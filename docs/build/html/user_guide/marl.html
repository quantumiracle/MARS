<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Multi-Agent RL &mdash; MARS 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Some Notes" href="notes.html" />
    <link rel="prev" title="Single-Agent RL" href="sarl.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MARS
            <img src="../_static/mars_label.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/intro.html">Introduction</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Install MARS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">A Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="general.html">General Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="sarl.html">Single-Agent RL</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Multi-Agent RL</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#marl-algorithms">MARL algorithms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#self-play">Self-Play</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#description">Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reference">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neural-fictitious-self-play">Neural Fictitious Self-Play</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example">Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">Reference</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#nash-dqn-with-exploiter">Nash DQN with Exploiter</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">Reference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#general-usage">General Usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#testing">Testing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exploitation">Exploitation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="notes.html">Some Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MARS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mars/algorithm/agent.html">Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/algorithm/dqn.html">DQN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/algorithm/ppo.html">PPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/data_struct.html">Data Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/func.html">Functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/env/env.html">Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Primal Experiments</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../experiments/single_agent.html">Single-Agent Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiments/exploit.html">Exploitability Test</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MARS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Multi-Agent RL</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guide/marl.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="multi-agent-rl">
<h1>Multi-Agent RL<a class="headerlink" href="#multi-agent-rl" title="Permalink to this headline">¶</a></h1>
<section id="marl-algorithms">
<h2>MARL algorithms<a class="headerlink" href="#marl-algorithms" title="Permalink to this headline">¶</a></h2>
<p><strong>Table of contents</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="#self-play">Self-Play</a></p></li>
<li><p><a class="reference external" href="#neural-fictitious-self-play">Neural Fictitious Self-Play</a></p></li>
</ul>
<section id="self-play">
<h3>Self-Play<a class="headerlink" href="#self-play" title="Permalink to this headline">¶</a></h3>
<section id="description">
<h4>Description<a class="headerlink" href="#description" title="Permalink to this headline">¶</a></h4>
<p>We provide several algorithms, in either reinforcement learning (RL) or evolutionary strategy (ES), with self-play learning mechanism in multi-agent environments, including deep-Q networks (DQN), proximal policy optimization (PPO), genetic algorithm (GA), etc. Therefore, they can be generally classified as <strong>Self-Play + RL</strong> or <strong>Self-Play + ES</strong>.</p>
<ul>
<li><p>Self-Play + RL:</p>
<div class="highlight-tex notranslate"><div class="highlight"><pre><span></span>Champion List:
Initially, this list contains a random policy agent.

Environment:
At the beginning of each episode, load the most recent agent archived in the Champion List.
Set this agent to be the Opponent.

Agent:
Trains inside the Environment against the Opponent with our choice of RL method.
Once the performance exceeds some threshold, checkpoint the agent into the Champion List.
</pre></div>
</div>
<p>There are some requirements for an environment to be conveniently learned with self-play + RL method: (1) the environment needs to be symmetric for each agent, including their state space, action space, transition dynamics, random start, etc; (2) to conveniently apply the above mechanism for learning a single model controlling the two sides of the game, the perspectives for different agents needs to be the same, which means, the same model could be applied on each side of the game without modification. The <em>first</em> point is obviously satisfied in some games like Go, <em>SlimeVolley</em>, and most of the Atari games like <em>Boxing</em> and <em>Pong</em> , etc. The <em>second</em> point is not always available for most multi-agent game although it seems rather like a trivial implementation issue. For example, in all Atari games (OpenAI Gym or PettingZoo), there is only one perspective of observation for all agents in the game, which is the full view of the game (either image or RAM) and contains all information for both the current agent and its opponent. Thus, all agents have the same observation in Atari games by default, which makes the model lack of knowledge it is currently taking charge of. An <a class="reference external" href="https://github.com/PettingZoo-Team/PettingZoo/issues/423">issue</a> for reference is provided. The direct solution for making a game to  provide same observation perspective for each agent is to transform the perspectives of observation from all agents to the same one. If this is hard or infeasible in practice (e.g. transforming and recoloring the Atari games can take considerable efforts),  an alternative to achieve a similar effect is to add an indicator of the agent to its observation, which is a one-hot vector indicating which agent the sample is collected with, and use all samples to update the model.</p>
<p>If both the above two points are satisfied in an environment, we can simply learn one model to control each agent in a game. Moreover, samples from all agents will be symmetric for the model, therefore the model can and should learn from all those samples to maximize its learning efficiency. As an example, with DQN algorithm, we should put samples from all agents into the buffer of the model for it to learn from. Due to this reason, the implementation of self-play in our repository is different from <a class="reference external" href="https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo_selfplay.py">the previous one</a>. Since the perspective transformation is provide with in the <em>SlimeVolley</em> environment, it can use samples from only one agent to update the model.</p>
<p>An example to run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils.func</span> <span class="kn">import</span> <span class="n">LoadYAML2Dict</span>
<span class="kn">from</span> <span class="nn">env.import_env</span> <span class="kn">import</span> <span class="n">make_env</span>
<span class="kn">from</span> <span class="nn">rollout</span> <span class="kn">import</span> <span class="n">rollout</span>
<span class="kn">from</span> <span class="nn">rl.algorithm</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;confs/slimevolley_slimevolleyv0_selfplay_dqn&#39;</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mergeDefault</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model1</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">model2</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Self-Play + ES:</p>
<p>In the category of self-play with evolutionary strategy algorithms, we implement self-play with genetic algorithm (GA).</p>
<div class="highlight-tex notranslate"><div class="highlight"><pre><span></span>Create a population of agents with random initial parameters.
Play a total of N games. For each game:
  Randomly choose two agents in the population and have them play against each other.
  If the game is tied, add a bit of noise to the second agent&#39;s parameters.
  Otherwise, replace the loser with a clone of the winner, and add a bit of noise to the clone.
</pre></div>
</div>
<p>An example to run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils.func</span> <span class="kn">import</span> <span class="n">LoadYAML2Dict</span>
<span class="kn">from</span> <span class="nn">env.import_env</span> <span class="kn">import</span> <span class="n">make_env</span>
<span class="kn">from</span> <span class="nn">rollout</span> <span class="kn">import</span> <span class="n">rollout</span>
<span class="kn">from</span> <span class="nn">rl.algorithm</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;confs/slimevolley_slimevolleyv0_selfplay_ga&#39;</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mergeDefault</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="reference">
<h4>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/hardmaru/slimevolleygym/blob/master/TRAINING.md"><em>The repository and tutorials of SlimeVolley environment</em></a><em>, Hardmaru, et. al</em>.</p></li>
</ul>
</section>
</section>
<section id="neural-fictitious-self-play">
<h3>Neural Fictitious Self-Play<a class="headerlink" href="#neural-fictitious-self-play" title="Permalink to this headline">¶</a></h3>
<section id="id1">
<h4>Description<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Neural Fictitious Self-Play (NFSP) is a generalization of fictitious self-play (which proves to solve Nash equilibrium in two-player zero-sum games and multi-player potential games) with neural networks. In NFSP, each agent is learning through a certain RL algorithm (<em>e.g.</em>, DQN, PPO, etc) as well as maintaining a policy network through supervised learning on previous samples, <em>i.e.</em>, the average strategy profile, in a sense of the average of the best responses to opponent’s historical strategies.</p>
<p>The hyper-parameter <img alt="\eta" src="https://latex.codecogs.com/svg.latex?%5Ceta" /> in NFSP is set to choose the action either as the best response to current opponent strategy or from the average strategy that the agent applied as previous best responses.  It can be specified with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">args</span><span class="o">.</span><span class="n">marl_spec</span><span class="p">[</span><span class="s1">&#39;eta&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.1</span>
</pre></div>
</div>
<p>In testing, the value of <img alt="\eta" src="https://latex.codecogs.com/svg.latex?%5Ceta" /> is automatically set as 0, so that all actions come from the learned average policy. However, as in original <a class="reference external" href="https://arxiv.org/pdf/1603.01121.pdf">paper</a>, there are at least three types of strategies can be derived from NFSP: (1) best response strategy (setting <img alt="\eta" src="https://latex.codecogs.com/svg.latex?%5Ceta" /> as 1, also greedy); (2) average strategy (setting <img alt="\eta" src="https://latex.codecogs.com/svg.latex?%5Ceta" /> as 0); (3) greedy-average strategy (setting <img alt="\eta" src="https://latex.codecogs.com/svg.latex?%5Ceta" /> as 0, but also taking greedy action that maximizes the action value or probability rather than sampling from a probabilistic distribution). Our default choice in implementation for testing is the second one.</p>
</section>
<section id="example">
<h4>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h4>
<p>An example to run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils.func</span> <span class="kn">import</span> <span class="n">LoadYAML2Dict</span>
<span class="kn">from</span> <span class="nn">env.import_env</span> <span class="kn">import</span> <span class="n">make_env</span>
<span class="kn">from</span> <span class="nn">rollout</span> <span class="kn">import</span> <span class="n">rollout</span>
<span class="kn">from</span> <span class="nn">rl.algorithm</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;confs/pettingzoo_boxingv1_nfsp&#39;</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mergeDefault</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model1</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">model2</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h4>Reference<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p><em><a class="reference external" href="https://arxiv.org/pdf/1603.01121.pdf">Deep Reinforcement Learning from Self-Play in Imperfect-Information Games</a></em>, <em>Johannes Heinrich and David Silver.</em></p></li>
</ul>
</section>
</section>
<section id="nash-dqn-with-exploiter">
<h3>Nash DQN with Exploiter<a class="headerlink" href="#nash-dqn-with-exploiter" title="Permalink to this headline">¶</a></h3>
<section id="id3">
<h4>Description<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
</section>
<section id="id4">
<h4>Example<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>An example to run:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils.func</span> <span class="kn">import</span> <span class="n">LoadYAML2Dict</span>
<span class="kn">from</span> <span class="nn">env.import_env</span> <span class="kn">import</span> <span class="n">make_env</span>
<span class="kn">from</span> <span class="nn">rollout</span> <span class="kn">import</span> <span class="n">rollout</span>
<span class="kn">from</span> <span class="nn">rl.algorithm</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;confs/slimevolley_slimevolleyv0_nash_dqn_exploiter&#39;</span>

<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mergeDefault</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model1</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model1</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id5">
<h4>Reference<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li></li>
</ul>
</section>
</section>
</section>
<section id="general-usage">
<h2>General Usage<a class="headerlink" href="#general-usage" title="Permalink to this headline">¶</a></h2>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>The followings are required in the main script, for either training/testing/exploitation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils.func</span> <span class="kn">import</span> <span class="n">LoadYAML2Dict</span>
<span class="kn">from</span> <span class="nn">env.import_env</span> <span class="kn">import</span> <span class="n">make_env</span>
<span class="kn">from</span> <span class="nn">rollout</span> <span class="kn">import</span> <span class="n">rollout</span>
<span class="kn">from</span> <span class="nn">rl.algorithm</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</li>
<li><p>Typical usage for a two-agent game, e.g. <em>boxing-v1 PettingZoo</em>, the algorithm details including which MARL algorithm is used are specified in the <em>yaml</em> file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;PATH TO YAML&#39;</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mergeDefault</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model1</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">model2</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="c1"># model1.fix()  # fix a model if you don&#39;t want it to learn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

</pre></div>
</div>
</li>
</ul>
</section>
<section id="testing">
<h3>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>Typical usage for a two-agent game, e.g. <em>boxing-v1 PettingZoo</em>, similar as above, the algorithm details including which MARL algorithm is used are specified in the <em>yaml</em> file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;PATH TO YAML&#39;</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="c1">## Change/specify some arguments if necessary</span>
<span class="n">args</span><span class="o">.</span><span class="n">test</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># the test mode will automatically fix all models</span>
<span class="n">args</span><span class="o">.</span><span class="n">render</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">args</span><span class="o">.</span><span class="n">load_model_full_path</span> <span class="o">=</span> <span class="s1">&#39;PATH TO THE TRAINED MODEL&#39;</span> <span class="c1"># or use args.load_model_idx</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model1</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">model2</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="exploitation">
<h3>Exploitation<a class="headerlink" href="#exploitation" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p>When you use SlimeVolley environments and want to exploit a trained model in this type of environment, you need to set the <em>yaml</em> file with <em>against_baseline</em> as <em>False</em> and <em>exploit</em> as <em>True</em>, so that you can input two models to the <em>MultiAgent</em> object, one is the trained model you want to exploit, another one is the exploiter with whatever model you want to use. A typical example would be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;PATH TO YAML&#39;</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="c1">## Change/specify some arguments if necessary</span>
<span class="n">args</span><span class="o">.</span><span class="n">against_baseline</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">args</span><span class="o">.</span><span class="n">exploit</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">args</span><span class="o">.</span><span class="n">load_model_full_path</span> <span class="o">=</span> <span class="s1">&#39;PATH TO THE TRAINED MODEL&#39;</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">trained_model</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">exploiter</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">trained_model</span><span class="o">.</span><span class="n">fix</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">trained_model</span><span class="p">,</span> <span class="n">exploiter</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="sarl.html" class="btn btn-neutral float-left" title="Single-Agent RL" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="notes.html" class="btn btn-neutral float-right" title="Some Notes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Zihan Ding.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>