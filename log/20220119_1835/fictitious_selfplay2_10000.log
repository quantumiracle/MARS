pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 12.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fafaef4add8>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Agents No. [0] (index starting from 0) are not learnable.
Load meta strategy:  [[0.2 0.2 0.2 0.2 0.2]
 [0.2 0.2 0.2 0.2 0.2]]
Load checkpoints (policy family):  [['50' '5253' '7615' '8835' '9107']
 ['193' '5289' '7712' '9011' '9134']]
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'cpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220117153310/epi_10000/', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/quantumiracle/research/MARS/data/model/20220117153310_exploit_10000/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/quantumiracle/research/MARS/data/log/20220117153310_exploit_10000/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0194s / 0.0194 s
agent0:                 episode reward: 0.3963,                 loss: nan
agent1:                 episode reward: -0.3963,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0478s / 0.0672 s
agent0:                 episode reward: -0.1781,                 loss: nan
agent1:                 episode reward: 0.1781,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0463s / 0.1135 s
agent0:                 episode reward: -0.0675,                 loss: nan
agent1:                 episode reward: 0.0675,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0474s / 0.1609 s
agent0:                 episode reward: 0.0264,                 loss: nan
agent1:                 episode reward: -0.0264,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0454s / 0.2063 s
agent0:                 episode reward: 0.1697,                 loss: nan
agent1:                 episode reward: -0.1697,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0476s / 0.2539 s
agent0:                 episode reward: -0.3480,                 loss: nan
agent1:                 episode reward: 0.3480,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0458s / 0.2997 s
agent0:                 episode reward: 0.2421,                 loss: nan
agent1:                 episode reward: -0.2421,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0438s / 0.3435 s
agent0:                 episode reward: 0.1073,                 loss: nan
agent1:                 episode reward: -0.1073,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0470s / 0.3906 s
agent0:                 episode reward: 0.0929,                 loss: nan
agent1:                 episode reward: -0.0929,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0485s / 0.4391 s
agent0:                 episode reward: 0.6131,                 loss: nan
agent1:                 episode reward: -0.6131,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.0397s / 0.4788 s
agent0:                 episode reward: 0.1151,                 loss: nan
agent1:                 episode reward: -0.1151,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1139s / 0.5926 s
agent0:                 episode reward: 0.0162,                 loss: nan
agent1:                 episode reward: -0.0162,                 loss: 0.1998
Episode: 241/30000 (0.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1692s / 0.7619 s
agent0:                 episode reward: -0.1561,                 loss: nan
agent1:                 episode reward: 0.1561,                 loss: 0.1908
Episode: 261/30000 (0.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1637s / 0.9256 s
agent0:                 episode reward: 0.1238,                 loss: nan
agent1:                 episode reward: -0.1238,                 loss: 0.1807
Episode: 281/30000 (0.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1640s / 1.0895 s
agent0:                 episode reward: 0.0953,                 loss: nan
agent1:                 episode reward: -0.0953,                 loss: 0.1773
Episode: 301/30000 (1.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1648s / 1.2543 s
agent0:                 episode reward: 0.0280,                 loss: nan
agent1:                 episode reward: -0.0280,                 loss: 0.1745
Episode: 321/30000 (1.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1736s / 1.4280 s
agent0:                 episode reward: 0.0264,                 loss: nan
agent1:                 episode reward: -0.0264,                 loss: 0.1729
Episode: 341/30000 (1.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1617s / 1.5897 s
agent0:                 episode reward: -0.1238,                 loss: nan
agent1:                 episode reward: 0.1238,                 loss: 0.1703
Episode: 361/30000 (1.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1628s / 1.7525 s
agent0:                 episode reward: 0.0320,                 loss: nan
agent1:                 episode reward: -0.0320,                 loss: 0.1667
Episode: 381/30000 (1.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1679s / 1.9203 s
agent0:                 episode reward: 0.1219,                 loss: nan
agent1:                 episode reward: -0.1219,                 loss: 0.1637
Episode: 401/30000 (1.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2081s / 2.1285 s
agent0:                 episode reward: 0.0043,                 loss: nan
agent1:                 episode reward: -0.0043,                 loss: 0.1604
Episode: 421/30000 (1.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1771s / 2.3055 s
agent0:                 episode reward: -0.1577,                 loss: nan
agent1:                 episode reward: 0.1577,                 loss: 0.1583
Episode: 441/30000 (1.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2007s / 2.5062 s
agent0:                 episode reward: -0.0913,                 loss: nan
agent1:                 episode reward: 0.0913,                 loss: 0.1523
Episode: 461/30000 (1.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2025s / 2.7087 s
agent0:                 episode reward: 0.2429,                 loss: nan
agent1:                 episode reward: -0.2429,                 loss: 0.1468
Episode: 481/30000 (1.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2022s / 2.9109 s
agent0:                 episode reward: 0.2072,                 loss: nan
agent1:                 episode reward: -0.2072,                 loss: 0.1440
Episode: 501/30000 (1.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1994s / 3.1103 s
agent0:                 episode reward: -0.0273,                 loss: nan
agent1:                 episode reward: 0.0273,                 loss: 0.1422
Episode: 521/30000 (1.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1741s / 3.2845 s
agent0:                 episode reward: -0.2801,                 loss: nan
agent1:                 episode reward: 0.2801,                 loss: 0.1398
Episode: 541/30000 (1.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1750s / 3.4595 s
agent0:                 episode reward: -0.0263,                 loss: nan
agent1:                 episode reward: 0.0263,                 loss: 0.1375
Episode: 561/30000 (1.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1941s / 3.6536 s
agent0:                 episode reward: 0.0356,                 loss: nan
agent1:                 episode reward: -0.0356,                 loss: 0.1468
Episode: 581/30000 (1.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1810s / 3.8346 s
agent0:                 episode reward: -0.1949,                 loss: nan
agent1:                 episode reward: 0.1949,                 loss: 0.1408
Episode: 601/30000 (2.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1691s / 4.0037 s
agent0:                 episode reward: -0.1484,                 loss: nan
agent1:                 episode reward: 0.1484,                 loss: 0.1400
Episode: 621/30000 (2.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1640s / 4.1677 s
agent0:                 episode reward: -0.1433,                 loss: nan
agent1:                 episode reward: 0.1433,                 loss: 0.1401
Episode: 641/30000 (2.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1783s / 4.3460 s
agent0:                 episode reward: -0.1555,                 loss: nan
agent1:                 episode reward: 0.1555,                 loss: 0.1387
Episode: 661/30000 (2.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2044s / 4.5503 s
agent0:                 episode reward: -0.1987,                 loss: nan
agent1:                 episode reward: 0.1987,                 loss: 0.1385
Episode: 681/30000 (2.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1939s / 4.7442 s
agent0:                 episode reward: 0.1598,                 loss: nan
agent1:                 episode reward: -0.1598,                 loss: 0.1405
Episode: 701/30000 (2.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2041s / 4.9483 s
agent0:                 episode reward: -0.3439,                 loss: nan
agent1:                 episode reward: 0.3439,                 loss: 0.1390
Episode: 721/30000 (2.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2255s / 5.1738 s
agent0:                 episode reward: 0.2116,                 loss: nan
agent1:                 episode reward: -0.2116,                 loss: 0.1400
Episode: 741/30000 (2.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2437s / 5.4175 s
agent0:                 episode reward: 0.1958,                 loss: nan
agent1:                 episode reward: -0.1958,                 loss: 0.1396
Episode: 761/30000 (2.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2125s / 5.6300 s
agent0:                 episode reward: 0.2192,                 loss: nan
agent1:                 episode reward: -0.2192,                 loss: 0.1390
Episode: 781/30000 (2.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1688s / 5.7988 s
agent0:                 episode reward: 0.4799,                 loss: nan
agent1:                 episode reward: -0.4799,                 loss: 0.1403
Episode: 801/30000 (2.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1713s / 5.9701 s
agent0:                 episode reward: -0.0666,                 loss: nan
agent1:                 episode reward: 0.0666,                 loss: 0.1402
Episode: 821/30000 (2.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1770s / 6.1470 s
agent0:                 episode reward: -0.3756,                 loss: nan
agent1:                 episode reward: 0.3756,                 loss: 0.1401
Episode: 841/30000 (2.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1812s / 6.3283 s
agent0:                 episode reward: -0.1308,                 loss: nan
agent1:                 episode reward: 0.1308,                 loss: 0.1402
Episode: 861/30000 (2.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1908s / 6.5191 s
agent0:                 episode reward: -0.3204,                 loss: nan
agent1:                 episode reward: 0.3204,                 loss: 0.1402
Episode: 881/30000 (2.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1722s / 6.6913 s
agent0:                 episode reward: -0.3533,                 loss: nan
agent1:                 episode reward: 0.3533,                 loss: 0.1414
Episode: 901/30000 (3.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1787s / 6.8700 s
agent0:                 episode reward: -0.0600,                 loss: nan
agent1:                 episode reward: 0.0600,                 loss: 0.1441
Episode: 921/30000 (3.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1736s / 7.0436 s
agent0:                 episode reward: 0.0277,                 loss: nan
agent1:                 episode reward: -0.0277,                 loss: 0.1446/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

Episode: 941/30000 (3.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1695s / 7.2131 s
agent0:                 episode reward: 0.2808,                 loss: nan
agent1:                 episode reward: -0.2808,                 loss: 0.1445
Episode: 961/30000 (3.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1737s / 7.3868 s
agent0:                 episode reward: -0.0369,                 loss: nan
agent1:                 episode reward: 0.0369,                 loss: 0.1447
Episode: 981/30000 (3.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1736s / 7.5604 s
agent0:                 episode reward: -0.4702,                 loss: nan
agent1:                 episode reward: 0.4702,                 loss: 0.1446
Episode: 1001/30000 (3.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1734s / 7.7338 s
agent0:                 episode reward: 0.1467,                 loss: nan
agent1:                 episode reward: -0.1467,                 loss: 0.1442
Episode: 1021/30000 (3.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1745s / 7.9082 s
agent0:                 episode reward: -0.0418,                 loss: nan
agent1:                 episode reward: 0.0418,                 loss: 0.1443
Episode: 1041/30000 (3.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1999s / 8.1081 s
agent0:                 episode reward: -0.1260,                 loss: nan
agent1:                 episode reward: 0.1260,                 loss: 0.1448
Episode: 1061/30000 (3.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2130s / 8.3211 s
agent0:                 episode reward: -0.5469,                 loss: nan
agent1:                 episode reward: 0.5469,                 loss: 0.1438
Episode: 1081/30000 (3.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2358s / 8.5570 s
agent0:                 episode reward: -0.2586,                 loss: nan
agent1:                 episode reward: 0.2586,                 loss: 0.1425
Episode: 1101/30000 (3.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2047s / 8.7617 s
agent0:                 episode reward: 0.2336,                 loss: nan
agent1:                 episode reward: -0.2336,                 loss: 0.1425
Episode: 1121/30000 (3.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2020s / 8.9637 s
agent0:                 episode reward: -0.0307,                 loss: nan
agent1:                 episode reward: 0.0307,                 loss: 0.1435
Episode: 1141/30000 (3.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2197s / 9.1834 s
agent0:                 episode reward: 0.0554,                 loss: nan
agent1:                 episode reward: -0.0554,                 loss: 0.1435
Episode: 1161/30000 (3.8700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2058s / 9.3893 s
agent0:                 episode reward: -0.0099,                 loss: nan
agent1:                 episode reward: 0.0099,                 loss: 0.1437
Episode: 1181/30000 (3.9367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2195s / 9.6087 s
agent0:                 episode reward: 0.0487,                 loss: nan
agent1:                 episode reward: -0.0487,                 loss: 0.1426
Episode: 1201/30000 (4.0033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1723s / 9.7810 s
agent0:                 episode reward: -0.3937,                 loss: nan
agent1:                 episode reward: 0.3937,                 loss: 0.1430
Episode: 1221/30000 (4.0700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2015s / 9.9825 s
agent0:                 episode reward: -0.0963,                 loss: nan
agent1:                 episode reward: 0.0963,                 loss: 0.1428
Episode: 1241/30000 (4.1367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2267s / 10.2093 s
agent0:                 episode reward: 0.0904,                 loss: nan
agent1:                 episode reward: -0.0904,                 loss: 0.1419
Episode: 1261/30000 (4.2033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1895s / 10.3988 s
agent0:                 episode reward: -0.1038,                 loss: nan
agent1:                 episode reward: 0.1038,                 loss: 0.1412
Episode: 1281/30000 (4.2700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1958s / 10.5946 s
agent0:                 episode reward: -0.2780,                 loss: nan
agent1:                 episode reward: 0.2780,                 loss: 0.1408
Episode: 1301/30000 (4.3367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1903s / 10.7849 s
agent0:                 episode reward: -0.2124,                 loss: nan
agent1:                 episode reward: 0.2124,                 loss: 0.1428
Episode: 1321/30000 (4.4033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2030s / 10.9880 s
agent0:                 episode reward: -0.4813,                 loss: nan
agent1:                 episode reward: 0.4813,                 loss: 0.1423
Episode: 1341/30000 (4.4700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1762s / 11.1641 s
agent0:                 episode reward: -0.3544,                 loss: nan
agent1:                 episode reward: 0.3544,                 loss: 0.1429
Episode: 1361/30000 (4.5367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.1873s / 11.3514 s
agent0:                 episode reward: -0.3367,                 loss: nan
agent1:                 episode reward: 0.3367,                 loss: 0.1408
Episode: 1381/30000 (4.6033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2007s / 11.5521 s
agent0:                 episode reward: -0.3900,                 loss: nan
agent1:                 episode reward: 0.3900,                 loss: 0.1419
Episode: 1401/30000 (4.6700%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2349s / 11.7870 s
agent0:                 episode reward: 0.1774,                 loss: nan
agent1:                 episode reward: -0.1774,                 loss: 0.1420
Episode: 1421/30000 (4.7367%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2396s / 12.0266 s
agent0:                 episode reward: -0.1739,                 loss: nan
agent1:                 episode reward: 0.1739,                 loss: 0.1437
Episode: 1441/30000 (4.8033%),                 avg. length: 2.0,                last time consumption/overall running time: 0.2171s / 12.2437 s
agent0:                 episode reward: -0.1309,                 loss: nan
agent1:                 episode reward: 0.1309,                 loss: 0.1426
Traceback (most recent call last):
  File "exploit_arbitrary_mdp2.py", line 50, in <module>
    launch_rollout(parser_args.method, parser_args.load_id, parser_args.epi)
  File "exploit_arbitrary_mdp2.py", line 42, in launch_rollout
    rollout(env, model, args, load_id+f'_exploit_{epi}')
  File "/home/quantumiracle/research/MARS/mars/rollout.py", line 21, in rollout
    rollout_normal(env, model, save_id, args)
  File "/home/quantumiracle/research/MARS/mars/rollout.py", line 112, in rollout_normal
    loss = model.update(
  File "/home/quantumiracle/research/MARS/mars/rl/agents/multiagent.py", line 274, in update
    loss = agent.update()
  File "/home/quantumiracle/research/MARS/mars/rl/agents/dqn.py", line 139, in update
    self.optimizer.step()
  File "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/torch/optim/adam.py", line 118, in step
    eps=group['eps'])
  File "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/torch/optim/_functional.py", line 86, in adam
    exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)
KeyboardInterrupt
