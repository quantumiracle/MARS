pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220204_1545/pettingzoo_surround_v1_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220204_1545/pettingzoo_surround_v1_nash_ppo.
Episode: 1/30000 (0.0033%),                 avg. length: 1427.0,                last time consumption/overall running time: 8.7746s / 8.7746 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0058
env0_second_0:                 episode reward: 4.0000,                 loss: -0.0106
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 1436.25,                last time consumption/overall running time: 156.6050s / 165.3796 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.0370
env0_second_0:                 episode reward: 3.2500,                 loss: -0.0350
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 1519.25,                last time consumption/overall running time: 165.1424s / 330.5221 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.0432
env0_second_0:                 episode reward: 2.2000,                 loss: -0.0411
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 1761.6,                last time consumption/overall running time: 199.1127s / 529.6348 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0613
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0636
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 2352.3,                last time consumption/overall running time: 231.6802s / 761.3150 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.0539
env0_second_0:                 episode reward: 2.0000,                 loss: -0.0520
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 2415.65,                last time consumption/overall running time: 242.9395s / 1004.2545 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0607
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0575
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 2554.8,                last time consumption/overall running time: 254.5425s / 1258.7969 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0701
env0_second_0:                 episode reward: 0.9500,                 loss: -0.0666
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 2489.75,                last time consumption/overall running time: 268.7165s / 1527.5135 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0778
env0_second_0:                 episode reward: 0.7500,                 loss: -0.0745
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 2057.65,                last time consumption/overall running time: 218.8398s / 1746.3533 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.0788
env0_second_0:                 episode reward: 5.6000,                 loss: -0.0775
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 2238.1,                last time consumption/overall running time: 231.6657s / 1978.0191 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.0619
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0557
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 2093.4,                last time consumption/overall running time: 238.5950s / 2216.6141 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.0534
env0_second_0:                 episode reward: 4.3500,                 loss: -0.0514
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 2145.65,                last time consumption/overall running time: 219.4582s / 2436.0724 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.0687
env0_second_0:                 episode reward: 5.5500,                 loss: -0.0570
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 2261.4,                last time consumption/overall running time: 236.4322s / 2672.5046 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.0629
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0562
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 2278.25,                last time consumption/overall running time: 248.4544s / 2920.9590 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.0779
env0_second_0:                 episode reward: 4.9500,                 loss: -0.0721
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 2146.7,                last time consumption/overall running time: 227.6992s / 3148.6582 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.0556
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0504
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 2013.45,                last time consumption/overall running time: 221.7491s / 3370.4074 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0508
env0_second_0:                 episode reward: 5.8000,                 loss: -0.0393
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 2257.6,                last time consumption/overall running time: 251.0139s / 3621.4212 s
env0_first_0:                 episode reward: -4.8000,                 loss: -0.0420
env0_second_0:                 episode reward: 4.8000,                 loss: -0.0295
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 2119.7,                last time consumption/overall running time: 248.5855s / 3870.0067 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0539
env0_second_0:                 episode reward: 5.4000,                 loss: -0.0365
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 2146.95,                last time consumption/overall running time: 250.2449s / 4120.2517 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.0494
env0_second_0:                 episode reward: 5.3500,                 loss: -0.0397
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 1957.4,                last time consumption/overall running time: 201.9777s / 4322.2293 s
env0_first_0:                 episode reward: -6.6500,                 loss: -0.0725
env0_second_0:                 episode reward: 6.6500,                 loss: -0.0617
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 1909.85,                last time consumption/overall running time: 194.0063s / 4516.2356 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.0632
env0_second_0:                 episode reward: 8.2500,                 loss: -0.0502
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 1813.65,                last time consumption/overall running time: 202.6120s / 4718.8476 s
env0_first_0:                 episode reward: -8.5000,                 loss: -0.0600
env0_second_0:                 episode reward: 8.5000,                 loss: -0.0416
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 1981.2,                last time consumption/overall running time: 209.9889s / 4928.8366 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.0742
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0524
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 2002.15,                last time consumption/overall running time: 192.1616s / 5120.9982 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.0815
env0_second_0:                 episode reward: 6.2000,                 loss: -0.0597
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 1825.15,                last time consumption/overall running time: 191.7736s / 5312.7718 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.0909
env0_second_0:                 episode reward: 8.3000,                 loss: -0.0702
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 2057.9,                last time consumption/overall running time: 233.3792s / 5546.1510 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.0772
env0_second_0:                 episode reward: 6.5000,                 loss: -0.0573
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 2052.85,                last time consumption/overall running time: 224.2684s / 5770.4195 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.0591
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0417
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 1942.15,                last time consumption/overall running time: 194.3047s / 5964.7242 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.1059
env0_second_0:                 episode reward: 7.5500,                 loss: -0.0856
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 2044.15,                last time consumption/overall running time: 219.5146s / 6184.2388 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.1283
env0_second_0:                 episode reward: 6.0500,                 loss: -0.1045
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 1989.3,                last time consumption/overall running time: 216.4906s / 6400.7294 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.1167
env0_second_0:                 episode reward: 4.9500,                 loss: -0.0828
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 2221.9,                last time consumption/overall running time: 222.4364s / 6623.1659 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0878
env0_second_0:                 episode reward: -1.0500,                 loss: -0.0597
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 2301.9,                last time consumption/overall running time: 236.9394s / 6860.1053 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1001
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0639
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 2265.8,                last time consumption/overall running time: 237.4089s / 7097.5142 s
env0_first_0:                 episode reward: -4.1500,                 loss: -0.1135
env0_second_0:                 episode reward: 4.1500,                 loss: -0.0760
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 2543.55,                last time consumption/overall running time: 253.8406s / 7351.3548 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0849
env0_second_0:                 episode reward: 5.1000,                 loss: -0.0452
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 2909.6,                last time consumption/overall running time: 296.9828s / 7648.3376 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.0667
env0_second_0:                 episode reward: 2.3000,                 loss: -0.0201
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 2805.1,                last time consumption/overall running time: 310.0068s / 7958.3444 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.0814
env0_second_0:                 episode reward: 1.1500,                 loss: -0.0466
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 2799.05,                last time consumption/overall running time: 306.2358s / 8264.5803 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0803
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0338
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 741/30000 (2.4700%),                 avg. length: 2828.15,                last time consumption/overall running time: 291.6415s / 8556.2218 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0707
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0423
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 761/30000 (2.5367%),                 avg. length: 2894.6,                last time consumption/overall running time: 330.5446s / 8886.7665 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0565
env0_second_0:                 episode reward: -3.2500,                 loss: -0.0107
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 781/30000 (2.6033%),                 avg. length: 2891.8,                last time consumption/overall running time: 311.6189s / 9198.3854 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.0586
env0_second_0:                 episode reward: -1.8000,                 loss: -0.0216
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 801/30000 (2.6700%),                 avg. length: 2579.55,                last time consumption/overall running time: 294.7230s / 9493.1084 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.0534
env0_second_0:                 episode reward: -5.2000,                 loss: -0.0165
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 821/30000 (2.7367%),                 avg. length: 1884.25,                last time consumption/overall running time: 207.9546s / 9701.0629 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0671
env0_second_0:                 episode reward: -6.7000,                 loss: -0.0240
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 841/30000 (2.8033%),                 avg. length: 2456.05,                last time consumption/overall running time: 265.4120s / 9966.4749 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0427
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0187
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 861/30000 (2.8700%),                 avg. length: 1951.35,                last time consumption/overall running time: 225.5523s / 10192.0272 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.0814
env0_second_0:                 episode reward: 8.2000,                 loss: -0.0496
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 881/30000 (2.9367%),                 avg. length: 2095.4,                last time consumption/overall running time: 218.6508s / 10410.6780 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.0574
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0029
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 901/30000 (3.0033%),                 avg. length: 2380.0,                last time consumption/overall running time: 275.0591s / 10685.7371 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0568
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0306
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 921/30000 (3.0700%),                 avg. length: 2049.3,                last time consumption/overall running time: 232.6959s / 10918.4331 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0639
env0_second_0:                 episode reward: 5.1000,                 loss: 0.2036
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 941/30000 (3.1367%),                 avg. length: 2536.3,                last time consumption/overall running time: 291.9838s / 11210.4169 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.0808
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0329
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 961/30000 (3.2033%),                 avg. length: 2825.75,                last time consumption/overall running time: 305.8879s / 11516.3048 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0686
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0130
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 981/30000 (3.2700%),                 avg. length: 2486.45,                last time consumption/overall running time: 272.8706s / 11789.1754 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.0499
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0208
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 1001/30000 (3.3367%),                 avg. length: 2659.3,                last time consumption/overall running time: 278.4727s / 12067.6481 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.0588
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0407
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1021/30000 (3.4033%),                 avg. length: 2583.25,                last time consumption/overall running time: 271.2408s / 12338.8888 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0650
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0099
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 1041/30000 (3.4700%),                 avg. length: 2897.9,                last time consumption/overall running time: 322.4699s / 12661.3587 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0562
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0291
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1061/30000 (3.5367%),                 avg. length: 2721.0,                last time consumption/overall running time: 282.9606s / 12944.3193 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.0902
env0_second_0:                 episode reward: -3.9500,                 loss: -0.0240
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1081/30000 (3.6033%),                 avg. length: 2711.75,                last time consumption/overall running time: 294.2285s / 13238.5477 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0614
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0138
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1101/30000 (3.6700%),                 avg. length: 2434.65,                last time consumption/overall running time: 247.5712s / 13486.1189 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.0700
env0_second_0:                 episode reward: -5.1500,                 loss: -0.0068
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1121/30000 (3.7367%),                 avg. length: 2906.55,                last time consumption/overall running time: 326.6421s / 13812.7610 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.0611
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0038
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 1141/30000 (3.8033%),                 avg. length: 2873.15,                last time consumption/overall running time: 304.1860s / 14116.9470 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0565
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0192
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1161/30000 (3.8700%),                 avg. length: 2504.55,                last time consumption/overall running time: 259.3677s / 14376.3146 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.0641
env0_second_0:                 episode reward: -5.0000,                 loss: -0.0055
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 1181/30000 (3.9367%),                 avg. length: 2393.4,                last time consumption/overall running time: 237.3581s / 14613.6727 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0837
env0_second_0:                 episode reward: -5.4500,                 loss: -0.0312
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1201/30000 (4.0033%),                 avg. length: 2190.45,                last time consumption/overall running time: 244.2208s / 14857.8935 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.0780
env0_second_0:                 episode reward: -6.6500,                 loss: -0.0314
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 1221/30000 (4.0700%),                 avg. length: 2030.15,                last time consumption/overall running time: 198.2485s / 15056.1421 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.0717
env0_second_0:                 episode reward: -6.8000,                 loss: -0.0160
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 1241/30000 (4.1367%),                 avg. length: 1938.15,                last time consumption/overall running time: 193.4863s / 15249.6284 s
env0_first_0:                 episode reward: 7.9000,                 loss: -0.0496
env0_second_0:                 episode reward: -7.9000,                 loss: 0.4182
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 1261/30000 (4.2033%),                 avg. length: 1993.55,                last time consumption/overall running time: 203.9969s / 15453.6253 s
env0_first_0:                 episode reward: 8.1500,                 loss: -0.0885
env0_second_0:                 episode reward: -8.1500,                 loss: -0.0178
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 1281/30000 (4.2700%),                 avg. length: 2304.3,                last time consumption/overall running time: 236.5748s / 15690.2001 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.0671
env0_second_0:                 episode reward: -5.2500,                 loss: 0.0002
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 1301/30000 (4.3367%),                 avg. length: 2785.75,                last time consumption/overall running time: 291.9722s / 15982.1723 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0606
env0_second_0:                 episode reward: -4.9000,                 loss: 0.0064
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 1321/30000 (4.4033%),                 avg. length: 2533.3,                last time consumption/overall running time: 278.2633s / 16260.4357 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0599
env0_second_0:                 episode reward: -5.6500,                 loss: 0.0511
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1341/30000 (4.4700%),                 avg. length: 2561.85,                last time consumption/overall running time: 269.2545s / 16529.6901 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0481
env0_second_0:                 episode reward: -5.7500,                 loss: 0.0541
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 1361/30000 (4.5367%),                 avg. length: 2850.3,                last time consumption/overall running time: 285.8422s / 16815.5324 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.0201
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0804
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1381/30000 (4.6033%),                 avg. length: 2548.45,                last time consumption/overall running time: 260.8573s / 17076.3897 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.0096
env0_second_0:                 episode reward: -4.7000,                 loss: 0.2243
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 1401/30000 (4.6700%),                 avg. length: 3057.8,                last time consumption/overall running time: 310.3086s / 17386.6983 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.0095
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0788
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1421/30000 (4.7367%),                 avg. length: 2944.35,                last time consumption/overall running time: 305.3718s / 17692.0701 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.0283
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0531
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 1441/30000 (4.8033%),                 avg. length: 2508.95,                last time consumption/overall running time: 282.5552s / 17974.6253 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0380
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0232
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 1461/30000 (4.8700%),                 avg. length: 2745.7,                last time consumption/overall running time: 326.6533s / 18301.2786 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.0414
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0197
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1481/30000 (4.9367%),                 avg. length: 2872.65,                last time consumption/overall running time: 342.2568s / 18643.5354 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0311
env0_second_0:                 episode reward: -0.6000,                 loss: 0.1434
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1501/30000 (5.0033%),                 avg. length: 2698.8,                last time consumption/overall running time: 307.9801s / 18951.5155 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.0458
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0444
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 1521/30000 (5.0700%),                 avg. length: 2809.0,                last time consumption/overall running time: 297.5159s / 19249.0314 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.0518
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0502
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1541/30000 (5.1367%),                 avg. length: 2563.25,                last time consumption/overall running time: 277.2144s / 19526.2458 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0447
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0671
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1561/30000 (5.2033%),                 avg. length: 2697.4,                last time consumption/overall running time: 294.5739s / 19820.8197 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.0000,                 loss: 0.8756
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1581/30000 (5.2700%),                 avg. length: 2445.25,                last time consumption/overall running time: 253.6324s / 20074.4522 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0180
env0_second_0:                 episode reward: -4.6500,                 loss: 0.2898
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 1601/30000 (5.3367%),                 avg. length: 2212.25,                last time consumption/overall running time: 218.3060s / 20292.7581 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.0597
env0_second_0:                 episode reward: -6.0000,                 loss: 0.3504
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 1621/30000 (5.4033%),                 avg. length: 2660.9,                last time consumption/overall running time: 297.9954s / 20590.7535 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.0243
env0_second_0:                 episode reward: -2.8000,                 loss: 0.2249
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1641/30000 (5.4700%),                 avg. length: 3042.75,                last time consumption/overall running time: 365.6457s / 20956.3992 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0514
env0_second_0:                 episode reward: -3.2000,                 loss: 0.2637
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 1661/30000 (5.5367%),                 avg. length: 2795.55,                last time consumption/overall running time: 308.3553s / 21264.7545 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.0507
env0_second_0:                 episode reward: -4.2500,                 loss: 0.1316
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1681/30000 (5.6033%),                 avg. length: 2821.45,                last time consumption/overall running time: 294.0373s / 21558.7919 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0756
env0_second_0:                 episode reward: -4.6000,                 loss: 0.0228
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 1701/30000 (5.6700%),                 avg. length: 2662.5,                last time consumption/overall running time: 314.7248s / 21873.5167 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0651
env0_second_0:                 episode reward: -5.7500,                 loss: 0.2950
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1721/30000 (5.7367%),                 avg. length: 2535.25,                last time consumption/overall running time: 284.5868s / 22158.1035 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0665
env0_second_0:                 episode reward: -6.3000,                 loss: 0.0513
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 1741/30000 (5.8033%),                 avg. length: 2844.7,                last time consumption/overall running time: 303.9477s / 22462.0512 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.0255
env0_second_0:                 episode reward: -3.7000,                 loss: 0.1677
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 1761/30000 (5.8700%),                 avg. length: 2736.9,                last time consumption/overall running time: 280.7952s / 22742.8464 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0147
env0_second_0:                 episode reward: -5.4000,                 loss: 0.7013
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1781/30000 (5.9367%),                 avg. length: 2498.15,                last time consumption/overall running time: 257.7890s / 23000.6355 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0630
env0_second_0:                 episode reward: -4.9000,                 loss: 0.6164
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 1801/30000 (6.0033%),                 avg. length: 2808.15,                last time consumption/overall running time: 300.1103s / 23300.7457 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0588
env0_second_0:                 episode reward: -4.0500,                 loss: 0.6493
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1821/30000 (6.0700%),                 avg. length: 2811.9,                last time consumption/overall running time: 342.0217s / 23642.7675 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.0223
env0_second_0:                 episode reward: -4.1000,                 loss: 0.2698
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 1841/30000 (6.1367%),                 avg. length: 2561.05,                last time consumption/overall running time: 291.6329s / 23934.4004 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0504
env0_second_0:                 episode reward: -4.9500,                 loss: 0.2398
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 1861/30000 (6.2033%),                 avg. length: 2500.95,                last time consumption/overall running time: 275.1304s / 24209.5307 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0756
env0_second_0:                 episode reward: -6.0500,                 loss: 0.0642
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1881/30000 (6.2700%),                 avg. length: 2557.6,                last time consumption/overall running time: 311.2407s / 24520.7715 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.0431
env0_second_0:                 episode reward: -6.2500,                 loss: 0.1400
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 1901/30000 (6.3367%),                 avg. length: 2358.0,                last time consumption/overall running time: 288.6811s / 24809.4526 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0278
env0_second_0:                 episode reward: -7.1000,                 loss: 0.2279
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1921/30000 (6.4033%),                 avg. length: 2706.7,                last time consumption/overall running time: 323.7522s / 25133.2047 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0139
env0_second_0:                 episode reward: -3.6500,                 loss: 0.1700
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 1941/30000 (6.4700%),                 avg. length: 2474.0,                last time consumption/overall running time: 285.4548s / 25418.6595 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0598
env0_second_0:                 episode reward: -5.8500,                 loss: 0.1465
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1961/30000 (6.5367%),                 avg. length: 2649.3,                last time consumption/overall running time: 316.3534s / 25735.0129 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0487
env0_second_0:                 episode reward: -5.8500,                 loss: 0.0871
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1981/30000 (6.6033%),                 avg. length: 1359.35,                last time consumption/overall running time: 165.0618s / 25900.0747 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0467
env0_second_0:                 episode reward: -3.8000,                 loss: 0.2002
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 2001/30000 (6.6700%),                 avg. length: 1266.9,                last time consumption/overall running time: 154.5050s / 26054.5798 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.0526
env0_second_0:                 episode reward: -5.6000,                 loss: 0.0421
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 2021/30000 (6.7367%),                 avg. length: 1417.95,                last time consumption/overall running time: 158.6782s / 26213.2580 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0275
env0_second_0:                 episode reward: -5.4000,                 loss: 0.0528
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2041/30000 (6.8033%),                 avg. length: 1546.1,                last time consumption/overall running time: 173.7425s / 26387.0005 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.0095
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0181
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2061/30000 (6.8700%),                 avg. length: 1538.1,                last time consumption/overall running time: 172.1002s / 26559.1007 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.0154
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0094
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2081/30000 (6.9367%),                 avg. length: 1356.6,                last time consumption/overall running time: 154.2214s / 26713.3221 s
env0_first_0:                 episode reward: 6.2000,                 loss: -0.0531
env0_second_0:                 episode reward: -6.2000,                 loss: -0.0204
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 2101/30000 (7.0033%),                 avg. length: 1366.15,                last time consumption/overall running time: 171.3140s / 26884.6361 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.0567
env0_second_0:                 episode reward: -7.2500,                 loss: 0.0210
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 2121/30000 (7.0700%),                 avg. length: 1367.25,                last time consumption/overall running time: 151.2372s / 27035.8733 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0577
env0_second_0:                 episode reward: -7.7500,                 loss: 0.0343
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 2141/30000 (7.1367%),                 avg. length: 1459.5,                last time consumption/overall running time: 166.4016s / 27202.2750 s
env0_first_0:                 episode reward: 7.2000,                 loss: -0.0503
env0_second_0:                 episode reward: -7.2000,                 loss: 0.0514
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 2161/30000 (7.2033%),                 avg. length: 1936.7,                last time consumption/overall running time: 232.0842s / 27434.3591 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.0290
env0_second_0:                 episode reward: -6.1000,                 loss: 0.1415
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 2181/30000 (7.2700%),                 avg. length: 2804.55,                last time consumption/overall running time: 286.5653s / 27720.9245 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0366
env0_second_0:                 episode reward: -1.9500,                 loss: 0.1211
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2201/30000 (7.3367%),                 avg. length: 2828.65,                last time consumption/overall running time: 309.2054s / 28030.1299 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0494
env0_second_0:                 episode reward: -3.8500,                 loss: 0.1694
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2221/30000 (7.4033%),                 avg. length: 2308.25,                last time consumption/overall running time: 237.0565s / 28267.1865 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.0476
env0_second_0:                 episode reward: -6.1500,                 loss: 0.0779
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 2241/30000 (7.4700%),                 avg. length: 2437.9,                last time consumption/overall running time: 259.2475s / 28526.4340 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.0366
env0_second_0:                 episode reward: -6.1500,                 loss: 0.1016
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2261/30000 (7.5367%),                 avg. length: 2561.0,                last time consumption/overall running time: 268.7086s / 28795.1425 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0389
env0_second_0:                 episode reward: -5.4500,                 loss: 0.0466
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 2281/30000 (7.6033%),                 avg. length: 2359.95,                last time consumption/overall running time: 257.5445s / 29052.6870 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0360
env0_second_0:                 episode reward: -6.4000,                 loss: 0.0609
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 2301/30000 (7.6700%),                 avg. length: 2383.95,                last time consumption/overall running time: 258.3478s / 29311.0348 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0512
env0_second_0:                 episode reward: -5.6500,                 loss: 0.0535
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 2321/30000 (7.7367%),                 avg. length: 1796.9,                last time consumption/overall running time: 190.7934s / 29501.8283 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0570
env0_second_0:                 episode reward: -7.7500,                 loss: 0.0748
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 2341/30000 (7.8033%),                 avg. length: 2369.65,                last time consumption/overall running time: 262.6839s / 29764.5122 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.0445
env0_second_0:                 episode reward: -5.5500,                 loss: 0.1098
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 2361/30000 (7.8700%),                 avg. length: 2701.35,                last time consumption/overall running time: 316.1965s / 30080.7087 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.0433
env0_second_0:                 episode reward: -4.7500,                 loss: 0.0803
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 2381/30000 (7.9367%),                 avg. length: 2887.35,                last time consumption/overall running time: 323.3406s / 30404.0493 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.0348
env0_second_0:                 episode reward: -4.5000,                 loss: 0.0410
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2401/30000 (8.0033%),                 avg. length: 2822.05,                last time consumption/overall running time: 301.3909s / 30705.4401 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0339
env0_second_0:                 episode reward: -4.8000,                 loss: 0.0749
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2421/30000 (8.0700%),                 avg. length: 2918.3,                last time consumption/overall running time: 326.5175s / 31031.9577 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.0335
env0_second_0:                 episode reward: -4.2500,                 loss: 0.0364
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2441/30000 (8.1367%),                 avg. length: 2650.75,                last time consumption/overall running time: 305.8059s / 31337.7636 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.0138
env0_second_0:                 episode reward: -5.6000,                 loss: 0.1138
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 2461/30000 (8.2033%),                 avg. length: 2205.8,                last time consumption/overall running time: 265.1473s / 31602.9108 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.0572
env0_second_0:                 episode reward: -7.3000,                 loss: 0.1802
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 2481/30000 (8.2700%),                 avg. length: 2188.1,                last time consumption/overall running time: 257.7534s / 31860.6642 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.0429
env0_second_0:                 episode reward: -6.9500,                 loss: 0.0481
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 2501/30000 (8.3367%),                 avg. length: 2848.3,                last time consumption/overall running time: 311.6141s / 32172.2782 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.0473
env0_second_0:                 episode reward: -4.7000,                 loss: 0.1522
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 2521/30000 (8.4033%),                 avg. length: 2683.95,                last time consumption/overall running time: 321.6443s / 32493.9226 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.0581
env0_second_0:                 episode reward: -4.3500,                 loss: 0.0562
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 2541/30000 (8.4700%),                 avg. length: 2500.15,                last time consumption/overall running time: 261.0733s / 32754.9959 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.0477
env0_second_0:                 episode reward: -5.6000,                 loss: 0.0546
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 2561/30000 (8.5367%),                 avg. length: 2848.7,                last time consumption/overall running time: 303.5283s / 33058.5241 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0614
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0745
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 2581/30000 (8.6033%),                 avg. length: 2743.3,                last time consumption/overall running time: 308.8486s / 33367.3728 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.0696
env0_second_0:                 episode reward: -5.0500,                 loss: 0.0365
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2601/30000 (8.6700%),                 avg. length: 2848.95,                last time consumption/overall running time: 346.5675s / 33713.9402 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0840
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0061
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 2621/30000 (8.7367%),                 avg. length: 2919.7,                last time consumption/overall running time: 321.9523s / 34035.8925 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.0746
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0425
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 2641/30000 (8.8033%),                 avg. length: 3122.75,                last time consumption/overall running time: 356.2581s / 34392.1506 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0601
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0580
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2661/30000 (8.8700%),                 avg. length: 2916.7,                last time consumption/overall running time: 313.6642s / 34705.8148 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.0509
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0578
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2681/30000 (8.9367%),                 avg. length: 2553.05,                last time consumption/overall running time: 291.3100s / 34997.1248 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0415
env0_second_0:                 episode reward: -5.4500,                 loss: 0.0985
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 2701/30000 (9.0033%),                 avg. length: 3176.05,                last time consumption/overall running time: 351.3206s / 35348.4454 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.0562
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0361
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2721/30000 (9.0700%),                 avg. length: 2884.05,                last time consumption/overall running time: 331.0448s / 35679.4903 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.0590
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0234
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2741/30000 (9.1367%),                 avg. length: 2843.4,                last time consumption/overall running time: 306.3600s / 35985.8503 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0614
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0151
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 2761/30000 (9.2033%),                 avg. length: 2822.8,                last time consumption/overall running time: 290.0507s / 36275.9010 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.0327
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0334
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2781/30000 (9.2700%),                 avg. length: 2748.25,                last time consumption/overall running time: 309.7873s / 36585.6883 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0420
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0142
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 2801/30000 (9.3367%),                 avg. length: 2516.85,                last time consumption/overall running time: 273.1508s / 36858.8390 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0453
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0340
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 2821/30000 (9.4033%),                 avg. length: 2796.75,                last time consumption/overall running time: 315.0715s / 37173.9105 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.0602
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0147
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 2841/30000 (9.4700%),                 avg. length: 2714.1,                last time consumption/overall running time: 304.7351s / 37478.6456 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0065
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0956
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2861/30000 (9.5367%),                 avg. length: 2896.9,                last time consumption/overall running time: 303.2465s / 37781.8921 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0389
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1418
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 2881/30000 (9.6033%),                 avg. length: 1571.35,                last time consumption/overall running time: 161.4793s / 37943.3714 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0206
env0_second_0:                 episode reward: 0.8500,                 loss: 0.1377
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2901/30000 (9.6700%),                 avg. length: 1442.65,                last time consumption/overall running time: 167.2347s / 38110.6061 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.0683
env0_second_0:                 episode reward: -6.5000,                 loss: 0.0147
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 2921/30000 (9.7367%),                 avg. length: 1580.85,                last time consumption/overall running time: 175.4405s / 38286.0466 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.0449
env0_second_0:                 episode reward: -5.1000,                 loss: 0.0103
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 2941/30000 (9.8033%),                 avg. length: 1882.5,                last time consumption/overall running time: 215.5829s / 38501.6295 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0178
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0554
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2961/30000 (9.8700%),                 avg. length: 2119.25,                last time consumption/overall running time: 252.5659s / 38754.1954 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0057
env0_second_0:                 episode reward: -0.8500,                 loss: 0.1018
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 2981/30000 (9.9367%),                 avg. length: 2157.55,                last time consumption/overall running time: 254.6236s / 39008.8190 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0066
env0_second_0:                 episode reward: -0.6000,                 loss: 0.1162
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 3001/30000 (10.0033%),                 avg. length: 2088.6,                last time consumption/overall running time: 255.7448s / 39264.5639 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0132
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0985
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3021/30000 (10.0700%),                 avg. length: 2050.7,                last time consumption/overall running time: 225.4507s / 39490.0146 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.0378
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0962
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3041/30000 (10.1367%),                 avg. length: 1906.35,                last time consumption/overall running time: 204.9440s / 39694.9587 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0406
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0305
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 3061/30000 (10.2033%),                 avg. length: 1938.05,                last time consumption/overall running time: 225.5511s / 39920.5097 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.0549
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0014
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 3081/30000 (10.2700%),                 avg. length: 2198.95,                last time consumption/overall running time: 258.4217s / 40178.9314 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0435
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0417
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3101/30000 (10.3367%),                 avg. length: 2329.9,                last time consumption/overall running time: 271.7438s / 40450.6752 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.0405
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0469
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 3121/30000 (10.4033%),                 avg. length: 1897.3,                last time consumption/overall running time: 217.6100s / 40668.2852 s
env0_first_0:                 episode reward: -4.2500,                 loss: -0.0412
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0187
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 3141/30000 (10.4700%),                 avg. length: 2355.5,                last time consumption/overall running time: 262.3878s / 40930.6730 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.0069
env0_second_0:                 episode reward: 2.4000,                 loss: 0.1142
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3161/30000 (10.5367%),                 avg. length: 2174.25,                last time consumption/overall running time: 259.6251s / 41190.2981 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0228
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1387
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 3181/30000 (10.6033%),                 avg. length: 2233.3,                last time consumption/overall running time: 236.5359s / 41426.8340 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0192
env0_second_0:                 episode reward: 2.1000,                 loss: -0.3809
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3201/30000 (10.6700%),                 avg. length: 2150.3,                last time consumption/overall running time: 226.7032s / 41653.5372 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0350
env0_second_0:                 episode reward: 0.0000,                 loss: 1.4514
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3221/30000 (10.7367%),                 avg. length: 2191.4,                last time consumption/overall running time: 216.2129s / 41869.7501 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0271
env0_second_0:                 episode reward: -2.8500,                 loss: 0.5156
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3241/30000 (10.8033%),                 avg. length: 1938.85,                last time consumption/overall running time: 222.8374s / 42092.5875 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0211
env0_second_0:                 episode reward: -1.3500,                 loss: 0.5887
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3261/30000 (10.8700%),                 avg. length: 1914.2,                last time consumption/overall running time: 193.1429s / 42285.7304 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0309
env0_second_0:                 episode reward: 0.4000,                 loss: 0.3363
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3281/30000 (10.9367%),                 avg. length: 1938.45,                last time consumption/overall running time: 192.8651s / 42478.5955 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0638
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0733
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 3301/30000 (11.0033%),                 avg. length: 2288.15,                last time consumption/overall running time: 248.0530s / 42726.6485 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0635
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0800
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3321/30000 (11.0700%),                 avg. length: 2175.0,                last time consumption/overall running time: 239.5153s / 42966.1638 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.0662
env0_second_0:                 episode reward: -5.5000,                 loss: 0.0400
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 3341/30000 (11.1367%),                 avg. length: 2158.7,                last time consumption/overall running time: 231.8517s / 43198.0155 s
env0_first_0:                 episode reward: 5.9500,                 loss: -0.0883
env0_second_0:                 episode reward: -5.9500,                 loss: 0.0295
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 3361/30000 (11.2033%),                 avg. length: 2202.65,                last time consumption/overall running time: 244.0664s / 43442.0820 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0950
env0_second_0:                 episode reward: -5.6500,                 loss: -0.0205
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3381/30000 (11.2700%),                 avg. length: 2183.5,                last time consumption/overall running time: 256.2404s / 43698.3224 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0630
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0542
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3401/30000 (11.3367%),                 avg. length: 2422.4,                last time consumption/overall running time: 280.3973s / 43978.7196 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.0510
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0464
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 3421/30000 (11.4033%),                 avg. length: 2361.9,                last time consumption/overall running time: 242.5200s / 44221.2397 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.0426
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0422
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3441/30000 (11.4700%),                 avg. length: 2721.5,                last time consumption/overall running time: 277.3369s / 44498.5766 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0251
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0729
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 3461/30000 (11.5367%),                 avg. length: 2657.1,                last time consumption/overall running time: 284.0443s / 44782.6209 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.0373
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0228
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 3481/30000 (11.6033%),                 avg. length: 2712.3,                last time consumption/overall running time: 299.1387s / 45081.7596 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.0470
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0180
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 3501/30000 (11.6700%),                 avg. length: 2618.6,                last time consumption/overall running time: 273.2779s / 45355.0375 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0722
env0_second_0:                 episode reward: -4.6500,                 loss: -0.0171
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3521/30000 (11.7367%),                 avg. length: 2894.25,                last time consumption/overall running time: 317.2089s / 45672.2464 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0468
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0116
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3541/30000 (11.8033%),                 avg. length: 2896.8,                last time consumption/overall running time: 341.4109s / 46013.6573 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0474
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0128
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3561/30000 (11.8700%),                 avg. length: 2788.5,                last time consumption/overall running time: 332.4313s / 46346.0886 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0700
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0269
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 3581/30000 (11.9367%),                 avg. length: 2430.5,                last time consumption/overall running time: 258.0160s / 46604.1046 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0680
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0239
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 3601/30000 (12.0033%),                 avg. length: 2759.1,                last time consumption/overall running time: 320.8766s / 46924.9812 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0693
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0216
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 3621/30000 (12.0700%),                 avg. length: 2794.1,                last time consumption/overall running time: 324.3584s / 47249.3396 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.0481
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0077
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3641/30000 (12.1367%),                 avg. length: 2818.25,                last time consumption/overall running time: 319.0775s / 47568.4171 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0532
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0114
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3661/30000 (12.2033%),                 avg. length: 2930.8,                last time consumption/overall running time: 345.1236s / 47913.5407 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0394
env0_second_0:                 episode reward: 0.1500,                 loss: 0.1172
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3681/30000 (12.2700%),                 avg. length: 2872.8,                last time consumption/overall running time: 323.3959s / 48236.9366 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0464
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0985
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3701/30000 (12.3367%),                 avg. length: 2815.95,                last time consumption/overall running time: 331.3961s / 48568.3326 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.0647
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0080
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 3721/30000 (12.4033%),                 avg. length: 2715.35,                last time consumption/overall running time: 331.3134s / 48899.6460 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.0649
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0202
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 3741/30000 (12.4700%),                 avg. length: 2947.3,                last time consumption/overall running time: 355.8865s / 49255.5325 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.0626
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0040
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3761/30000 (12.5367%),                 avg. length: 2749.4,                last time consumption/overall running time: 331.3957s / 49586.9282 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.0481
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0183
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3781/30000 (12.6033%),                 avg. length: 3035.3,                last time consumption/overall running time: 365.8836s / 49952.8118 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0523
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0187
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 3801/30000 (12.6700%),                 avg. length: 2943.25,                last time consumption/overall running time: 358.9145s / 50311.7263 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0270
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0454
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 3821/30000 (12.7367%),                 avg. length: 3047.65,                last time consumption/overall running time: 324.8232s / 50636.5496 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0405
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0541
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 3841/30000 (12.8033%),                 avg. length: 2983.9,                last time consumption/overall running time: 292.8073s / 50929.3568 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0489
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0468
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3861/30000 (12.8700%),                 avg. length: 3052.2,                last time consumption/overall running time: 341.5370s / 51270.8938 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.0512
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0375
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 3881/30000 (12.9367%),                 avg. length: 3270.5,                last time consumption/overall running time: 376.6099s / 51647.5038 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0632
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0435
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 3901/30000 (13.0033%),                 avg. length: 3018.5,                last time consumption/overall running time: 334.7052s / 51982.2089 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0608
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0555
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3921/30000 (13.0700%),                 avg. length: 3307.25,                last time consumption/overall running time: 394.3472s / 52376.5561 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.0566
env0_second_0:                 episode reward: 1.7000,                 loss: 0.1006
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3941/30000 (13.1367%),                 avg. length: 2597.65,                last time consumption/overall running time: 288.2980s / 52664.8541 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0098
env0_second_0:                 episode reward: -0.2000,                 loss: 0.2445
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3961/30000 (13.2033%),                 avg. length: 2392.15,                last time consumption/overall running time: 283.1804s / 52948.0345 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0328
env0_second_0:                 episode reward: -5.7500,                 loss: 0.1142
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 3981/30000 (13.2700%),                 avg. length: 2467.9,                last time consumption/overall running time: 274.3729s / 53222.4075 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.0069
env0_second_0:                 episode reward: 2.9000,                 loss: 0.2774
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 4001/30000 (13.3367%),                 avg. length: 2995.05,                last time consumption/overall running time: 341.1858s / 53563.5932 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0297
env0_second_0:                 episode reward: -0.8000,                 loss: 0.1386
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4021/30000 (13.4033%),                 avg. length: 1489.55,                last time consumption/overall running time: 159.1041s / 53722.6974 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.0206
env0_second_0:                 episode reward: -4.2000,                 loss: 0.1462
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 4041/30000 (13.4700%),                 avg. length: 1621.2,                last time consumption/overall running time: 175.0455s / 53897.7428 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0510
env0_second_0:                 episode reward: -4.8500,                 loss: 0.0398
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 4061/30000 (13.5367%),                 avg. length: 2253.8,                last time consumption/overall running time: 264.7784s / 54162.5212 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0402
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0557
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4081/30000 (13.6033%),                 avg. length: 1821.35,                last time consumption/overall running time: 203.7444s / 54366.2657 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.0266
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0762
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4101/30000 (13.6700%),                 avg. length: 1937.7,                last time consumption/overall running time: 208.4756s / 54574.7413 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0269
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0807
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4121/30000 (13.7367%),                 avg. length: 2164.9,                last time consumption/overall running time: 224.5187s / 54799.2600 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.0606
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0113
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4141/30000 (13.8033%),                 avg. length: 1954.35,                last time consumption/overall running time: 228.6687s / 55027.9287 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0778
env0_second_0:                 episode reward: 5.9500,                 loss: -0.0186
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 4161/30000 (13.8700%),                 avg. length: 1938.5,                last time consumption/overall running time: 214.0968s / 55242.0255 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.0460
env0_second_0:                 episode reward: 2.8500,                 loss: 0.1068
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4181/30000 (13.9367%),                 avg. length: 2061.7,                last time consumption/overall running time: 249.7791s / 55491.8047 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.0738
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0243
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 4201/30000 (14.0033%),                 avg. length: 2145.2,                last time consumption/overall running time: 255.1970s / 55747.0017 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.0735
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0145
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4221/30000 (14.0700%),                 avg. length: 2282.05,                last time consumption/overall running time: 266.7110s / 56013.7126 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0697
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0700
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 4241/30000 (14.1367%),                 avg. length: 1971.05,                last time consumption/overall running time: 235.1207s / 56248.8333 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.0977
env0_second_0:                 episode reward: 6.1000,                 loss: -0.0230
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 4261/30000 (14.2033%),                 avg. length: 1813.25,                last time consumption/overall running time: 222.6073s / 56471.4406 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0549
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0546
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 4281/30000 (14.2700%),                 avg. length: 1766.85,                last time consumption/overall running time: 209.1598s / 56680.6004 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0776
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0412
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4301/30000 (14.3367%),                 avg. length: 2217.15,                last time consumption/overall running time: 245.2540s / 56925.8545 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0660
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0571
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4321/30000 (14.4033%),                 avg. length: 2497.05,                last time consumption/overall running time: 278.3845s / 57204.2389 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.0813
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0056
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 4341/30000 (14.4700%),                 avg. length: 2225.25,                last time consumption/overall running time: 273.5981s / 57477.8370 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.0936
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0388
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 4361/30000 (14.5367%),                 avg. length: 2328.9,                last time consumption/overall running time: 284.4358s / 57762.2729 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0854
env0_second_0:                 episode reward: 5.1000,                 loss: -0.0277
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4381/30000 (14.6033%),                 avg. length: 1755.4,                last time consumption/overall running time: 218.3568s / 57980.6296 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0835
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0179
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4401/30000 (14.6700%),                 avg. length: 2181.95,                last time consumption/overall running time: 266.5556s / 58247.1853 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.0643
env0_second_0:                 episode reward: 2.8000,                 loss: -0.0033
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 4421/30000 (14.7367%),                 avg. length: 2274.95,                last time consumption/overall running time: 232.7199s / 58479.9052 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.1027
env0_second_0:                 episode reward: 5.7000,                 loss: -0.0251
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 4441/30000 (14.8033%),                 avg. length: 1946.15,                last time consumption/overall running time: 216.3593s / 58696.2645 s
env0_first_0:                 episode reward: -8.1000,                 loss: -0.1058
env0_second_0:                 episode reward: 8.1000,                 loss: -0.0329
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 4461/30000 (14.8700%),                 avg. length: 2097.65,                last time consumption/overall running time: 224.6090s / 58920.8735 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.0899
env0_second_0:                 episode reward: 7.2500,                 loss: -0.0101
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 4481/30000 (14.9367%),                 avg. length: 2089.3,                last time consumption/overall running time: 236.8399s / 59157.7134 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0767
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0186
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4501/30000 (15.0033%),                 avg. length: 2597.9,                last time consumption/overall running time: 306.1267s / 59463.8401 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0670
env0_second_0:                 episode reward: 4.7000,                 loss: -0.0125
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4521/30000 (15.0700%),                 avg. length: 2558.55,                last time consumption/overall running time: 298.6377s / 59762.4778 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0846
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0167
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4541/30000 (15.1367%),                 avg. length: 2728.4,                last time consumption/overall running time: 317.3605s / 60079.8383 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0723
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0442
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 4561/30000 (15.2033%),                 avg. length: 2413.65,                last time consumption/overall running time: 290.9717s / 60370.8099 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.0901
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0343
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 4581/30000 (15.2700%),                 avg. length: 2695.4,                last time consumption/overall running time: 283.0312s / 60653.8411 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0842
env0_second_0:                 episode reward: 3.1000,                 loss: 0.3576
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 4601/30000 (15.3367%),                 avg. length: 2679.3,                last time consumption/overall running time: 274.9912s / 60928.8323 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.0869
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0184
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 4621/30000 (15.4033%),                 avg. length: 2489.6,                last time consumption/overall running time: 265.8039s / 61194.6362 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1010
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0194
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4641/30000 (15.4700%),                 avg. length: 2475.45,                last time consumption/overall running time: 245.4384s / 61440.0745 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.0974
env0_second_0:                 episode reward: 2.5500,                 loss: -0.0181
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4661/30000 (15.5367%),                 avg. length: 2425.25,                last time consumption/overall running time: 291.0514s / 61731.1260 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0762
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0320
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4681/30000 (15.6033%),                 avg. length: 2395.9,                last time consumption/overall running time: 243.4780s / 61974.6039 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.0882
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0800
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 4701/30000 (15.6700%),                 avg. length: 2116.1,                last time consumption/overall running time: 218.0935s / 62192.6974 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0723
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0417
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 4721/30000 (15.7367%),                 avg. length: 2707.35,                last time consumption/overall running time: 311.2392s / 62503.9367 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.0898
env0_second_0:                 episode reward: -2.2500,                 loss: -0.0360
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4741/30000 (15.8033%),                 avg. length: 3082.25,                last time consumption/overall running time: 344.0812s / 62848.0179 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0815
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0095
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4761/30000 (15.8700%),                 avg. length: 2964.5,                last time consumption/overall running time: 338.4610s / 63186.4789 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0858
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0082
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 4781/30000 (15.9367%),                 avg. length: 2914.95,                last time consumption/overall running time: 354.1382s / 63540.6171 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0996
env0_second_0:                 episode reward: 0.4000,                 loss: -0.0181
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4801/30000 (16.0033%),                 avg. length: 2811.75,                last time consumption/overall running time: 321.0496s / 63861.6667 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.0995
env0_second_0:                 episode reward: 2.0000,                 loss: -0.0225
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 4821/30000 (16.0700%),                 avg. length: 2495.15,                last time consumption/overall running time: 281.3631s / 64143.0298 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.1196
env0_second_0:                 episode reward: 4.4500,                 loss: -0.0389
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 4841/30000 (16.1367%),                 avg. length: 3168.9,                last time consumption/overall running time: 358.4641s / 64501.4939 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0965
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0307
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4861/30000 (16.2033%),                 avg. length: 3103.3,                last time consumption/overall running time: 350.0081s / 64851.5020 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.0877
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0141
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 4881/30000 (16.2700%),                 avg. length: 3170.6,                last time consumption/overall running time: 381.5672s / 65233.0692 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0859
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0119
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 4901/30000 (16.3367%),                 avg. length: 2826.4,                last time consumption/overall running time: 325.4152s / 65558.4844 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0769
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0358
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 4921/30000 (16.4033%),                 avg. length: 2881.05,                last time consumption/overall running time: 346.2297s / 65904.7141 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0904
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0080
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4941/30000 (16.4700%),                 avg. length: 2733.8,                last time consumption/overall running time: 316.2986s / 66221.0127 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.0722
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0514
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4961/30000 (16.5367%),                 avg. length: 3101.15,                last time consumption/overall running time: 368.1823s / 66589.1950 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0940
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0162
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4981/30000 (16.6033%),                 avg. length: 3238.65,                last time consumption/overall running time: 358.1217s / 66947.3167 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0993
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0034
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 5001/30000 (16.6700%),                 avg. length: 2974.5,                last time consumption/overall running time: 349.4905s / 67296.8071 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0957
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0118
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5021/30000 (16.7367%),                 avg. length: 2517.5,                last time consumption/overall running time: 291.5511s / 67588.3582 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.0635
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0559
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5041/30000 (16.8033%),                 avg. length: 2999.45,                last time consumption/overall running time: 293.5072s / 67881.8654 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0891
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0097
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 5061/30000 (16.8700%),                 avg. length: 3115.8,                last time consumption/overall running time: 309.5839s / 68191.4493 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0829
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0295
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 5081/30000 (16.9367%),                 avg. length: 3210.9,                last time consumption/overall running time: 353.0601s / 68544.5093 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.0837
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0067
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 5101/30000 (17.0033%),                 avg. length: 3003.4,                last time consumption/overall running time: 350.7495s / 68895.2589 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0806
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5121/30000 (17.0700%),                 avg. length: 2503.4,                last time consumption/overall running time: 285.7409s / 69180.9998 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.0643
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0204
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 5141/30000 (17.1367%),                 avg. length: 2460.15,                last time consumption/overall running time: 295.1549s / 69476.1548 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0435
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0304
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 5161/30000 (17.2033%),                 avg. length: 2747.55,                last time consumption/overall running time: 278.8473s / 69755.0021 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0835
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0245
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5181/30000 (17.2700%),                 avg. length: 3051.05,                last time consumption/overall running time: 335.5383s / 70090.5404 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0798
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0549
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 5201/30000 (17.3367%),                 avg. length: 3074.6,                last time consumption/overall running time: 305.4382s / 70395.9786 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0685
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0422
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 5221/30000 (17.4033%),                 avg. length: 3241.95,                last time consumption/overall running time: 392.2785s / 70788.2571 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0770
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0692
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 5241/30000 (17.4700%),                 avg. length: 2881.65,                last time consumption/overall running time: 308.0254s / 71096.2825 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0625
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0485
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 5261/30000 (17.5367%),                 avg. length: 2859.8,                last time consumption/overall running time: 316.6577s / 71412.9401 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.0828
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0439
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 5281/30000 (17.6033%),                 avg. length: 2762.75,                last time consumption/overall running time: 315.7350s / 71728.6751 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0216
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0949
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 5301/30000 (17.6700%),                 avg. length: 3026.1,                last time consumption/overall running time: 331.5301s / 72060.2052 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0877
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0098
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 5321/30000 (17.7367%),                 avg. length: 2931.25,                last time consumption/overall running time: 279.7410s / 72339.9462 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.0868
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0399
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5341/30000 (17.8033%),                 avg. length: 2967.3,                last time consumption/overall running time: 338.2213s / 72678.1675 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0930
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0207
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5361/30000 (17.8700%),                 avg. length: 2784.25,                last time consumption/overall running time: 300.6819s / 72978.8494 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.0930
env0_second_0:                 episode reward: 5.4500,                 loss: 0.4316
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5381/30000 (17.9367%),                 avg. length: 2942.3,                last time consumption/overall running time: 354.3981s / 73333.2475 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0891
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0453
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 5401/30000 (18.0033%),                 avg. length: 3010.0,                last time consumption/overall running time: 335.9147s / 73669.1622 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0851
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0260
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5421/30000 (18.0700%),                 avg. length: 3164.2,                last time consumption/overall running time: 360.6524s / 74029.8146 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.0726
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0402
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 5441/30000 (18.1367%),                 avg. length: 2878.9,                last time consumption/overall running time: 318.8974s / 74348.7120 s
env0_first_0:                 episode reward: -5.6500,                 loss: -0.0683
env0_second_0:                 episode reward: 5.6500,                 loss: 0.1459
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 5461/30000 (18.2033%),                 avg. length: 3141.25,                last time consumption/overall running time: 366.8225s / 74715.5344 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0701
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0963
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5481/30000 (18.2700%),                 avg. length: 2924.75,                last time consumption/overall running time: 331.3455s / 75046.8799 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0880
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0677
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5501/30000 (18.3367%),                 avg. length: 2471.5,                last time consumption/overall running time: 303.4614s / 75350.3414 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0737
env0_second_0:                 episode reward: -0.1500,                 loss: 0.1024
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 5521/30000 (18.4033%),                 avg. length: 3195.25,                last time consumption/overall running time: 328.0527s / 75678.3941 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0811
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0317
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5541/30000 (18.4700%),                 avg. length: 3054.65,                last time consumption/overall running time: 323.6317s / 76002.0257 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0726
env0_second_0:                 episode reward: -5.8500,                 loss: 0.0701
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 5561/30000 (18.5367%),                 avg. length: 3018.25,                last time consumption/overall running time: 326.5627s / 76328.5884 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0820
env0_second_0:                 episode reward: -5.4000,                 loss: 0.0470
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 5581/30000 (18.6033%),                 avg. length: 3257.5,                last time consumption/overall running time: 341.3319s / 76669.9202 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.0659
env0_second_0:                 episode reward: -5.1500,                 loss: 0.0658
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 5601/30000 (18.6700%),                 avg. length: 3571.55,                last time consumption/overall running time: 365.3189s / 77035.2391 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.0616
env0_second_0:                 episode reward: -2.1000,                 loss: 0.2449
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5621/30000 (18.7367%),                 avg. length: 3589.35,                last time consumption/overall running time: 407.8831s / 77443.1222 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.0853
env0_second_0:                 episode reward: -3.0500,                 loss: 0.1670
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5641/30000 (18.8033%),                 avg. length: 3502.1,                last time consumption/overall running time: 367.5945s / 77810.7167 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0611
env0_second_0:                 episode reward: -2.6500,                 loss: 0.2064
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5661/30000 (18.8700%),                 avg. length: 3721.15,                last time consumption/overall running time: 430.9119s / 78241.6286 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.0519
env0_second_0:                 episode reward: -3.7500,                 loss: 0.6000
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5681/30000 (18.9367%),                 avg. length: 3027.1,                last time consumption/overall running time: 340.0373s / 78581.6659 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.0409
env0_second_0:                 episode reward: -1.3000,                 loss: 0.3172
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 5701/30000 (19.0033%),                 avg. length: 3141.4,                last time consumption/overall running time: 348.6486s / 78930.3145 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0776
env0_second_0:                 episode reward: -4.6000,                 loss: 0.3537
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 5721/30000 (19.0700%),                 avg. length: 3404.7,                last time consumption/overall running time: 393.2645s / 79323.5790 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.0851
env0_second_0:                 episode reward: -4.3000,                 loss: 0.3759
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5741/30000 (19.1367%),                 avg. length: 3868.6,                last time consumption/overall running time: 447.1965s / 79770.7754 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0764
env0_second_0:                 episode reward: -1.7500,                 loss: 0.3347
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5761/30000 (19.2033%),                 avg. length: 3368.65,                last time consumption/overall running time: 356.3406s / 80127.1160 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0784
env0_second_0:                 episode reward: -2.6000,                 loss: 0.4758
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5781/30000 (19.2700%),                 avg. length: 3712.5,                last time consumption/overall running time: 434.7275s / 80561.8436 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0465
env0_second_0:                 episode reward: 1.8500,                 loss: 0.1988
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5801/30000 (19.3367%),                 avg. length: 3812.1,                last time consumption/overall running time: 455.5607s / 81017.4043 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.0513
env0_second_0:                 episode reward: 0.4500,                 loss: 0.1700
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 5821/30000 (19.4033%),                 avg. length: 4031.95,                last time consumption/overall running time: 452.3693s / 81469.7736 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0798
env0_second_0:                 episode reward: 1.8500,                 loss: 0.1285
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5841/30000 (19.4700%),                 avg. length: 3294.2,                last time consumption/overall running time: 378.5839s / 81848.3576 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.0511
env0_second_0:                 episode reward: -1.4000,                 loss: 0.1906
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 5861/30000 (19.5367%),                 avg. length: 1819.2,                last time consumption/overall running time: 187.7238s / 82036.0813 s
env0_first_0:                 episode reward: 6.9000,                 loss: -0.0065
env0_second_0:                 episode reward: -6.9000,                 loss: 0.6143
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 5881/30000 (19.6033%),                 avg. length: 2109.55,                last time consumption/overall running time: 249.5361s / 82285.6174 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0009
env0_second_0:                 episode reward: -5.2500,                 loss: 0.4099
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5901/30000 (19.6700%),                 avg. length: 2470.5,                last time consumption/overall running time: 258.4103s / 82544.0277 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.0061
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0951
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5921/30000 (19.7367%),                 avg. length: 2234.75,                last time consumption/overall running time: 225.4809s / 82769.5086 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.0239
env0_second_0:                 episode reward: 4.1000,                 loss: 0.2912
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5941/30000 (19.8033%),                 avg. length: 2012.45,                last time consumption/overall running time: 214.9499s / 82984.4585 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0373
env0_second_0:                 episode reward: 2.6500,                 loss: 0.7640
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 5961/30000 (19.8700%),                 avg. length: 1174.25,                last time consumption/overall running time: 140.8025s / 83125.2610 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0705
env0_second_0:                 episode reward: 7.8500,                 loss: 0.5061
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 5981/30000 (19.9367%),                 avg. length: 1114.25,                last time consumption/overall running time: 123.1131s / 83248.3741 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.1113
env0_second_0:                 episode reward: 8.3500,                 loss: 0.1702
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 6001/30000 (20.0033%),                 avg. length: 1103.1,                last time consumption/overall running time: 139.0371s / 83387.4113 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.1182
env0_second_0:                 episode reward: 8.6500,                 loss: -0.0236
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 6021/30000 (20.0700%),                 avg. length: 1095.15,                last time consumption/overall running time: 137.5721s / 83524.9834 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.1197
env0_second_0:                 episode reward: 8.3000,                 loss: -0.0455
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 6041/30000 (20.1367%),                 avg. length: 963.85,                last time consumption/overall running time: 119.6243s / 83644.6077 s
env0_first_0:                 episode reward: -8.9000,                 loss: -0.1251
env0_second_0:                 episode reward: 8.9000,                 loss: -0.0080
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 6061/30000 (20.2033%),                 avg. length: 937.15,                last time consumption/overall running time: 99.7440s / 83744.3517 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.0945
env0_second_0:                 episode reward: 9.3500,                 loss: 0.2102
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 6081/30000 (20.2700%),                 avg. length: 988.2,                last time consumption/overall running time: 99.9082s / 83844.2599 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.0533
env0_second_0:                 episode reward: 8.8500,                 loss: 0.3117
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 6101/30000 (20.3367%),                 avg. length: 925.4,                last time consumption/overall running time: 119.4383s / 83963.6981 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.1093
env0_second_0:                 episode reward: 8.8500,                 loss: -0.0087
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 6121/30000 (20.4033%),                 avg. length: 912.7,                last time consumption/overall running time: 112.3145s / 84076.0126 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0985
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0381
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 6141/30000 (20.4700%),                 avg. length: 918.45,                last time consumption/overall running time: 121.0001s / 84197.0127 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.0903
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0481
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 6161/30000 (20.5367%),                 avg. length: 1044.75,                last time consumption/overall running time: 117.0620s / 84314.0747 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0282
env0_second_0:                 episode reward: 8.0000,                 loss: 0.2518
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 6181/30000 (20.6033%),                 avg. length: 1722.75,                last time consumption/overall running time: 185.6063s / 84499.6810 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0221
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1314
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 6201/30000 (20.6700%),                 avg. length: 1435.05,                last time consumption/overall running time: 142.4527s / 84642.1337 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0126
env0_second_0:                 episode reward: 6.6500,                 loss: 0.1272
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 6221/30000 (20.7367%),                 avg. length: 1348.65,                last time consumption/overall running time: 145.0877s / 84787.2215 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.0244
env0_second_0:                 episode reward: 7.7500,                 loss: 0.3780
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 6241/30000 (20.8033%),                 avg. length: 1374.7,                last time consumption/overall running time: 142.9657s / 84930.1872 s
env0_first_0:                 episode reward: -8.2500,                 loss: 0.0141
env0_second_0:                 episode reward: 8.2500,                 loss: 0.4566
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 6261/30000 (20.8700%),                 avg. length: 1544.9,                last time consumption/overall running time: 191.4647s / 85121.6519 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0178
env0_second_0:                 episode reward: 4.8000,                 loss: 1.5195
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 6281/30000 (20.9367%),                 avg. length: 1540.6,                last time consumption/overall running time: 167.7322s / 85289.3841 s
env0_first_0:                 episode reward: -7.0500,                 loss: -0.0178
env0_second_0:                 episode reward: 7.0500,                 loss: 1.5528
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 6301/30000 (21.0033%),                 avg. length: 1575.9,                last time consumption/overall running time: 187.3164s / 85476.7006 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.0366
env0_second_0:                 episode reward: 7.7000,                 loss: 1.2312
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 6321/30000 (21.0700%),                 avg. length: 1545.0,                last time consumption/overall running time: 172.6577s / 85649.3582 s
env0_first_0:                 episode reward: -8.8000,                 loss: -0.0552
env0_second_0:                 episode reward: 8.8000,                 loss: 0.8088
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 6341/30000 (21.1367%),                 avg. length: 1562.65,                last time consumption/overall running time: 172.4501s / 85821.8084 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0811
env0_second_0:                 episode reward: 7.8500,                 loss: 0.8201
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 6361/30000 (21.2033%),                 avg. length: 1689.7,                last time consumption/overall running time: 190.0981s / 86011.9065 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.0249
env0_second_0:                 episode reward: 5.2500,                 loss: 0.7590
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 6381/30000 (21.2700%),                 avg. length: 1654.7,                last time consumption/overall running time: 203.6652s / 86215.5717 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.0519
env0_second_0:                 episode reward: 5.9000,                 loss: 0.2704
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 6401/30000 (21.3367%),                 avg. length: 1655.1,                last time consumption/overall running time: 186.1763s / 86401.7480 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.0710
env0_second_0:                 episode reward: 8.7500,                 loss: 0.2743
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 6421/30000 (21.4033%),                 avg. length: 1836.25,                last time consumption/overall running time: 229.0515s / 86630.7995 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.0931
env0_second_0:                 episode reward: 7.5500,                 loss: 0.1890
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 6441/30000 (21.4700%),                 avg. length: 1840.6,                last time consumption/overall running time: 214.6568s / 86845.4563 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.0839
env0_second_0:                 episode reward: 8.3500,                 loss: 0.6126
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 6461/30000 (21.5367%),                 avg. length: 1782.2,                last time consumption/overall running time: 191.9054s / 87037.3618 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0736
env0_second_0:                 episode reward: 7.8500,                 loss: 0.2907
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 6481/30000 (21.6033%),                 avg. length: 1913.55,                last time consumption/overall running time: 208.3833s / 87245.7451 s
env0_first_0:                 episode reward: -8.9000,                 loss: -0.1134
env0_second_0:                 episode reward: 8.9000,                 loss: 0.3039
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 6501/30000 (21.6700%),                 avg. length: 1957.95,                last time consumption/overall running time: 230.3009s / 87476.0459 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.1057
env0_second_0:                 episode reward: 8.6000,                 loss: 0.3090
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 6521/30000 (21.7367%),                 avg. length: 2396.15,                last time consumption/overall running time: 259.2542s / 87735.3001 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.1157
env0_second_0:                 episode reward: 5.5000,                 loss: 0.2321
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 6541/30000 (21.8033%),                 avg. length: 2268.35,                last time consumption/overall running time: 258.4365s / 87993.7366 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.1124
env0_second_0:                 episode reward: 5.7500,                 loss: 0.1814
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 6561/30000 (21.8700%),                 avg. length: 2540.65,                last time consumption/overall running time: 310.5294s / 88304.2660 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.1151
env0_second_0:                 episode reward: 5.2500,                 loss: 0.1660
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 6581/30000 (21.9367%),                 avg. length: 2910.9,                last time consumption/overall running time: 312.7417s / 88617.0078 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1160
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1633
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 6601/30000 (22.0033%),                 avg. length: 3011.7,                last time consumption/overall running time: 301.0121s / 88918.0199 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1083
env0_second_0:                 episode reward: 1.0500,                 loss: 0.1220
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 6621/30000 (22.0700%),                 avg. length: 3258.05,                last time consumption/overall running time: 348.1372s / 89266.1570 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1062
env0_second_0:                 episode reward: -0.6500,                 loss: 0.1277
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 6641/30000 (22.1367%),                 avg. length: 3158.4,                last time consumption/overall running time: 330.6248s / 89596.7819 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.0858
env0_second_0:                 episode reward: -2.3000,                 loss: 0.1448
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6661/30000 (22.2033%),                 avg. length: 3193.5,                last time consumption/overall running time: 343.5065s / 89940.2884 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.0552
env0_second_0:                 episode reward: -3.4500,                 loss: 0.3500
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 6681/30000 (22.2700%),                 avg. length: 3086.3,                last time consumption/overall running time: 302.2029s / 90242.4913 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0543
env0_second_0:                 episode reward: -4.6500,                 loss: 0.3608
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 6701/30000 (22.3367%),                 avg. length: 3553.75,                last time consumption/overall running time: 391.5911s / 90634.0824 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0693
env0_second_0:                 episode reward: -2.5000,                 loss: 0.3492
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 6721/30000 (22.4033%),                 avg. length: 3468.8,                last time consumption/overall running time: 390.9301s / 91025.0125 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.0599
env0_second_0:                 episode reward: -2.8000,                 loss: 0.2382
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 6741/30000 (22.4700%),                 avg. length: 4131.4,                last time consumption/overall running time: 464.1690s / 91489.1815 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0619
env0_second_0:                 episode reward: 2.1000,                 loss: 0.2174
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6761/30000 (22.5367%),                 avg. length: 3604.9,                last time consumption/overall running time: 359.5445s / 91848.7260 s
env0_first_0:                 episode reward: -5.4500,                 loss: -0.0838
env0_second_0:                 episode reward: 5.4500,                 loss: 0.2779
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6781/30000 (22.6033%),                 avg. length: 3966.75,                last time consumption/overall running time: 459.6349s / 92308.3609 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.0736
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1860
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 6801/30000 (22.6700%),                 avg. length: 4004.95,                last time consumption/overall running time: 479.2156s / 92787.5765 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0811
env0_second_0:                 episode reward: 0.8500,                 loss: 0.3208
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 6821/30000 (22.7367%),                 avg. length: 2940.7,                last time consumption/overall running time: 331.4072s / 93118.9837 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0348
env0_second_0:                 episode reward: -3.6500,                 loss: 0.3636
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6841/30000 (22.8033%),                 avg. length: 2893.9,                last time consumption/overall running time: 308.7124s / 93427.6961 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.0693
env0_second_0:                 episode reward: -6.8000,                 loss: 0.3887
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 6861/30000 (22.8700%),                 avg. length: 3865.8,                last time consumption/overall running time: 422.9108s / 93850.6068 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0713
env0_second_0:                 episode reward: -1.9500,                 loss: 0.1959
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 6881/30000 (22.9367%),                 avg. length: 3992.4,                last time consumption/overall running time: 422.1852s / 94272.7920 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0795
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0656
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 6901/30000 (23.0033%),                 avg. length: 4302.3,                last time consumption/overall running time: 473.8968s / 94746.6888 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0815
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1869
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6921/30000 (23.0700%),                 avg. length: 3917.4,                last time consumption/overall running time: 460.7001s / 95207.3890 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0794
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2167
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 6941/30000 (23.1367%),                 avg. length: 3861.25,                last time consumption/overall running time: 443.2794s / 95650.6683 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0765
env0_second_0:                 episode reward: 1.2500,                 loss: 0.4719
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6961/30000 (23.2033%),                 avg. length: 3658.95,                last time consumption/overall running time: 422.2899s / 96072.9583 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0843
env0_second_0:                 episode reward: -2.5500,                 loss: 0.2940
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 6981/30000 (23.2700%),                 avg. length: 3532.45,                last time consumption/overall running time: 416.9263s / 96489.8846 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.0834
env0_second_0:                 episode reward: 2.2000,                 loss: 0.3589
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7001/30000 (23.3367%),                 avg. length: 3027.05,                last time consumption/overall running time: 363.4067s / 96853.2912 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.0768
env0_second_0:                 episode reward: 2.7000,                 loss: 0.3720
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7021/30000 (23.4033%),                 avg. length: 3768.35,                last time consumption/overall running time: 413.9975s / 97267.2887 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.0736
env0_second_0:                 episode reward: -0.3500,                 loss: 0.1904
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 7041/30000 (23.4700%),                 avg. length: 3208.5,                last time consumption/overall running time: 343.6873s / 97610.9761 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0660
env0_second_0:                 episode reward: -4.9000,                 loss: 0.2602
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 7061/30000 (23.5367%),                 avg. length: 2593.4,                last time consumption/overall running time: 310.6033s / 97921.5794 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0683
env0_second_0:                 episode reward: -6.4000,                 loss: 0.2702
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 7081/30000 (23.6033%),                 avg. length: 2546.55,                last time consumption/overall running time: 276.0862s / 98197.6656 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0620
env0_second_0:                 episode reward: -5.8500,                 loss: 0.4555
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 7101/30000 (23.6700%),                 avg. length: 3811.1,                last time consumption/overall running time: 416.4035s / 98614.0691 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0461
env0_second_0:                 episode reward: -1.6500,                 loss: 0.3990
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7121/30000 (23.7367%),                 avg. length: 3440.4,                last time consumption/overall running time: 407.2827s / 99021.3518 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0359
env0_second_0:                 episode reward: 0.6500,                 loss: 0.5651
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 7141/30000 (23.8033%),                 avg. length: 3771.4,                last time consumption/overall running time: 416.5208s / 99437.8726 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.0507
env0_second_0:                 episode reward: -2.2500,                 loss: 0.3353
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 7161/30000 (23.8700%),                 avg. length: 4277.2,                last time consumption/overall running time: 456.1985s / 99894.0712 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0786
env0_second_0:                 episode reward: -2.6500,                 loss: 0.1532
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 7181/30000 (23.9367%),                 avg. length: 3691.6,                last time consumption/overall running time: 413.0427s / 100307.1139 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0839
env0_second_0:                 episode reward: -3.3000,                 loss: 0.2972
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 7201/30000 (24.0033%),                 avg. length: 4065.65,                last time consumption/overall running time: 429.0261s / 100736.1400 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.0760
env0_second_0:                 episode reward: -2.7500,                 loss: 0.2791
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 7221/30000 (24.0700%),                 avg. length: 3429.15,                last time consumption/overall running time: 337.0251s / 101073.1651 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0339
env0_second_0:                 episode reward: -2.5500,                 loss: 0.3666
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7241/30000 (24.1367%),                 avg. length: 2234.8,                last time consumption/overall running time: 267.5874s / 101340.7525 s
env0_first_0:                 episode reward: 5.9500,                 loss: -0.0332
env0_second_0:                 episode reward: -5.9500,                 loss: 0.4602
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 7261/30000 (24.2033%),                 avg. length: 2757.65,                last time consumption/overall running time: 321.4479s / 101662.2004 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0696
env0_second_0:                 episode reward: -2.0000,                 loss: 0.3374
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 7281/30000 (24.2700%),                 avg. length: 2769.2,                last time consumption/overall running time: 310.0035s / 101972.2039 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0645
env0_second_0:                 episode reward: -4.9000,                 loss: 0.4263
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 7301/30000 (24.3367%),                 avg. length: 2702.4,                last time consumption/overall running time: 319.7347s / 102291.9386 s
env0_first_0:                 episode reward: 6.3500,                 loss: -0.0855
env0_second_0:                 episode reward: -6.3500,                 loss: 0.3305
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 7321/30000 (24.4033%),                 avg. length: 2666.0,                last time consumption/overall running time: 325.9578s / 102617.8964 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.0951
env0_second_0:                 episode reward: -6.6500,                 loss: 0.3697
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 7341/30000 (24.4700%),                 avg. length: 2919.3,                last time consumption/overall running time: 333.2700s / 102951.1664 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.0715
env0_second_0:                 episode reward: -6.6000,                 loss: 0.3282
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 7361/30000 (24.5367%),                 avg. length: 3090.7,                last time consumption/overall running time: 313.6959s / 103264.8622 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0702
env0_second_0:                 episode reward: -4.8000,                 loss: 0.4217
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 7381/30000 (24.6033%),                 avg. length: 3261.05,                last time consumption/overall running time: 307.5081s / 103572.3703 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.0759
env0_second_0:                 episode reward: -6.6000,                 loss: 0.2253
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 7401/30000 (24.6700%),                 avg. length: 2913.0,                last time consumption/overall running time: 354.0633s / 103926.4336 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0637
env0_second_0:                 episode reward: -6.7000,                 loss: 0.3887
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 7421/30000 (24.7367%),                 avg. length: 3030.1,                last time consumption/overall running time: 323.0890s / 104249.5226 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0846
env0_second_0:                 episode reward: -3.6500,                 loss: 0.4406
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 7441/30000 (24.8033%),                 avg. length: 3632.2,                last time consumption/overall running time: 424.6467s / 104674.1694 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.0783
env0_second_0:                 episode reward: -2.4000,                 loss: 0.1730
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 7461/30000 (24.8700%),                 avg. length: 3045.1,                last time consumption/overall running time: 338.3101s / 105012.4795 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.0597
env0_second_0:                 episode reward: -6.0000,                 loss: 0.1310
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 7481/30000 (24.9367%),                 avg. length: 2881.95,                last time consumption/overall running time: 323.4327s / 105335.9122 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.0135
env0_second_0:                 episode reward: -1.8500,                 loss: 0.5090
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 7501/30000 (25.0033%),                 avg. length: 3194.85,                last time consumption/overall running time: 342.6428s / 105678.5550 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.0550
env0_second_0:                 episode reward: -1.9000,                 loss: 0.8380
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7521/30000 (25.0700%),                 avg. length: 3906.1,                last time consumption/overall running time: 467.2240s / 106145.7790 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0575
env0_second_0:                 episode reward: -3.8500,                 loss: 0.2706
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 7541/30000 (25.1367%),                 avg. length: 3305.6,                last time consumption/overall running time: 340.0261s / 106485.8051 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.0808
env0_second_0:                 episode reward: -5.2500,                 loss: 0.2108
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 7561/30000 (25.2033%),                 avg. length: 3778.7,                last time consumption/overall running time: 401.3881s / 106887.1931 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.0815
env0_second_0:                 episode reward: -3.4500,                 loss: 0.3343
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 7581/30000 (25.2700%),                 avg. length: 3596.75,                last time consumption/overall running time: 370.3783s / 107257.5714 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0712
env0_second_0:                 episode reward: -0.6000,                 loss: 0.4569
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 7601/30000 (25.3367%),                 avg. length: 2916.4,                last time consumption/overall running time: 308.4669s / 107566.0383 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.0731
env0_second_0:                 episode reward: -2.4000,                 loss: 0.3206
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 7621/30000 (25.4033%),                 avg. length: 2830.1,                last time consumption/overall running time: 335.3902s / 107901.4285 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0341
env0_second_0:                 episode reward: -4.9500,                 loss: 0.3760
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 7641/30000 (25.4700%),                 avg. length: 3462.5,                last time consumption/overall running time: 410.6057s / 108312.0342 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.0513
env0_second_0:                 episode reward: -3.0000,                 loss: 0.3264
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 7661/30000 (25.5367%),                 avg. length: 2941.5,                last time consumption/overall running time: 318.7220s / 108630.7562 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0599
env0_second_0:                 episode reward: -6.0500,                 loss: 0.2272
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 7681/30000 (25.6033%),                 avg. length: 2878.1,                last time consumption/overall running time: 318.0379s / 108948.7941 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0511
env0_second_0:                 episode reward: -4.0500,                 loss: 0.3546
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 7701/30000 (25.6700%),                 avg. length: 3358.55,                last time consumption/overall running time: 372.2857s / 109321.0798 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0521
env0_second_0:                 episode reward: -0.4500,                 loss: 0.4056
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 7721/30000 (25.7367%),                 avg. length: 3472.35,                last time consumption/overall running time: 406.2387s / 109727.3185 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0325
env0_second_0:                 episode reward: -3.6500,                 loss: 0.6691
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 7741/30000 (25.8033%),                 avg. length: 2973.85,                last time consumption/overall running time: 355.3020s / 110082.6205 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0658
env0_second_0:                 episode reward: -4.5500,                 loss: 0.5052
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 7761/30000 (25.8700%),                 avg. length: 2392.5,                last time consumption/overall running time: 281.2752s / 110363.8957 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.0283
env0_second_0:                 episode reward: -5.5500,                 loss: 0.5546
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 7781/30000 (25.9367%),                 avg. length: 1610.1,                last time consumption/overall running time: 199.4872s / 110563.3829 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0003
env0_second_0:                 episode reward: -3.4000,                 loss: 0.5520
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 7801/30000 (26.0033%),                 avg. length: 1713.3,                last time consumption/overall running time: 203.4122s / 110766.7950 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0197
env0_second_0:                 episode reward: -3.8500,                 loss: 0.3610
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 7821/30000 (26.0700%),                 avg. length: 1875.9,                last time consumption/overall running time: 226.3513s / 110993.1463 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0219
env0_second_0:                 episode reward: 0.6500,                 loss: 0.3889
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7841/30000 (26.1367%),                 avg. length: 1976.5,                last time consumption/overall running time: 232.2990s / 111225.4453 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0056
env0_second_0:                 episode reward: -1.9000,                 loss: 1.2159
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 7861/30000 (26.2033%),                 avg. length: 2321.25,                last time consumption/overall running time: 266.7114s / 111492.1567 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.0435
env0_second_0:                 episode reward: 3.2000,                 loss: 0.6323
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 7881/30000 (26.2700%),                 avg. length: 2193.35,                last time consumption/overall running time: 211.0386s / 111703.1954 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.0452
env0_second_0:                 episode reward: 5.7000,                 loss: 0.6723
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 7901/30000 (26.3367%),                 avg. length: 1148.15,                last time consumption/overall running time: 135.6850s / 111838.8803 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1049
env0_second_0:                 episode reward: 8.7500,                 loss: 0.5832
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 7921/30000 (26.4033%),                 avg. length: 1074.4,                last time consumption/overall running time: 125.0827s / 111963.9630 s
env0_first_0:                 episode reward: -8.9500,                 loss: -0.1185
env0_second_0:                 episode reward: 8.9500,                 loss: -0.0004
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 7941/30000 (26.4700%),                 avg. length: 933.35,                last time consumption/overall running time: 95.0968s / 112059.0598 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.0872
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0391
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 7961/30000 (26.5367%),                 avg. length: 947.3,                last time consumption/overall running time: 100.4826s / 112159.5424 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0939
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0302
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 7981/30000 (26.6033%),                 avg. length: 919.1,                last time consumption/overall running time: 109.7045s / 112269.2469 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.0732
env0_second_0:                 episode reward: 9.2000,                 loss: -0.0236
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8001/30000 (26.6700%),                 avg. length: 932.3,                last time consumption/overall running time: 99.9334s / 112369.1803 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0595
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0148
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 8021/30000 (26.7367%),                 avg. length: 948.45,                last time consumption/overall running time: 113.2025s / 112482.3827 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.0405
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0739
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8041/30000 (26.8033%),                 avg. length: 927.1,                last time consumption/overall running time: 115.9305s / 112598.3132 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0636
env0_second_0:                 episode reward: 9.3000,                 loss: -0.0109
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8061/30000 (26.8700%),                 avg. length: 913.15,                last time consumption/overall running time: 101.4538s / 112699.7670 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0494
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0063
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 8081/30000 (26.9367%),                 avg. length: 947.05,                last time consumption/overall running time: 109.4719s / 112809.2389 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0447
env0_second_0:                 episode reward: 9.3000,                 loss: 0.0104
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 8101/30000 (27.0033%),                 avg. length: 910.25,                last time consumption/overall running time: 115.3901s / 112924.6290 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0404
env0_second_0:                 episode reward: 9.3000,                 loss: 0.0233
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 8121/30000 (27.0700%),                 avg. length: 917.55,                last time consumption/overall running time: 92.1830s / 113016.8120 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.0570
env0_second_0:                 episode reward: 9.1500,                 loss: -0.0116
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 8141/30000 (27.1367%),                 avg. length: 939.7,                last time consumption/overall running time: 99.9608s / 113116.7728 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0525
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0146
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 8161/30000 (27.2033%),                 avg. length: 917.05,                last time consumption/overall running time: 99.2372s / 113216.0101 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0719
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0384
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 8181/30000 (27.2700%),                 avg. length: 925.75,                last time consumption/overall running time: 113.9466s / 113329.9567 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.0289
env0_second_0:                 episode reward: 9.2500,                 loss: 0.1962
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 8201/30000 (27.3367%),                 avg. length: 925.2,                last time consumption/overall running time: 120.3315s / 113450.2881 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.0483
env0_second_0:                 episode reward: 9.0500,                 loss: 0.0842
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 8221/30000 (27.4033%),                 avg. length: 909.4,                last time consumption/overall running time: 101.9763s / 113552.2644 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.0538
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0072
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 8241/30000 (27.4700%),                 avg. length: 919.75,                last time consumption/overall running time: 101.4999s / 113653.7643 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.0969
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0616
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 8261/30000 (27.5367%),                 avg. length: 916.35,                last time consumption/overall running time: 113.9042s / 113767.6685 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0806
env0_second_0:                 episode reward: 9.5000,                 loss: -0.0231
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 8281/30000 (27.6033%),                 avg. length: 909.2,                last time consumption/overall running time: 115.2261s / 113882.8946 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0517
env0_second_0:                 episode reward: 9.5500,                 loss: 0.0637
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 8301/30000 (27.6700%),                 avg. length: 916.8,                last time consumption/overall running time: 97.7370s / 113980.6316 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0692
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0080
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8321/30000 (27.7367%),                 avg. length: 923.75,                last time consumption/overall running time: 93.6603s / 114074.2919 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.1075
env0_second_0:                 episode reward: 9.4500,                 loss: -0.0584
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8341/30000 (27.8033%),                 avg. length: 915.4,                last time consumption/overall running time: 106.9249s / 114181.2168 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.0698
env0_second_0:                 episode reward: 9.6000,                 loss: 0.2043
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 8361/30000 (27.8700%),                 avg. length: 917.8,                last time consumption/overall running time: 112.9202s / 114294.1370 s
env0_first_0:                 episode reward: -8.8000,                 loss: -0.0754
env0_second_0:                 episode reward: 8.8000,                 loss: 0.1519
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 8381/30000 (27.9367%),                 avg. length: 912.2,                last time consumption/overall running time: 120.5657s / 114414.7027 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.0982
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0389
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 8401/30000 (28.0033%),                 avg. length: 917.6,                last time consumption/overall running time: 110.1421s / 114524.8448 s
env0_first_0:                 episode reward: -9.3500,                 loss: 0.0996
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0293
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8421/30000 (28.0700%),                 avg. length: 918.4,                last time consumption/overall running time: 92.5767s / 114617.4215 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.1345
env0_second_0:                 episode reward: 9.0000,                 loss: -0.0483
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 8441/30000 (28.1367%),                 avg. length: 932.9,                last time consumption/overall running time: 110.7154s / 114728.1369 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1238
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0011
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 8461/30000 (28.2033%),                 avg. length: 1045.05,                last time consumption/overall running time: 117.3553s / 114845.4922 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.0420
env0_second_0:                 episode reward: 6.7500,                 loss: 0.1251
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 8481/30000 (28.2700%),                 avg. length: 956.45,                last time consumption/overall running time: 104.9169s / 114950.4091 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.0697
env0_second_0:                 episode reward: 9.3500,                 loss: 0.0298
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 8501/30000 (28.3367%),                 avg. length: 924.8,                last time consumption/overall running time: 115.7661s / 115066.1752 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0929
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0805
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 8521/30000 (28.4033%),                 avg. length: 915.45,                last time consumption/overall running time: 118.0829s / 115184.2580 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1227
env0_second_0:                 episode reward: 9.8000,                 loss: 0.2323
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8541/30000 (28.4700%),                 avg. length: 912.3,                last time consumption/overall running time: 115.6567s / 115299.9148 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1133
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0972
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 8561/30000 (28.5367%),                 avg. length: 964.25,                last time consumption/overall running time: 125.6869s / 115425.6017 s
env0_first_0:                 episode reward: -8.1500,                 loss: -0.0828
env0_second_0:                 episode reward: 8.1500,                 loss: 0.3055
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 8581/30000 (28.6033%),                 avg. length: 926.15,                last time consumption/overall running time: 105.5881s / 115531.1898 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0997
env0_second_0:                 episode reward: 9.4000,                 loss: 0.3036
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 8601/30000 (28.6700%),                 avg. length: 907.2,                last time consumption/overall running time: 109.9919s / 115641.1816 s
env0_first_0:                 episode reward: -9.8500,                 loss: -0.1130
env0_second_0:                 episode reward: 9.8500,                 loss: 0.0470
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 8621/30000 (28.7367%),                 avg. length: 924.9,                last time consumption/overall running time: 108.8887s / 115750.0703 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1139
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0518
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 8641/30000 (28.8033%),                 avg. length: 915.75,                last time consumption/overall running time: 101.2405s / 115851.3108 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1214
env0_second_0:                 episode reward: 9.6000,                 loss: 0.2519
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 8661/30000 (28.8700%),                 avg. length: 921.9,                last time consumption/overall running time: 97.4803s / 115948.7911 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1282
env0_second_0:                 episode reward: 9.3500,                 loss: 0.1891
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8681/30000 (28.9367%),                 avg. length: 908.35,                last time consumption/overall running time: 104.2922s / 116053.0834 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1180
env0_second_0:                 episode reward: 9.3000,                 loss: 0.1270
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 8701/30000 (29.0033%),                 avg. length: 907.8,                last time consumption/overall running time: 98.4848s / 116151.5682 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.1191
env0_second_0:                 episode reward: 9.5000,                 loss: 0.1933
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 8721/30000 (29.0700%),                 avg. length: 908.95,                last time consumption/overall running time: 92.5083s / 116244.0765 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1179
env0_second_0:                 episode reward: 9.6000,                 loss: 0.1702
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 8741/30000 (29.1367%),                 avg. length: 916.0,                last time consumption/overall running time: 95.2950s / 116339.3714 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1187
env0_second_0:                 episode reward: 8.7500,                 loss: 0.1606
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 8761/30000 (29.2033%),                 avg. length: 918.7,                last time consumption/overall running time: 103.5394s / 116442.9109 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.0882
env0_second_0:                 episode reward: 8.2500,                 loss: 8.4531
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 8781/30000 (29.2700%),                 avg. length: 916.3,                last time consumption/overall running time: 110.0364s / 116552.9473 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.0959
env0_second_0:                 episode reward: 9.2500,                 loss: 0.1845
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 8801/30000 (29.3367%),                 avg. length: 908.8,                last time consumption/overall running time: 114.8261s / 116667.7733 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1203
env0_second_0:                 episode reward: 9.4000,                 loss: 0.1303
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 8821/30000 (29.4033%),                 avg. length: 908.2,                last time consumption/overall running time: 117.7563s / 116785.5297 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1288
env0_second_0:                 episode reward: 9.3500,                 loss: 0.0672
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 8841/30000 (29.4700%),                 avg. length: 907.95,                last time consumption/overall running time: 106.9588s / 116892.4885 s
env0_first_0:                 episode reward: -9.8500,                 loss: -0.1627
env0_second_0:                 episode reward: 9.8500,                 loss: -0.0555
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 8861/30000 (29.5367%),                 avg. length: 907.2,                last time consumption/overall running time: 98.7050s / 116991.1935 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1631
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0475
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 8881/30000 (29.6033%),                 avg. length: 908.8,                last time consumption/overall running time: 95.5546s / 117086.7482 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.1673
env0_second_0:                 episode reward: 9.6500,                 loss: -0.0639
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 8901/30000 (29.6700%),                 avg. length: 981.05,                last time consumption/overall running time: 117.0822s / 117203.8303 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0968
env0_second_0:                 episode reward: 5.4000,                 loss: 0.3302
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 8921/30000 (29.7367%),                 avg. length: 932.05,                last time consumption/overall running time: 120.2130s / 117324.0433 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.1112
env0_second_0:                 episode reward: 8.6000,                 loss: 0.0931
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 8941/30000 (29.8033%),                 avg. length: 937.95,                last time consumption/overall running time: 115.3775s / 117439.4208 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.1247
env0_second_0:                 episode reward: 9.1000,                 loss: 0.0140
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 8961/30000 (29.8700%),                 avg. length: 914.4,                last time consumption/overall running time: 116.4720s / 117555.8928 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1332
env0_second_0:                 episode reward: 9.4000,                 loss: -0.0454
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 8981/30000 (29.9367%),                 avg. length: 909.75,                last time consumption/overall running time: 114.8088s / 117670.7017 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1438
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0306
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 9001/30000 (30.0033%),                 avg. length: 919.5,                last time consumption/overall running time: 110.4994s / 117781.2011 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.1268
env0_second_0:                 episode reward: 8.8500,                 loss: -0.0302
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 9021/30000 (30.0700%),                 avg. length: 907.75,                last time consumption/overall running time: 116.7298s / 117897.9309 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.1645
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0276
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 9041/30000 (30.1367%),                 avg. length: 907.8,                last time consumption/overall running time: 107.7562s / 118005.6871 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1426
env0_second_0:                 episode reward: 9.7000,                 loss: -0.0225
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 9061/30000 (30.2033%),                 avg. length: 916.75,                last time consumption/overall running time: 117.9780s / 118123.6651 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1529
env0_second_0:                 episode reward: 9.3500,                 loss: -0.0414
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 9081/30000 (30.2700%),                 avg. length: 1264.9,                last time consumption/overall running time: 152.6142s / 118276.2793 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0368
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1953
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 9101/30000 (30.3367%),                 avg. length: 2299.05,                last time consumption/overall running time: 264.8905s / 118541.1697 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.0194
env0_second_0:                 episode reward: -1.4000,                 loss: 0.1761
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9121/30000 (30.4033%),                 avg. length: 2448.65,                last time consumption/overall running time: 285.1526s / 118826.3224 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0212
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0988
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9141/30000 (30.4700%),                 avg. length: 2579.05,                last time consumption/overall running time: 271.1385s / 119097.4608 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0392
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0480
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 9161/30000 (30.5367%),                 avg. length: 2766.15,                last time consumption/overall running time: 297.1654s / 119394.6262 s
env0_first_0:                 episode reward: -3.1500,                 loss: -0.0427
env0_second_0:                 episode reward: 3.1500,                 loss: 0.0420
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 9181/30000 (30.6033%),                 avg. length: 2335.25,                last time consumption/overall running time: 286.5350s / 119681.1612 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.0476
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0987
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 9201/30000 (30.6700%),                 avg. length: 2638.8,                last time consumption/overall running time: 320.2810s / 120001.4422 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.0741
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0632
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 9221/30000 (30.7367%),                 avg. length: 2662.4,                last time consumption/overall running time: 324.4212s / 120325.8634 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0509
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0956
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 9241/30000 (30.8033%),                 avg. length: 2609.25,                last time consumption/overall running time: 318.6576s / 120644.5209 s
env0_first_0:                 episode reward: -4.7500,                 loss: -0.0513
env0_second_0:                 episode reward: 4.7500,                 loss: 0.1150
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 9261/30000 (30.8700%),                 avg. length: 2512.0,                last time consumption/overall running time: 309.2598s / 120953.7808 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.0535
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0738
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 9281/30000 (30.9367%),                 avg. length: 2843.8,                last time consumption/overall running time: 347.7690s / 121301.5498 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0648
env0_second_0:                 episode reward: 0.2500,                 loss: 0.1298
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 9301/30000 (31.0033%),                 avg. length: 2665.9,                last time consumption/overall running time: 326.6790s / 121628.2288 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0620
env0_second_0:                 episode reward: -0.3000,                 loss: 0.1369
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 9321/30000 (31.0700%),                 avg. length: 2362.85,                last time consumption/overall running time: 291.1253s / 121919.3541 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0772
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1070
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 9341/30000 (31.1367%),                 avg. length: 2060.55,                last time consumption/overall running time: 256.1797s / 122175.5338 s
env0_first_0:                 episode reward: -7.1500,                 loss: -0.0824
env0_second_0:                 episode reward: 7.1500,                 loss: 0.1227
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 9361/30000 (31.2033%),                 avg. length: 2525.8,                last time consumption/overall running time: 310.8267s / 122486.3606 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.0802
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0551
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 9381/30000 (31.2700%),                 avg. length: 2896.1,                last time consumption/overall running time: 354.9051s / 122841.2657 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.0685
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0475
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 9401/30000 (31.3367%),                 avg. length: 3068.2,                last time consumption/overall running time: 373.2375s / 123214.5032 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0697
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0482
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 9421/30000 (31.4033%),                 avg. length: 2791.2,                last time consumption/overall running time: 341.4826s / 123555.9857 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.0798
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0167
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9441/30000 (31.4700%),                 avg. length: 2938.6,                last time consumption/overall running time: 360.0593s / 123916.0450 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.0737
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0217
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 9461/30000 (31.5367%),                 avg. length: 2909.7,                last time consumption/overall running time: 355.4307s / 124271.4757 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0925
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0004
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 9481/30000 (31.6033%),                 avg. length: 2992.65,                last time consumption/overall running time: 365.1967s / 124636.6724 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0834
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0286
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 9501/30000 (31.6700%),                 avg. length: 2844.05,                last time consumption/overall running time: 347.7593s / 124984.4317 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.0836
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0979
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 9521/30000 (31.7367%),                 avg. length: 3074.1,                last time consumption/overall running time: 374.0387s / 125358.4704 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0821
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0707
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 9541/30000 (31.8033%),                 avg. length: 3021.85,                last time consumption/overall running time: 366.3122s / 125724.7826 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0900
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0949
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 9561/30000 (31.8700%),                 avg. length: 3026.8,                last time consumption/overall running time: 367.7843s / 126092.5669 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.0986
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0610
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 9581/30000 (31.9367%),                 avg. length: 3045.35,                last time consumption/overall running time: 370.8054s / 126463.3723 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.0879
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0725
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9601/30000 (32.0033%),                 avg. length: 2844.1,                last time consumption/overall running time: 346.2575s / 126809.6298 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0887
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0966
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 9621/30000 (32.0700%),                 avg. length: 2803.6,                last time consumption/overall running time: 341.0315s / 127150.6613 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0688
env0_second_0:                 episode reward: 1.8500,                 loss: 0.2347
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 9641/30000 (32.1367%),                 avg. length: 2813.9,                last time consumption/overall running time: 343.8251s / 127494.4864 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.0660
env0_second_0:                 episode reward: -1.1000,                 loss: 0.2496
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 9661/30000 (32.2033%),                 avg. length: 3207.05,                last time consumption/overall running time: 390.9809s / 127885.4674 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0771
env0_second_0:                 episode reward: -1.5000,                 loss: 0.2150
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 9681/30000 (32.2700%),                 avg. length: 3023.45,                last time consumption/overall running time: 368.9572s / 128254.4245 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.0884
env0_second_0:                 episode reward: -4.1500,                 loss: 0.1572
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 9701/30000 (32.3367%),                 avg. length: 2852.65,                last time consumption/overall running time: 346.6285s / 128601.0530 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.0577
env0_second_0:                 episode reward: -3.8000,                 loss: 0.2191
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 9721/30000 (32.4033%),                 avg. length: 2425.2,                last time consumption/overall running time: 297.2948s / 128898.3478 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0638
env0_second_0:                 episode reward: -6.7000,                 loss: 0.1459
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 9741/30000 (32.4700%),                 avg. length: 2736.4,                last time consumption/overall running time: 330.3214s / 129228.6692 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.0600
env0_second_0:                 episode reward: -5.9000,                 loss: 0.0627
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 9761/30000 (32.5367%),                 avg. length: 2571.6,                last time consumption/overall running time: 313.3270s / 129541.9962 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.0850
env0_second_0:                 episode reward: -6.5500,                 loss: 0.0852
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 9781/30000 (32.6033%),                 avg. length: 2581.4,                last time consumption/overall running time: 298.0074s / 129840.0036 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0855
env0_second_0:                 episode reward: -7.1500,                 loss: 0.0540
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 9801/30000 (32.6700%),                 avg. length: 2547.9,                last time consumption/overall running time: 271.4850s / 130111.4886 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0760
env0_second_0:                 episode reward: -7.1000,                 loss: 0.0313
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 9821/30000 (32.7367%),                 avg. length: 2608.5,                last time consumption/overall running time: 280.1092s / 130391.5978 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0704
env0_second_0:                 episode reward: -7.1000,                 loss: 0.0793
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 9841/30000 (32.8033%),                 avg. length: 2844.9,                last time consumption/overall running time: 326.6562s / 130718.2541 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0730
env0_second_0:                 episode reward: -5.7500,                 loss: 0.0675
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 9861/30000 (32.8700%),                 avg. length: 2809.1,                last time consumption/overall running time: 308.6525s / 131026.9065 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0610
env0_second_0:                 episode reward: -6.3000,                 loss: 0.0560
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 9881/30000 (32.9367%),                 avg. length: 2624.1,                last time consumption/overall running time: 290.9092s / 131317.8157 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.0628
env0_second_0:                 episode reward: -6.5000,                 loss: 0.1169
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 9901/30000 (33.0033%),                 avg. length: 2393.5,                last time consumption/overall running time: 246.0471s / 131563.8628 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0868
env0_second_0:                 episode reward: -7.1500,                 loss: 0.0683
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 9921/30000 (33.0700%),                 avg. length: 2438.2,                last time consumption/overall running time: 278.1579s / 131842.0207 s
env0_first_0:                 episode reward: 7.8000,                 loss: -0.0726
env0_second_0:                 episode reward: -7.8000,                 loss: 0.1087
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 9941/30000 (33.1367%),                 avg. length: 2575.9,                last time consumption/overall running time: 287.4380s / 132129.4587 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0560
env0_second_0:                 episode reward: -5.8000,                 loss: 0.1622
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 9961/30000 (33.2033%),                 avg. length: 2550.05,                last time consumption/overall running time: 264.0198s / 132393.4785 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.0341
env0_second_0:                 episode reward: -3.0500,                 loss: 0.1046
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 9981/30000 (33.2700%),                 avg. length: 2089.95,                last time consumption/overall running time: 220.9488s / 132614.4273 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.0145
env0_second_0:                 episode reward: 4.3000,                 loss: 0.3264
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 10001/30000 (33.3367%),                 avg. length: 1658.05,                last time consumption/overall running time: 174.8605s / 132789.2877 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0184
env0_second_0:                 episode reward: 5.9500,                 loss: 0.3435
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 10021/30000 (33.4033%),                 avg. length: 2514.6,                last time consumption/overall running time: 260.3291s / 133049.6168 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.0153
env0_second_0:                 episode reward: 3.6000,                 loss: 0.2420
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 10041/30000 (33.4700%),                 avg. length: 2728.2,                last time consumption/overall running time: 284.2595s / 133333.8763 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0162
env0_second_0:                 episode reward: -3.2000,                 loss: 0.2304
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 10061/30000 (33.5367%),                 avg. length: 2733.45,                last time consumption/overall running time: 311.1969s / 133645.0732 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.0395
env0_second_0:                 episode reward: -5.5000,                 loss: 0.1694
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 10081/30000 (33.6033%),                 avg. length: 2650.45,                last time consumption/overall running time: 283.6712s / 133928.7444 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.0459
env0_second_0:                 episode reward: -6.7500,                 loss: 0.5156
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 10101/30000 (33.6700%),                 avg. length: 2714.65,                last time consumption/overall running time: 286.1654s / 134214.9098 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.0560
env0_second_0:                 episode reward: -5.3000,                 loss: 0.6819
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 10121/30000 (33.7367%),                 avg. length: 2037.65,                last time consumption/overall running time: 219.1624s / 134434.0722 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0196
env0_second_0:                 episode reward: -4.4000,                 loss: 0.6155
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 10141/30000 (33.8033%),                 avg. length: 863.0,                last time consumption/overall running time: 102.3843s / 134536.4565 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0597
env0_second_0:                 episode reward: -10.0000,                 loss: 0.1753
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10161/30000 (33.8700%),                 avg. length: 863.0,                last time consumption/overall running time: 101.9695s / 134638.4260 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0386
env0_second_0:                 episode reward: -10.0000,                 loss: 0.6407
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10181/30000 (33.9367%),                 avg. length: 863.0,                last time consumption/overall running time: 98.6700s / 134737.0960 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0685
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3995
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 10201/30000 (34.0033%),                 avg. length: 863.0,                last time consumption/overall running time: 97.7421s / 134834.8381 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0445
env0_second_0:                 episode reward: -9.9500,                 loss: 0.7459
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10221/30000 (34.0700%),                 avg. length: 863.0,                last time consumption/overall running time: 97.9960s / 134932.8341 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0502
env0_second_0:                 episode reward: -10.0000,                 loss: 1.8254
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10241/30000 (34.1367%),                 avg. length: 863.0,                last time consumption/overall running time: 100.0652s / 135032.8993 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0256
env0_second_0:                 episode reward: -10.0000,                 loss: 2.4532
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10261/30000 (34.2033%),                 avg. length: 863.0,                last time consumption/overall running time: 107.0170s / 135139.9162 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0023
env0_second_0:                 episode reward: -10.0000,                 loss: 2.3626
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10281/30000 (34.2700%),                 avg. length: 863.0,                last time consumption/overall running time: 98.9821s / 135238.8983 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0206
env0_second_0:                 episode reward: -10.0000,                 loss: 1.6913
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10301/30000 (34.3367%),                 avg. length: 863.0,                last time consumption/overall running time: 98.3234s / 135337.2218 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0366
env0_second_0:                 episode reward: -10.0000,                 loss: 4.3287
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10321/30000 (34.4033%),                 avg. length: 863.0,                last time consumption/overall running time: 102.9788s / 135440.2005 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0118
env0_second_0:                 episode reward: -10.0000,                 loss: 1.6219
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10341/30000 (34.4700%),                 avg. length: 863.0,                last time consumption/overall running time: 96.7944s / 135536.9950 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0435
env0_second_0:                 episode reward: -10.0000,                 loss: 1.8709
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 10361/30000 (34.5367%),                 avg. length: 1178.4,                last time consumption/overall running time: 128.8622s / 135665.8571 s
env0_first_0:                 episode reward: 6.9000,                 loss: -0.0372
env0_second_0:                 episode reward: -6.9000,                 loss: 3.0669
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 10381/30000 (34.6033%),                 avg. length: 1679.4,                last time consumption/overall running time: 191.2872s / 135857.1444 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.0230
env0_second_0:                 episode reward: -6.5000,                 loss: 1.0608
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 10401/30000 (34.6700%),                 avg. length: 2360.15,                last time consumption/overall running time: 265.1344s / 136122.2788 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.0474
env0_second_0:                 episode reward: -4.3000,                 loss: 0.7936
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 10421/30000 (34.7367%),                 avg. length: 2190.05,                last time consumption/overall running time: 226.3982s / 136348.6770 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0689
env0_second_0:                 episode reward: -6.0500,                 loss: 0.9890
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 10441/30000 (34.8033%),                 avg. length: 2373.15,                last time consumption/overall running time: 264.7532s / 136613.4302 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.0241
env0_second_0:                 episode reward: -3.8000,                 loss: 0.9283
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 10461/30000 (34.8700%),                 avg. length: 2225.35,                last time consumption/overall running time: 243.0808s / 136856.5110 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.0589
env0_second_0:                 episode reward: -6.0000,                 loss: 0.5255
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 10481/30000 (34.9367%),                 avg. length: 2107.45,                last time consumption/overall running time: 236.9871s / 137093.4981 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.0641
env0_second_0:                 episode reward: -5.3000,                 loss: 0.5757
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 10501/30000 (35.0033%),                 avg. length: 1613.9,                last time consumption/overall running time: 181.6735s / 137275.1716 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.0327
env0_second_0:                 episode reward: -5.7000,                 loss: 0.6876
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 10521/30000 (35.0700%),                 avg. length: 2391.55,                last time consumption/overall running time: 262.1163s / 137537.2879 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.0560
env0_second_0:                 episode reward: -6.5000,                 loss: 0.9737
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 10541/30000 (35.1367%),                 avg. length: 2361.95,                last time consumption/overall running time: 245.9875s / 137783.2753 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0572
env0_second_0:                 episode reward: -6.0500,                 loss: 0.7144
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 10561/30000 (35.2033%),                 avg. length: 2373.65,                last time consumption/overall running time: 265.5355s / 138048.8108 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0328
env0_second_0:                 episode reward: -4.8500,                 loss: 0.9149
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 10581/30000 (35.2700%),                 avg. length: 2075.85,                last time consumption/overall running time: 215.6920s / 138264.5028 s
env0_first_0:                 episode reward: 7.4500,                 loss: -0.0965
env0_second_0:                 episode reward: -7.4500,                 loss: 0.5137
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 10601/30000 (35.3367%),                 avg. length: 2326.65,                last time consumption/overall running time: 242.9572s / 138507.4600 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0779
env0_second_0:                 episode reward: -6.7000,                 loss: 0.5936
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 10621/30000 (35.4033%),                 avg. length: 2676.8,                last time consumption/overall running time: 290.1159s / 138797.5760 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.0195
env0_second_0:                 episode reward: -3.3500,                 loss: 0.5729
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 10641/30000 (35.4700%),                 avg. length: 2274.4,                last time consumption/overall running time: 262.6177s / 139060.1937 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0774
env0_second_0:                 episode reward: -7.1500,                 loss: 0.5089
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 10661/30000 (35.5367%),                 avg. length: 2040.85,                last time consumption/overall running time: 232.2904s / 139292.4840 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0976
env0_second_0:                 episode reward: -7.7500,                 loss: 0.4121
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 10681/30000 (35.6033%),                 avg. length: 2331.85,                last time consumption/overall running time: 247.2597s / 139539.7437 s
env0_first_0:                 episode reward: 8.3000,                 loss: -0.1131
env0_second_0:                 episode reward: -8.3000,                 loss: 0.4145
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 10701/30000 (35.6700%),                 avg. length: 2371.4,                last time consumption/overall running time: 260.4706s / 139800.2142 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.1219
env0_second_0:                 episode reward: -9.0500,                 loss: 0.4423
env1_first_0:                 episode reward: 8.6500,                 loss: nan
env1_second_0:                 episode reward: -8.6500,                 loss: nan
Episode: 10721/30000 (35.7367%),                 avg. length: 2428.1,                last time consumption/overall running time: 263.8567s / 140064.0709 s
env0_first_0:                 episode reward: 8.5500,                 loss: -0.0862
env0_second_0:                 episode reward: -8.5500,                 loss: 0.2579
env1_first_0:                 episode reward: 8.6000,                 loss: nan
env1_second_0:                 episode reward: -8.6000,                 loss: nan
Episode: 10741/30000 (35.8033%),                 avg. length: 2704.85,                last time consumption/overall running time: 301.9635s / 140366.0345 s
env0_first_0:                 episode reward: 8.4000,                 loss: -0.0635
env0_second_0:                 episode reward: -8.4000,                 loss: 0.1961
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 10761/30000 (35.8700%),                 avg. length: 2741.1,                last time consumption/overall running time: 289.2944s / 140655.3289 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0074
env0_second_0:                 episode reward: -5.8500,                 loss: 0.6540
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 10781/30000 (35.9367%),                 avg. length: 2649.3,                last time consumption/overall running time: 294.4755s / 140949.8044 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0358
env0_second_0:                 episode reward: -6.3000,                 loss: 1.4608
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 10801/30000 (36.0033%),                 avg. length: 2466.55,                last time consumption/overall running time: 277.7109s / 141227.5153 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.0920
env0_second_0:                 episode reward: -8.6000,                 loss: 0.2358
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 10821/30000 (36.0700%),                 avg. length: 2585.6,                last time consumption/overall running time: 296.2326s / 141523.7479 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.0646
env0_second_0:                 episode reward: -7.6500,                 loss: 0.2548
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 10841/30000 (36.1367%),                 avg. length: 2845.4,                last time consumption/overall running time: 321.1964s / 141844.9444 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0437
env0_second_0:                 episode reward: -4.4000,                 loss: 1.0219
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 10861/30000 (36.2033%),                 avg. length: 2928.7,                last time consumption/overall running time: 319.3140s / 142164.2584 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0642
env0_second_0:                 episode reward: -4.9000,                 loss: 0.9388
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 10881/30000 (36.2700%),                 avg. length: 3333.45,                last time consumption/overall running time: 372.1462s / 142536.4046 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0335
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7322
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 10901/30000 (36.3367%),                 avg. length: 3834.05,                last time consumption/overall running time: 433.9144s / 142970.3190 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.0241
env0_second_0:                 episode reward: -2.9500,                 loss: 0.5255
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 10921/30000 (36.4033%),                 avg. length: 980.8,                last time consumption/overall running time: 109.8489s / 143080.1680 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.0138
env0_second_0:                 episode reward: 5.7500,                 loss: 0.4146
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 10941/30000 (36.4700%),                 avg. length: 909.8,                last time consumption/overall running time: 113.6823s / 143193.8503 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1065
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0361
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 10961/30000 (36.5367%),                 avg. length: 908.55,                last time consumption/overall running time: 110.6826s / 143304.5329 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0841
env0_second_0:                 episode reward: 9.5000,                 loss: 0.0403
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 10981/30000 (36.6033%),                 avg. length: 927.55,                last time consumption/overall running time: 105.1518s / 143409.6847 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.0910
env0_second_0:                 episode reward: 9.2000,                 loss: 0.0271
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 11001/30000 (36.6700%),                 avg. length: 922.6,                last time consumption/overall running time: 103.3026s / 143512.9872 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.1129
env0_second_0:                 episode reward: 9.0000,                 loss: 0.0844
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 11021/30000 (36.7367%),                 avg. length: 909.2,                last time consumption/overall running time: 109.9087s / 143622.8960 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.0897
env0_second_0:                 episode reward: 9.4500,                 loss: -0.0613
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 11041/30000 (36.8033%),                 avg. length: 909.75,                last time consumption/overall running time: 102.0566s / 143724.9526 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0864
env0_second_0:                 episode reward: 9.5500,                 loss: -0.1272
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 11061/30000 (36.8700%),                 avg. length: 920.2,                last time consumption/overall running time: 102.5204s / 143827.4730 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0978
env0_second_0:                 episode reward: 9.5000,                 loss: 0.1891
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 11081/30000 (36.9367%),                 avg. length: 935.25,                last time consumption/overall running time: 107.2420s / 143934.7150 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.0967
env0_second_0:                 episode reward: 9.2000,                 loss: 0.3180
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 11101/30000 (37.0033%),                 avg. length: 928.9,                last time consumption/overall running time: 100.9051s / 144035.6201 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0978
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0211
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 11121/30000 (37.0700%),                 avg. length: 925.15,                last time consumption/overall running time: 113.0921s / 144148.7122 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1080
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0359
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 11141/30000 (37.1367%),                 avg. length: 915.0,                last time consumption/overall running time: 106.7792s / 144255.4915 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1206
env0_second_0:                 episode reward: 9.8000,                 loss: -0.0060
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 11161/30000 (37.2033%),                 avg. length: 908.4,                last time consumption/overall running time: 108.7327s / 144364.2242 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.1191
env0_second_0:                 episode reward: 9.5000,                 loss: 1.7330
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 11181/30000 (37.2700%),                 avg. length: 907.2,                last time consumption/overall running time: 101.2454s / 144465.4696 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1225
env0_second_0:                 episode reward: 9.7000,                 loss: 0.2380
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 11201/30000 (37.3367%),                 avg. length: 908.4,                last time consumption/overall running time: 107.1691s / 144572.6387 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0814
env0_second_0:                 episode reward: 9.3000,                 loss: 0.2239
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 11221/30000 (37.4033%),                 avg. length: 909.75,                last time consumption/overall running time: 105.7081s / 144678.3468 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.1485
env0_second_0:                 episode reward: 9.2500,                 loss: 0.0132
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 11241/30000 (37.4700%),                 avg. length: 911.7,                last time consumption/overall running time: 104.0584s / 144782.4052 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1587
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0380
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 11261/30000 (37.5367%),                 avg. length: 909.2,                last time consumption/overall running time: 118.1842s / 144900.5894 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1413
env0_second_0:                 episode reward: 9.7000,                 loss: -0.0780
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 11281/30000 (37.6033%),                 avg. length: 907.0,                last time consumption/overall running time: 100.2135s / 145000.8029 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1541
env0_second_0:                 episode reward: 9.8000,                 loss: -0.1020
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 11301/30000 (37.6700%),                 avg. length: 1520.55,                last time consumption/overall running time: 167.0747s / 145167.8776 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0191
env0_second_0:                 episode reward: 1.4000,                 loss: 0.1690
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 11321/30000 (37.7367%),                 avg. length: 1643.15,                last time consumption/overall running time: 170.7977s / 145338.6753 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0051
env0_second_0:                 episode reward: -4.2000,                 loss: 0.3033
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 11341/30000 (37.8033%),                 avg. length: 1855.55,                last time consumption/overall running time: 201.8683s / 145540.5436 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.0040
env0_second_0:                 episode reward: 3.2500,                 loss: 0.8182
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 11361/30000 (37.8700%),                 avg. length: 2224.55,                last time consumption/overall running time: 239.5363s / 145780.0799 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0219
env0_second_0:                 episode reward: 2.2500,                 loss: 0.4352
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 11381/30000 (37.9367%),                 avg. length: 2320.65,                last time consumption/overall running time: 246.8459s / 146026.9257 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0408
env0_second_0:                 episode reward: -1.5000,                 loss: 0.1839
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 11401/30000 (38.0033%),                 avg. length: 2330.1,                last time consumption/overall running time: 246.3807s / 146273.3064 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0087
env0_second_0:                 episode reward: 2.2500,                 loss: 0.2338
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 11421/30000 (38.0700%),                 avg. length: 2059.45,                last time consumption/overall running time: 235.3927s / 146508.6992 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.0595
env0_second_0:                 episode reward: 6.4000,                 loss: 0.2259
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 11441/30000 (38.1367%),                 avg. length: 1718.6,                last time consumption/overall running time: 193.3278s / 146702.0270 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.0411
env0_second_0:                 episode reward: 8.6500,                 loss: 0.2867
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 11461/30000 (38.2033%),                 avg. length: 2017.6,                last time consumption/overall running time: 222.0917s / 146924.1187 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.0596
env0_second_0:                 episode reward: 7.6000,                 loss: 0.1725
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 11481/30000 (38.2700%),                 avg. length: 1782.8,                last time consumption/overall running time: 195.7718s / 147119.8906 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.0597
env0_second_0:                 episode reward: 8.2500,                 loss: 0.2579
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 11501/30000 (38.3367%),                 avg. length: 2357.9,                last time consumption/overall running time: 248.3999s / 147368.2905 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.0174
env0_second_0:                 episode reward: 2.9500,                 loss: 0.3054
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 11521/30000 (38.4033%),                 avg. length: 2178.1,                last time consumption/overall running time: 226.8094s / 147595.0999 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.0345
env0_second_0:                 episode reward: 4.1000,                 loss: 0.1879
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 11541/30000 (38.4700%),                 avg. length: 2005.25,                last time consumption/overall running time: 216.1201s / 147811.2200 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.0297
env0_second_0:                 episode reward: 4.4500,                 loss: 0.2022
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 11561/30000 (38.5367%),                 avg. length: 1981.5,                last time consumption/overall running time: 215.7126s / 148026.9325 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.0498
env0_second_0:                 episode reward: 4.8500,                 loss: 0.1713
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 11581/30000 (38.6033%),                 avg. length: 1713.8,                last time consumption/overall running time: 186.2610s / 148213.1936 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.0787
env0_second_0:                 episode reward: 8.0000,                 loss: 0.1566
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 11601/30000 (38.6700%),                 avg. length: 1876.05,                last time consumption/overall running time: 208.3609s / 148421.5544 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.0811
env0_second_0:                 episode reward: 7.4000,                 loss: 0.1784
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 11621/30000 (38.7367%),                 avg. length: 1930.35,                last time consumption/overall running time: 207.8922s / 148629.4466 s
env0_first_0:                 episode reward: -8.1000,                 loss: -0.0861
env0_second_0:                 episode reward: 8.1000,                 loss: 0.1411
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 11641/30000 (38.8033%),                 avg. length: 2330.35,                last time consumption/overall running time: 249.7168s / 148879.1634 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0009
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2558
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 11661/30000 (38.8700%),                 avg. length: 2479.1,                last time consumption/overall running time: 276.5717s / 149155.7352 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.0374
env0_second_0:                 episode reward: -2.4500,                 loss: 0.2863
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 11681/30000 (38.9367%),                 avg. length: 2216.75,                last time consumption/overall running time: 235.8788s / 149391.6139 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0124
env0_second_0:                 episode reward: 4.1500,                 loss: 0.7997
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 11701/30000 (39.0033%),                 avg. length: 1922.55,                last time consumption/overall running time: 212.1388s / 149603.7527 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.0503
env0_second_0:                 episode reward: 5.5000,                 loss: 0.5641
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 11721/30000 (39.0700%),                 avg. length: 2722.95,                last time consumption/overall running time: 311.4741s / 149915.2268 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0197
env0_second_0:                 episode reward: -0.8500,                 loss: 0.4618
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 11741/30000 (39.1367%),                 avg. length: 1996.7,                last time consumption/overall running time: 218.8342s / 150134.0610 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0460
env0_second_0:                 episode reward: -6.4000,                 loss: 0.3028
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 11761/30000 (39.2033%),                 avg. length: 2092.3,                last time consumption/overall running time: 219.9240s / 150353.9851 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.0347
env0_second_0:                 episode reward: -6.2500,                 loss: 0.4267
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 11781/30000 (39.2700%),                 avg. length: 2361.85,                last time consumption/overall running time: 261.7622s / 150615.7473 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0141
env0_second_0:                 episode reward: -5.8000,                 loss: 0.6918
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 11801/30000 (39.3367%),                 avg. length: 2630.05,                last time consumption/overall running time: 286.7871s / 150902.5344 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0090
env0_second_0:                 episode reward: 2.2500,                 loss: 0.5638
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 11821/30000 (39.4033%),                 avg. length: 2438.45,                last time consumption/overall running time: 271.1562s / 151173.6906 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0072
env0_second_0:                 episode reward: 2.1000,                 loss: 0.5903
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 11841/30000 (39.4700%),                 avg. length: 1720.1,                last time consumption/overall running time: 187.3857s / 151361.0763 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0091
env0_second_0:                 episode reward: 3.2000,                 loss: 0.8355
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11861/30000 (39.5367%),                 avg. length: 1742.45,                last time consumption/overall running time: 191.3299s / 151552.4061 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0699
env0_second_0:                 episode reward: 7.8500,                 loss: 0.7552
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 11881/30000 (39.6033%),                 avg. length: 2715.25,                last time consumption/overall running time: 307.5184s / 151859.9245 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0318
env0_second_0:                 episode reward: -0.5000,                 loss: 0.5176
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 11901/30000 (39.6700%),                 avg. length: 2502.85,                last time consumption/overall running time: 284.2651s / 152144.1896 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0393
env0_second_0:                 episode reward: 5.9500,                 loss: 0.6094
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 11921/30000 (39.7367%),                 avg. length: 2785.75,                last time consumption/overall running time: 305.7654s / 152449.9550 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.0506
env0_second_0:                 episode reward: 5.7000,                 loss: 0.6318
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 11941/30000 (39.8033%),                 avg. length: 2610.1,                last time consumption/overall running time: 289.2022s / 152739.1572 s
env0_first_0:                 episode reward: -6.2000,                 loss: -0.0392
env0_second_0:                 episode reward: 6.2000,                 loss: 0.9094
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 11961/30000 (39.8700%),                 avg. length: 2252.5,                last time consumption/overall running time: 252.1420s / 152991.2992 s
env0_first_0:                 episode reward: -6.1000,                 loss: -0.0369
env0_second_0:                 episode reward: 6.1000,                 loss: 1.4917
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 11981/30000 (39.9367%),                 avg. length: 2551.25,                last time consumption/overall running time: 277.2756s / 153268.5747 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0378
env0_second_0:                 episode reward: 2.0500,                 loss: 0.7369
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12001/30000 (40.0033%),                 avg. length: 2922.0,                last time consumption/overall running time: 309.4685s / 153578.0433 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0550
env0_second_0:                 episode reward: -2.5500,                 loss: 0.7901
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 12021/30000 (40.0700%),                 avg. length: 2927.6,                last time consumption/overall running time: 325.9376s / 153903.9809 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0770
env0_second_0:                 episode reward: 4.6000,                 loss: 0.4188
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 12041/30000 (40.1367%),                 avg. length: 2797.5,                last time consumption/overall running time: 297.8290s / 154201.8099 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.0546
env0_second_0:                 episode reward: 5.9000,                 loss: 0.7848
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 12061/30000 (40.2033%),                 avg. length: 2761.4,                last time consumption/overall running time: 291.5818s / 154493.3917 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.0693
env0_second_0:                 episode reward: 7.0000,                 loss: 0.6057
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 12081/30000 (40.2700%),                 avg. length: 2741.65,                last time consumption/overall running time: 294.7812s / 154788.1728 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.0581
env0_second_0:                 episode reward: 5.8500,                 loss: 0.6172
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 12101/30000 (40.3367%),                 avg. length: 2480.75,                last time consumption/overall running time: 274.7560s / 155062.9289 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0521
env0_second_0:                 episode reward: 5.9500,                 loss: 0.5485
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 12121/30000 (40.4033%),                 avg. length: 2688.4,                last time consumption/overall running time: 285.6463s / 155348.5752 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.0540
env0_second_0:                 episode reward: 5.5000,                 loss: 0.6069
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 12141/30000 (40.4700%),                 avg. length: 2404.6,                last time consumption/overall running time: 266.5852s / 155615.1604 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.0691
env0_second_0:                 episode reward: 7.2500,                 loss: 0.8155
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 12161/30000 (40.5367%),                 avg. length: 2152.25,                last time consumption/overall running time: 226.5887s / 155841.7490 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.0359
env0_second_0:                 episode reward: 8.2000,                 loss: 0.8955
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 12181/30000 (40.6033%),                 avg. length: 2420.45,                last time consumption/overall running time: 257.6532s / 156099.4023 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.0765
env0_second_0:                 episode reward: 7.6000,                 loss: 0.4624
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 12201/30000 (40.6700%),                 avg. length: 3010.85,                last time consumption/overall running time: 322.9268s / 156422.3290 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0517
env0_second_0:                 episode reward: 5.8000,                 loss: 0.6496
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 12221/30000 (40.7367%),                 avg. length: 2706.4,                last time consumption/overall running time: 309.6215s / 156731.9505 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.0755
env0_second_0:                 episode reward: 6.8000,                 loss: 0.7088
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 12241/30000 (40.8033%),                 avg. length: 2503.0,                last time consumption/overall running time: 272.9315s / 157004.8821 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.0722
env0_second_0:                 episode reward: 6.7500,                 loss: 0.8950
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 12261/30000 (40.8700%),                 avg. length: 2459.05,                last time consumption/overall running time: 279.4609s / 157284.3430 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.0109
env0_second_0:                 episode reward: 3.3000,                 loss: 0.9467
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 12281/30000 (40.9367%),                 avg. length: 2093.8,                last time consumption/overall running time: 226.3524s / 157510.6954 s
env0_first_0:                 episode reward: -7.6500,                 loss: -0.0788
env0_second_0:                 episode reward: 7.6500,                 loss: 0.6587
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 12301/30000 (41.0033%),                 avg. length: 2358.0,                last time consumption/overall running time: 247.9108s / 157758.6062 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.0642
env0_second_0:                 episode reward: 6.4000,                 loss: 0.6282
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 12321/30000 (41.0700%),                 avg. length: 1559.2,                last time consumption/overall running time: 169.7449s / 157928.3510 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.0576
env0_second_0:                 episode reward: 8.6000,                 loss: 0.6042
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 12341/30000 (41.1367%),                 avg. length: 2223.45,                last time consumption/overall running time: 238.4760s / 158166.8270 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.0186
env0_second_0:                 episode reward: 5.3000,                 loss: 0.5781
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 12361/30000 (41.2033%),                 avg. length: 2309.4,                last time consumption/overall running time: 245.0429s / 158411.8699 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0520
env0_second_0:                 episode reward: 0.6500,                 loss: 0.7011
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12381/30000 (41.2700%),                 avg. length: 2872.8,                last time consumption/overall running time: 311.5286s / 158723.3985 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.0745
env0_second_0:                 episode reward: 6.0000,                 loss: 1.0806
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 12401/30000 (41.3367%),                 avg. length: 2091.15,                last time consumption/overall running time: 231.1178s / 158954.5163 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.0560
env0_second_0:                 episode reward: 2.3500,                 loss: 0.5366
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 12421/30000 (41.4033%),                 avg. length: 1080.45,                last time consumption/overall running time: 123.3583s / 159077.8746 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.0589
env0_second_0:                 episode reward: -8.8000,                 loss: 0.4225
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 12441/30000 (41.4700%),                 avg. length: 1029.65,                last time consumption/overall running time: 118.8893s / 159196.7639 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0944
env0_second_0:                 episode reward: -9.6000,                 loss: 0.0608
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 12461/30000 (41.5367%),                 avg. length: 1219.75,                last time consumption/overall running time: 151.1598s / 159347.9237 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0623
env0_second_0:                 episode reward: -7.7500,                 loss: 0.2454
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 12481/30000 (41.6033%),                 avg. length: 2348.1,                last time consumption/overall running time: 256.1514s / 159604.0751 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0481
env0_second_0:                 episode reward: 5.4000,                 loss: 0.4213
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 12501/30000 (41.6700%),                 avg. length: 2559.15,                last time consumption/overall running time: 261.7753s / 159865.8504 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.0396
env0_second_0:                 episode reward: 4.6500,                 loss: 0.4709
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 12521/30000 (41.7367%),                 avg. length: 3098.75,                last time consumption/overall running time: 348.4248s / 160214.2752 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0476
env0_second_0:                 episode reward: -0.3000,                 loss: 0.5207
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 12541/30000 (41.8033%),                 avg. length: 2726.0,                last time consumption/overall running time: 289.1081s / 160503.3834 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0405
env0_second_0:                 episode reward: -4.5500,                 loss: 0.6514
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 12561/30000 (41.8700%),                 avg. length: 2302.05,                last time consumption/overall running time: 253.4131s / 160756.7965 s
env0_first_0:                 episode reward: 7.5500,                 loss: -0.0647
env0_second_0:                 episode reward: -7.5500,                 loss: 0.3225
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 12581/30000 (41.9367%),                 avg. length: 2482.4,                last time consumption/overall running time: 275.3086s / 161032.1051 s
env0_first_0:                 episode reward: 7.0500,                 loss: -0.0713
env0_second_0:                 episode reward: -7.0500,                 loss: 0.2608
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 12601/30000 (42.0033%),                 avg. length: 2592.6,                last time consumption/overall running time: 297.1665s / 161329.2717 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0773
env0_second_0:                 episode reward: -6.4000,                 loss: 0.4303
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 12621/30000 (42.0700%),                 avg. length: 2254.9,                last time consumption/overall running time: 253.0328s / 161582.3045 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.0821
env0_second_0:                 episode reward: -7.6500,                 loss: 0.6152
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 12641/30000 (42.1367%),                 avg. length: 2220.9,                last time consumption/overall running time: 229.2678s / 161811.5723 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.0835
env0_second_0:                 episode reward: -8.6000,                 loss: 0.6726
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 12661/30000 (42.2033%),                 avg. length: 2494.5,                last time consumption/overall running time: 265.4636s / 162077.0359 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0813
env0_second_0:                 episode reward: -7.7500,                 loss: 0.3831
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 12681/30000 (42.2700%),                 avg. length: 2862.6,                last time consumption/overall running time: 300.6383s / 162377.6742 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.0777
env0_second_0:                 episode reward: -6.6000,                 loss: 0.3126
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 12701/30000 (42.3367%),                 avg. length: 2945.45,                last time consumption/overall running time: 311.8819s / 162689.5562 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.0772
env0_second_0:                 episode reward: -5.0000,                 loss: 0.3808
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 12721/30000 (42.4033%),                 avg. length: 2717.25,                last time consumption/overall running time: 296.5430s / 162986.0992 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0597
env0_second_0:                 episode reward: -1.5000,                 loss: 0.5808
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 12741/30000 (42.4700%),                 avg. length: 2940.1,                last time consumption/overall running time: 301.5444s / 163287.6436 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0651
env0_second_0:                 episode reward: 1.2500,                 loss: 0.6859
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 12761/30000 (42.5367%),                 avg. length: 3300.35,                last time consumption/overall running time: 364.5305s / 163652.1742 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0716
env0_second_0:                 episode reward: -4.6000,                 loss: 0.3581
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 12781/30000 (42.6033%),                 avg. length: 3252.5,                last time consumption/overall running time: 350.9770s / 164003.1511 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.0704
env0_second_0:                 episode reward: -4.3500,                 loss: 0.3351
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 12801/30000 (42.6700%),                 avg. length: 2815.3,                last time consumption/overall running time: 303.7574s / 164306.9085 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0762
env0_second_0:                 episode reward: -5.8000,                 loss: 0.3720
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 12821/30000 (42.7367%),                 avg. length: 2947.85,                last time consumption/overall running time: 310.2191s / 164617.1276 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.0600
env0_second_0:                 episode reward: -6.5500,                 loss: 0.5466
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 12841/30000 (42.8033%),                 avg. length: 2858.9,                last time consumption/overall running time: 307.1909s / 164924.3185 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.0524
env0_second_0:                 episode reward: -1.3500,                 loss: 0.5618
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 12861/30000 (42.8700%),                 avg. length: 2187.0,                last time consumption/overall running time: 245.0986s / 165169.4171 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.0745
env0_second_0:                 episode reward: 7.0000,                 loss: 0.6269
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 12881/30000 (42.9367%),                 avg. length: 3076.25,                last time consumption/overall running time: 323.1046s / 165492.5217 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0637
env0_second_0:                 episode reward: -2.5500,                 loss: 0.7008
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 12901/30000 (43.0033%),                 avg. length: 2825.65,                last time consumption/overall running time: 319.1678s / 165811.6894 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.0500
env0_second_0:                 episode reward: -6.8000,                 loss: 0.7190
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 12921/30000 (43.0700%),                 avg. length: 2678.9,                last time consumption/overall running time: 298.1264s / 166109.8158 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0567
env0_second_0:                 episode reward: -7.1000,                 loss: 0.5888
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 12941/30000 (43.1367%),                 avg. length: 2823.8,                last time consumption/overall running time: 298.8164s / 166408.6322 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.0520
env0_second_0:                 episode reward: -6.1500,                 loss: 0.4517
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 12961/30000 (43.2033%),                 avg. length: 3002.6,                last time consumption/overall running time: 319.5231s / 166728.1553 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0883
env0_second_0:                 episode reward: -6.3000,                 loss: 0.3246
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 12981/30000 (43.2700%),                 avg. length: 2820.8,                last time consumption/overall running time: 318.9611s / 167047.1164 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.0897
env0_second_0:                 episode reward: -5.7000,                 loss: 0.3204
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 13001/30000 (43.3367%),                 avg. length: 3189.95,                last time consumption/overall running time: 349.2898s / 167396.4062 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.0927
env0_second_0:                 episode reward: -7.0000,                 loss: 0.3489
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 13021/30000 (43.4033%),                 avg. length: 3412.05,                last time consumption/overall running time: 376.6238s / 167773.0300 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0996
env0_second_0:                 episode reward: -4.6500,                 loss: 0.3005
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 13041/30000 (43.4700%),                 avg. length: 2500.9,                last time consumption/overall running time: 281.8502s / 168054.8802 s
env0_first_0:                 episode reward: 6.3500,                 loss: -0.0859
env0_second_0:                 episode reward: -6.3500,                 loss: 0.4307
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 13061/30000 (43.5367%),                 avg. length: 2543.65,                last time consumption/overall running time: 291.4215s / 168346.3017 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.0847
env0_second_0:                 episode reward: -7.2500,                 loss: 0.4843
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 13081/30000 (43.6033%),                 avg. length: 2479.75,                last time consumption/overall running time: 275.0691s / 168621.3708 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.0460
env0_second_0:                 episode reward: -7.0000,                 loss: 0.4836
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 13101/30000 (43.6700%),                 avg. length: 3445.95,                last time consumption/overall running time: 385.4427s / 169006.8135 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.0836
env0_second_0:                 episode reward: -3.7000,                 loss: 0.4075
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 13121/30000 (43.7367%),                 avg. length: 2836.45,                last time consumption/overall running time: 320.1931s / 169327.0066 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0431
env0_second_0:                 episode reward: -5.4500,                 loss: 0.3457
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 13141/30000 (43.8033%),                 avg. length: 3045.1,                last time consumption/overall running time: 331.0537s / 169658.0604 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0685
env0_second_0:                 episode reward: -4.9500,                 loss: 0.5682
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 13161/30000 (43.8700%),                 avg. length: 3856.3,                last time consumption/overall running time: 457.8977s / 170115.9580 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1063
env0_second_0:                 episode reward: -1.7500,                 loss: 0.3637
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 13181/30000 (43.9367%),                 avg. length: 3923.6,                last time consumption/overall running time: 481.6629s / 170597.6210 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0909
env0_second_0:                 episode reward: -2.1500,                 loss: 0.4025
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 13201/30000 (44.0033%),                 avg. length: 3594.9,                last time consumption/overall running time: 444.5850s / 171042.2060 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0640
env0_second_0:                 episode reward: -2.6000,                 loss: 0.3701
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 13221/30000 (44.0700%),                 avg. length: 3969.0,                last time consumption/overall running time: 467.2967s / 171509.5026 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.0765
env0_second_0:                 episode reward: -2.4500,                 loss: 0.4624
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 13241/30000 (44.1367%),                 avg. length: 3660.15,                last time consumption/overall running time: 443.1993s / 171952.7019 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.0941
env0_second_0:                 episode reward: -3.4000,                 loss: 0.4554
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 13261/30000 (44.2033%),                 avg. length: 2570.1,                last time consumption/overall running time: 314.2469s / 172266.9488 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0738
env0_second_0:                 episode reward: -5.4500,                 loss: 0.4331
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 13281/30000 (44.2700%),                 avg. length: 2869.85,                last time consumption/overall running time: 341.7623s / 172608.7112 s
env0_first_0:                 episode reward: 7.0500,                 loss: -0.1131
env0_second_0:                 episode reward: -7.0500,                 loss: 0.3948
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 13301/30000 (44.3367%),                 avg. length: 3026.0,                last time consumption/overall running time: 365.3545s / 172974.0657 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0852
env0_second_0:                 episode reward: -5.4500,                 loss: 0.5121
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 13321/30000 (44.4033%),                 avg. length: 3283.55,                last time consumption/overall running time: 401.3300s / 173375.3957 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.0553
env0_second_0:                 episode reward: -3.0500,                 loss: 0.4502
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 13341/30000 (44.4700%),                 avg. length: 3338.65,                last time consumption/overall running time: 405.3953s / 173780.7910 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0553
env0_second_0:                 episode reward: -5.6500,                 loss: 0.4234
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 13361/30000 (44.5367%),                 avg. length: 3290.35,                last time consumption/overall running time: 370.7525s / 174151.5435 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.0603
env0_second_0:                 episode reward: -3.9500,                 loss: 0.3756
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 13381/30000 (44.6033%),                 avg. length: 3424.8,                last time consumption/overall running time: 357.2708s / 174508.8143 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0721
env0_second_0:                 episode reward: -4.0500,                 loss: 0.4399
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 13401/30000 (44.6700%),                 avg. length: 4075.15,                last time consumption/overall running time: 419.9708s / 174928.7851 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0588
env0_second_0:                 episode reward: -1.0500,                 loss: 0.3595
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 13421/30000 (44.7367%),                 avg. length: 4267.2,                last time consumption/overall running time: 445.1870s / 175373.9721 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0789
env0_second_0:                 episode reward: -1.0500,                 loss: 0.3777
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 13441/30000 (44.8033%),                 avg. length: 3998.15,                last time consumption/overall running time: 431.8787s / 175805.8508 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0686
env0_second_0:                 episode reward: 1.8500,                 loss: 0.3838
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 13461/30000 (44.8700%),                 avg. length: 3473.15,                last time consumption/overall running time: 356.8438s / 176162.6945 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0513
env0_second_0:                 episode reward: 0.0500,                 loss: 0.4664
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 13481/30000 (44.9367%),                 avg. length: 2525.05,                last time consumption/overall running time: 273.4171s / 176436.1117 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.0477
env0_second_0:                 episode reward: -6.4500,                 loss: 0.5854
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 13501/30000 (45.0033%),                 avg. length: 2932.7,                last time consumption/overall running time: 313.6147s / 176749.7264 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.0245
env0_second_0:                 episode reward: -3.0500,                 loss: 0.5604
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 13521/30000 (45.0700%),                 avg. length: 3711.05,                last time consumption/overall running time: 415.0653s / 177164.7916 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0760
env0_second_0:                 episode reward: -2.6500,                 loss: 0.4627
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 13541/30000 (45.1367%),                 avg. length: 3689.55,                last time consumption/overall running time: 395.1043s / 177559.8960 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.0752
env0_second_0:                 episode reward: -3.1500,                 loss: 0.6313
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 13561/30000 (45.2033%),                 avg. length: 3767.15,                last time consumption/overall running time: 398.7255s / 177958.6215 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.0598
env0_second_0:                 episode reward: -1.4500,                 loss: 0.5490
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 13581/30000 (45.2700%),                 avg. length: 3102.9,                last time consumption/overall running time: 326.9965s / 178285.6179 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0394
env0_second_0:                 episode reward: -4.6000,                 loss: 0.5208
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 13601/30000 (45.3367%),                 avg. length: 2977.55,                last time consumption/overall running time: 312.0779s / 178597.6958 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.0615
env0_second_0:                 episode reward: -4.7000,                 loss: 0.4641
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 13621/30000 (45.4033%),                 avg. length: 3958.9,                last time consumption/overall running time: 423.9874s / 179021.6833 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0894
env0_second_0:                 episode reward: 1.2500,                 loss: 0.3558
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 13641/30000 (45.4700%),                 avg. length: 3917.25,                last time consumption/overall running time: 434.2838s / 179455.9671 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.0880
env0_second_0:                 episode reward: 1.2500,                 loss: 0.4051
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 13661/30000 (45.5367%),                 avg. length: 4340.1,                last time consumption/overall running time: 447.3570s / 179903.3241 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.0929
env0_second_0:                 episode reward: 2.1000,                 loss: 0.5280
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 13681/30000 (45.6033%),                 avg. length: 4245.25,                last time consumption/overall running time: 467.6574s / 180370.9814 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0834
env0_second_0:                 episode reward: -0.4000,                 loss: 0.4779
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 13701/30000 (45.6700%),                 avg. length: 4129.6,                last time consumption/overall running time: 451.1767s / 180822.1581 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.0758
env0_second_0:                 episode reward: 4.2000,                 loss: 0.4213
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 13721/30000 (45.7367%),                 avg. length: 4173.25,                last time consumption/overall running time: 444.3970s / 181266.5551 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0771
env0_second_0:                 episode reward: 1.0000,                 loss: 0.6045
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 13741/30000 (45.8033%),                 avg. length: 3953.85,                last time consumption/overall running time: 417.9182s / 181684.4734 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.0941
env0_second_0:                 episode reward: 3.4000,                 loss: 0.5053
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 13761/30000 (45.8700%),                 avg. length: 3578.1,                last time consumption/overall running time: 391.9955s / 182076.4689 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0608
env0_second_0:                 episode reward: 0.5000,                 loss: 0.7496
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 13781/30000 (45.9367%),                 avg. length: 3086.15,                last time consumption/overall running time: 317.8382s / 182394.3071 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0232
env0_second_0:                 episode reward: -0.6500,                 loss: 0.7955
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 13801/30000 (46.0033%),                 avg. length: 2666.9,                last time consumption/overall running time: 276.0301s / 182670.3372 s
env0_first_0:                 episode reward: 7.5500,                 loss: -0.0833
env0_second_0:                 episode reward: -7.5500,                 loss: 0.9582
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 13821/30000 (46.0700%),                 avg. length: 2772.35,                last time consumption/overall running time: 287.8555s / 182958.1927 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0858
env0_second_0:                 episode reward: -5.6500,                 loss: 0.4974
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 13841/30000 (46.1367%),                 avg. length: 3027.1,                last time consumption/overall running time: 313.6986s / 183271.8912 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0663
env0_second_0:                 episode reward: -6.3000,                 loss: 0.4522
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 13861/30000 (46.2033%),                 avg. length: 2736.55,                last time consumption/overall running time: 285.7266s / 183557.6178 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.0896
env0_second_0:                 episode reward: -7.2500,                 loss: 0.3378
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 13881/30000 (46.2700%),                 avg. length: 3115.4,                last time consumption/overall running time: 338.5739s / 183896.1917 s
env0_first_0:                 episode reward: 7.4500,                 loss: -0.0791
env0_second_0:                 episode reward: -7.4500,                 loss: 0.3213
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 13901/30000 (46.3367%),                 avg. length: 3493.8,                last time consumption/overall running time: 384.3792s / 184280.5709 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.0577
env0_second_0:                 episode reward: -4.3000,                 loss: 0.2889
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 13921/30000 (46.4033%),                 avg. length: 2475.25,                last time consumption/overall running time: 262.5610s / 184543.1319 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.0598
env0_second_0:                 episode reward: 5.8500,                 loss: 0.6858
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 13941/30000 (46.4700%),                 avg. length: 3011.45,                last time consumption/overall running time: 331.5271s / 184874.6590 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0759
env0_second_0:                 episode reward: -3.2500,                 loss: 0.6634
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 13961/30000 (46.5367%),                 avg. length: 3650.9,                last time consumption/overall running time: 391.6985s / 185266.3575 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0951
env0_second_0:                 episode reward: -3.3000,                 loss: 0.6670
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 13981/30000 (46.6033%),                 avg. length: 3245.65,                last time consumption/overall running time: 352.1584s / 185618.5159 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0896
env0_second_0:                 episode reward: -5.4000,                 loss: 0.5649
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 14001/30000 (46.6700%),                 avg. length: 4058.0,                last time consumption/overall running time: 442.7756s / 186061.2915 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0741
env0_second_0:                 episode reward: -1.9500,                 loss: 0.4399
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 14021/30000 (46.7367%),                 avg. length: 3100.15,                last time consumption/overall running time: 333.6088s / 186394.9002 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0651
env0_second_0:                 episode reward: -4.5500,                 loss: 0.3974
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 14041/30000 (46.8033%),                 avg. length: 3925.6,                last time consumption/overall running time: 422.4512s / 186817.3514 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0546
env0_second_0:                 episode reward: -0.8000,                 loss: 0.5144
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 14061/30000 (46.8700%),                 avg. length: 3490.75,                last time consumption/overall running time: 377.6147s / 187194.9661 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.0583
env0_second_0:                 episode reward: -2.2000,                 loss: 0.5969
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 14081/30000 (46.9367%),                 avg. length: 3134.8,                last time consumption/overall running time: 344.4910s / 187539.4571 s
env0_first_0:                 episode reward: 6.9000,                 loss: -0.0808
env0_second_0:                 episode reward: -6.9000,                 loss: 0.5658
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 14101/30000 (47.0033%),                 avg. length: 2915.05,                last time consumption/overall running time: 308.7295s / 187848.1866 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.0607
env0_second_0:                 episode reward: -7.3000,                 loss: 0.5320
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 14121/30000 (47.0700%),                 avg. length: 2760.2,                last time consumption/overall running time: 298.0565s / 188146.2431 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.0869
env0_second_0:                 episode reward: -8.6000,                 loss: 1.0368
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 14141/30000 (47.1367%),                 avg. length: 3162.3,                last time consumption/overall running time: 337.7972s / 188484.0403 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.0826
env0_second_0:                 episode reward: -6.4500,                 loss: 0.6575
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 14161/30000 (47.2033%),                 avg. length: 2631.25,                last time consumption/overall running time: 279.6637s / 188763.7040 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.0484
env0_second_0:                 episode reward: -6.0000,                 loss: 0.8108
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 14181/30000 (47.2700%),                 avg. length: 3124.25,                last time consumption/overall running time: 337.5068s / 189101.2108 s
env0_first_0:                 episode reward: 6.7000,                 loss: -0.0575
env0_second_0:                 episode reward: -6.7000,                 loss: 0.6810
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 14201/30000 (47.3367%),                 avg. length: 3066.0,                last time consumption/overall running time: 319.6663s / 189420.8771 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.0584
env0_second_0:                 episode reward: -6.7500,                 loss: 0.5671
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 14221/30000 (47.4033%),                 avg. length: 2650.55,                last time consumption/overall running time: 277.1119s / 189697.9890 s
env0_first_0:                 episode reward: 5.5000,                 loss: 0.0089
env0_second_0:                 episode reward: -5.5000,                 loss: 0.6754
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 14241/30000 (47.4700%),                 avg. length: 1217.1,                last time consumption/overall running time: 131.0671s / 189829.0561 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0340
env0_second_0:                 episode reward: -5.4500,                 loss: 0.4003
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 14261/30000 (47.5367%),                 avg. length: 1755.95,                last time consumption/overall running time: 183.7478s / 190012.8039 s
env0_first_0:                 episode reward: -8.7000,                 loss: -0.0872
env0_second_0:                 episode reward: 8.7000,                 loss: 0.9218
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 14281/30000 (47.6033%),                 avg. length: 2928.45,                last time consumption/overall running time: 313.3010s / 190326.1049 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.0588
env0_second_0:                 episode reward: 5.3000,                 loss: 0.6978
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 14301/30000 (47.6700%),                 avg. length: 3906.35,                last time consumption/overall running time: 421.3456s / 190747.4504 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0489
env0_second_0:                 episode reward: -1.7500,                 loss: 0.5482
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 14321/30000 (47.7367%),                 avg. length: 3491.65,                last time consumption/overall running time: 368.0442s / 191115.4946 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.0610
env0_second_0:                 episode reward: -4.5000,                 loss: 0.5791
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 14341/30000 (47.8033%),                 avg. length: 2638.45,                last time consumption/overall running time: 310.0414s / 191425.5360 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0744
env0_second_0:                 episode reward: -3.2500,                 loss: 1.3201
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 14361/30000 (47.8700%),                 avg. length: 2352.65,                last time consumption/overall running time: 248.2097s / 191673.7458 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.0417
env0_second_0:                 episode reward: 4.5000,                 loss: 0.8222
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 14381/30000 (47.9367%),                 avg. length: 3029.7,                last time consumption/overall running time: 324.1669s / 191997.9126 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0843
env0_second_0:                 episode reward: -1.9500,                 loss: 0.7796
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 14401/30000 (48.0033%),                 avg. length: 3587.9,                last time consumption/overall running time: 380.4172s / 192378.3298 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.0641
env0_second_0:                 episode reward: -1.1500,                 loss: 0.7129
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 14421/30000 (48.0700%),                 avg. length: 3010.5,                last time consumption/overall running time: 320.2409s / 192698.5707 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.0878
env0_second_0:                 episode reward: -6.1500,                 loss: 0.4750
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 14441/30000 (48.1367%),                 avg. length: 2773.7,                last time consumption/overall running time: 293.6629s / 192992.2336 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.0775
env0_second_0:                 episode reward: -6.1500,                 loss: 0.7264
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 14461/30000 (48.2033%),                 avg. length: 2419.55,                last time consumption/overall running time: 256.1961s / 193248.4297 s
env0_first_0:                 episode reward: 7.0500,                 loss: -0.0900
env0_second_0:                 episode reward: -7.0500,                 loss: 0.5789
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 14481/30000 (48.2700%),                 avg. length: 3606.25,                last time consumption/overall running time: 382.3081s / 193630.7378 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.0760
env0_second_0:                 episode reward: -2.9500,                 loss: 0.7625
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 14501/30000 (48.3367%),                 avg. length: 3230.5,                last time consumption/overall running time: 340.5746s / 193971.3124 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.0768
env0_second_0:                 episode reward: -3.8000,                 loss: 0.7276
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 14521/30000 (48.4033%),                 avg. length: 3149.0,                last time consumption/overall running time: 326.1105s / 194297.4229 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.0747
env0_second_0:                 episode reward: -4.0000,                 loss: 0.5474
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 14541/30000 (48.4700%),                 avg. length: 2461.5,                last time consumption/overall running time: 262.9046s / 194560.3275 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0484
env0_second_0:                 episode reward: -2.6000,                 loss: 1.7815
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 14561/30000 (48.5367%),                 avg. length: 1918.95,                last time consumption/overall running time: 212.0396s / 194772.3671 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0304
env0_second_0:                 episode reward: -0.3000,                 loss: 0.6184
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 14581/30000 (48.6033%),                 avg. length: 2083.05,                last time consumption/overall running time: 221.2258s / 194993.5929 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.0576
env0_second_0:                 episode reward: 4.5000,                 loss: 0.7095
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 14601/30000 (48.6700%),                 avg. length: 2149.65,                last time consumption/overall running time: 225.4740s / 195219.0669 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0575
env0_second_0:                 episode reward: 5.1000,                 loss: 0.7712
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 14621/30000 (48.7367%),                 avg. length: 2048.35,                last time consumption/overall running time: 220.4018s / 195439.4687 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.0413
env0_second_0:                 episode reward: 3.4500,                 loss: 0.6365
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 14641/30000 (48.8033%),                 avg. length: 2521.85,                last time consumption/overall running time: 277.5656s / 195717.0343 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0549
env0_second_0:                 episode reward: 0.6000,                 loss: 0.6023
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 14661/30000 (48.8700%),                 avg. length: 2408.55,                last time consumption/overall running time: 252.7021s / 195969.7365 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.0473
env0_second_0:                 episode reward: 2.7500,                 loss: 2.8177
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 14681/30000 (48.9367%),                 avg. length: 2329.25,                last time consumption/overall running time: 244.0859s / 196213.8223 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.0192
env0_second_0:                 episode reward: 2.5500,                 loss: 0.7665
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 14701/30000 (49.0033%),                 avg. length: 2149.2,                last time consumption/overall running time: 252.1406s / 196465.9629 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.0575
env0_second_0:                 episode reward: 5.0500,                 loss: 0.6336
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 14721/30000 (49.0700%),                 avg. length: 2100.8,                last time consumption/overall running time: 236.1851s / 196702.1480 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.0335
env0_second_0:                 episode reward: 5.0500,                 loss: 0.6825
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 14741/30000 (49.1367%),                 avg. length: 2190.15,                last time consumption/overall running time: 241.2559s / 196943.4039 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.0347
env0_second_0:                 episode reward: 4.3500,                 loss: 0.5853
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 14761/30000 (49.2033%),                 avg. length: 2368.0,                last time consumption/overall running time: 258.2813s / 197201.6852 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.0177
env0_second_0:                 episode reward: 1.1500,                 loss: 0.5228
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 14781/30000 (49.2700%),                 avg. length: 2173.6,                last time consumption/overall running time: 240.5194s / 197442.2046 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.0230
env0_second_0:                 episode reward: 4.7000,                 loss: 0.8032
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 14801/30000 (49.3367%),                 avg. length: 2407.6,                last time consumption/overall running time: 256.4244s / 197698.6290 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0278
env0_second_0:                 episode reward: -1.7500,                 loss: 0.7144
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 14821/30000 (49.4033%),                 avg. length: 2440.85,                last time consumption/overall running time: 273.5531s / 197972.1822 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.0229
env0_second_0:                 episode reward: 2.5500,                 loss: 1.4704
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 14841/30000 (49.4700%),                 avg. length: 2431.5,                last time consumption/overall running time: 257.3016s / 198229.4837 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0236
env0_second_0:                 episode reward: 1.8500,                 loss: 0.7023
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 14861/30000 (49.5367%),                 avg. length: 2085.0,                last time consumption/overall running time: 228.8006s / 198458.2844 s
env0_first_0:                 episode reward: -4.8500,                 loss: -0.0471
env0_second_0:                 episode reward: 4.8500,                 loss: 0.6537
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 14881/30000 (49.6033%),                 avg. length: 1923.4,                last time consumption/overall running time: 209.3589s / 198667.6433 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0603
env0_second_0:                 episode reward: 7.8500,                 loss: 0.5395
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 14901/30000 (49.6700%),                 avg. length: 1746.15,                last time consumption/overall running time: 191.7847s / 198859.4279 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.0532
env0_second_0:                 episode reward: 6.2500,                 loss: 0.5564
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 14921/30000 (49.7367%),                 avg. length: 1745.0,                last time consumption/overall running time: 184.7356s / 199044.1636 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.0347
env0_second_0:                 episode reward: 8.0000,                 loss: 0.7312
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 14941/30000 (49.8033%),                 avg. length: 1190.9,                last time consumption/overall running time: 129.7547s / 199173.9182 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.0361
env0_second_0:                 episode reward: -4.1500,                 loss: 0.5761
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 14961/30000 (49.8700%),                 avg. length: 982.45,                last time consumption/overall running time: 107.3301s / 199281.2484 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.0091
env0_second_0:                 episode reward: -8.8500,                 loss: 0.7331
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 14981/30000 (49.9367%),                 avg. length: 967.05,                last time consumption/overall running time: 113.7727s / 199395.0211 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0487
env0_second_0:                 episode reward: -9.5000,                 loss: -0.0255
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 15001/30000 (50.0033%),                 avg. length: 979.0,                last time consumption/overall running time: 110.8549s / 199505.8760 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0394
env0_second_0:                 episode reward: -9.5500,                 loss: 0.0849
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 15021/30000 (50.0700%),                 avg. length: 945.35,                last time consumption/overall running time: 107.5651s / 199613.4411 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0308
env0_second_0:                 episode reward: -9.7000,                 loss: 0.6723
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 15041/30000 (50.1367%),                 avg. length: 950.9,                last time consumption/overall running time: 108.2312s / 199721.6723 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.0195
env0_second_0:                 episode reward: -9.0500,                 loss: 0.5855
env1_first_0:                 episode reward: 9.1000,                 loss: nan
env1_second_0:                 episode reward: -9.1000,                 loss: nan
Episode: 15061/30000 (50.2033%),                 avg. length: 946.7,                last time consumption/overall running time: 121.8234s / 199843.4957 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0251
env0_second_0:                 episode reward: -9.1500,                 loss: 0.6356
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 15081/30000 (50.2700%),                 avg. length: 943.0,                last time consumption/overall running time: 104.8491s / 199948.3448 s
env0_first_0:                 episode reward: 7.4000,                 loss: 0.0313
env0_second_0:                 episode reward: -7.4000,                 loss: 0.5314
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 15101/30000 (50.3367%),                 avg. length: 940.0,                last time consumption/overall running time: 105.3742s / 200053.7190 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.0711
env0_second_0:                 episode reward: 0.2000,                 loss: 0.1629
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 15121/30000 (50.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 107.2727s / 200160.9916 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0856
env0_second_0:                 episode reward: -0.0500,                 loss: 0.1444
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 15141/30000 (50.4700%),                 avg. length: 976.4,                last time consumption/overall running time: 112.6834s / 200273.6751 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.0767
env0_second_0:                 episode reward: -1.4000,                 loss: 0.8359
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 15161/30000 (50.5367%),                 avg. length: 1044.15,                last time consumption/overall running time: 113.0976s / 200386.7727 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0776
env0_second_0:                 episode reward: -3.2000,                 loss: 1.1476
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 15181/30000 (50.6033%),                 avg. length: 1077.65,                last time consumption/overall running time: 120.5870s / 200507.3597 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.0250
env0_second_0:                 episode reward: -4.2000,                 loss: 1.3603
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 15201/30000 (50.6700%),                 avg. length: 1235.85,                last time consumption/overall running time: 135.9036s / 200643.2633 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.0160
env0_second_0:                 episode reward: -0.4000,                 loss: 1.4156
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 15221/30000 (50.7367%),                 avg. length: 1264.45,                last time consumption/overall running time: 137.2813s / 200780.5446 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.0320
env0_second_0:                 episode reward: -0.2000,                 loss: 1.8633
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 15241/30000 (50.8033%),                 avg. length: 1587.5,                last time consumption/overall running time: 166.4123s / 200946.9568 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.0002
env0_second_0:                 episode reward: 2.3000,                 loss: 1.5111
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 15261/30000 (50.8700%),                 avg. length: 2145.5,                last time consumption/overall running time: 234.9700s / 201181.9269 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0075
env0_second_0:                 episode reward: -0.5500,                 loss: 1.6524
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 15281/30000 (50.9367%),                 avg. length: 2110.05,                last time consumption/overall running time: 229.6171s / 201411.5440 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0028
env0_second_0:                 episode reward: 1.0500,                 loss: 1.2872
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 15301/30000 (51.0033%),                 avg. length: 2001.3,                last time consumption/overall running time: 219.7570s / 201631.3010 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0414
env0_second_0:                 episode reward: 2.3500,                 loss: 1.3534
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 15321/30000 (51.0700%),                 avg. length: 2071.6,                last time consumption/overall running time: 222.4492s / 201853.7502 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0352
env0_second_0:                 episode reward: -0.7500,                 loss: 1.5158
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 15341/30000 (51.1367%),                 avg. length: 1910.6,                last time consumption/overall running time: 211.3706s / 202065.1208 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0399
env0_second_0:                 episode reward: -1.6000,                 loss: 2.1153
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 15361/30000 (51.2033%),                 avg. length: 2246.3,                last time consumption/overall running time: 238.9558s / 202304.0766 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0265
env0_second_0:                 episode reward: 0.5000,                 loss: 3.4467
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 15381/30000 (51.2700%),                 avg. length: 2030.4,                last time consumption/overall running time: 214.4299s / 202518.5065 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.0105
env0_second_0:                 episode reward: 4.8500,                 loss: 1.9232
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 15401/30000 (51.3367%),                 avg. length: 1867.75,                last time consumption/overall running time: 199.1924s / 202717.6990 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0074
env0_second_0:                 episode reward: 5.0000,                 loss: 1.9856
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 15421/30000 (51.4033%),                 avg. length: 1988.6,                last time consumption/overall running time: 214.7941s / 202932.4931 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0317
env0_second_0:                 episode reward: -0.1000,                 loss: 2.3261
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 15441/30000 (51.4700%),                 avg. length: 1973.0,                last time consumption/overall running time: 217.8335s / 203150.3266 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0182
env0_second_0:                 episode reward: -3.7500,                 loss: 1.6260
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 15461/30000 (51.5367%),                 avg. length: 2253.5,                last time consumption/overall running time: 240.3813s / 203390.7079 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0328
env0_second_0:                 episode reward: -2.0500,                 loss: 1.4706
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 15481/30000 (51.6033%),                 avg. length: 2139.6,                last time consumption/overall running time: 235.9151s / 203626.6230 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.1368
env0_second_0:                 episode reward: 0.9000,                 loss: 1.9931
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 15501/30000 (51.6700%),                 avg. length: 1850.45,                last time consumption/overall running time: 196.1750s / 203822.7980 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0367
env0_second_0:                 episode reward: 4.6000,                 loss: 1.5624
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 15521/30000 (51.7367%),                 avg. length: 2176.8,                last time consumption/overall running time: 233.5406s / 204056.3386 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0486
env0_second_0:                 episode reward: 4.6000,                 loss: 1.6526
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 15541/30000 (51.8033%),                 avg. length: 2752.15,                last time consumption/overall running time: 293.2453s / 204349.5839 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0168
env0_second_0:                 episode reward: 1.0000,                 loss: 1.5273
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 15561/30000 (51.8700%),                 avg. length: 1519.1,                last time consumption/overall running time: 166.2459s / 204515.8298 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0154
env0_second_0:                 episode reward: 7.1500,                 loss: 4.1525
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 15581/30000 (51.9367%),                 avg. length: 1154.1,                last time consumption/overall running time: 134.6084s / 204650.4382 s
env0_first_0:                 episode reward: -8.5000,                 loss: -0.0315
env0_second_0:                 episode reward: 8.5000,                 loss: 1.5110
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 15601/30000 (52.0033%),                 avg. length: 919.6,                last time consumption/overall running time: 116.8995s / 204767.3376 s
env0_first_0:                 episode reward: -9.3500,                 loss: 0.0018
env0_second_0:                 episode reward: 9.3500,                 loss: 1.2598
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 15621/30000 (52.0700%),                 avg. length: 941.55,                last time consumption/overall running time: 109.8539s / 204877.1915 s
env0_first_0:                 episode reward: -9.0000,                 loss: 0.0032
env0_second_0:                 episode reward: 9.0000,                 loss: 1.0060
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 15641/30000 (52.1367%),                 avg. length: 934.55,                last time consumption/overall running time: 106.5768s / 204983.7683 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.0169
env0_second_0:                 episode reward: 9.0000,                 loss: 0.7608
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 15661/30000 (52.2033%),                 avg. length: 1186.95,                last time consumption/overall running time: 147.5543s / 205131.3226 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0685
env0_second_0:                 episode reward: 6.0500,                 loss: 0.9619
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 15681/30000 (52.2700%),                 avg. length: 1058.6,                last time consumption/overall running time: 120.2718s / 205251.5944 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0476
env0_second_0:                 episode reward: 7.6500,                 loss: 2.0952
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 15701/30000 (52.3367%),                 avg. length: 1069.0,                last time consumption/overall running time: 122.2023s / 205373.7967 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0738
env0_second_0:                 episode reward: 5.5500,                 loss: 1.8774
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 15721/30000 (52.4033%),                 avg. length: 1144.2,                last time consumption/overall running time: 128.1668s / 205501.9634 s
env0_first_0:                 episode reward: -8.3000,                 loss: 0.1124
env0_second_0:                 episode reward: 8.3000,                 loss: 1.5486
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 15741/30000 (52.4700%),                 avg. length: 951.4,                last time consumption/overall running time: 108.3376s / 205610.3011 s
env0_first_0:                 episode reward: -10.0000,                 loss: -0.0337
env0_second_0:                 episode reward: 10.0000,                 loss: 1.7770
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 15761/30000 (52.5367%),                 avg. length: 861.95,                last time consumption/overall running time: 99.3340s / 205709.6351 s
env0_first_0:                 episode reward: -10.0000,                 loss: 0.1425
env0_second_0:                 episode reward: 10.0000,                 loss: 3.7856
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 15781/30000 (52.6033%),                 avg. length: 855.85,                last time consumption/overall running time: 98.1873s / 205807.8224 s
env0_first_0:                 episode reward: -10.0000,                 loss: 0.0561
env0_second_0:                 episode reward: 10.0000,                 loss: 1.5843
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 15801/30000 (52.6700%),                 avg. length: 979.8,                last time consumption/overall running time: 113.1627s / 205920.9851 s
env0_first_0:                 episode reward: -9.9000,                 loss: -0.0078
env0_second_0:                 episode reward: 9.9000,                 loss: 1.8162
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 15821/30000 (52.7367%),                 avg. length: 933.8,                last time consumption/overall running time: 109.1755s / 206030.1606 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0711
env0_second_0:                 episode reward: 9.5500,                 loss: 2.0439
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 15841/30000 (52.8033%),                 avg. length: 962.9,                last time consumption/overall running time: 106.5844s / 206136.7450 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.0560
env0_second_0:                 episode reward: 9.7000,                 loss: 2.2444
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 15861/30000 (52.8700%),                 avg. length: 931.9,                last time consumption/overall running time: 106.1969s / 206242.9420 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.0704
env0_second_0:                 episode reward: 9.8000,                 loss: 2.8120
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 15881/30000 (52.9367%),                 avg. length: 1003.0,                last time consumption/overall running time: 113.8931s / 206356.8350 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.0869
env0_second_0:                 episode reward: 9.7500,                 loss: 3.5049
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 15901/30000 (53.0033%),                 avg. length: 1510.9,                last time consumption/overall running time: 168.0412s / 206524.8762 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.0360
env0_second_0:                 episode reward: 5.1000,                 loss: 1.7597
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 15921/30000 (53.0700%),                 avg. length: 1961.6,                last time consumption/overall running time: 214.5738s / 206739.4501 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0140
env0_second_0:                 episode reward: 1.8000,                 loss: 1.9427
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 15941/30000 (53.1367%),                 avg. length: 2302.5,                last time consumption/overall running time: 242.5875s / 206982.0376 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0022
env0_second_0:                 episode reward: 3.0000,                 loss: 1.7113
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 15961/30000 (53.2033%),                 avg. length: 2224.45,                last time consumption/overall running time: 235.4967s / 207217.5342 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.0398
env0_second_0:                 episode reward: 6.0500,                 loss: 1.1776
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 15981/30000 (53.2700%),                 avg. length: 2223.25,                last time consumption/overall running time: 238.2671s / 207455.8014 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.0614
env0_second_0:                 episode reward: 4.1000,                 loss: 1.8453
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 16001/30000 (53.3367%),                 avg. length: 2564.45,                last time consumption/overall running time: 274.2242s / 207730.0256 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0313
env0_second_0:                 episode reward: 1.6000,                 loss: 1.6763
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 16021/30000 (53.4033%),                 avg. length: 2404.95,                last time consumption/overall running time: 257.6718s / 207987.6974 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.0346
env0_second_0:                 episode reward: 3.8500,                 loss: 1.6081
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 16041/30000 (53.4700%),                 avg. length: 2088.9,                last time consumption/overall running time: 226.3702s / 208214.0676 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.0599
env0_second_0:                 episode reward: 7.0000,                 loss: 1.8405
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 16061/30000 (53.5367%),                 avg. length: 2696.45,                last time consumption/overall running time: 286.2664s / 208500.3340 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0200
env0_second_0:                 episode reward: -1.6500,                 loss: 1.4899
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 16081/30000 (53.6033%),                 avg. length: 2679.3,                last time consumption/overall running time: 288.2926s / 208788.6266 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.0573
env0_second_0:                 episode reward: -3.7000,                 loss: 1.4011
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 16101/30000 (53.6700%),                 avg. length: 2711.3,                last time consumption/overall running time: 286.6340s / 209075.2606 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.0396
env0_second_0:                 episode reward: -2.7000,                 loss: 0.9630
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 16121/30000 (53.7367%),                 avg. length: 3233.35,                last time consumption/overall running time: 339.1256s / 209414.3863 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0229
env0_second_0:                 episode reward: 2.0500,                 loss: 0.9379
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 16141/30000 (53.8033%),                 avg. length: 2939.65,                last time consumption/overall running time: 312.6801s / 209727.0664 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.0545
env0_second_0:                 episode reward: 4.4000,                 loss: 1.3678
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 16161/30000 (53.8700%),                 avg. length: 3275.4,                last time consumption/overall running time: 342.9447s / 210070.0110 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.0237
env0_second_0:                 episode reward: 2.3000,                 loss: 3.6375
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 16181/30000 (53.9367%),                 avg. length: 3659.55,                last time consumption/overall running time: 385.5330s / 210455.5440 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.0321
env0_second_0:                 episode reward: -4.0000,                 loss: 2.5456
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 16201/30000 (54.0033%),                 avg. length: 3616.4,                last time consumption/overall running time: 382.7360s / 210838.2800 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.0638
env0_second_0:                 episode reward: -2.8000,                 loss: 1.2354
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 16221/30000 (54.0700%),                 avg. length: 3998.4,                last time consumption/overall running time: 414.7749s / 211253.0549 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.0723
env0_second_0:                 episode reward: 0.7000,                 loss: 1.0928
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 16241/30000 (54.1367%),                 avg. length: 3812.25,                last time consumption/overall running time: 389.1554s / 211642.2103 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0645
env0_second_0:                 episode reward: 1.0000,                 loss: 0.9285
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 16261/30000 (54.2033%),                 avg. length: 1924.0,                last time consumption/overall running time: 209.3619s / 211851.5722 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0334
env0_second_0:                 episode reward: 4.0000,                 loss: 1.3094
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 16281/30000 (54.2700%),                 avg. length: 2789.95,                last time consumption/overall running time: 305.0573s / 212156.6295 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0516
env0_second_0:                 episode reward: -1.5000,                 loss: 1.3385
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 16301/30000 (54.3367%),                 avg. length: 2653.25,                last time consumption/overall running time: 297.5609s / 212454.1904 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.0703
env0_second_0:                 episode reward: -7.3500,                 loss: 0.6795
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 16321/30000 (54.4033%),                 avg. length: 2708.4,                last time consumption/overall running time: 288.5409s / 212742.7313 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.0450
env0_second_0:                 episode reward: -6.4500,                 loss: 1.1245
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 16341/30000 (54.4700%),                 avg. length: 2828.3,                last time consumption/overall running time: 295.9335s / 213038.6647 s
env0_first_0:                 episode reward: 7.5500,                 loss: -0.0754
env0_second_0:                 episode reward: -7.5500,                 loss: 0.7110
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 16361/30000 (54.5367%),                 avg. length: 3479.85,                last time consumption/overall running time: 371.4361s / 213410.1008 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.0675
env0_second_0:                 episode reward: -4.3500,                 loss: 0.3581
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 16381/30000 (54.6033%),                 avg. length: 3302.65,                last time consumption/overall running time: 349.0177s / 213759.1185 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0885
env0_second_0:                 episode reward: -4.8000,                 loss: 0.5066
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 16401/30000 (54.6700%),                 avg. length: 2940.3,                last time consumption/overall running time: 310.7085s / 214069.8270 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.0662
env0_second_0:                 episode reward: -2.2500,                 loss: 0.6835
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 16421/30000 (54.7367%),                 avg. length: 3317.6,                last time consumption/overall running time: 341.9376s / 214411.7646 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.0840
env0_second_0:                 episode reward: -3.3500,                 loss: 0.5550
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 16441/30000 (54.8033%),                 avg. length: 2589.0,                last time consumption/overall running time: 277.1856s / 214688.9502 s
env0_first_0:                 episode reward: 5.3500,                 loss: -0.0448
env0_second_0:                 episode reward: -5.3500,                 loss: 0.7098
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 16461/30000 (54.8700%),                 avg. length: 2374.5,                last time consumption/overall running time: 253.5984s / 214942.5486 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.0672
env0_second_0:                 episode reward: -6.1000,                 loss: 1.9896
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 16481/30000 (54.9367%),                 avg. length: 863.0,                last time consumption/overall running time: 107.7765s / 215050.3251 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0650
env0_second_0:                 episode reward: -10.0000,                 loss: 1.4637
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16501/30000 (55.0033%),                 avg. length: 863.0,                last time consumption/overall running time: 98.5862s / 215148.9113 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0697
env0_second_0:                 episode reward: -10.0000,                 loss: 1.0131
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16521/30000 (55.0700%),                 avg. length: 863.0,                last time consumption/overall running time: 97.0180s / 215245.9293 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0834
env0_second_0:                 episode reward: -10.0000,                 loss: 0.8240
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16541/30000 (55.1367%),                 avg. length: 863.0,                last time consumption/overall running time: 103.3878s / 215349.3171 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0657
env0_second_0:                 episode reward: -10.0000,                 loss: 0.8855
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16561/30000 (55.2033%),                 avg. length: 863.0,                last time consumption/overall running time: 109.3574s / 215458.6746 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0788
env0_second_0:                 episode reward: -9.9000,                 loss: 1.9683
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16581/30000 (55.2700%),                 avg. length: 863.0,                last time consumption/overall running time: 99.3607s / 215558.0353 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0695
env0_second_0:                 episode reward: -9.9000,                 loss: 1.8861
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16601/30000 (55.3367%),                 avg. length: 863.0,                last time consumption/overall running time: 105.9881s / 215664.0233 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0805
env0_second_0:                 episode reward: -9.8000,                 loss: 1.7627
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 16621/30000 (55.4033%),                 avg. length: 863.0,                last time consumption/overall running time: 97.3584s / 215761.3818 s
env0_first_0:                 episode reward: 9.7500,                 loss: 0.0206
env0_second_0:                 episode reward: -9.7500,                 loss: 1.3952
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 16641/30000 (55.4700%),                 avg. length: 863.0,                last time consumption/overall running time: 100.4762s / 215861.8579 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0841
env0_second_0:                 episode reward: -9.9500,                 loss: 1.8366
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16661/30000 (55.5367%),                 avg. length: 863.0,                last time consumption/overall running time: 99.7305s / 215961.5884 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0845
env0_second_0:                 episode reward: -10.0000,                 loss: 1.1019
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16681/30000 (55.6033%),                 avg. length: 863.0,                last time consumption/overall running time: 95.7898s / 216057.3782 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0983
env0_second_0:                 episode reward: -10.0000,                 loss: 1.2019
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 16701/30000 (55.6700%),                 avg. length: 864.2,                last time consumption/overall running time: 96.7945s / 216154.1727 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0915
env0_second_0:                 episode reward: -9.8000,                 loss: 0.7350
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 16721/30000 (55.7367%),                 avg. length: 1550.5,                last time consumption/overall running time: 166.3472s / 216320.5199 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.0198
env0_second_0:                 episode reward: -3.4500,                 loss: 0.4602
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 16741/30000 (55.8033%),                 avg. length: 2352.95,                last time consumption/overall running time: 247.8389s / 216568.3589 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.0318
env0_second_0:                 episode reward: -2.4000,                 loss: 0.4268
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 16761/30000 (55.8700%),                 avg. length: 2714.4,                last time consumption/overall running time: 287.5317s / 216855.8906 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0367
env0_second_0:                 episode reward: -1.9500,                 loss: 0.6889
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 16781/30000 (55.9367%),                 avg. length: 2332.8,                last time consumption/overall running time: 247.6128s / 217103.5034 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0233
env0_second_0:                 episode reward: -4.6500,                 loss: 0.7084
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 16801/30000 (56.0033%),                 avg. length: 2783.0,                last time consumption/overall running time: 298.5487s / 217402.0521 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.0403
env0_second_0:                 episode reward: -5.1000,                 loss: 0.6837
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 16821/30000 (56.0700%),                 avg. length: 2587.6,                last time consumption/overall running time: 271.9868s / 217674.0389 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.0370
env0_second_0:                 episode reward: -5.9000,                 loss: 0.7116
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 16841/30000 (56.1367%),                 avg. length: 2461.25,                last time consumption/overall running time: 268.3734s / 217942.4123 s
env0_first_0:                 episode reward: 6.9000,                 loss: -0.0765
env0_second_0:                 episode reward: -6.9000,                 loss: 0.6665
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 16861/30000 (56.2033%),                 avg. length: 2813.55,                last time consumption/overall running time: 296.7818s / 218239.1941 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.0843
env0_second_0:                 episode reward: -6.7500,                 loss: 0.5908
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 16881/30000 (56.2700%),                 avg. length: 2717.1,                last time consumption/overall running time: 302.5744s / 218541.7685 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0825
env0_second_0:                 episode reward: -7.1500,                 loss: 0.8002
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 16901/30000 (56.3367%),                 avg. length: 3024.85,                last time consumption/overall running time: 325.3824s / 218867.1508 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0566
env0_second_0:                 episode reward: -4.5500,                 loss: 0.8424
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 16921/30000 (56.4033%),                 avg. length: 2980.65,                last time consumption/overall running time: 309.0638s / 219176.2146 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.0600
env0_second_0:                 episode reward: -5.0500,                 loss: 0.7842
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 16941/30000 (56.4700%),                 avg. length: 3268.1,                last time consumption/overall running time: 335.1675s / 219511.3821 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.0501
env0_second_0:                 episode reward: -6.1000,                 loss: 0.5980
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 16961/30000 (56.5367%),                 avg. length: 3382.2,                last time consumption/overall running time: 358.6949s / 219870.0770 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.0638
env0_second_0:                 episode reward: -5.2000,                 loss: 0.6246
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 16981/30000 (56.6033%),                 avg. length: 3104.2,                last time consumption/overall running time: 342.6797s / 220212.7567 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0494
env0_second_0:                 episode reward: -4.9500,                 loss: 0.6111
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 17001/30000 (56.6700%),                 avg. length: 3200.0,                last time consumption/overall running time: 348.1887s / 220560.9455 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.0739
env0_second_0:                 episode reward: -4.7500,                 loss: 0.4934
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 17021/30000 (56.7367%),                 avg. length: 3118.95,                last time consumption/overall running time: 360.1164s / 220921.0619 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0818
env0_second_0:                 episode reward: -6.3000,                 loss: 0.6750
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 17041/30000 (56.8033%),                 avg. length: 3334.75,                last time consumption/overall running time: 352.1905s / 221273.2524 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0633
env0_second_0:                 episode reward: -4.9000,                 loss: 0.9481
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 17061/30000 (56.8700%),                 avg. length: 4035.1,                last time consumption/overall running time: 428.1994s / 221701.4518 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0641
env0_second_0:                 episode reward: -1.5500,                 loss: 0.5812
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 17081/30000 (56.9367%),                 avg. length: 4219.15,                last time consumption/overall running time: 447.6695s / 222149.1213 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.0647
env0_second_0:                 episode reward: -0.9000,                 loss: 0.5761
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 17101/30000 (57.0033%),                 avg. length: 2450.1,                last time consumption/overall running time: 256.7704s / 222405.8916 s
env0_first_0:                 episode reward: 6.9000,                 loss: -0.0399
env0_second_0:                 episode reward: -6.9000,                 loss: 0.7258
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 17121/30000 (57.0700%),                 avg. length: 2211.95,                last time consumption/overall running time: 242.7742s / 222648.6658 s
env0_first_0:                 episode reward: 7.4000,                 loss: -0.0733
env0_second_0:                 episode reward: -7.4000,                 loss: 1.0118
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 17141/30000 (57.1367%),                 avg. length: 2285.25,                last time consumption/overall running time: 249.8328s / 222898.4986 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.0901
env0_second_0:                 episode reward: -7.3500,                 loss: 0.5707
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 17161/30000 (57.2033%),                 avg. length: 3352.0,                last time consumption/overall running time: 364.9447s / 223263.4433 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0621
env0_second_0:                 episode reward: -4.6000,                 loss: 0.4514
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 17181/30000 (57.2700%),                 avg. length: 3572.85,                last time consumption/overall running time: 375.4261s / 223638.8694 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.0613
env0_second_0:                 episode reward: -3.9500,                 loss: 0.5566
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 17201/30000 (57.3367%),                 avg. length: 3781.75,                last time consumption/overall running time: 399.2855s / 224038.1549 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0628
env0_second_0:                 episode reward: 1.8500,                 loss: 0.4018
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 17221/30000 (57.4033%),                 avg. length: 3192.95,                last time consumption/overall running time: 335.8551s / 224374.0101 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0134
env0_second_0:                 episode reward: -0.1000,                 loss: 0.8246
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 17241/30000 (57.4700%),                 avg. length: 2308.85,                last time consumption/overall running time: 252.4675s / 224626.4776 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.0335
env0_second_0:                 episode reward: -1.9000,                 loss: 1.1347
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 17261/30000 (57.5367%),                 avg. length: 2442.45,                last time consumption/overall running time: 255.8855s / 224882.3631 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.0448
env0_second_0:                 episode reward: -3.5500,                 loss: 0.8019
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 17281/30000 (57.6033%),                 avg. length: 2008.4,                last time consumption/overall running time: 212.7751s / 225095.1382 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.0429
env0_second_0:                 episode reward: -6.9500,                 loss: 0.8844
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 17301/30000 (57.6700%),                 avg. length: 2483.5,                last time consumption/overall running time: 267.1616s / 225362.2998 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.0712
env0_second_0:                 episode reward: -6.3000,                 loss: 0.7557
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 17321/30000 (57.7367%),                 avg. length: 2692.65,                last time consumption/overall running time: 283.8279s / 225646.1277 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.0746
env0_second_0:                 episode reward: -5.7000,                 loss: 0.6067
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 17341/30000 (57.8033%),                 avg. length: 3536.35,                last time consumption/overall running time: 366.0424s / 226012.1702 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.0641
env0_second_0:                 episode reward: -2.0500,                 loss: 0.7894
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 17361/30000 (57.8700%),                 avg. length: 3406.25,                last time consumption/overall running time: 362.7903s / 226374.9605 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.0550
env0_second_0:                 episode reward: -3.3500,                 loss: 0.8168
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 17381/30000 (57.9367%),                 avg. length: 2599.15,                last time consumption/overall running time: 281.5546s / 226656.5151 s
env0_first_0:                 episode reward: 7.5500,                 loss: -0.0786
env0_second_0:                 episode reward: -7.5500,                 loss: 0.6965
env1_first_0:                 episode reward: 8.2500,                 loss: nan
env1_second_0:                 episode reward: -8.2500,                 loss: nan
Episode: 17401/30000 (58.0033%),                 avg. length: 2811.0,                last time consumption/overall running time: 296.3023s / 226952.8174 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.0847
env0_second_0:                 episode reward: -5.6000,                 loss: 0.6489
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 17421/30000 (58.0700%),                 avg. length: 3713.9,                last time consumption/overall running time: 392.2610s / 227345.0784 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0487
env0_second_0:                 episode reward: -4.6000,                 loss: 0.5807
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 17441/30000 (58.1367%),                 avg. length: 3644.8,                last time consumption/overall running time: 386.4378s / 227731.5161 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0637
env0_second_0:                 episode reward: -4.5500,                 loss: 0.5776
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 17461/30000 (58.2033%),                 avg. length: 4182.95,                last time consumption/overall running time: 441.2817s / 228172.7978 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.0680
env0_second_0:                 episode reward: 1.6000,                 loss: 0.6072
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17481/30000 (58.2700%),                 avg. length: 2855.4,                last time consumption/overall running time: 311.0198s / 228483.8177 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.0348
env0_second_0:                 episode reward: -5.7000,                 loss: 0.8457
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 17501/30000 (58.3367%),                 avg. length: 2144.1,                last time consumption/overall running time: 238.2993s / 228722.1170 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.0904
env0_second_0:                 episode reward: -8.8000,                 loss: 1.8460
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 17521/30000 (58.4033%),                 avg. length: 2298.95,                last time consumption/overall running time: 245.8492s / 228967.9662 s
env0_first_0:                 episode reward: 8.9000,                 loss: -0.0160
env0_second_0:                 episode reward: -8.9000,                 loss: 1.0931
env1_first_0:                 episode reward: 8.9500,                 loss: nan
env1_second_0:                 episode reward: -8.9500,                 loss: nan
Episode: 17541/30000 (58.4700%),                 avg. length: 2387.75,                last time consumption/overall running time: 261.3736s / 229229.3398 s
env0_first_0:                 episode reward: 8.0000,                 loss: -0.0496
env0_second_0:                 episode reward: -8.0000,                 loss: 0.7928
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 17561/30000 (58.5367%),                 avg. length: 1871.9,                last time consumption/overall running time: 198.5736s / 229427.9134 s
env0_first_0:                 episode reward: 7.9000,                 loss: -0.0851
env0_second_0:                 episode reward: -7.9000,                 loss: 1.3212
env1_first_0:                 episode reward: 8.2500,                 loss: nan
env1_second_0:                 episode reward: -8.2500,                 loss: nan
Episode: 17581/30000 (58.6033%),                 avg. length: 2055.75,                last time consumption/overall running time: 217.5750s / 229645.4884 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.0775
env0_second_0:                 episode reward: -6.9500,                 loss: 0.4238
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 17601/30000 (58.6700%),                 avg. length: 2505.75,                last time consumption/overall running time: 266.9906s / 229912.4790 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.0321
env0_second_0:                 episode reward: -4.5000,                 loss: 0.4538
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 17621/30000 (58.7367%),                 avg. length: 2562.45,                last time consumption/overall running time: 270.8323s / 230183.3113 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.0424
env0_second_0:                 episode reward: -6.9500,                 loss: 0.5727
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 17641/30000 (58.8033%),                 avg. length: 2578.0,                last time consumption/overall running time: 275.6645s / 230458.9758 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.0871
env0_second_0:                 episode reward: -6.8000,                 loss: 0.7627
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 17661/30000 (58.8700%),                 avg. length: 2451.4,                last time consumption/overall running time: 253.9248s / 230712.9006 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0829
env0_second_0:                 episode reward: -6.4000,                 loss: 0.7074
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 17681/30000 (58.9367%),                 avg. length: 3935.2,                last time consumption/overall running time: 412.1254s / 231125.0260 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.0492
env0_second_0:                 episode reward: -2.0500,                 loss: 0.5719
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 17701/30000 (59.0033%),                 avg. length: 4110.35,                last time consumption/overall running time: 432.5696s / 231557.5956 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0521
env0_second_0:                 episode reward: 0.5000,                 loss: 0.4692
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 17721/30000 (59.0700%),                 avg. length: 3941.6,                last time consumption/overall running time: 416.7034s / 231974.2991 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0653
env0_second_0:                 episode reward: -4.0500,                 loss: 0.5281
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 17741/30000 (59.1367%),                 avg. length: 3714.2,                last time consumption/overall running time: 388.2097s / 232362.5088 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.0738
env0_second_0:                 episode reward: -4.0000,                 loss: 0.4711
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 17761/30000 (59.2033%),                 avg. length: 3438.9,                last time consumption/overall running time: 364.8930s / 232727.4018 s
env0_first_0:                 episode reward: 5.9500,                 loss: -0.0739
env0_second_0:                 episode reward: -5.9500,                 loss: 0.5777
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 17781/30000 (59.2700%),                 avg. length: 3989.35,                last time consumption/overall running time: 430.7937s / 233158.1955 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0576
env0_second_0:                 episode reward: -1.6500,                 loss: 0.9447
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 17801/30000 (59.3367%),                 avg. length: 3967.35,                last time consumption/overall running time: 419.1895s / 233577.3850 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.0749
env0_second_0:                 episode reward: -4.1500,                 loss: 0.8583
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 17821/30000 (59.4033%),                 avg. length: 3669.2,                last time consumption/overall running time: 377.2872s / 233954.6722 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.0573
env0_second_0:                 episode reward: -4.5000,                 loss: 0.7551
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 17841/30000 (59.4700%),                 avg. length: 3063.05,                last time consumption/overall running time: 316.8392s / 234271.5114 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.0682
env0_second_0:                 episode reward: -5.2500,                 loss: 0.8803
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 17861/30000 (59.5367%),                 avg. length: 3270.1,                last time consumption/overall running time: 382.9480s / 234654.4594 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0699
env0_second_0:                 episode reward: -4.8000,                 loss: 0.5538
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 17881/30000 (59.6033%),                 avg. length: 3230.8,                last time consumption/overall running time: 342.1795s / 234996.6388 s
env0_first_0:                 episode reward: 5.9500,                 loss: -0.0886
env0_second_0:                 episode reward: -5.9500,                 loss: 1.1046
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 17901/30000 (59.6700%),                 avg. length: 3349.0,                last time consumption/overall running time: 359.9310s / 235356.5698 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.0611
env0_second_0:                 episode reward: -3.6000,                 loss: 0.5765
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 17921/30000 (59.7367%),                 avg. length: 3029.3,                last time consumption/overall running time: 325.5223s / 235682.0921 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.0561
env0_second_0:                 episode reward: -5.1500,                 loss: 0.9812
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 17941/30000 (59.8033%),                 avg. length: 3315.3,                last time consumption/overall running time: 360.1229s / 236042.2150 s
env0_first_0:                 episode reward: 5.3500,                 loss: -0.0545
env0_second_0:                 episode reward: -5.3500,                 loss: 0.4349
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 17961/30000 (59.8700%),                 avg. length: 4044.05,                last time consumption/overall running time: 417.5268s / 236459.7418 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0647
env0_second_0:                 episode reward: -1.9500,                 loss: 0.4193
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 17981/30000 (59.9367%),                 avg. length: 2081.2,                last time consumption/overall running time: 218.8832s / 236678.6249 s
env0_first_0:                 episode reward: -4.8000,                 loss: -0.0485
env0_second_0:                 episode reward: 4.8000,                 loss: 0.3806
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 18001/30000 (60.0033%),                 avg. length: 913.3,                last time consumption/overall running time: 102.0897s / 236780.7146 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1059
env0_second_0:                 episode reward: 9.3000,                 loss: -0.0229
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 18021/30000 (60.0700%),                 avg. length: 938.3,                last time consumption/overall running time: 109.4866s / 236890.2012 s
env0_first_0:                 episode reward: -8.7000,                 loss: -0.1151
env0_second_0:                 episode reward: 8.7000,                 loss: 0.2180
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 18041/30000 (60.1367%),                 avg. length: 924.4,                last time consumption/overall running time: 104.1205s / 236994.3218 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1223
env0_second_0:                 episode reward: 8.7500,                 loss: 0.1097
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 18061/30000 (60.2033%),                 avg. length: 913.75,                last time consumption/overall running time: 107.9044s / 237102.2262 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.1319
env0_second_0:                 episode reward: 9.2500,                 loss: 0.0862
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 18081/30000 (60.2700%),                 avg. length: 916.5,                last time consumption/overall running time: 106.3202s / 237208.5464 s
env0_first_0:                 episode reward: -8.7000,                 loss: -0.1300
env0_second_0:                 episode reward: 8.7000,                 loss: 0.1420
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 18101/30000 (60.3367%),                 avg. length: 918.5,                last time consumption/overall running time: 101.6298s / 237310.1762 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.1472
env0_second_0:                 episode reward: 9.0500,                 loss: 0.6923
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 18121/30000 (60.4033%),                 avg. length: 910.95,                last time consumption/overall running time: 104.7386s / 237414.9148 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1360
env0_second_0:                 episode reward: 9.2000,                 loss: 0.4060
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 18141/30000 (60.4700%),                 avg. length: 910.55,                last time consumption/overall running time: 105.1465s / 237520.0613 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.1371
env0_second_0:                 episode reward: 9.4500,                 loss: 0.7049
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 18161/30000 (60.5367%),                 avg. length: 914.6,                last time consumption/overall running time: 107.4235s / 237627.4848 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.1009
env0_second_0:                 episode reward: 9.6500,                 loss: 0.8897
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 18181/30000 (60.6033%),                 avg. length: 909.55,                last time consumption/overall running time: 107.8063s / 237735.2911 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.1022
env0_second_0:                 episode reward: 9.1000,                 loss: 0.4572
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 18201/30000 (60.6700%),                 avg. length: 918.6,                last time consumption/overall running time: 104.8481s / 237840.1392 s
env0_first_0:                 episode reward: -8.9500,                 loss: -0.0933
env0_second_0:                 episode reward: 8.9500,                 loss: 0.5732
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 18221/30000 (60.7367%),                 avg. length: 957.2,                last time consumption/overall running time: 108.2691s / 237948.4083 s
env0_first_0:                 episode reward: -8.8000,                 loss: -0.0738
env0_second_0:                 episode reward: 8.8000,                 loss: 1.3648
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 18241/30000 (60.8033%),                 avg. length: 912.0,                last time consumption/overall running time: 102.7099s / 238051.1182 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1280
env0_second_0:                 episode reward: 9.3000,                 loss: 0.9861
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 18261/30000 (60.8700%),                 avg. length: 908.4,                last time consumption/overall running time: 101.1677s / 238152.2860 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1292
env0_second_0:                 episode reward: 9.3500,                 loss: 0.7619
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 18281/30000 (60.9367%),                 avg. length: 912.9,                last time consumption/overall running time: 102.5029s / 238254.7889 s
env0_first_0:                 episode reward: -8.8000,                 loss: -0.0966
env0_second_0:                 episode reward: 8.8000,                 loss: 0.6413
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 18301/30000 (61.0033%),                 avg. length: 916.8,                last time consumption/overall running time: 106.6128s / 238361.4017 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.0994
env0_second_0:                 episode reward: 9.3500,                 loss: 0.3259
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 18321/30000 (61.0700%),                 avg. length: 1009.3,                last time consumption/overall running time: 112.5798s / 238473.9815 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.0145
env0_second_0:                 episode reward: 7.4000,                 loss: 1.0971
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 18341/30000 (61.1367%),                 avg. length: 1576.4,                last time consumption/overall running time: 184.4677s / 238658.4492 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0960
env0_second_0:                 episode reward: -0.7500,                 loss: 1.2953
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 18361/30000 (61.2033%),                 avg. length: 1460.7,                last time consumption/overall running time: 159.3018s / 238817.7509 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0574
env0_second_0:                 episode reward: 4.9500,                 loss: 1.1924
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 18381/30000 (61.2700%),                 avg. length: 1677.2,                last time consumption/overall running time: 179.4705s / 238997.2214 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0111
env0_second_0:                 episode reward: 6.5000,                 loss: 1.4505
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 18401/30000 (61.3367%),                 avg. length: 1878.3,                last time consumption/overall running time: 201.1159s / 239198.3373 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0373
env0_second_0:                 episode reward: -0.2500,                 loss: 2.2920
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 18421/30000 (61.4033%),                 avg. length: 2319.75,                last time consumption/overall running time: 246.9476s / 239445.2849 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0224
env0_second_0:                 episode reward: -4.0500,                 loss: 1.7891
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 18441/30000 (61.4700%),                 avg. length: 2269.55,                last time consumption/overall running time: 242.3253s / 239687.6103 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0235
env0_second_0:                 episode reward: 1.3500,                 loss: 0.9821
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 18461/30000 (61.5367%),                 avg. length: 1530.25,                last time consumption/overall running time: 163.4897s / 239851.0999 s
env0_first_0:                 episode reward: -7.4500,                 loss: -0.0308
env0_second_0:                 episode reward: 7.4500,                 loss: 0.6728
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 18481/30000 (61.6033%),                 avg. length: 921.0,                last time consumption/overall running time: 102.2286s / 239953.3285 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0960
env0_second_0:                 episode reward: 9.5000,                 loss: 0.8418
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 18501/30000 (61.6700%),                 avg. length: 1004.15,                last time consumption/overall running time: 109.6254s / 240062.9539 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.0717
env0_second_0:                 episode reward: 8.3500,                 loss: 0.9924
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 18521/30000 (61.7367%),                 avg. length: 1816.75,                last time consumption/overall running time: 192.8680s / 240255.8219 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0612
env0_second_0:                 episode reward: -4.5500,                 loss: 1.1663
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 18541/30000 (61.8033%),                 avg. length: 1848.4,                last time consumption/overall running time: 197.7554s / 240453.5773 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0547
env0_second_0:                 episode reward: -3.3000,                 loss: 0.8235
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 18561/30000 (61.8700%),                 avg. length: 2535.15,                last time consumption/overall running time: 268.1506s / 240721.7279 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0295
env0_second_0:                 episode reward: 0.7500,                 loss: 0.8324
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18581/30000 (61.9367%),                 avg. length: 2776.5,                last time consumption/overall running time: 298.0225s / 241019.7505 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0042
env0_second_0:                 episode reward: 1.8500,                 loss: 0.8362
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 18601/30000 (62.0033%),                 avg. length: 2845.3,                last time consumption/overall running time: 290.0121s / 241309.7625 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0047
env0_second_0:                 episode reward: 0.7500,                 loss: 0.9414
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 18621/30000 (62.0700%),                 avg. length: 2760.65,                last time consumption/overall running time: 292.7381s / 241602.5007 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0353
env0_second_0:                 episode reward: 1.9500,                 loss: 0.7594
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 18641/30000 (62.1367%),                 avg. length: 2831.65,                last time consumption/overall running time: 297.2219s / 241899.7225 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0128
env0_second_0:                 episode reward: 1.1500,                 loss: 1.0237
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 18661/30000 (62.2033%),                 avg. length: 3145.0,                last time consumption/overall running time: 325.7377s / 242225.4602 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.0151
env0_second_0:                 episode reward: -1.5500,                 loss: 0.7895
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 18681/30000 (62.2700%),                 avg. length: 2785.75,                last time consumption/overall running time: 286.1374s / 242511.5976 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0070
env0_second_0:                 episode reward: -0.7500,                 loss: 0.6808
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 18701/30000 (62.3367%),                 avg. length: 2739.25,                last time consumption/overall running time: 294.5132s / 242806.1109 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.0253
env0_second_0:                 episode reward: -2.1500,                 loss: 1.7631
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 18721/30000 (62.4033%),                 avg. length: 2525.65,                last time consumption/overall running time: 271.8025s / 243077.9133 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.0129
env0_second_0:                 episode reward: 2.6500,                 loss: 0.8928
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 18741/30000 (62.4700%),                 avg. length: 3088.4,                last time consumption/overall running time: 319.0650s / 243396.9784 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.0338
env0_second_0:                 episode reward: -1.8000,                 loss: 0.9379
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 18761/30000 (62.5367%),                 avg. length: 2857.6,                last time consumption/overall running time: 312.4877s / 243709.4661 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.0112
env0_second_0:                 episode reward: -2.3500,                 loss: 1.0344
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 18781/30000 (62.6033%),                 avg. length: 3022.35,                last time consumption/overall running time: 317.4078s / 244026.8738 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.0171
env0_second_0:                 episode reward: -3.1000,                 loss: 1.2259
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 18801/30000 (62.6700%),                 avg. length: 3033.1,                last time consumption/overall running time: 335.6569s / 244362.5307 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.0237
env0_second_0:                 episode reward: -1.8000,                 loss: 1.0761
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 18821/30000 (62.7367%),                 avg. length: 3239.0,                last time consumption/overall running time: 337.5760s / 244700.1067 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.0333
env0_second_0:                 episode reward: 1.5500,                 loss: 2.2924
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 18841/30000 (62.8033%),                 avg. length: 3153.4,                last time consumption/overall running time: 331.4702s / 245031.5769 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0070
env0_second_0:                 episode reward: 0.5000,                 loss: 1.5018
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 18861/30000 (62.8700%),                 avg. length: 3220.9,                last time consumption/overall running time: 346.0462s / 245377.6231 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0162
env0_second_0:                 episode reward: 0.5000,                 loss: 3.1614
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18881/30000 (62.9367%),                 avg. length: 2270.45,                last time consumption/overall running time: 246.3928s / 245624.0159 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0044
env0_second_0:                 episode reward: 4.5500,                 loss: 1.8039
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 18901/30000 (63.0033%),                 avg. length: 3041.05,                last time consumption/overall running time: 335.7256s / 245959.7415 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.0272
env0_second_0:                 episode reward: 3.2000,                 loss: 1.3556
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 18921/30000 (63.0700%),                 avg. length: 3523.7,                last time consumption/overall running time: 370.3206s / 246330.0621 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0206
env0_second_0:                 episode reward: 0.2500,                 loss: 1.1125
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 18941/30000 (63.1367%),                 avg. length: 3202.75,                last time consumption/overall running time: 339.9072s / 246669.9693 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.0157
env0_second_0:                 episode reward: -1.7000,                 loss: 0.9042
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 18961/30000 (63.2033%),                 avg. length: 3294.55,                last time consumption/overall running time: 351.9110s / 247021.8803 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0162
env0_second_0:                 episode reward: -0.6000,                 loss: 1.0899
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 18981/30000 (63.2700%),                 avg. length: 2734.7,                last time consumption/overall running time: 296.2359s / 247318.1162 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0364
env0_second_0:                 episode reward: -2.8500,                 loss: 1.2573
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 19001/30000 (63.3367%),                 avg. length: 2963.55,                last time consumption/overall running time: 313.1732s / 247631.2894 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.0266
env0_second_0:                 episode reward: -1.4500,                 loss: 1.1117
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 19021/30000 (63.4033%),                 avg. length: 3301.4,                last time consumption/overall running time: 340.6228s / 247971.9122 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.0316
env0_second_0:                 episode reward: 1.4000,                 loss: 1.2174
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 19041/30000 (63.4700%),                 avg. length: 3363.5,                last time consumption/overall running time: 361.7621s / 248333.6743 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.0323
env0_second_0:                 episode reward: -0.5500,                 loss: 0.8255
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 19061/30000 (63.5367%),                 avg. length: 2891.9,                last time consumption/overall running time: 299.1781s / 248632.8525 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.0385
env0_second_0:                 episode reward: 4.0500,                 loss: 0.7619
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 19081/30000 (63.6033%),                 avg. length: 3317.65,                last time consumption/overall running time: 341.3176s / 248974.1700 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0344
env0_second_0:                 episode reward: 0.9500,                 loss: 1.0616
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 19101/30000 (63.6700%),                 avg. length: 3149.0,                last time consumption/overall running time: 320.3126s / 249294.4826 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.0565
env0_second_0:                 episode reward: 3.3500,                 loss: 1.1487
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 19121/30000 (63.7367%),                 avg. length: 3361.5,                last time consumption/overall running time: 336.5919s / 249631.0745 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0529
env0_second_0:                 episode reward: 4.0000,                 loss: 0.7617
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 19141/30000 (63.8033%),                 avg. length: 3255.8,                last time consumption/overall running time: 334.7395s / 249965.8139 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.0173
env0_second_0:                 episode reward: 0.6000,                 loss: 0.9937
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 19161/30000 (63.8700%),                 avg. length: 3423.8,                last time consumption/overall running time: 353.4663s / 250319.2802 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.0389
env0_second_0:                 episode reward: 1.7000,                 loss: 1.1969
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 19181/30000 (63.9367%),                 avg. length: 3145.05,                last time consumption/overall running time: 325.1169s / 250644.3972 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0295
env0_second_0:                 episode reward: 0.9500,                 loss: 0.7187
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 19201/30000 (64.0033%),                 avg. length: 2978.05,                last time consumption/overall running time: 301.4860s / 250945.8831 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.0000
env0_second_0:                 episode reward: -1.8500,                 loss: 1.7541
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 19221/30000 (64.0700%),                 avg. length: 3442.05,                last time consumption/overall running time: 349.1897s / 251295.0729 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.0363
env0_second_0:                 episode reward: 0.5500,                 loss: 0.7777
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 19241/30000 (64.1367%),                 avg. length: 2914.35,                last time consumption/overall running time: 316.2289s / 251611.3018 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.0155
env0_second_0:                 episode reward: -2.9000,                 loss: 0.6243
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 19261/30000 (64.2033%),                 avg. length: 3029.9,                last time consumption/overall running time: 318.2682s / 251929.5700 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.0191
env0_second_0:                 episode reward: -3.0000,                 loss: 0.6064
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 19281/30000 (64.2700%),                 avg. length: 2948.9,                last time consumption/overall running time: 303.9410s / 252233.5110 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.0167
env0_second_0:                 episode reward: -3.5500,                 loss: 0.7307
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 19301/30000 (64.3367%),                 avg. length: 2223.8,                last time consumption/overall running time: 240.9730s / 252474.4840 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0328
env0_second_0:                 episode reward: -4.8000,                 loss: 0.6525
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 19321/30000 (64.4033%),                 avg. length: 2422.3,                last time consumption/overall running time: 255.4790s / 252729.9630 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0510
env0_second_0:                 episode reward: -4.6000,                 loss: 0.8579
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 19341/30000 (64.4700%),                 avg. length: 2624.6,                last time consumption/overall running time: 287.0581s / 253017.0210 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0362
env0_second_0:                 episode reward: -2.0000,                 loss: 1.6084
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 19361/30000 (64.5367%),                 avg. length: 2705.95,                last time consumption/overall running time: 297.0938s / 253314.1149 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0428
env0_second_0:                 episode reward: -0.1500,                 loss: 0.6265
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 19381/30000 (64.6033%),                 avg. length: 2049.4,                last time consumption/overall running time: 212.4576s / 253526.5725 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0378
env0_second_0:                 episode reward: 2.1500,                 loss: 0.9391
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 19401/30000 (64.6700%),                 avg. length: 2256.6,                last time consumption/overall running time: 232.4327s / 253759.0052 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.0299
env0_second_0:                 episode reward: -1.8000,                 loss: 0.8071
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 19421/30000 (64.7367%),                 avg. length: 2996.95,                last time consumption/overall running time: 314.4468s / 254073.4520 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.0701
env0_second_0:                 episode reward: -2.0500,                 loss: 0.5015
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 19441/30000 (64.8033%),                 avg. length: 2996.3,                last time consumption/overall running time: 307.2945s / 254380.7464 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0672
env0_second_0:                 episode reward: -1.0500,                 loss: 0.4831
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 19461/30000 (64.8700%),                 avg. length: 2935.8,                last time consumption/overall running time: 316.6600s / 254697.4065 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.0314
env0_second_0:                 episode reward: 1.5000,                 loss: 0.4879
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 19481/30000 (64.9367%),                 avg. length: 3047.65,                last time consumption/overall running time: 320.7860s / 255018.1925 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0292
env0_second_0:                 episode reward: -0.8000,                 loss: 0.3126
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 19501/30000 (65.0033%),                 avg. length: 3237.15,                last time consumption/overall running time: 336.1693s / 255354.3617 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0316
env0_second_0:                 episode reward: 0.7500,                 loss: 0.6448
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 19521/30000 (65.0700%),                 avg. length: 2559.2,                last time consumption/overall running time: 276.8903s / 255631.2521 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0239
env0_second_0:                 episode reward: 0.0500,                 loss: 0.4328
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 19541/30000 (65.1367%),                 avg. length: 1810.95,                last time consumption/overall running time: 183.3925s / 255814.6445 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.0245
env0_second_0:                 episode reward: 0.1000,                 loss: 0.4667
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19561/30000 (65.2033%),                 avg. length: 1907.85,                last time consumption/overall running time: 197.8613s / 256012.5058 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.0151
env0_second_0:                 episode reward: 3.5500,                 loss: 0.5775
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 19581/30000 (65.2700%),                 avg. length: 2190.75,                last time consumption/overall running time: 224.9183s / 256237.4242 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0403
env0_second_0:                 episode reward: -3.8500,                 loss: 0.8024
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 19601/30000 (65.3367%),                 avg. length: 1913.55,                last time consumption/overall running time: 206.0465s / 256443.4707 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0162
env0_second_0:                 episode reward: 3.1500,                 loss: 0.9152
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 19621/30000 (65.4033%),                 avg. length: 2141.4,                last time consumption/overall running time: 229.9830s / 256673.4537 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.0202
env0_second_0:                 episode reward: 3.4500,                 loss: 0.6494
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 19641/30000 (65.4700%),                 avg. length: 2247.1,                last time consumption/overall running time: 231.7036s / 256905.1573 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.0140
env0_second_0:                 episode reward: 1.5500,                 loss: 0.3323
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 19661/30000 (65.5367%),                 avg. length: 1845.6,                last time consumption/overall running time: 199.6289s / 257104.7862 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.0216
env0_second_0:                 episode reward: -1.3500,                 loss: 0.5592
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 19681/30000 (65.6033%),                 avg. length: 1827.55,                last time consumption/overall running time: 190.1865s / 257294.9727 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0244
env0_second_0:                 episode reward: 3.2500,                 loss: 0.6944
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 19701/30000 (65.6700%),                 avg. length: 1571.45,                last time consumption/overall running time: 163.2909s / 257458.2636 s
env0_first_0:                 episode reward: -7.7000,                 loss: -0.0186
env0_second_0:                 episode reward: 7.7000,                 loss: 0.9597
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 19721/30000 (65.7367%),                 avg. length: 1644.45,                last time consumption/overall running time: 169.6913s / 257627.9550 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.0024
env0_second_0:                 episode reward: 7.3500,                 loss: 1.4485
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 19741/30000 (65.8033%),                 avg. length: 2080.8,                last time consumption/overall running time: 210.7578s / 257838.7127 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0108
env0_second_0:                 episode reward: 1.7000,                 loss: 1.7226
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 19761/30000 (65.8700%),                 avg. length: 2091.9,                last time consumption/overall running time: 215.1639s / 258053.8767 s
env0_first_0:                 episode reward: -7.4500,                 loss: -0.0281
env0_second_0:                 episode reward: 7.4500,                 loss: 1.8045
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 19781/30000 (65.9367%),                 avg. length: 2090.15,                last time consumption/overall running time: 219.5432s / 258273.4199 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.0359
env0_second_0:                 episode reward: 7.0000,                 loss: 2.1559
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 19801/30000 (66.0033%),                 avg. length: 1885.45,                last time consumption/overall running time: 198.5765s / 258471.9964 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.0713
env0_second_0:                 episode reward: 8.2000,                 loss: 2.9053
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 19821/30000 (66.0700%),                 avg. length: 1645.45,                last time consumption/overall running time: 170.4441s / 258642.4405 s
env0_first_0:                 episode reward: -7.4500,                 loss: -0.0683
env0_second_0:                 episode reward: 7.4500,                 loss: 1.3761
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 19841/30000 (66.1367%),                 avg. length: 2139.5,                last time consumption/overall running time: 221.0854s / 258863.5259 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.0606
env0_second_0:                 episode reward: 6.7000,                 loss: 2.1254
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 19861/30000 (66.2033%),                 avg. length: 2133.95,                last time consumption/overall running time: 222.9326s / 259086.4585 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0572
env0_second_0:                 episode reward: 7.8500,                 loss: 2.0461
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 19881/30000 (66.2700%),                 avg. length: 2102.35,                last time consumption/overall running time: 218.7205s / 259305.1790 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.0792
env0_second_0:                 episode reward: 8.3500,                 loss: 0.6638
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 19901/30000 (66.3367%),                 avg. length: 2114.65,                last time consumption/overall running time: 221.1201s / 259526.2991 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.0440
env0_second_0:                 episode reward: 7.3000,                 loss: 0.6839
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 19921/30000 (66.4033%),                 avg. length: 2095.35,                last time consumption/overall running time: 215.7739s / 259742.0729 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.0687
env0_second_0:                 episode reward: 8.6500,                 loss: 0.6365
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 19941/30000 (66.4700%),                 avg. length: 2055.1,                last time consumption/overall running time: 210.8092s / 259952.8821 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.0608
env0_second_0:                 episode reward: 9.2000,                 loss: 0.7218
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 19961/30000 (66.5367%),                 avg. length: 1881.7,                last time consumption/overall running time: 193.9990s / 260146.8811 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.0151
env0_second_0:                 episode reward: 8.0000,                 loss: 0.9223
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 19981/30000 (66.6033%),                 avg. length: 2048.9,                last time consumption/overall running time: 208.7580s / 260355.6391 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0741
env0_second_0:                 episode reward: 9.4000,                 loss: 0.7972
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 20001/30000 (66.6700%),                 avg. length: 2053.35,                last time consumption/overall running time: 211.0708s / 260566.7099 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0825
env0_second_0:                 episode reward: 9.4000,                 loss: 0.7824
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 20021/30000 (66.7367%),                 avg. length: 2100.65,                last time consumption/overall running time: 216.2880s / 260782.9980 s
env0_first_0:                 episode reward: -8.0500,                 loss: -0.0318
env0_second_0:                 episode reward: 8.0500,                 loss: 0.7914
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 20041/30000 (66.8033%),                 avg. length: 2113.0,                last time consumption/overall running time: 216.7304s / 260999.7283 s
env0_first_0:                 episode reward: -8.7000,                 loss: -0.0377
env0_second_0:                 episode reward: 8.7000,                 loss: 4.1121
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 20061/30000 (66.8700%),                 avg. length: 1883.1,                last time consumption/overall running time: 206.3049s / 261206.0332 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.0264
env0_second_0:                 episode reward: -2.1000,                 loss: 1.1505
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 20081/30000 (66.9367%),                 avg. length: 1962.1,                last time consumption/overall running time: 213.4417s / 261419.4749 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0286
env0_second_0:                 episode reward: 8.4000,                 loss: 1.3211
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 20101/30000 (67.0033%),                 avg. length: 2481.1,                last time consumption/overall running time: 253.8347s / 261673.3096 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0006
env0_second_0:                 episode reward: -2.0500,                 loss: 0.5339
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 20121/30000 (67.0700%),                 avg. length: 2362.5,                last time consumption/overall running time: 243.4766s / 261916.7861 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.0377
env0_second_0:                 episode reward: 6.4000,                 loss: 0.8936
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 20141/30000 (67.1367%),                 avg. length: 2167.55,                last time consumption/overall running time: 220.6327s / 262137.4189 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0066
env0_second_0:                 episode reward: 8.4000,                 loss: 0.7951
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 20161/30000 (67.2033%),                 avg. length: 1792.9,                last time consumption/overall running time: 186.4037s / 262323.8225 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0047
env0_second_0:                 episode reward: 0.2500,                 loss: 1.0073
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 20181/30000 (67.2700%),                 avg. length: 1850.3,                last time consumption/overall running time: 190.0061s / 262513.8286 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0040
env0_second_0:                 episode reward: -3.6500,                 loss: 0.8000
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 20201/30000 (67.3367%),                 avg. length: 1987.3,                last time consumption/overall running time: 216.3108s / 262730.1395 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0238
env0_second_0:                 episode reward: 5.9500,                 loss: 0.5713
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 20221/30000 (67.4033%),                 avg. length: 1934.4,                last time consumption/overall running time: 204.0220s / 262934.1615 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.0125
env0_second_0:                 episode reward: -0.6500,                 loss: 0.5708
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 20241/30000 (67.4700%),                 avg. length: 2274.5,                last time consumption/overall running time: 242.6683s / 263176.8297 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.0445
env0_second_0:                 episode reward: 2.4000,                 loss: 0.4356
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 20261/30000 (67.5367%),                 avg. length: 2015.9,                last time consumption/overall running time: 214.2312s / 263391.0609 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.0571
env0_second_0:                 episode reward: 6.0000,                 loss: 0.4537
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 20281/30000 (67.6033%),                 avg. length: 2330.4,                last time consumption/overall running time: 248.0382s / 263639.0991 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0477
env0_second_0:                 episode reward: 3.7000,                 loss: 0.6378
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 20301/30000 (67.6700%),                 avg. length: 2541.9,                last time consumption/overall running time: 264.0090s / 263903.1080 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.0403
env0_second_0:                 episode reward: 2.4000,                 loss: 0.5456
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 20321/30000 (67.7367%),                 avg. length: 2248.4,                last time consumption/overall running time: 233.4399s / 264136.5480 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.0287
env0_second_0:                 episode reward: -2.4000,                 loss: 0.6166
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 20341/30000 (67.8033%),                 avg. length: 2536.3,                last time consumption/overall running time: 263.2065s / 264399.7545 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.0384
env0_second_0:                 episode reward: -0.8500,                 loss: 0.5915
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 20361/30000 (67.8700%),                 avg. length: 2725.9,                last time consumption/overall running time: 288.3516s / 264688.1061 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.0202
env0_second_0:                 episode reward: -1.1500,                 loss: 0.4968
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 20381/30000 (67.9367%),                 avg. length: 1880.15,                last time consumption/overall running time: 197.7214s / 264885.8275 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0401
env0_second_0:                 episode reward: 1.3500,                 loss: 0.7471
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 20401/30000 (68.0033%),                 avg. length: 2826.1,                last time consumption/overall running time: 295.1477s / 265180.9752 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0411
env0_second_0:                 episode reward: 2.2500,                 loss: 0.5549
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 20421/30000 (68.0700%),                 avg. length: 2314.1,                last time consumption/overall running time: 243.9436s / 265424.9188 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.0658
env0_second_0:                 episode reward: 7.0000,                 loss: 0.4109
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 20441/30000 (68.1367%),                 avg. length: 2590.7,                last time consumption/overall running time: 272.9884s / 265697.9072 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.0417
env0_second_0:                 episode reward: 5.3000,                 loss: 0.6675
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 20461/30000 (68.2033%),                 avg. length: 2346.75,                last time consumption/overall running time: 244.8501s / 265942.7573 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.0315
env0_second_0:                 episode reward: 5.1500,                 loss: 1.1559
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 20481/30000 (68.2700%),                 avg. length: 1894.85,                last time consumption/overall running time: 198.2265s / 266140.9838 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.0167
env0_second_0:                 episode reward: 7.0000,                 loss: 0.4285
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 20501/30000 (68.3367%),                 avg. length: 2731.65,                last time consumption/overall running time: 283.2372s / 266424.2210 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.0134
env0_second_0:                 episode reward: 2.0000,                 loss: 0.4654
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 20521/30000 (68.4033%),                 avg. length: 2289.95,                last time consumption/overall running time: 236.7471s / 266660.9681 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.0622
env0_second_0:                 episode reward: 7.5500,                 loss: 0.4791
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 20541/30000 (68.4700%),                 avg. length: 2347.8,                last time consumption/overall running time: 242.8047s / 266903.7728 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.0790
env0_second_0:                 episode reward: 7.4000,                 loss: 0.3507
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 20561/30000 (68.5367%),                 avg. length: 2655.9,                last time consumption/overall running time: 288.8975s / 267192.6703 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.0439
env0_second_0:                 episode reward: 4.6500,                 loss: 0.3445
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 20581/30000 (68.6033%),                 avg. length: 2193.1,                last time consumption/overall running time: 234.7121s / 267427.3824 s
env0_first_0:                 episode reward: -7.8500,                 loss: -0.0351
env0_second_0:                 episode reward: 7.8500,                 loss: 0.5464
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 20601/30000 (68.6700%),                 avg. length: 1806.55,                last time consumption/overall running time: 188.0884s / 267615.4707 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.0222
env0_second_0:                 episode reward: -1.1000,                 loss: 0.5946
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 20621/30000 (68.7367%),                 avg. length: 1910.55,                last time consumption/overall running time: 198.0421s / 267813.5128 s
env0_first_0:                 episode reward: -6.4500,                 loss: -0.0255
env0_second_0:                 episode reward: 6.4500,                 loss: 0.6621
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 20641/30000 (68.8033%),                 avg. length: 2147.85,                last time consumption/overall running time: 232.6650s / 268046.1779 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0176
env0_second_0:                 episode reward: -0.9500,                 loss: 0.5571
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 20661/30000 (68.8700%),                 avg. length: 2886.55,                last time consumption/overall running time: 298.9024s / 268345.0803 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.0443
env0_second_0:                 episode reward: 3.4500,                 loss: 0.3320
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 20681/30000 (68.9367%),                 avg. length: 2604.6,                last time consumption/overall running time: 269.0793s / 268614.1596 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0607
env0_second_0:                 episode reward: 5.8000,                 loss: 0.3095
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 20701/30000 (69.0033%),                 avg. length: 2330.0,                last time consumption/overall running time: 238.7041s / 268852.8637 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.1011
env0_second_0:                 episode reward: 8.0000,                 loss: 0.2979
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 20721/30000 (69.0700%),                 avg. length: 2318.2,                last time consumption/overall running time: 241.3042s / 269094.1679 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.1046
env0_second_0:                 episode reward: 9.1500,                 loss: 0.2943
env1_first_0:                 episode reward: -8.2500,                 loss: nan
env1_second_0:                 episode reward: 8.2500,                 loss: nan
Episode: 20741/30000 (69.1367%),                 avg. length: 2311.0,                last time consumption/overall running time: 244.4963s / 269338.6643 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.0906
env0_second_0:                 episode reward: 9.1500,                 loss: 0.2242
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 20761/30000 (69.2033%),                 avg. length: 2278.55,                last time consumption/overall running time: 232.6894s / 269571.3537 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.0798
env0_second_0:                 episode reward: 8.8500,                 loss: 0.3518
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 20781/30000 (69.2700%),                 avg. length: 2289.05,                last time consumption/overall running time: 233.7973s / 269805.1509 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0226
env0_second_0:                 episode reward: 4.6000,                 loss: 0.6551
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 20801/30000 (69.3367%),                 avg. length: 2349.35,                last time consumption/overall running time: 255.4719s / 270060.6228 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.0826
env0_second_0:                 episode reward: 7.5500,                 loss: 1.7266
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 20821/30000 (69.4033%),                 avg. length: 2307.65,                last time consumption/overall running time: 237.6501s / 270298.2730 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.1131
env0_second_0:                 episode reward: 9.4500,                 loss: 1.1935
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 20841/30000 (69.4700%),                 avg. length: 2303.55,                last time consumption/overall running time: 249.8069s / 270548.0799 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.0986
env0_second_0:                 episode reward: 9.1000,                 loss: 1.3130
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 20861/30000 (69.5367%),                 avg. length: 1398.2,                last time consumption/overall running time: 151.7586s / 270699.8385 s
env0_first_0:                 episode reward: -8.4500,                 loss: -0.0865
env0_second_0:                 episode reward: 8.4500,                 loss: 1.0143
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 20881/30000 (69.6033%),                 avg. length: 913.4,                last time consumption/overall running time: 99.3437s / 270799.1822 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.0986
env0_second_0:                 episode reward: 9.7500,                 loss: -0.0239
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 20901/30000 (69.6700%),                 avg. length: 914.4,                last time consumption/overall running time: 97.2984s / 270896.4807 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.1126
env0_second_0:                 episode reward: 9.6500,                 loss: 0.0226
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 20921/30000 (69.7367%),                 avg. length: 909.6,                last time consumption/overall running time: 98.2054s / 270994.6860 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.1155
env0_second_0:                 episode reward: 9.0500,                 loss: 0.1481
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 20941/30000 (69.8033%),                 avg. length: 908.6,                last time consumption/overall running time: 97.9172s / 271092.6033 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1193
env0_second_0:                 episode reward: 9.5500,                 loss: -0.0616
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 20961/30000 (69.8700%),                 avg. length: 1050.55,                last time consumption/overall running time: 112.1668s / 271204.7700 s
env0_first_0:                 episode reward: -8.2500,                 loss: -0.0851
env0_second_0:                 episode reward: 8.2500,                 loss: 0.2667
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 20981/30000 (69.9367%),                 avg. length: 1111.4,                last time consumption/overall running time: 118.3872s / 271323.1573 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.0485
env0_second_0:                 episode reward: 9.3000,                 loss: 0.1525
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 21001/30000 (70.0033%),                 avg. length: 908.75,                last time consumption/overall running time: 98.3467s / 271421.5040 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1168
env0_second_0:                 episode reward: 9.6000,                 loss: -0.0235
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 21021/30000 (70.0700%),                 avg. length: 910.2,                last time consumption/overall running time: 97.6070s / 271519.1109 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.1087
env0_second_0:                 episode reward: 8.5500,                 loss: 0.0139
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 21041/30000 (70.1367%),                 avg. length: 921.2,                last time consumption/overall running time: 99.4224s / 271618.5334 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1188
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0112
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 21061/30000 (70.2033%),                 avg. length: 907.95,                last time consumption/overall running time: 97.2720s / 271715.8054 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1035
env0_second_0:                 episode reward: 9.7000,                 loss: 0.0630
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 21081/30000 (70.2700%),                 avg. length: 924.55,                last time consumption/overall running time: 99.2743s / 271815.0797 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.0914
env0_second_0:                 episode reward: 9.0000,                 loss: 0.1576
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 21101/30000 (70.3367%),                 avg. length: 922.2,                last time consumption/overall running time: 98.7588s / 271913.8385 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.0937
env0_second_0:                 episode reward: 9.0500,                 loss: 0.1478
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 21121/30000 (70.4033%),                 avg. length: 910.0,                last time consumption/overall running time: 98.4554s / 272012.2939 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1051
env0_second_0:                 episode reward: 9.2000,                 loss: 0.2790
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 21141/30000 (70.4700%),                 avg. length: 924.4,                last time consumption/overall running time: 100.0580s / 272112.3519 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.1130
env0_second_0:                 episode reward: 9.6500,                 loss: 0.5159
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 21161/30000 (70.5367%),                 avg. length: 921.6,                last time consumption/overall running time: 99.5752s / 272211.9271 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.1201
env0_second_0:                 episode reward: 9.7500,                 loss: 0.2372
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 21181/30000 (70.6033%),                 avg. length: 936.15,                last time consumption/overall running time: 100.6528s / 272312.5799 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.1111
env0_second_0:                 episode reward: 9.5000,                 loss: 0.2062
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 21201/30000 (70.6700%),                 avg. length: 925.25,                last time consumption/overall running time: 99.9513s / 272412.5312 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.0942
env0_second_0:                 episode reward: 9.3500,                 loss: 0.1649
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 21221/30000 (70.7367%),                 avg. length: 915.75,                last time consumption/overall running time: 99.4426s / 272511.9738 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.0863
env0_second_0:                 episode reward: 9.4000,                 loss: 0.1620
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 21241/30000 (70.8033%),                 avg. length: 926.2,                last time consumption/overall running time: 97.7786s / 272609.7523 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.0757
env0_second_0:                 episode reward: 9.1500,                 loss: 0.3876
env1_first_0:                 episode reward: -8.2500,                 loss: nan
env1_second_0:                 episode reward: 8.2500,                 loss: nan
Episode: 21261/30000 (70.8700%),                 avg. length: 910.6,                last time consumption/overall running time: 97.1770s / 272706.9293 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.0812
env0_second_0:                 episode reward: 9.5000,                 loss: 0.5624
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 21281/30000 (70.9367%),                 avg. length: 919.8,                last time consumption/overall running time: 105.8514s / 272812.7807 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.0924
env0_second_0:                 episode reward: 9.3500,                 loss: 0.1099
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 21301/30000 (71.0033%),                 avg. length: 966.25,                last time consumption/overall running time: 108.1049s / 272920.8856 s
env0_first_0:                 episode reward: -7.8000,                 loss: -0.0441
env0_second_0:                 episode reward: 7.8000,                 loss: 0.4662
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 21321/30000 (71.0700%),                 avg. length: 2028.1,                last time consumption/overall running time: 206.3792s / 273127.2648 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0388
env0_second_0:                 episode reward: 0.3500,                 loss: 2.0144
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21341/30000 (71.1367%),                 avg. length: 1737.6,                last time consumption/overall running time: 177.8629s / 273305.1277 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0154
env0_second_0:                 episode reward: 6.1500,                 loss: 1.9601
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 21361/30000 (71.2033%),                 avg. length: 2435.6,                last time consumption/overall running time: 246.9380s / 273552.0656 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0037
env0_second_0:                 episode reward: -1.6500,                 loss: 1.0146
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 21381/30000 (71.2700%),                 avg. length: 2427.1,                last time consumption/overall running time: 249.2418s / 273801.3075 s
env0_first_0:                 episode reward: -4.8000,                 loss: -0.0126
env0_second_0:                 episode reward: 4.8000,                 loss: 2.7878
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 21401/30000 (71.3367%),                 avg. length: 2382.8,                last time consumption/overall running time: 242.2897s / 274043.5972 s
env0_first_0:                 episode reward: -5.6000,                 loss: -0.0349
env0_second_0:                 episode reward: 5.6000,                 loss: 1.1546
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 21421/30000 (71.4033%),                 avg. length: 2452.05,                last time consumption/overall running time: 250.3932s / 274293.9904 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.0001
env0_second_0:                 episode reward: 3.3500,                 loss: 0.9834
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 21441/30000 (71.4700%),                 avg. length: 2572.7,                last time consumption/overall running time: 267.8871s / 274561.8774 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.0360
env0_second_0:                 episode reward: 2.7000,                 loss: 0.9917
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 21461/30000 (71.5367%),                 avg. length: 2539.05,                last time consumption/overall running time: 259.8790s / 274821.7565 s
env0_first_0:                 episode reward: -6.4000,                 loss: -0.0503
env0_second_0:                 episode reward: 6.4000,                 loss: 1.3986
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 21481/30000 (71.6033%),                 avg. length: 2933.65,                last time consumption/overall running time: 301.7688s / 275123.5253 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.0591
env0_second_0:                 episode reward: 1.1500,                 loss: 1.2399
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 21501/30000 (71.6700%),                 avg. length: 2677.0,                last time consumption/overall running time: 267.4494s / 275390.9747 s
env0_first_0:                 episode reward: -6.6500,                 loss: -0.0963
env0_second_0:                 episode reward: 6.6500,                 loss: 1.0668
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 21521/30000 (71.7367%),                 avg. length: 2572.0,                last time consumption/overall running time: 260.9951s / 275651.9698 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.1048
env0_second_0:                 episode reward: 8.6000,                 loss: 0.5623
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 21541/30000 (71.8033%),                 avg. length: 2394.95,                last time consumption/overall running time: 238.8053s / 275890.7751 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0714
env0_second_0:                 episode reward: 1.8500,                 loss: 0.6489
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 21561/30000 (71.8700%),                 avg. length: 2514.4,                last time consumption/overall running time: 241.1996s / 276131.9748 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.0895
env0_second_0:                 episode reward: 4.5000,                 loss: 0.9078
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 21581/30000 (71.9367%),                 avg. length: 2762.0,                last time consumption/overall running time: 268.3884s / 276400.3631 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.0897
env0_second_0:                 episode reward: 6.2500,                 loss: 0.6994
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 21601/30000 (72.0033%),                 avg. length: 2642.0,                last time consumption/overall running time: 264.7802s / 276665.1433 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.0962
env0_second_0:                 episode reward: 6.3000,                 loss: 0.5915
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 21621/30000 (72.0700%),                 avg. length: 2508.1,                last time consumption/overall running time: 246.1664s / 276911.3097 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.0988
env0_second_0:                 episode reward: 7.7500,                 loss: 0.5739
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 21641/30000 (72.1367%),                 avg. length: 2620.85,                last time consumption/overall running time: 276.7049s / 277188.0147 s
env0_first_0:                 episode reward: -8.0500,                 loss: -0.0956
env0_second_0:                 episode reward: 8.0500,                 loss: 0.3933
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 21661/30000 (72.2033%),                 avg. length: 2913.35,                last time consumption/overall running time: 320.1569s / 277508.1716 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.0554
env0_second_0:                 episode reward: 3.2500,                 loss: 0.6695
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 21681/30000 (72.2700%),                 avg. length: 2176.7,                last time consumption/overall running time: 221.1709s / 277729.3425 s
env0_first_0:                 episode reward: -7.4500,                 loss: -0.0784
env0_second_0:                 episode reward: 7.4500,                 loss: 0.6050
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 21701/30000 (72.3367%),                 avg. length: 2567.2,                last time consumption/overall running time: 250.1155s / 277979.4580 s
env0_first_0:                 episode reward: -7.2500,                 loss: -0.0741
env0_second_0:                 episode reward: 7.2500,                 loss: 0.8436
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 21721/30000 (72.4033%),                 avg. length: 2071.0,                last time consumption/overall running time: 202.6250s / 278182.0830 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0780
env0_second_0:                 episode reward: -0.2500,                 loss: 0.4200
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 21741/30000 (72.4700%),                 avg. length: 948.7,                last time consumption/overall running time: 97.0581s / 278279.1412 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0997
env0_second_0:                 episode reward: -9.5000,                 loss: 0.2358
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 21761/30000 (72.5367%),                 avg. length: 981.05,                last time consumption/overall running time: 99.7233s / 278378.8644 s
env0_first_0:                 episode reward: 8.6500,                 loss: -0.0597
env0_second_0:                 episode reward: -8.6500,                 loss: 0.1791
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 21781/30000 (72.6033%),                 avg. length: 949.4,                last time consumption/overall running time: 97.9850s / 278476.8495 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.0801
env0_second_0:                 episode reward: -8.8000,                 loss: 0.1674
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 21801/30000 (72.6700%),                 avg. length: 960.95,                last time consumption/overall running time: 105.4389s / 278582.2884 s
env0_first_0:                 episode reward: 9.3000,                 loss: 0.0023
env0_second_0:                 episode reward: -9.3000,                 loss: 0.2003
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 21821/30000 (72.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 103.3708s / 278685.6592 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0335
env0_second_0:                 episode reward: -9.8500,                 loss: 0.5948
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 21841/30000 (72.8033%),                 avg. length: 943.8,                last time consumption/overall running time: 102.2178s / 278787.8770 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0487
env0_second_0:                 episode reward: -9.7500,                 loss: 0.6465
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 21861/30000 (72.8700%),                 avg. length: 951.3,                last time consumption/overall running time: 106.5101s / 278894.3871 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0526
env0_second_0:                 episode reward: -9.6500,                 loss: 0.7699
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 21881/30000 (72.9367%),                 avg. length: 957.95,                last time consumption/overall running time: 105.1409s / 278999.5280 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0468
env0_second_0:                 episode reward: -9.1500,                 loss: 0.6400
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 21901/30000 (73.0033%),                 avg. length: 958.25,                last time consumption/overall running time: 106.1830s / 279105.7110 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0932
env0_second_0:                 episode reward: -9.6500,                 loss: 0.2195
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 21921/30000 (73.0700%),                 avg. length: 948.95,                last time consumption/overall running time: 103.0616s / 279208.7726 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0276
env0_second_0:                 episode reward: -9.8500,                 loss: 0.6280
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 21941/30000 (73.1367%),                 avg. length: 951.75,                last time consumption/overall running time: 100.7709s / 279309.5435 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.0533
env0_second_0:                 episode reward: -9.4000,                 loss: 0.6808
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 21961/30000 (73.2033%),                 avg. length: 946.3,                last time consumption/overall running time: 103.2341s / 279412.7776 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0827
env0_second_0:                 episode reward: -9.7000,                 loss: 3.3983
env1_first_0:                 episode reward: 9.1000,                 loss: nan
env1_second_0:                 episode reward: -9.1000,                 loss: nan
Episode: 21981/30000 (73.2700%),                 avg. length: 945.6,                last time consumption/overall running time: 104.5730s / 279517.3507 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0729
env0_second_0:                 episode reward: -9.9000,                 loss: 0.8397
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 22001/30000 (73.3367%),                 avg. length: 943.6,                last time consumption/overall running time: 101.3001s / 279618.6508 s
env0_first_0:                 episode reward: 9.2000,                 loss: -0.0593
env0_second_0:                 episode reward: -9.2000,                 loss: 1.0417
env1_first_0:                 episode reward: 8.9000,                 loss: nan
env1_second_0:                 episode reward: -8.9000,                 loss: nan
Episode: 22021/30000 (73.4033%),                 avg. length: 950.35,                last time consumption/overall running time: 101.4618s / 279720.1126 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0457
env0_second_0:                 episode reward: -9.8000,                 loss: 0.7159
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 22041/30000 (73.4700%),                 avg. length: 944.75,                last time consumption/overall running time: 104.6533s / 279824.7659 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0684
env0_second_0:                 episode reward: -9.7000,                 loss: 1.2047
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 22061/30000 (73.5367%),                 avg. length: 952.75,                last time consumption/overall running time: 106.4203s / 279931.1862 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0815
env0_second_0:                 episode reward: -9.7500,                 loss: 1.1020
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 22081/30000 (73.6033%),                 avg. length: 944.2,                last time consumption/overall running time: 110.6237s / 280041.8099 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.0363
env0_second_0:                 episode reward: -9.0500,                 loss: 1.2428
env1_first_0:                 episode reward: 8.6000,                 loss: nan
env1_second_0:                 episode reward: -8.6000,                 loss: nan
Episode: 22101/30000 (73.6700%),                 avg. length: 944.0,                last time consumption/overall running time: 99.9486s / 280141.7585 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0531
env0_second_0:                 episode reward: -9.7000,                 loss: 1.5585
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 22121/30000 (73.7367%),                 avg. length: 966.0,                last time consumption/overall running time: 98.2230s / 280239.9815 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0759
env0_second_0:                 episode reward: -9.4500,                 loss: 1.6217
env1_first_0:                 episode reward: 8.9500,                 loss: nan
env1_second_0:                 episode reward: -8.9500,                 loss: nan
Episode: 22141/30000 (73.8033%),                 avg. length: 949.75,                last time consumption/overall running time: 97.7777s / 280337.7592 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0771
env0_second_0:                 episode reward: -9.5500,                 loss: 0.9544
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 22161/30000 (73.8700%),                 avg. length: 944.5,                last time consumption/overall running time: 96.5440s / 280434.3032 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0690
env0_second_0:                 episode reward: -9.8000,                 loss: 0.8628
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 22181/30000 (73.9367%),                 avg. length: 980.65,                last time consumption/overall running time: 98.6594s / 280532.9626 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0627
env0_second_0:                 episode reward: -9.4500,                 loss: 0.6015
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 22201/30000 (74.0033%),                 avg. length: 958.1,                last time consumption/overall running time: 96.3949s / 280629.3576 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0828
env0_second_0:                 episode reward: -9.7000,                 loss: 0.4410
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 22221/30000 (74.0700%),                 avg. length: 943.8,                last time consumption/overall running time: 95.3569s / 280724.7144 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0617
env0_second_0:                 episode reward: -9.5000,                 loss: 1.0398
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 22241/30000 (74.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 96.8585s / 280821.5729 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0514
env0_second_0:                 episode reward: -9.8000,                 loss: 0.4972
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 22261/30000 (74.2033%),                 avg. length: 943.4,                last time consumption/overall running time: 96.6722s / 280918.2451 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0728
env0_second_0:                 episode reward: -9.8000,                 loss: 0.7837
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 22281/30000 (74.2700%),                 avg. length: 948.6,                last time consumption/overall running time: 97.4513s / 281015.6964 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0673
env0_second_0:                 episode reward: -9.7500,                 loss: 0.7080
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 22301/30000 (74.3367%),                 avg. length: 944.2,                last time consumption/overall running time: 96.3674s / 281112.0638 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0685
env0_second_0:                 episode reward: -9.8000,                 loss: 0.4950
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 22321/30000 (74.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 98.3882s / 281210.4520 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0810
env0_second_0:                 episode reward: -9.9000,                 loss: 0.5008
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 22341/30000 (74.4700%),                 avg. length: 944.2,                last time consumption/overall running time: 101.8141s / 281312.2661 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.1059
env0_second_0:                 episode reward: -9.8500,                 loss: 0.4708
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 22361/30000 (74.5367%),                 avg. length: 943.0,                last time consumption/overall running time: 97.8284s / 281410.0944 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0985
env0_second_0:                 episode reward: -9.8500,                 loss: 0.3876
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 22381/30000 (74.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.6986s / 281507.7930 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0975
env0_second_0:                 episode reward: -9.8500,                 loss: 0.2542
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 22401/30000 (74.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.4757s / 281605.2687 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0947
env0_second_0:                 episode reward: -9.6500,                 loss: 0.5013
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 22421/30000 (74.7367%),                 avg. length: 947.65,                last time consumption/overall running time: 95.6629s / 281700.9316 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0879
env0_second_0:                 episode reward: -9.7000,                 loss: 0.2557
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 22441/30000 (74.8033%),                 avg. length: 945.2,                last time consumption/overall running time: 95.5674s / 281796.4990 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.1051
env0_second_0:                 episode reward: -9.8500,                 loss: -0.0113
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 22461/30000 (74.8700%),                 avg. length: 943.0,                last time consumption/overall running time: 95.0974s / 281891.5964 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.1051
env0_second_0:                 episode reward: -9.9000,                 loss: 0.0871
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 22481/30000 (74.9367%),                 avg. length: 943.95,                last time consumption/overall running time: 95.2449s / 281986.8413 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.1188
env0_second_0:                 episode reward: -9.6500,                 loss: 1.1325
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 22501/30000 (75.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 96.7981s / 282083.6394 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.1181
env0_second_0:                 episode reward: -9.4500,                 loss: 0.1249
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 22521/30000 (75.0700%),                 avg. length: 956.15,                last time consumption/overall running time: 98.6667s / 282182.3061 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.0990
env0_second_0:                 episode reward: -9.3500,                 loss: 0.1532
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 22541/30000 (75.1367%),                 avg. length: 944.15,                last time consumption/overall running time: 102.3527s / 282284.6588 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0964
env0_second_0:                 episode reward: -9.7000,                 loss: 0.1109
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 22561/30000 (75.2033%),                 avg. length: 944.1,                last time consumption/overall running time: 96.6836s / 282381.3425 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.1228
env0_second_0:                 episode reward: -9.7500,                 loss: 0.1711
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 22581/30000 (75.2700%),                 avg. length: 945.75,                last time consumption/overall running time: 96.4701s / 282477.8125 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.1093
env0_second_0:                 episode reward: -9.4500,                 loss: 0.4265
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 22601/30000 (75.3367%),                 avg. length: 947.3,                last time consumption/overall running time: 95.9436s / 282573.7561 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.1121
env0_second_0:                 episode reward: -9.7000,                 loss: 0.2525
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 22621/30000 (75.4033%),                 avg. length: 944.4,                last time consumption/overall running time: 95.8634s / 282669.6195 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.1047
env0_second_0:                 episode reward: -9.7000,                 loss: 0.3463
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 22641/30000 (75.4700%),                 avg. length: 943.2,                last time consumption/overall running time: 95.0648s / 282764.6843 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0982
env0_second_0:                 episode reward: -9.9500,                 loss: 0.3052
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 22661/30000 (75.5367%),                 avg. length: 945.2,                last time consumption/overall running time: 95.3384s / 282860.0227 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.1105
env0_second_0:                 episode reward: -9.7500,                 loss: 0.1804
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 22681/30000 (75.6033%),                 avg. length: 944.0,                last time consumption/overall running time: 95.2544s / 282955.2771 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.1150
env0_second_0:                 episode reward: -9.7000,                 loss: 0.2100
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 22701/30000 (75.6700%),                 avg. length: 947.3,                last time consumption/overall running time: 100.5881s / 283055.8651 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.1061
env0_second_0:                 episode reward: -9.3500,                 loss: 0.2842
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 22721/30000 (75.7367%),                 avg. length: 943.2,                last time consumption/overall running time: 111.2316s / 283167.0968 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0845
env0_second_0:                 episode reward: -9.9500,                 loss: 0.3773
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 22741/30000 (75.8033%),                 avg. length: 960.15,                last time consumption/overall running time: 102.5958s / 283269.6926 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0734
env0_second_0:                 episode reward: -9.5500,                 loss: 0.4200
env1_first_0:                 episode reward: 8.9500,                 loss: nan
env1_second_0:                 episode reward: -8.9500,                 loss: nan
Episode: 22761/30000 (75.8700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.5518s / 283367.2444 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1073
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0065
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 22781/30000 (75.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 110.4211s / 283477.6655 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0963
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0575
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 22801/30000 (76.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 102.4107s / 283580.0762 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1008
env0_second_0:                 episode reward: -9.9500,                 loss: 0.2395
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 22821/30000 (76.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 102.9129s / 283682.9891 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.1044
env0_second_0:                 episode reward: -9.7500,                 loss: 0.0621
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 22841/30000 (76.1367%),                 avg. length: 943.8,                last time consumption/overall running time: 102.9967s / 283785.9858 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0976
env0_second_0:                 episode reward: -9.8000,                 loss: 0.2408
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 22861/30000 (76.2033%),                 avg. length: 943.2,                last time consumption/overall running time: 103.8907s / 283889.8765 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0905
env0_second_0:                 episode reward: -9.9000,                 loss: 0.1145
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 22881/30000 (76.2700%),                 avg. length: 943.6,                last time consumption/overall running time: 97.6044s / 283987.4809 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0854
env0_second_0:                 episode reward: -9.9000,                 loss: 0.0615
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 22901/30000 (76.3367%),                 avg. length: 943.2,                last time consumption/overall running time: 96.2478s / 284083.7287 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0873
env0_second_0:                 episode reward: -9.5000,                 loss: 0.1030
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 22921/30000 (76.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.3087s / 284181.0374 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0876
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0928
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 22941/30000 (76.4700%),                 avg. length: 943.8,                last time consumption/overall running time: 103.0865s / 284284.1239 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0886
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0265
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 22961/30000 (76.5367%),                 avg. length: 943.0,                last time consumption/overall running time: 100.8067s / 284384.9306 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1011
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0047
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 22981/30000 (76.6033%),                 avg. length: 943.2,                last time consumption/overall running time: 103.4007s / 284488.3313 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1160
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0395
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 23001/30000 (76.6700%),                 avg. length: 944.0,                last time consumption/overall running time: 102.2407s / 284590.5719 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.1006
env0_second_0:                 episode reward: -9.9000,                 loss: 0.0415
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 23021/30000 (76.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 95.2268s / 284685.7987 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1046
env0_second_0:                 episode reward: -10.0000,                 loss: 0.6441
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 23041/30000 (76.8033%),                 avg. length: 943.95,                last time consumption/overall running time: 96.4969s / 284782.2956 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1001
env0_second_0:                 episode reward: -9.9500,                 loss: 0.1725
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 23061/30000 (76.8700%),                 avg. length: 943.0,                last time consumption/overall running time: 96.2180s / 284878.5135 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0950
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0866
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23081/30000 (76.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 97.7033s / 284976.2168 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0745
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3401
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23101/30000 (77.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.3675s / 285073.5844 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0901
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3070
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23121/30000 (77.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 101.1132s / 285174.6975 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1043
env0_second_0:                 episode reward: -10.0000,                 loss: 0.2251
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23141/30000 (77.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 96.5188s / 285271.2163 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0114
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3985
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23161/30000 (77.2033%),                 avg. length: 943.0,                last time consumption/overall running time: 103.1083s / 285374.3246 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0770
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3655
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 23181/30000 (77.2700%),                 avg. length: 943.0,                last time consumption/overall running time: 104.6697s / 285478.9943 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0908
env0_second_0:                 episode reward: -9.9500,                 loss: 0.2725
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23201/30000 (77.3367%),                 avg. length: 943.0,                last time consumption/overall running time: 103.0221s / 285582.0163 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0911
env0_second_0:                 episode reward: -9.9500,                 loss: 0.2946
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23221/30000 (77.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.1640s / 285679.1804 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0624
env0_second_0:                 episode reward: -9.7500,                 loss: 0.2330
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23241/30000 (77.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 96.7619s / 285775.9423 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0564
env0_second_0:                 episode reward: -9.8000,                 loss: 0.1331
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 23261/30000 (77.5367%),                 avg. length: 943.0,                last time consumption/overall running time: 95.6690s / 285871.6113 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0954
env0_second_0:                 episode reward: -9.4500,                 loss: 0.2452
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 23281/30000 (77.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 96.4255s / 285968.0368 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.1039
env0_second_0:                 episode reward: -5.0000,                 loss: 0.2534
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 23301/30000 (77.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 102.1226s / 286070.1594 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1362
env0_second_0:                 episode reward: 0.0000,                 loss: 0.5039
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23321/30000 (77.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 109.5783s / 286179.7377 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1103
env0_second_0:                 episode reward: -1.0000,                 loss: 0.5743
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 23341/30000 (77.8033%),                 avg. length: 951.7,                last time consumption/overall running time: 97.0644s / 286276.8022 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.0711
env0_second_0:                 episode reward: -9.3500,                 loss: 0.4168
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 23361/30000 (77.8700%),                 avg. length: 944.35,                last time consumption/overall running time: 96.4407s / 286373.2429 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1145
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0387
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 23381/30000 (77.9367%),                 avg. length: 944.2,                last time consumption/overall running time: 96.5191s / 286469.7620 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1335
env0_second_0:                 episode reward: -9.9500,                 loss: 0.1990
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 23401/30000 (78.0033%),                 avg. length: 955.0,                last time consumption/overall running time: 97.6207s / 286567.3827 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.1028
env0_second_0:                 episode reward: -9.4000,                 loss: 0.2155
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 23421/30000 (78.0700%),                 avg. length: 958.6,                last time consumption/overall running time: 96.7148s / 286664.0975 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.1112
env0_second_0:                 episode reward: -9.4000,                 loss: 0.0315
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 23441/30000 (78.1367%),                 avg. length: 950.6,                last time consumption/overall running time: 95.3177s / 286759.4152 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.1174
env0_second_0:                 episode reward: -9.7000,                 loss: 0.4857
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 23461/30000 (78.2033%),                 avg. length: 952.1,                last time consumption/overall running time: 95.9527s / 286855.3679 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.1128
env0_second_0:                 episode reward: -9.5500,                 loss: 0.1454
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 23481/30000 (78.2700%),                 avg. length: 950.55,                last time consumption/overall running time: 95.6154s / 286950.9833 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0825
env0_second_0:                 episode reward: -9.5500,                 loss: 0.1799
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 23501/30000 (78.3367%),                 avg. length: 960.65,                last time consumption/overall running time: 111.4883s / 287062.4715 s
env0_first_0:                 episode reward: 8.9500,                 loss: -0.0510
env0_second_0:                 episode reward: -8.9500,                 loss: 0.2011
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 23521/30000 (78.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 101.2744s / 287163.7460 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.0413
env0_second_0:                 episode reward: -7.7500,                 loss: 0.4928
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 23541/30000 (78.4700%),                 avg. length: 944.75,                last time consumption/overall running time: 97.7219s / 287261.4679 s
env0_first_0:                 episode reward: 8.0000,                 loss: -0.0533
env0_second_0:                 episode reward: -8.0000,                 loss: 0.4789
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 23561/30000 (78.5367%),                 avg. length: 954.85,                last time consumption/overall running time: 105.9761s / 287367.4440 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0622
env0_second_0:                 episode reward: -9.9500,                 loss: 0.3706
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 23581/30000 (78.6033%),                 avg. length: 966.9,                last time consumption/overall running time: 100.2591s / 287467.7031 s
env0_first_0:                 episode reward: 8.9000,                 loss: -0.0279
env0_second_0:                 episode reward: -8.9000,                 loss: 0.4873
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 23601/30000 (78.6700%),                 avg. length: 970.05,                last time consumption/overall running time: 99.0036s / 287566.7067 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0482
env0_second_0:                 episode reward: -9.1500,                 loss: 0.2537
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 23621/30000 (78.7367%),                 avg. length: 966.75,                last time consumption/overall running time: 98.5831s / 287665.2898 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0479
env0_second_0:                 episode reward: -9.6000,                 loss: 0.4647
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 23641/30000 (78.8033%),                 avg. length: 944.4,                last time consumption/overall running time: 98.0785s / 287763.3683 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0511
env0_second_0:                 episode reward: -9.7000,                 loss: 0.9905
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 23661/30000 (78.8700%),                 avg. length: 992.65,                last time consumption/overall running time: 101.4487s / 287864.8170 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.0206
env0_second_0:                 episode reward: -9.3500,                 loss: 1.6849
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 23681/30000 (78.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 100.6650s / 287965.4820 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0284
env0_second_0:                 episode reward: -9.1500,                 loss: 1.1607
env1_first_0:                 episode reward: 8.9500,                 loss: nan
env1_second_0:                 episode reward: -8.9500,                 loss: nan
Episode: 23701/30000 (79.0033%),                 avg. length: 945.0,                last time consumption/overall running time: 105.2290s / 288070.7109 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0680
env0_second_0:                 episode reward: -9.5000,                 loss: 0.9070
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 23721/30000 (79.0700%),                 avg. length: 1024.0,                last time consumption/overall running time: 111.9111s / 288182.6220 s
env0_first_0:                 episode reward: 8.5000,                 loss: -0.0274
env0_second_0:                 episode reward: -8.5000,                 loss: 1.6479
env1_first_0:                 episode reward: 8.6000,                 loss: nan
env1_second_0:                 episode reward: -8.6000,                 loss: nan
Episode: 23741/30000 (79.1367%),                 avg. length: 984.25,                last time consumption/overall running time: 100.0451s / 288282.6671 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.0285
env0_second_0:                 episode reward: -8.8500,                 loss: 0.4072
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 23761/30000 (79.2033%),                 avg. length: 965.75,                last time consumption/overall running time: 112.3493s / 288395.0164 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.0266
env0_second_0:                 episode reward: -9.4000,                 loss: 0.4825
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 23781/30000 (79.2700%),                 avg. length: 955.75,                last time consumption/overall running time: 106.7726s / 288501.7890 s
env0_first_0:                 episode reward: 9.2000,                 loss: 0.0518
env0_second_0:                 episode reward: -9.2000,                 loss: 0.8057
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 23801/30000 (79.3367%),                 avg. length: 947.1,                last time consumption/overall running time: 97.4561s / 288599.2451 s
env0_first_0:                 episode reward: 8.2500,                 loss: -0.0377
env0_second_0:                 episode reward: -8.2500,                 loss: 1.6134
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 23821/30000 (79.4033%),                 avg. length: 972.7,                last time consumption/overall running time: 103.1430s / 288702.3881 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0105
env0_second_0:                 episode reward: -9.5500,                 loss: 0.9168
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 23841/30000 (79.4700%),                 avg. length: 1008.8,                last time consumption/overall running time: 106.3352s / 288808.7234 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0294
env0_second_0:                 episode reward: -9.7000,                 loss: 0.3884
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 23861/30000 (79.5367%),                 avg. length: 955.25,                last time consumption/overall running time: 103.9172s / 288912.6406 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0401
env0_second_0:                 episode reward: -9.8000,                 loss: 1.3310
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 23881/30000 (79.6033%),                 avg. length: 944.55,                last time consumption/overall running time: 100.4421s / 289013.0827 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0569
env0_second_0:                 episode reward: -4.9500,                 loss: 1.2750
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 23901/30000 (79.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 102.1468s / 289115.2295 s
env0_first_0:                 episode reward: 9.0000,                 loss: -0.0280
env0_second_0:                 episode reward: -9.0000,                 loss: 0.8548
env1_first_0:                 episode reward: 8.9500,                 loss: nan
env1_second_0:                 episode reward: -8.9500,                 loss: nan
Episode: 23921/30000 (79.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 105.1051s / 289220.3346 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0418
env0_second_0:                 episode reward: -9.5000,                 loss: 0.9659
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 23941/30000 (79.8033%),                 avg. length: 943.2,                last time consumption/overall running time: 103.1152s / 289323.4498 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0499
env0_second_0:                 episode reward: -9.5000,                 loss: 1.1317
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 23961/30000 (79.8700%),                 avg. length: 958.9,                last time consumption/overall running time: 101.7079s / 289425.1577 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0628
env0_second_0:                 episode reward: -9.9500,                 loss: 1.3496
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 23981/30000 (79.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 95.9720s / 289521.1298 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0747
env0_second_0:                 episode reward: -9.8500,                 loss: 1.9916
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 24001/30000 (80.0033%),                 avg. length: 945.1,                last time consumption/overall running time: 98.4952s / 289619.6249 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0730
env0_second_0:                 episode reward: -9.8000,                 loss: 1.5494
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 24021/30000 (80.0700%),                 avg. length: 954.35,                last time consumption/overall running time: 100.7135s / 289720.3385 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.0710
env0_second_0:                 episode reward: -9.3500,                 loss: 1.4570
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 24041/30000 (80.1367%),                 avg. length: 947.2,                last time consumption/overall running time: 99.7744s / 289820.1128 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0327
env0_second_0:                 episode reward: -9.4500,                 loss: 1.7278
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 24061/30000 (80.2033%),                 avg. length: 952.05,                last time consumption/overall running time: 103.4594s / 289923.5723 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.0508
env0_second_0:                 episode reward: -9.3000,                 loss: 1.7681
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 24081/30000 (80.2700%),                 avg. length: 950.25,                last time consumption/overall running time: 97.4118s / 290020.9840 s
env0_first_0:                 episode reward: 9.3000,                 loss: 0.0581
env0_second_0:                 episode reward: -9.3000,                 loss: 1.6266
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 24101/30000 (80.3367%),                 avg. length: 945.45,                last time consumption/overall running time: 101.7727s / 290122.7567 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.0022
env0_second_0:                 episode reward: -9.3000,                 loss: 1.2684
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 24121/30000 (80.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 96.1724s / 290218.9291 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0090
env0_second_0:                 episode reward: -9.8500,                 loss: 1.0842
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 24141/30000 (80.4700%),                 avg. length: 944.4,                last time consumption/overall running time: 96.6588s / 290315.5880 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0128
env0_second_0:                 episode reward: -9.6500,                 loss: 1.5123
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 24161/30000 (80.5367%),                 avg. length: 943.6,                last time consumption/overall running time: 96.8674s / 290412.4553 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0195
env0_second_0:                 episode reward: -9.9000,                 loss: 356.9309
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 24181/30000 (80.6033%),                 avg. length: 943.75,                last time consumption/overall running time: 96.5284s / 290508.9837 s
env0_first_0:                 episode reward: 9.9000,                 loss: 0.0026
env0_second_0:                 episode reward: -9.9000,                 loss: 0.9305
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 24201/30000 (80.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 101.0192s / 290610.0030 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0084
env0_second_0:                 episode reward: -9.8000,                 loss: 0.7382
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 24221/30000 (80.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 98.1822s / 290708.1852 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0041
env0_second_0:                 episode reward: -9.8000,                 loss: 0.7353
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 24241/30000 (80.8033%),                 avg. length: 943.2,                last time consumption/overall running time: 100.8361s / 290809.0213 s
env0_first_0:                 episode reward: 9.4500,                 loss: 0.0290
env0_second_0:                 episode reward: -9.4500,                 loss: 0.7663
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 24261/30000 (80.8700%),                 avg. length: 943.2,                last time consumption/overall running time: 96.5645s / 290905.5858 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0224
env0_second_0:                 episode reward: -9.6500,                 loss: 1.8183
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 24281/30000 (80.9367%),                 avg. length: 943.2,                last time consumption/overall running time: 97.5868s / 291003.1726 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0750
env0_second_0:                 episode reward: -9.7500,                 loss: 2.9613
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 24301/30000 (81.0033%),                 avg. length: 944.95,                last time consumption/overall running time: 97.7590s / 291100.9316 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0758
env0_second_0:                 episode reward: -9.7500,                 loss: 4.2998
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 24321/30000 (81.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 98.0830s / 291199.0146 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0496
env0_second_0:                 episode reward: -9.9500,                 loss: 3.8979
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 24341/30000 (81.1367%),                 avg. length: 943.6,                last time consumption/overall running time: 97.4507s / 291296.4653 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0851
env0_second_0:                 episode reward: -9.8500,                 loss: 4.1632
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 24361/30000 (81.2033%),                 avg. length: 943.4,                last time consumption/overall running time: 97.3239s / 291393.7892 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0767
env0_second_0:                 episode reward: -9.7500,                 loss: 4.9867
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 24381/30000 (81.2700%),                 avg. length: 944.0,                last time consumption/overall running time: 100.9631s / 291494.7522 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0847
env0_second_0:                 episode reward: -9.6000,                 loss: 4.4014
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 24401/30000 (81.3367%),                 avg. length: 943.2,                last time consumption/overall running time: 97.9429s / 291592.6951 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0695
env0_second_0:                 episode reward: -9.8500,                 loss: 4.0537
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 24421/30000 (81.4033%),                 avg. length: 953.2,                last time consumption/overall running time: 99.8031s / 291692.4982 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0596
env0_second_0:                 episode reward: -9.6500,                 loss: 3.1761
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 24441/30000 (81.4700%),                 avg. length: 943.75,                last time consumption/overall running time: 98.1915s / 291790.6897 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0889
env0_second_0:                 episode reward: -9.7000,                 loss: 4.1032
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 24461/30000 (81.5367%),                 avg. length: 943.4,                last time consumption/overall running time: 98.2790s / 291888.9688 s
env0_first_0:                 episode reward: 9.2500,                 loss: -0.0660
env0_second_0:                 episode reward: -9.2500,                 loss: 2.6087
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 24481/30000 (81.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 101.9704s / 291990.9392 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0877
env0_second_0:                 episode reward: -9.9000,                 loss: 1.0167
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 24501/30000 (81.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 99.0549s / 292089.9941 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0864
env0_second_0:                 episode reward: -9.7500,                 loss: 1.2585
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 24521/30000 (81.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 97.6388s / 292187.6329 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0683
env0_second_0:                 episode reward: -9.4500,                 loss: 1.3760
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 24541/30000 (81.8033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.8277s / 292285.4606 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0798
env0_second_0:                 episode reward: -9.7000,                 loss: 1.0532
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 24561/30000 (81.8700%),                 avg. length: 943.0,                last time consumption/overall running time: 99.2596s / 292384.7202 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0805
env0_second_0:                 episode reward: -9.8000,                 loss: 1.0970
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 24581/30000 (81.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 100.3778s / 292485.0980 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0817
env0_second_0:                 episode reward: -9.6000,                 loss: 1.2555
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 24601/30000 (82.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 99.1997s / 292584.2977 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0590
env0_second_0:                 episode reward: -9.6000,                 loss: 1.2748
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 24621/30000 (82.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.5158s / 292681.8135 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0849
env0_second_0:                 episode reward: -9.9500,                 loss: 385.5915
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 24641/30000 (82.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 103.0664s / 292784.8799 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0193
env0_second_0:                 episode reward: -9.8000,                 loss: 1.1041
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 24661/30000 (82.2033%),                 avg. length: 943.0,                last time consumption/overall running time: 98.8218s / 292883.7017 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0904
env0_second_0:                 episode reward: -9.9500,                 loss: 1.0583
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 24681/30000 (82.2700%),                 avg. length: 943.4,                last time consumption/overall running time: 101.7390s / 292985.4408 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0544
env0_second_0:                 episode reward: -10.0000,                 loss: 1.6919
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 24701/30000 (82.3367%),                 avg. length: 944.8,                last time consumption/overall running time: 106.4393s / 293091.8801 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0644
env0_second_0:                 episode reward: -9.6000,                 loss: 1.1889
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 24721/30000 (82.4033%),                 avg. length: 943.4,                last time consumption/overall running time: 104.6548s / 293196.5349 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0705
env0_second_0:                 episode reward: -9.4500,                 loss: 1.1039
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 24741/30000 (82.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 105.1886s / 293301.7235 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0784
env0_second_0:                 episode reward: -9.9500,                 loss: 0.6426
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 24761/30000 (82.5367%),                 avg. length: 943.0,                last time consumption/overall running time: 103.6223s / 293405.3459 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0916
env0_second_0:                 episode reward: -10.0000,                 loss: 0.4416
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 24781/30000 (82.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.1568s / 293502.5027 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.0827
env0_second_0:                 episode reward: -9.4000,                 loss: 4.3582
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 24801/30000 (82.6700%),                 avg. length: 946.3,                last time consumption/overall running time: 96.7002s / 293599.2028 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0735
env0_second_0:                 episode reward: -9.8000,                 loss: 0.9130
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 24821/30000 (82.7367%),                 avg. length: 950.15,                last time consumption/overall running time: 101.9711s / 293701.1740 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0516
env0_second_0:                 episode reward: -9.7500,                 loss: 0.6988
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 24841/30000 (82.8033%),                 avg. length: 943.0,                last time consumption/overall running time: 96.2649s / 293797.4389 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0512
env0_second_0:                 episode reward: -9.5500,                 loss: 0.5084
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 24861/30000 (82.8700%),                 avg. length: 948.5,                last time consumption/overall running time: 96.3331s / 293893.7720 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0899
env0_second_0:                 episode reward: -9.4500,                 loss: 0.3973
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 24881/30000 (82.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 95.7527s / 293989.5247 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0829
env0_second_0:                 episode reward: -9.9000,                 loss: 0.2082
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 24901/30000 (83.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.4446s / 294086.9693 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0887
env0_second_0:                 episode reward: -9.9500,                 loss: 0.2415
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 24921/30000 (83.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.7149s / 294184.6841 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0871
env0_second_0:                 episode reward: -9.9500,                 loss: 0.1539
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 24941/30000 (83.1367%),                 avg. length: 943.2,                last time consumption/overall running time: 98.0702s / 294282.7544 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0778
env0_second_0:                 episode reward: -9.7500,                 loss: 0.1365
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 24961/30000 (83.2033%),                 avg. length: 947.7,                last time consumption/overall running time: 102.0237s / 294384.7781 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0818
env0_second_0:                 episode reward: -9.6500,                 loss: 0.3284
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 24981/30000 (83.2700%),                 avg. length: 945.25,                last time consumption/overall running time: 98.2434s / 294483.0215 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0496
env0_second_0:                 episode reward: -9.5000,                 loss: 0.2475
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 25001/30000 (83.3367%),                 avg. length: 943.0,                last time consumption/overall running time: 96.0978s / 294579.1193 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0355
env0_second_0:                 episode reward: -9.5000,                 loss: 0.1133
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 25021/30000 (83.4033%),                 avg. length: 943.75,                last time consumption/overall running time: 95.6264s / 294674.7458 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0722
env0_second_0:                 episode reward: -9.8500,                 loss: 0.2551
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 25041/30000 (83.4700%),                 avg. length: 945.1,                last time consumption/overall running time: 97.4513s / 294772.1970 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.0664
env0_second_0:                 episode reward: -9.3500,                 loss: 0.2057
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 25061/30000 (83.5367%),                 avg. length: 943.2,                last time consumption/overall running time: 96.6424s / 294868.8394 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0882
env0_second_0:                 episode reward: -9.7000,                 loss: 0.2488
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 25081/30000 (83.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 98.9634s / 294967.8029 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0701
env0_second_0:                 episode reward: -9.5500,                 loss: 0.3579
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 25101/30000 (83.6700%),                 avg. length: 957.9,                last time consumption/overall running time: 103.2775s / 295071.0804 s
env0_first_0:                 episode reward: 9.2000,                 loss: -0.0757
env0_second_0:                 episode reward: -9.2000,                 loss: 0.2144
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 25121/30000 (83.7367%),                 avg. length: 946.1,                last time consumption/overall running time: 105.2458s / 295176.3262 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0423
env0_second_0:                 episode reward: -9.4500,                 loss: 0.2063
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 25141/30000 (83.8033%),                 avg. length: 945.35,                last time consumption/overall running time: 105.9042s / 295282.2304 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0695
env0_second_0:                 episode reward: -9.6500,                 loss: 0.4675
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 25161/30000 (83.8700%),                 avg. length: 951.0,                last time consumption/overall running time: 103.9615s / 295386.1919 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0662
env0_second_0:                 episode reward: -9.8500,                 loss: 0.1704
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 25181/30000 (83.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 103.0316s / 295489.2236 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.0692
env0_second_0:                 episode reward: -9.3000,                 loss: 0.6960
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 25201/30000 (84.0033%),                 avg. length: 947.75,                last time consumption/overall running time: 120.2338s / 295609.4574 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0772
env0_second_0:                 episode reward: -9.8000,                 loss: 0.1379
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 25221/30000 (84.0700%),                 avg. length: 944.95,                last time consumption/overall running time: 120.2794s / 295729.7367 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.0614
env0_second_0:                 episode reward: -9.3000,                 loss: 0.3847
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 25241/30000 (84.1367%),                 avg. length: 943.95,                last time consumption/overall running time: 117.9304s / 295847.6671 s
env0_first_0:                 episode reward: 9.2000,                 loss: -0.0797
env0_second_0:                 episode reward: -9.2000,                 loss: 0.5267
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 25261/30000 (84.2033%),                 avg. length: 951.4,                last time consumption/overall running time: 118.1775s / 295965.8446 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.0817
env0_second_0:                 episode reward: -9.4000,                 loss: 0.4440
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 25281/30000 (84.2700%),                 avg. length: 951.45,                last time consumption/overall running time: 97.8907s / 296063.7353 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0810
env0_second_0:                 episode reward: -9.5500,                 loss: 0.5696
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 25301/30000 (84.3367%),                 avg. length: 944.2,                last time consumption/overall running time: 95.3787s / 296159.1140 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0622
env0_second_0:                 episode reward: -9.5000,                 loss: 1.3147
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 25321/30000 (84.4033%),                 avg. length: 943.8,                last time consumption/overall running time: 94.5933s / 296253.7073 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0469
env0_second_0:                 episode reward: -9.8000,                 loss: 0.9931
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 25341/30000 (84.4700%),                 avg. length: 945.75,                last time consumption/overall running time: 99.6257s / 296353.3330 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0820
env0_second_0:                 episode reward: -9.4500,                 loss: 0.8332
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 25361/30000 (84.5367%),                 avg. length: 946.3,                last time consumption/overall running time: 95.6558s / 296448.9888 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0844
env0_second_0:                 episode reward: -9.7500,                 loss: 0.5051
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 25381/30000 (84.6033%),                 avg. length: 948.15,                last time consumption/overall running time: 96.9299s / 296545.9187 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.0866
env0_second_0:                 episode reward: -9.4000,                 loss: 0.3039
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 25401/30000 (84.6700%),                 avg. length: 943.4,                last time consumption/overall running time: 100.2311s / 296646.1497 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0836
env0_second_0:                 episode reward: -9.7000,                 loss: 0.3455
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 25421/30000 (84.7367%),                 avg. length: 943.15,                last time consumption/overall running time: 102.7842s / 296748.9339 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0521
env0_second_0:                 episode reward: -9.8500,                 loss: 0.2375
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 25441/30000 (84.8033%),                 avg. length: 948.35,                last time consumption/overall running time: 99.8794s / 296848.8134 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0572
env0_second_0:                 episode reward: -9.8000,                 loss: 0.0797
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 25461/30000 (84.8700%),                 avg. length: 943.2,                last time consumption/overall running time: 101.7476s / 296950.5609 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0662
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0080
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 25481/30000 (84.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 120.8538s / 297071.4148 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0719
env0_second_0:                 episode reward: -9.9000,                 loss: 0.0101
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 25501/30000 (85.0033%),                 avg. length: 959.35,                last time consumption/overall running time: 108.7559s / 297180.1707 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0553
env0_second_0:                 episode reward: -9.7000,                 loss: 0.2337
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 25521/30000 (85.0700%),                 avg. length: 976.05,                last time consumption/overall running time: 108.6339s / 297288.8046 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0594
env0_second_0:                 episode reward: -9.4500,                 loss: 0.4013
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 25541/30000 (85.1367%),                 avg. length: 945.1,                last time consumption/overall running time: 105.9580s / 297394.7626 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0416
env0_second_0:                 episode reward: -9.8500,                 loss: 0.3423
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 25561/30000 (85.2033%),                 avg. length: 945.2,                last time consumption/overall running time: 105.7579s / 297500.5205 s
env0_first_0:                 episode reward: 9.3500,                 loss: -0.0448
env0_second_0:                 episode reward: -9.3500,                 loss: 0.1748
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 25581/30000 (85.2700%),                 avg. length: 943.55,                last time consumption/overall running time: 105.6701s / 297606.1905 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0708
env0_second_0:                 episode reward: -9.9000,                 loss: 0.2149
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 25601/30000 (85.3367%),                 avg. length: 943.0,                last time consumption/overall running time: 104.2030s / 297710.3935 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0524
env0_second_0:                 episode reward: -9.6000,                 loss: 0.1693
env1_first_0:                 episode reward: 8.9500,                 loss: nan
env1_second_0:                 episode reward: -8.9500,                 loss: nan
Episode: 25621/30000 (85.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 101.7631s / 297812.1566 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0606
env0_second_0:                 episode reward: -9.9000,                 loss: 0.4026
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 25641/30000 (85.4700%),                 avg. length: 943.4,                last time consumption/overall running time: 98.6249s / 297910.7815 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0535
env0_second_0:                 episode reward: -9.9000,                 loss: 0.5453
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 25661/30000 (85.5367%),                 avg. length: 943.2,                last time consumption/overall running time: 97.7043s / 298008.4858 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0855
env0_second_0:                 episode reward: -9.8500,                 loss: 0.4298
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 25681/30000 (85.6033%),                 avg. length: 943.2,                last time consumption/overall running time: 98.2243s / 298106.7101 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0649
env0_second_0:                 episode reward: -9.9000,                 loss: 0.5848
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 25701/30000 (85.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 96.2340s / 298202.9441 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0590
env0_second_0:                 episode reward: -10.0000,                 loss: 0.1431
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 25721/30000 (85.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 98.4802s / 298301.4243 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0864
env0_second_0:                 episode reward: -10.0000,                 loss: 0.6351
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 25741/30000 (85.8033%),                 avg. length: 943.75,                last time consumption/overall running time: 103.8921s / 298405.3164 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0470
env0_second_0:                 episode reward: -9.8500,                 loss: 0.7303
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 25761/30000 (85.8700%),                 avg. length: 943.15,                last time consumption/overall running time: 103.7764s / 298509.0928 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0855
env0_second_0:                 episode reward: -10.0000,                 loss: 0.8411
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 25781/30000 (85.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 103.8688s / 298612.9615 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0723
env0_second_0:                 episode reward: -9.8000,                 loss: 0.9597
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 25801/30000 (86.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 104.5174s / 298717.4790 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0756
env0_second_0:                 episode reward: -9.9000,                 loss: 1.2318
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 25821/30000 (86.0700%),                 avg. length: 943.6,                last time consumption/overall running time: 104.3506s / 298821.8296 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0579
env0_second_0:                 episode reward: -9.7000,                 loss: 3.3448
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 25841/30000 (86.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 104.6514s / 298926.4810 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.0005
env0_second_0:                 episode reward: -9.4000,                 loss: 1.7103
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 25861/30000 (86.2033%),                 avg. length: 947.6,                last time consumption/overall running time: 105.0725s / 299031.5535 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0543
env0_second_0:                 episode reward: -9.8500,                 loss: 1.8094
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 25881/30000 (86.2700%),                 avg. length: 943.0,                last time consumption/overall running time: 99.3888s / 299130.9423 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0738
env0_second_0:                 episode reward: -9.8500,                 loss: 2.3625
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 25901/30000 (86.3367%),                 avg. length: 943.0,                last time consumption/overall running time: 96.1314s / 299227.0737 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0179
env0_second_0:                 episode reward: -9.7500,                 loss: 2.5007
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 25921/30000 (86.4033%),                 avg. length: 944.6,                last time consumption/overall running time: 95.9516s / 299323.0253 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0109
env0_second_0:                 episode reward: -9.6500,                 loss: 2.3741
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 25941/30000 (86.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 94.3553s / 299417.3807 s
env0_first_0:                 episode reward: 9.9500,                 loss: 0.0129
env0_second_0:                 episode reward: -9.9500,                 loss: 0.6852
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 25961/30000 (86.5367%),                 avg. length: 943.0,                last time consumption/overall running time: 94.3192s / 299511.6999 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0099
env0_second_0:                 episode reward: -9.9000,                 loss: 0.7112
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 25981/30000 (86.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 95.9558s / 299607.6557 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0202
env0_second_0:                 episode reward: -9.7500,                 loss: 1.1803
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 26001/30000 (86.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 101.0994s / 299708.7551 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0895
env0_second_0:                 episode reward: -10.0000,                 loss: 0.9244
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 26021/30000 (86.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 95.8511s / 299804.6062 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0712
env0_second_0:                 episode reward: -9.4500,                 loss: 1.3638
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 26041/30000 (86.8033%),                 avg. length: 943.35,                last time consumption/overall running time: 94.8618s / 299899.4680 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0824
env0_second_0:                 episode reward: -9.7500,                 loss: 1.2187
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 26061/30000 (86.8700%),                 avg. length: 944.95,                last time consumption/overall running time: 96.4446s / 299995.9126 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0279
env0_second_0:                 episode reward: -9.7000,                 loss: 0.8724
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 26081/30000 (86.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 99.5031s / 300095.4157 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0806
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3036
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 26101/30000 (87.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 104.8568s / 300200.2725 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0695
env0_second_0:                 episode reward: -10.0000,                 loss: 0.8195
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 26121/30000 (87.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 103.8318s / 300304.1044 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0740
env0_second_0:                 episode reward: -9.9500,                 loss: 1.3512
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 26141/30000 (87.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 103.4506s / 300407.5550 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0722
env0_second_0:                 episode reward: -9.5000,                 loss: 0.9526
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 26161/30000 (87.2033%),                 avg. length: 943.0,                last time consumption/overall running time: 102.1626s / 300509.7175 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0688
env0_second_0:                 episode reward: -9.7500,                 loss: 0.3651
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 26181/30000 (87.2700%),                 avg. length: 943.0,                last time consumption/overall running time: 100.9742s / 300610.6918 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0792
env0_second_0:                 episode reward: -9.9000,                 loss: 0.7574
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 26201/30000 (87.3367%),                 avg. length: 943.0,                last time consumption/overall running time: 101.7682s / 300712.4600 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0773
env0_second_0:                 episode reward: -9.5500,                 loss: 0.7189
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 26221/30000 (87.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 102.3412s / 300814.8012 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1040
env0_second_0:                 episode reward: -9.9500,                 loss: 0.6231
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 26241/30000 (87.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.6557s / 300912.4568 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1003
env0_second_0:                 episode reward: -9.9500,                 loss: 0.3916
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 26261/30000 (87.5367%),                 avg. length: 943.0,                last time consumption/overall running time: 95.5631s / 301008.0200 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0904
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3577
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 26281/30000 (87.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 95.6780s / 301103.6980 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0955
env0_second_0:                 episode reward: -9.9000,                 loss: 0.3514
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 26301/30000 (87.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 100.9522s / 301204.6502 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0771
env0_second_0:                 episode reward: -9.9500,                 loss: 0.2093
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 26321/30000 (87.7367%),                 avg. length: 943.0,                last time consumption/overall running time: 105.7161s / 301310.3663 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0949
env0_second_0:                 episode reward: -9.9000,                 loss: 0.3049
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 26341/30000 (87.8033%),                 avg. length: 943.0,                last time consumption/overall running time: 95.3639s / 301405.7303 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0836
env0_second_0:                 episode reward: -10.0000,                 loss: 0.9342
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 26361/30000 (87.8700%),                 avg. length: 943.15,                last time consumption/overall running time: 96.2476s / 301501.9779 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0638
env0_second_0:                 episode reward: -9.8000,                 loss: 1.4713
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 26381/30000 (87.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 96.4836s / 301598.4615 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0747
env0_second_0:                 episode reward: -9.7000,                 loss: 0.4494
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 26401/30000 (88.0033%),                 avg. length: 943.8,                last time consumption/overall running time: 96.6158s / 301695.0772 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0879
env0_second_0:                 episode reward: -9.8500,                 loss: 0.9498
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 26421/30000 (88.0700%),                 avg. length: 944.2,                last time consumption/overall running time: 95.9734s / 301791.0506 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.1041
env0_second_0:                 episode reward: -9.8000,                 loss: 1.0395
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 26441/30000 (88.1367%),                 avg. length: 944.3,                last time consumption/overall running time: 96.7237s / 301887.7743 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.0930
env0_second_0:                 episode reward: -9.4000,                 loss: 0.7394
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 26461/30000 (88.2033%),                 avg. length: 943.35,                last time consumption/overall running time: 98.0378s / 301985.8121 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0993
env0_second_0:                 episode reward: -9.8000,                 loss: 0.9788
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 26481/30000 (88.2700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.0905s / 302082.9026 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0990
env0_second_0:                 episode reward: -9.9000,                 loss: 0.9127
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 26501/30000 (88.3367%),                 avg. length: 943.4,                last time consumption/overall running time: 98.3399s / 302181.2425 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0892
env0_second_0:                 episode reward: -10.0000,                 loss: 0.3067
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 26521/30000 (88.4033%),                 avg. length: 943.2,                last time consumption/overall running time: 97.4059s / 302278.6485 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.1003
env0_second_0:                 episode reward: -9.6500,                 loss: 0.2629
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 26541/30000 (88.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 98.7271s / 302377.3756 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.1081
env0_second_0:                 episode reward: -9.9000,                 loss: 0.0646
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 26561/30000 (88.5367%),                 avg. length: 945.3,                last time consumption/overall running time: 109.5310s / 302486.9066 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.1034
env0_second_0:                 episode reward: -9.6500,                 loss: 0.0359
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 26581/30000 (88.6033%),                 avg. length: 946.1,                last time consumption/overall running time: 108.4936s / 302595.4002 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0842
env0_second_0:                 episode reward: -9.7000,                 loss: 0.0903
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 26601/30000 (88.6700%),                 avg. length: 952.0,                last time consumption/overall running time: 99.3442s / 302694.7444 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0614
env0_second_0:                 episode reward: -9.6000,                 loss: 0.4425
env1_first_0:                 episode reward: 9.1000,                 loss: nan
env1_second_0:                 episode reward: -9.1000,                 loss: nan
Episode: 26621/30000 (88.7367%),                 avg. length: 943.75,                last time consumption/overall running time: 98.4370s / 302793.1814 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1037
env0_second_0:                 episode reward: -9.9500,                 loss: 0.5152
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 26641/30000 (88.8033%),                 avg. length: 946.1,                last time consumption/overall running time: 107.0340s / 302900.2154 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0821
env0_second_0:                 episode reward: -9.9000,                 loss: 0.6887
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 26661/30000 (88.8700%),                 avg. length: 943.0,                last time consumption/overall running time: 99.9504s / 303000.1658 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0921
env0_second_0:                 episode reward: -9.8500,                 loss: 0.6531
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 26681/30000 (88.9367%),                 avg. length: 944.4,                last time consumption/overall running time: 94.0628s / 303094.2287 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0621
env0_second_0:                 episode reward: -9.9500,                 loss: 0.2518
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 26701/30000 (89.0033%),                 avg. length: 947.0,                last time consumption/overall running time: 93.6161s / 303187.8448 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0575
env0_second_0:                 episode reward: -9.7500,                 loss: 0.1760
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 26721/30000 (89.0700%),                 avg. length: 949.3,                last time consumption/overall running time: 94.9722s / 303282.8170 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0300
env0_second_0:                 episode reward: -9.6000,                 loss: 0.2896
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 26741/30000 (89.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 93.7751s / 303376.5921 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0303
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0482
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 26761/30000 (89.2033%),                 avg. length: 943.0,                last time consumption/overall running time: 93.8433s / 303470.4354 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0275
env0_second_0:                 episode reward: -9.9000,                 loss: 0.1133
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 26781/30000 (89.2700%),                 avg. length: 943.4,                last time consumption/overall running time: 95.3479s / 303565.7833 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0180
env0_second_0:                 episode reward: -10.0000,                 loss: 0.2133
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 26801/30000 (89.3367%),                 avg. length: 943.0,                last time consumption/overall running time: 94.9333s / 303660.7166 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0378
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0912
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 26821/30000 (89.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 94.9904s / 303755.7070 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0131
env0_second_0:                 episode reward: -9.6500,                 loss: 0.1614
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 26841/30000 (89.4700%),                 avg. length: 943.2,                last time consumption/overall running time: 101.7243s / 303857.4314 s
env0_first_0:                 episode reward: 9.5500,                 loss: 0.0003
env0_second_0:                 episode reward: -9.5500,                 loss: 0.1629
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 26861/30000 (89.5367%),                 avg. length: 943.4,                last time consumption/overall running time: 101.4907s / 303958.9221 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0234
env0_second_0:                 episode reward: -9.7000,                 loss: 0.0326
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 26881/30000 (89.6033%),                 avg. length: 943.4,                last time consumption/overall running time: 94.2512s / 304053.1734 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0153
env0_second_0:                 episode reward: -9.7500,                 loss: 0.0550
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 26901/30000 (89.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.5294s / 304150.7028 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0178
env0_second_0:                 episode reward: -9.7500,                 loss: -0.0445
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 26921/30000 (89.7367%),                 avg. length: 945.15,                last time consumption/overall running time: 97.8683s / 304248.5711 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.0280
env0_second_0:                 episode reward: -9.3000,                 loss: 0.0040
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 26941/30000 (89.8033%),                 avg. length: 943.0,                last time consumption/overall running time: 94.6691s / 304343.2402 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0159
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0125
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 26961/30000 (89.8700%),                 avg. length: 943.0,                last time consumption/overall running time: 94.2885s / 304437.5288 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0474
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0837
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 26981/30000 (89.9367%),                 avg. length: 943.6,                last time consumption/overall running time: 96.0266s / 304533.5554 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0178
env0_second_0:                 episode reward: -9.8500,                 loss: -0.0534
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 27001/30000 (90.0033%),                 avg. length: 943.55,                last time consumption/overall running time: 93.4227s / 304626.9781 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0054
env0_second_0:                 episode reward: -9.7500,                 loss: 0.0638
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 27021/30000 (90.0700%),                 avg. length: 947.6,                last time consumption/overall running time: 112.3453s / 304739.3234 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0764
env0_second_0:                 episode reward: -9.9000,                 loss: 0.2637
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 27041/30000 (90.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 95.0076s / 304834.3310 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0964
env0_second_0:                 episode reward: -9.9500,                 loss: -0.0456
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 27061/30000 (90.2033%),                 avg. length: 943.35,                last time consumption/overall running time: 94.0365s / 304928.3676 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0869
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0347
env1_first_0:                 episode reward: 9.1000,                 loss: nan
env1_second_0:                 episode reward: -9.1000,                 loss: nan
Episode: 27081/30000 (90.2700%),                 avg. length: 944.55,                last time consumption/overall running time: 98.0697s / 305026.4372 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.1007
env0_second_0:                 episode reward: -9.8500,                 loss: 0.0385
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 27101/30000 (90.3367%),                 avg. length: 943.2,                last time consumption/overall running time: 97.8450s / 305124.2822 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.1171
env0_second_0:                 episode reward: -9.9000,                 loss: -0.0202
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 27121/30000 (90.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 94.9348s / 305219.2171 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0841
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0047
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 27141/30000 (90.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.3839s / 305316.6009 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0775
env0_second_0:                 episode reward: -9.9500,                 loss: 0.5498
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 27161/30000 (90.5367%),                 avg. length: 943.6,                last time consumption/overall running time: 96.2159s / 305412.8169 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0694
env0_second_0:                 episode reward: -9.8500,                 loss: 0.2235
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 27181/30000 (90.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 93.3188s / 305506.1357 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0740
env0_second_0:                 episode reward: -9.9000,                 loss: 0.1812
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 27201/30000 (90.6700%),                 avg. length: 947.45,                last time consumption/overall running time: 96.0343s / 305602.1701 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0433
env0_second_0:                 episode reward: -9.9500,                 loss: 0.1732
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 27221/30000 (90.7367%),                 avg. length: 948.75,                last time consumption/overall running time: 94.8256s / 305696.9957 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0749
env0_second_0:                 episode reward: -9.9500,                 loss: 0.1557
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 27241/30000 (90.8033%),                 avg. length: 943.4,                last time consumption/overall running time: 93.7524s / 305790.7480 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0655
env0_second_0:                 episode reward: -9.7000,                 loss: 0.0657
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 27261/30000 (90.8700%),                 avg. length: 943.4,                last time consumption/overall running time: 93.9087s / 305884.6567 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0890
env0_second_0:                 episode reward: -10.0000,                 loss: 0.1009
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 27281/30000 (90.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 94.2048s / 305978.8615 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1028
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0453
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 27301/30000 (91.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 95.4587s / 306074.3203 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0812
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0024
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 27321/30000 (91.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 98.3921s / 306172.7123 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0869
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0426
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 27341/30000 (91.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 112.3187s / 306285.0311 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0944
env0_second_0:                 episode reward: -9.8500,                 loss: 0.0725
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 27361/30000 (91.2033%),                 avg. length: 945.2,                last time consumption/overall running time: 93.4760s / 306378.5070 s
env0_first_0:                 episode reward: 6.8500,                 loss: -0.0659
env0_second_0:                 episode reward: -6.8500,                 loss: 0.1242
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 27381/30000 (91.2700%),                 avg. length: 944.0,                last time consumption/overall running time: 94.8482s / 306473.3552 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0728
env0_second_0:                 episode reward: -9.8500,                 loss: -0.0886
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 27401/30000 (91.3367%),                 avg. length: 947.6,                last time consumption/overall running time: 95.0489s / 306568.4042 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0694
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0226
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 27421/30000 (91.4033%),                 avg. length: 949.2,                last time consumption/overall running time: 95.9886s / 306664.3928 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0637
env0_second_0:                 episode reward: -9.9000,                 loss: -0.0708
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 27441/30000 (91.4700%),                 avg. length: 965.65,                last time consumption/overall running time: 97.7849s / 306762.1776 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0553
env0_second_0:                 episode reward: -9.6500,                 loss: 0.2481
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 27461/30000 (91.5367%),                 avg. length: 968.0,                last time consumption/overall running time: 96.7430s / 306858.9207 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0611
env0_second_0:                 episode reward: -9.5000,                 loss: 0.3949
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 27481/30000 (91.6033%),                 avg. length: 944.0,                last time consumption/overall running time: 101.6320s / 306960.5527 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0719
env0_second_0:                 episode reward: -9.7500,                 loss: 0.9866
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 27501/30000 (91.6700%),                 avg. length: 948.6,                last time consumption/overall running time: 110.5864s / 307071.1390 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0784
env0_second_0:                 episode reward: -9.7000,                 loss: 1.2828
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 27521/30000 (91.7367%),                 avg. length: 947.15,                last time consumption/overall running time: 96.5531s / 307167.6921 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0796
env0_second_0:                 episode reward: -9.9000,                 loss: 1.2055
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 27541/30000 (91.8033%),                 avg. length: 951.75,                last time consumption/overall running time: 94.0935s / 307261.7856 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0824
env0_second_0:                 episode reward: -9.8500,                 loss: 0.5064
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 27561/30000 (91.8700%),                 avg. length: 948.9,                last time consumption/overall running time: 93.1806s / 307354.9662 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0723
env0_second_0:                 episode reward: -9.9000,                 loss: 1.0405
env1_first_0:                 episode reward: 9.1000,                 loss: nan
env1_second_0:                 episode reward: -9.1000,                 loss: nan
Episode: 27581/30000 (91.9367%),                 avg. length: 955.15,                last time consumption/overall running time: 95.8669s / 307450.8331 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0754
env0_second_0:                 episode reward: -9.7500,                 loss: 0.4489
env1_first_0:                 episode reward: 8.9500,                 loss: nan
env1_second_0:                 episode reward: -8.9500,                 loss: nan
Episode: 27601/30000 (92.0033%),                 avg. length: 949.25,                last time consumption/overall running time: 94.6619s / 307545.4950 s
env0_first_0:                 episode reward: 9.2500,                 loss: -0.0830
env0_second_0:                 episode reward: -9.2500,                 loss: 0.8058
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 27621/30000 (92.0700%),                 avg. length: 945.55,                last time consumption/overall running time: 93.7852s / 307639.2802 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0779
env0_second_0:                 episode reward: -9.7000,                 loss: 1.4027
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 27641/30000 (92.1367%),                 avg. length: 943.8,                last time consumption/overall running time: 100.6865s / 307739.9667 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0936
env0_second_0:                 episode reward: -9.8500,                 loss: 0.3611
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 27661/30000 (92.2033%),                 avg. length: 944.8,                last time consumption/overall running time: 103.7362s / 307843.7030 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0944
env0_second_0:                 episode reward: -9.8500,                 loss: 0.6797
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 27681/30000 (92.2700%),                 avg. length: 952.05,                last time consumption/overall running time: 95.0645s / 307938.7675 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0632
env0_second_0:                 episode reward: -9.5500,                 loss: 0.3748
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 27701/30000 (92.3367%),                 avg. length: 943.95,                last time consumption/overall running time: 94.0099s / 308032.7774 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0987
env0_second_0:                 episode reward: -9.7500,                 loss: 0.6080
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 27721/30000 (92.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 96.2037s / 308128.9812 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0855
env0_second_0:                 episode reward: -9.7500,                 loss: 0.4392
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 27741/30000 (92.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 93.5892s / 308222.5704 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.0476
env0_second_0:                 episode reward: -9.3000,                 loss: 4.1729
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 27761/30000 (92.5367%),                 avg. length: 943.0,                last time consumption/overall running time: 94.0935s / 308316.6638 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0778
env0_second_0:                 episode reward: -9.5500,                 loss: 0.7594
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 27781/30000 (92.6033%),                 avg. length: 943.4,                last time consumption/overall running time: 97.8259s / 308414.4897 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0867
env0_second_0:                 episode reward: -9.7000,                 loss: 0.7925
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 27801/30000 (92.6700%),                 avg. length: 943.35,                last time consumption/overall running time: 93.9270s / 308508.4167 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0834
env0_second_0:                 episode reward: -9.9500,                 loss: 1.1764
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 27821/30000 (92.7367%),                 avg. length: 944.2,                last time consumption/overall running time: 94.5298s / 308602.9465 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0975
env0_second_0:                 episode reward: -9.5000,                 loss: 1.7204
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 27841/30000 (92.8033%),                 avg. length: 944.6,                last time consumption/overall running time: 93.3152s / 308696.2618 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0710
env0_second_0:                 episode reward: -9.8500,                 loss: 0.4711
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 27861/30000 (92.8700%),                 avg. length: 943.35,                last time consumption/overall running time: 94.0020s / 308790.2638 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.0626
env0_second_0:                 episode reward: -9.5000,                 loss: 0.3862
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 27881/30000 (92.9367%),                 avg. length: 943.55,                last time consumption/overall running time: 103.4988s / 308893.7626 s
env0_first_0:                 episode reward: 9.6500,                 loss: -0.0849
env0_second_0:                 episode reward: -9.6500,                 loss: 0.4535
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 27901/30000 (93.0033%),                 avg. length: 943.4,                last time consumption/overall running time: 108.9947s / 309002.7573 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0779
env0_second_0:                 episode reward: -9.8000,                 loss: 1.4121
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 27921/30000 (93.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 96.4351s / 309099.1924 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.1178
env0_second_0:                 episode reward: -9.9000,                 loss: 0.3588
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 27941/30000 (93.1367%),                 avg. length: 943.2,                last time consumption/overall running time: 93.3227s / 309192.5151 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1145
env0_second_0:                 episode reward: -9.9500,                 loss: 0.2648
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 27961/30000 (93.2033%),                 avg. length: 943.0,                last time consumption/overall running time: 103.0827s / 309295.5979 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.1117
env0_second_0:                 episode reward: -9.9000,                 loss: 0.2646
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 27981/30000 (93.2700%),                 avg. length: 943.0,                last time consumption/overall running time: 103.4417s / 309399.0395 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.1112
env0_second_0:                 episode reward: -9.9000,                 loss: 0.1015
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 28001/30000 (93.3367%),                 avg. length: 943.0,                last time consumption/overall running time: 102.6008s / 309501.6403 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.1096
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0757
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 28021/30000 (93.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 96.4911s / 309598.1314 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0849
env0_second_0:                 episode reward: -10.0000,                 loss: 0.9078
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 28041/30000 (93.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 94.3994s / 309692.5308 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.1003
env0_second_0:                 episode reward: -9.6000,                 loss: 0.7092
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 28061/30000 (93.5367%),                 avg. length: 943.0,                last time consumption/overall running time: 94.3045s / 309786.8353 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0876
env0_second_0:                 episode reward: -9.9000,                 loss: 0.4573
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28081/30000 (93.6033%),                 avg. length: 943.0,                last time consumption/overall running time: 95.0319s / 309881.8672 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0755
env0_second_0:                 episode reward: -9.9500,                 loss: 0.1629
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 28101/30000 (93.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 93.7590s / 309975.6263 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0712
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0611
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 28121/30000 (93.7367%),                 avg. length: 944.5,                last time consumption/overall running time: 93.7089s / 310069.3352 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0687
env0_second_0:                 episode reward: -9.9000,                 loss: 0.1783
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 28141/30000 (93.8033%),                 avg. length: 943.0,                last time consumption/overall running time: 94.3091s / 310163.6442 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0706
env0_second_0:                 episode reward: -9.9000,                 loss: 0.1088
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28161/30000 (93.8700%),                 avg. length: 943.6,                last time consumption/overall running time: 93.7702s / 310257.4144 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0800
env0_second_0:                 episode reward: -9.9000,                 loss: 0.1115
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 28181/30000 (93.9367%),                 avg. length: 943.2,                last time consumption/overall running time: 96.7984s / 310354.2128 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0929
env0_second_0:                 episode reward: -9.8500,                 loss: 0.7728
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 28201/30000 (94.0033%),                 avg. length: 943.8,                last time consumption/overall running time: 102.4207s / 310456.6335 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0712
env0_second_0:                 episode reward: -9.6000,                 loss: 0.1414
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 28221/30000 (94.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 94.1038s / 310550.7373 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0698
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0984
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 28241/30000 (94.1367%),                 avg. length: 943.0,                last time consumption/overall running time: 101.1711s / 310651.9084 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0836
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0853
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 28261/30000 (94.2033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.2974s / 310749.2058 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1001
env0_second_0:                 episode reward: -10.0000,                 loss: 0.0494
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28281/30000 (94.2700%),                 avg. length: 943.0,                last time consumption/overall running time: 97.7759s / 310846.9817 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1131
env0_second_0:                 episode reward: -10.0000,                 loss: 0.5368
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 28301/30000 (94.3367%),                 avg. length: 943.0,                last time consumption/overall running time: 98.4485s / 310945.4302 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0962
env0_second_0:                 episode reward: -10.0000,                 loss: 0.5891
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28321/30000 (94.4033%),                 avg. length: 943.0,                last time consumption/overall running time: 112.1982s / 311057.6283 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0999
env0_second_0:                 episode reward: -10.0000,                 loss: 0.8498
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28341/30000 (94.4700%),                 avg. length: 943.0,                last time consumption/overall running time: 101.3134s / 311158.9418 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0898
env0_second_0:                 episode reward: -10.0000,                 loss: 1.2941
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28361/30000 (94.5367%),                 avg. length: 945.35,                last time consumption/overall running time: 96.9054s / 311255.8472 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0640
env0_second_0:                 episode reward: -9.7500,                 loss: 1.7140
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 28381/30000 (94.6033%),                 avg. length: 944.75,                last time consumption/overall running time: 103.0593s / 311358.9064 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.0091
env0_second_0:                 episode reward: -9.0500,                 loss: 0.7890
env1_first_0:                 episode reward: 8.9000,                 loss: nan
env1_second_0:                 episode reward: -8.9000,                 loss: nan
Episode: 28401/30000 (94.6700%),                 avg. length: 943.0,                last time consumption/overall running time: 93.7502s / 311452.6566 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0785
env0_second_0:                 episode reward: -9.9500,                 loss: 1.1461
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 28421/30000 (94.7367%),                 avg. length: 944.0,                last time consumption/overall running time: 97.3188s / 311549.9754 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.0855
env0_second_0:                 episode reward: -9.7500,                 loss: 0.8648
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 28441/30000 (94.8033%),                 avg. length: 943.0,                last time consumption/overall running time: 97.4619s / 311647.4373 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.0384
env0_second_0:                 episode reward: -9.3000,                 loss: 1.0425
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 28461/30000 (94.8700%),                 avg. length: 943.0,                last time consumption/overall running time: 103.2237s / 311750.6610 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0823
env0_second_0:                 episode reward: -9.9500,                 loss: 1.2738
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 28481/30000 (94.9367%),                 avg. length: 943.0,                last time consumption/overall running time: 95.1834s / 311845.8445 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0950
env0_second_0:                 episode reward: -10.0000,                 loss: 0.4954
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28501/30000 (95.0033%),                 avg. length: 943.0,                last time consumption/overall running time: 94.6187s / 311940.4632 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1106
env0_second_0:                 episode reward: -10.0000,                 loss: 2.9372
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28521/30000 (95.0700%),                 avg. length: 943.0,                last time consumption/overall running time: 95.6244s / 312036.0876 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1002
env0_second_0:                 episode reward: -10.0000,                 loss: 1.7633
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 28541/30000 (95.1367%),                 avg. length: 943.2,                last time consumption/overall running time: 99.6450s / 312135.7326 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0998
env0_second_0:                 episode reward: -9.7000,                 loss: 0.8169
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 28561/30000 (95.2033%),                 avg. length: 957.85,                last time consumption/overall running time: 98.7070s / 312234.4396 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0441
env0_second_0:                 episode reward: -9.4500,                 loss: 0.8366
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 28581/30000 (95.2700%),                 avg. length: 975.4,                last time consumption/overall running time: 103.6329s / 312338.0725 s
env0_first_0:                 episode reward: 8.7500,                 loss: -0.0432
env0_second_0:                 episode reward: -8.7500,                 loss: 0.7335
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 28601/30000 (95.3367%),                 avg. length: 1174.95,                last time consumption/overall running time: 137.5398s / 312475.6123 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0048
env0_second_0:                 episode reward: -6.4000,                 loss: 1.4873
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 28621/30000 (95.4033%),                 avg. length: 1153.05,                last time consumption/overall running time: 114.1663s / 312589.7786 s
env0_first_0:                 episode reward: 7.9500,                 loss: 0.1423
env0_second_0:                 episode reward: -7.9500,                 loss: 1.4734
env1_first_0:                 episode reward: 8.6000,                 loss: nan
env1_second_0:                 episode reward: -8.6000,                 loss: nan
Episode: 28641/30000 (95.4700%),                 avg. length: 1245.35,                last time consumption/overall running time: 122.0157s / 312711.7944 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.0246
env0_second_0:                 episode reward: -8.8000,                 loss: 1.1737
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 28661/30000 (95.5367%),                 avg. length: 1277.9,                last time consumption/overall running time: 125.0582s / 312836.8525 s
env0_first_0:                 episode reward: 7.8500,                 loss: 0.0230
env0_second_0:                 episode reward: -7.8500,                 loss: 1.5200
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 28681/30000 (95.6033%),                 avg. length: 1346.05,                last time consumption/overall running time: 130.6464s / 312967.4989 s
env0_first_0:                 episode reward: 7.4000,                 loss: -0.0155
env0_second_0:                 episode reward: -7.4000,                 loss: 1.2457
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 28701/30000 (95.6700%),                 avg. length: 1816.25,                last time consumption/overall running time: 188.3608s / 313155.8597 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0213
env0_second_0:                 episode reward: -3.9500,                 loss: 1.3442
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 28721/30000 (95.7367%),                 avg. length: 1871.45,                last time consumption/overall running time: 183.3105s / 313339.1702 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0291
env0_second_0:                 episode reward: -2.4500,                 loss: 1.4748
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 28741/30000 (95.8033%),                 avg. length: 2355.7,                last time consumption/overall running time: 235.9752s / 313575.1454 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0072
env0_second_0:                 episode reward: -0.6000,                 loss: 1.0850
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 28761/30000 (95.8700%),                 avg. length: 2537.75,                last time consumption/overall running time: 243.5684s / 313818.7138 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.0145
env0_second_0:                 episode reward: -2.2000,                 loss: 1.3046
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 28781/30000 (95.9367%),                 avg. length: 2514.5,                last time consumption/overall running time: 252.6290s / 314071.3428 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.0079
env0_second_0:                 episode reward: 3.3500,                 loss: 1.7423
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 28801/30000 (96.0033%),                 avg. length: 2847.3,                last time consumption/overall running time: 296.4116s / 314367.7543 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0078
env0_second_0:                 episode reward: -0.9000,                 loss: 1.4453
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 28821/30000 (96.0700%),                 avg. length: 2618.55,                last time consumption/overall running time: 263.6955s / 314631.4498 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0128
env0_second_0:                 episode reward: -2.1500,                 loss: 1.2532
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 28841/30000 (96.1367%),                 avg. length: 927.95,                last time consumption/overall running time: 94.6360s / 314726.0858 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0509
env0_second_0:                 episode reward: 9.5500,                 loss: 1.1417
env1_first_0:                 episode reward: -9.3000,                 loss: nan
env1_second_0:                 episode reward: 9.3000,                 loss: nan
Episode: 28861/30000 (96.2033%),                 avg. length: 930.75,                last time consumption/overall running time: 103.3216s / 314829.4074 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.0970
env0_second_0:                 episode reward: 9.2500,                 loss: 0.7109
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 28881/30000 (96.2700%),                 avg. length: 916.75,                last time consumption/overall running time: 103.3106s / 314932.7180 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.1150
env0_second_0:                 episode reward: 9.2500,                 loss: 0.9747
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 28901/30000 (96.3367%),                 avg. length: 924.1,                last time consumption/overall running time: 97.8673s / 315030.5853 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.1057
env0_second_0:                 episode reward: 9.0500,                 loss: 0.2675
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 28921/30000 (96.4033%),                 avg. length: 917.6,                last time consumption/overall running time: 93.8014s / 315124.3867 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.1185
env0_second_0:                 episode reward: 9.4500,                 loss: 0.1297
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 28941/30000 (96.4700%),                 avg. length: 908.0,                last time consumption/overall running time: 100.6522s / 315225.0389 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.0958
env0_second_0:                 episode reward: 9.5500,                 loss: 0.0545
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 28961/30000 (96.5367%),                 avg. length: 908.4,                last time consumption/overall running time: 109.7413s / 315334.7802 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.1012
env0_second_0:                 episode reward: 9.7500,                 loss: 0.1676
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 28981/30000 (96.6033%),                 avg. length: 912.2,                last time consumption/overall running time: 91.3957s / 315426.1759 s
env0_first_0:                 episode reward: -8.9500,                 loss: -0.0776
env0_second_0:                 episode reward: 8.9500,                 loss: 1.0774
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 29001/30000 (96.6700%),                 avg. length: 911.8,                last time consumption/overall running time: 90.6954s / 315516.8713 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.0580
env0_second_0:                 episode reward: 9.0500,                 loss: 0.6105
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 29021/30000 (96.7367%),                 avg. length: 915.75,                last time consumption/overall running time: 92.2579s / 315609.1291 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.0931
env0_second_0:                 episode reward: 9.0500,                 loss: 0.0408
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 29041/30000 (96.8033%),                 avg. length: 917.9,                last time consumption/overall running time: 92.5144s / 315701.6436 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.0643
env0_second_0:                 episode reward: 8.6500,                 loss: 0.3468
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 29061/30000 (96.8700%),                 avg. length: 907.8,                last time consumption/overall running time: 90.4071s / 315792.0507 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.0944
env0_second_0:                 episode reward: 9.7500,                 loss: 0.4479
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 29081/30000 (96.9367%),                 avg. length: 910.2,                last time consumption/overall running time: 92.7203s / 315884.7709 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.0645
env0_second_0:                 episode reward: 9.1000,                 loss: 0.1760
env1_first_0:                 episode reward: -9.2500,                 loss: nan
env1_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 29101/30000 (97.0033%),                 avg. length: 910.8,                last time consumption/overall running time: 92.7618s / 315977.5327 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1192
env0_second_0:                 episode reward: 9.8000,                 loss: 0.5179
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 29121/30000 (97.0700%),                 avg. length: 916.55,                last time consumption/overall running time: 92.7003s / 316070.2330 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.0879
env0_second_0:                 episode reward: 8.7500,                 loss: 0.2695
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 29141/30000 (97.1367%),                 avg. length: 908.35,                last time consumption/overall running time: 92.0803s / 316162.3133 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1112
env0_second_0:                 episode reward: 9.7000,                 loss: 0.7020
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 29161/30000 (97.2033%),                 avg. length: 908.35,                last time consumption/overall running time: 93.0267s / 316255.3401 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.1109
env0_second_0:                 episode reward: 9.5000,                 loss: 0.7551
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 29181/30000 (97.2700%),                 avg. length: 925.25,                last time consumption/overall running time: 95.2111s / 316350.5512 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.0941
env0_second_0:                 episode reward: 9.1000,                 loss: 1.1567
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 29201/30000 (97.3367%),                 avg. length: 918.2,                last time consumption/overall running time: 95.8980s / 316446.4492 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.0825
env0_second_0:                 episode reward: 9.0000,                 loss: 0.2724
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 29221/30000 (97.4033%),                 avg. length: 923.0,                last time consumption/overall running time: 94.7402s / 316541.1894 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.1075
env0_second_0:                 episode reward: 9.2500,                 loss: 0.4526
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 29241/30000 (97.4700%),                 avg. length: 913.25,                last time consumption/overall running time: 94.4096s / 316635.5990 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1196
env0_second_0:                 episode reward: 9.4000,                 loss: 0.2047
env1_first_0:                 episode reward: -9.3000,                 loss: nan
env1_second_0:                 episode reward: 9.3000,                 loss: nan
Episode: 29261/30000 (97.5367%),                 avg. length: 912.4,                last time consumption/overall running time: 91.8211s / 316727.4201 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.0956
env0_second_0:                 episode reward: 8.6000,                 loss: 0.3752
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 29281/30000 (97.6033%),                 avg. length: 916.15,                last time consumption/overall running time: 92.4598s / 316819.8800 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1043
env0_second_0:                 episode reward: 9.2000,                 loss: 0.3271
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 29301/30000 (97.6700%),                 avg. length: 907.8,                last time consumption/overall running time: 90.9083s / 316910.7882 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1166
env0_second_0:                 episode reward: 9.5500,                 loss: 0.1381
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 29321/30000 (97.7367%),                 avg. length: 917.55,                last time consumption/overall running time: 91.1950s / 317001.9833 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1206
env0_second_0:                 episode reward: 9.5500,                 loss: 0.3992
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 29341/30000 (97.8033%),                 avg. length: 907.55,                last time consumption/overall running time: 92.2042s / 317094.1874 s
env0_first_0:                 episode reward: -9.2500,                 loss: -0.1211
env0_second_0:                 episode reward: 9.2500,                 loss: 0.3813
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 29361/30000 (97.8700%),                 avg. length: 907.8,                last time consumption/overall running time: 93.2844s / 317187.4719 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1112
env0_second_0:                 episode reward: 9.5500,                 loss: 0.2617
env1_first_0:                 episode reward: -9.8000,                 loss: nan
env1_second_0:                 episode reward: 9.8000,                 loss: nan
Episode: 29381/30000 (97.9367%),                 avg. length: 908.4,                last time consumption/overall running time: 90.8768s / 317278.3486 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1216
env0_second_0:                 episode reward: 9.7000,                 loss: 0.7673
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 29401/30000 (98.0033%),                 avg. length: 907.4,                last time consumption/overall running time: 92.4723s / 317370.8209 s
env0_first_0:                 episode reward: -9.8500,                 loss: -0.1157
env0_second_0:                 episode reward: 9.8500,                 loss: 0.4877
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 29421/30000 (98.0700%),                 avg. length: 907.55,                last time consumption/overall running time: 100.7536s / 317471.5745 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1101
env0_second_0:                 episode reward: 9.6000,                 loss: 0.3937
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 29441/30000 (98.1367%),                 avg. length: 908.15,                last time consumption/overall running time: 95.9353s / 317567.5098 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1203
env0_second_0:                 episode reward: 9.7000,                 loss: 0.2888
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 29461/30000 (98.2033%),                 avg. length: 907.8,                last time consumption/overall running time: 99.0191s / 317666.5289 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1161
env0_second_0:                 episode reward: 9.6000,                 loss: 0.3566
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 29481/30000 (98.2700%),                 avg. length: 907.0,                last time consumption/overall running time: 95.1916s / 317761.7206 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1071
env0_second_0:                 episode reward: 9.8000,                 loss: 0.3620
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 29501/30000 (98.3367%),                 avg. length: 910.3,                last time consumption/overall running time: 101.8536s / 317863.5742 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.0889
env0_second_0:                 episode reward: 9.2000,                 loss: 0.5349
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 29521/30000 (98.4033%),                 avg. length: 908.0,                last time consumption/overall running time: 99.1549s / 317962.7290 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.1018
env0_second_0:                 episode reward: 9.4500,                 loss: 0.1370
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 29541/30000 (98.4700%),                 avg. length: 911.2,                last time consumption/overall running time: 93.6670s / 318056.3961 s
env0_first_0:                 episode reward: -9.4000,                 loss: -0.1072
env0_second_0:                 episode reward: 9.4000,                 loss: 0.3923
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 29561/30000 (98.5367%),                 avg. length: 919.2,                last time consumption/overall running time: 107.4502s / 318163.8463 s
env0_first_0:                 episode reward: -9.0000,                 loss: -0.1132
env0_second_0:                 episode reward: 9.0000,                 loss: 0.3089
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 29581/30000 (98.6033%),                 avg. length: 908.4,                last time consumption/overall running time: 98.4673s / 318262.3136 s
env0_first_0:                 episode reward: -8.9000,                 loss: -0.1179
env0_second_0:                 episode reward: 8.9000,                 loss: 0.2144
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 29601/30000 (98.6700%),                 avg. length: 912.75,                last time consumption/overall running time: 90.7672s / 318353.0808 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1047
env0_second_0:                 episode reward: 9.3000,                 loss: 1.6384
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 29621/30000 (98.7367%),                 avg. length: 915.35,                last time consumption/overall running time: 93.7691s / 318446.8498 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.1250
env0_second_0:                 episode reward: 8.7500,                 loss: 1.0173
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 29641/30000 (98.8033%),                 avg. length: 907.2,                last time consumption/overall running time: 90.1437s / 318536.9935 s
env0_first_0:                 episode reward: -9.3500,                 loss: -0.1182
env0_second_0:                 episode reward: 9.3500,                 loss: 1.0613
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 29661/30000 (98.8700%),                 avg. length: 908.4,                last time consumption/overall running time: 91.3390s / 318628.3325 s
env0_first_0:                 episode reward: -9.1000,                 loss: -0.1192
env0_second_0:                 episode reward: 9.1000,                 loss: 0.7297
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 29681/30000 (98.9367%),                 avg. length: 916.75,                last time consumption/overall running time: 91.9246s / 318720.2571 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.1204
env0_second_0:                 episode reward: 9.2000,                 loss: 0.8217
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 29701/30000 (99.0033%),                 avg. length: 909.2,                last time consumption/overall running time: 92.0984s / 318812.3555 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.1036
env0_second_0:                 episode reward: 9.7500,                 loss: 0.7451
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 29721/30000 (99.0700%),                 avg. length: 907.4,                last time consumption/overall running time: 96.9897s / 318909.3453 s
env0_first_0:                 episode reward: -9.7000,                 loss: -0.1187
env0_second_0:                 episode reward: 9.7000,                 loss: 0.4107
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 29741/30000 (99.1367%),                 avg. length: 907.55,                last time consumption/overall running time: 90.7732s / 319000.1185 s
env0_first_0:                 episode reward: -9.8500,                 loss: -0.1523
env0_second_0:                 episode reward: 9.8500,                 loss: 0.0561
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 29761/30000 (99.2033%),                 avg. length: 907.8,                last time consumption/overall running time: 98.8791s / 319098.9976 s
env0_first_0:                 episode reward: -9.8500,                 loss: -0.1467
env0_second_0:                 episode reward: 9.8500,                 loss: -0.0314
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 29781/30000 (99.2700%),                 avg. length: 909.0,                last time consumption/overall running time: 104.5173s / 319203.5149 s
env0_first_0:                 episode reward: -9.5500,                 loss: -0.1412
env0_second_0:                 episode reward: 9.5500,                 loss: 0.1368
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 29801/30000 (99.3367%),                 avg. length: 907.0,                last time consumption/overall running time: 102.5697s / 319306.0846 s
env0_first_0:                 episode reward: -9.9000,                 loss: -0.1075
env0_second_0:                 episode reward: 9.9000,                 loss: 0.0985
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 29821/30000 (99.4033%),                 avg. length: 907.4,                last time consumption/overall running time: 95.7662s / 319401.8508 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.1368
env0_second_0:                 episode reward: 9.6000,                 loss: 0.4119
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 29841/30000 (99.4700%),                 avg. length: 907.0,                last time consumption/overall running time: 92.5085s / 319494.3593 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1455
env0_second_0:                 episode reward: 9.3000,                 loss: 0.2822Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
/home/zihan/research/MARS/mars/rl/agents/nash_ppo.py:181: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  s,a,r,s_prime,prob_a,done_mask =    torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst).to(self.device), \
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 29861/30000 (99.5367%),                 avg. length: 907.0,                last time consumption/overall running time: 98.0454s / 319592.4047 s
env0_first_0:                 episode reward: -9.9500,                 loss: -0.1341
env0_second_0:                 episode reward: 9.9500,                 loss: 0.4410
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 29881/30000 (99.6033%),                 avg. length: 907.0,                last time consumption/overall running time: 90.7836s / 319683.1883 s
env0_first_0:                 episode reward: -10.0000,                 loss: -0.1175
env0_second_0:                 episode reward: 10.0000,                 loss: 0.9984
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 29901/30000 (99.6700%),                 avg. length: 917.65,                last time consumption/overall running time: 94.7441s / 319777.9325 s
env0_first_0:                 episode reward: -9.6000,                 loss: -0.0941
env0_second_0:                 episode reward: 9.6000,                 loss: 0.9803
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 29921/30000 (99.7367%),                 avg. length: 908.4,                last time consumption/overall running time: 95.0066s / 319872.9391 s
env0_first_0:                 episode reward: -9.8000,                 loss: -0.1285
env0_second_0:                 episode reward: 9.8000,                 loss: 0.3019
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 29941/30000 (99.8033%),                 avg. length: 969.15,                last time consumption/overall running time: 96.6892s / 319969.6283 s
env0_first_0:                 episode reward: -9.1500,                 loss: -0.0834
env0_second_0:                 episode reward: 9.1500,                 loss: 0.1624
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 29961/30000 (99.8700%),                 avg. length: 910.1,                last time consumption/overall running time: 91.8447s / 320061.4730 s
env0_first_0:                 episode reward: -9.7500,                 loss: -0.0987
env0_second_0:                 episode reward: 9.7500,                 loss: -0.0754
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 29981/30000 (99.9367%),                 avg. length: 1075.15,                last time consumption/overall running time: 107.1772s / 320168.6502 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.0169
env0_second_0:                 episode reward: 7.5500,                 loss: 0.5332
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
