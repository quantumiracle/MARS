pygame 2.0.0 (SDL 2.0.12, python 3.7.10)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 33.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f51978a3c90>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): ReLU()
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ReLU()
      (4): Linear(in_features=32, out_features=32, bias=True)
      (5): ReLU()
      (6): Linear(in_features=32, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=32, bias=True)
      (1): ReLU()
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): ReLU()
      (4): Linear(in_features=32, out_features=32, bias=True)
      (5): ReLU()
      (6): Linear(in_features=32, out_features=3, bias=True)
    )
  )
)
Error: no exploited model index given!
Agents No. [0] (index starting from 0) are not learnable.
Agent No. [0] loads model from:  data/model/20220510123946/mdp_arbitrary_mdp_selfplay2/2000_0
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 1.0, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 100000}, 'num_process': 1, 'batch_size': 640, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220510123946/mdp_arbitrary_mdp_selfplay2/2000_0', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 1000, 'net_architecture': {'hidden_dim_list': [32, 32, 32], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/zihan/research/MARS/data/model/20220510123946_exploit_2000/mdp_arbitrary_mdp_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220510123946_exploit_2000/mdp_arbitrary_mdp_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.0830s / 1.0830 s
agent0:                 episode reward: 1.3973,                 loss: nan
agent1:                 episode reward: -1.3973,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 9.0,                last time consumption/overall running time: 0.1113s / 1.1943 s
agent0:                 episode reward: 1.1086,                 loss: nan
agent1:                 episode reward: -1.1086,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 9.0,                last time consumption/overall running time: 0.1116s / 1.3059 s
agent0:                 episode reward: 0.9108,                 loss: nan
agent1:                 episode reward: -0.9108,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 9.0,                last time consumption/overall running time: 0.1123s / 1.4182 s
agent0:                 episode reward: 0.1543,                 loss: nan
agent1:                 episode reward: -0.1543,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.1063s / 2.5245 s
agent0:                 episode reward: 1.3165,                 loss: nan
agent1:                 episode reward: -1.3165,                 loss: 0.4490
Episode: 101/10000 (1.0100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.2822s / 3.8067 s
agent0:                 episode reward: 0.3490,                 loss: nan
agent1:                 episode reward: -0.3490,                 loss: 0.4342
Episode: 121/10000 (1.2100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.2681s / 5.0748 s
agent0:                 episode reward: 1.2893,                 loss: nan
agent1:                 episode reward: -1.2893,                 loss: 0.4154
Episode: 141/10000 (1.4100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.2812s / 6.3561 s
agent0:                 episode reward: 1.1892,                 loss: nan
agent1:                 episode reward: -1.1892,                 loss: 0.4068
Episode: 161/10000 (1.6100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.2759s / 7.6320 s
agent0:                 episode reward: 0.8369,                 loss: nan
agent1:                 episode reward: -0.8369,                 loss: 0.4090
Episode: 181/10000 (1.8100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3154s / 8.9474 s
agent0:                 episode reward: 1.1773,                 loss: nan
agent1:                 episode reward: -1.1773,                 loss: 0.3741
Episode: 201/10000 (2.0100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3042s / 10.2516 s
agent0:                 episode reward: 1.3368,                 loss: nan
agent1:                 episode reward: -1.3368,                 loss: 0.3600
Episode: 221/10000 (2.2100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3257s / 11.5773 s
agent0:                 episode reward: 1.1287,                 loss: nan
agent1:                 episode reward: -1.1287,                 loss: 0.3559
Episode: 241/10000 (2.4100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.2925s / 12.8698 s
agent0:                 episode reward: 1.1707,                 loss: nan
agent1:                 episode reward: -1.1707,                 loss: 0.3506
Episode: 261/10000 (2.6100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.2929s / 14.1627 s
agent0:                 episode reward: 0.9102,                 loss: nan
agent1:                 episode reward: -0.9102,                 loss: 0.3467
Episode: 281/10000 (2.8100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.2996s / 15.4623 s
agent0:                 episode reward: 1.0743,                 loss: nan
agent1:                 episode reward: -1.0743,                 loss: 0.3228
Episode: 301/10000 (3.0100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3387s / 16.8010 s
agent0:                 episode reward: 1.0736,                 loss: nan
agent1:                 episode reward: -1.0736,                 loss: 0.3179
Episode: 321/10000 (3.2100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3096s / 18.1106 s
agent0:                 episode reward: 1.2535,                 loss: nan
agent1:                 episode reward: -1.2535,                 loss: 0.3132
Episode: 341/10000 (3.4100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3102s / 19.4208 s
agent0:                 episode reward: 1.6354,                 loss: nan
agent1:                 episode reward: -1.6354,                 loss: 0.3127
Episode: 361/10000 (3.6100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3121s / 20.7329 s
agent0:                 episode reward: 0.3488,                 loss: nan
agent1:                 episode reward: -0.3488,                 loss: 0.3117
Episode: 381/10000 (3.8100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3692s / 22.1021 s/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

agent0:                 episode reward: 0.4649,                 loss: nan
agent1:                 episode reward: -0.4649,                 loss: 0.3140
Episode: 401/10000 (4.0100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3566s / 23.4587 s
agent0:                 episode reward: 1.6576,                 loss: nan
agent1:                 episode reward: -1.6576,                 loss: 0.3089
Episode: 421/10000 (4.2100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3276s / 24.7863 s
agent0:                 episode reward: 1.2857,                 loss: nan
agent1:                 episode reward: -1.2857,                 loss: 0.3084
Episode: 441/10000 (4.4100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3519s / 26.1382 s
agent0:                 episode reward: 0.8714,                 loss: nan
agent1:                 episode reward: -0.8714,                 loss: 0.3073
Episode: 461/10000 (4.6100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3319s / 27.4701 s
agent0:                 episode reward: 1.4047,                 loss: nan
agent1:                 episode reward: -1.4047,                 loss: 0.3063
Episode: 481/10000 (4.8100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3630s / 28.8331 s
agent0:                 episode reward: 0.9363,                 loss: nan
agent1:                 episode reward: -0.9363,                 loss: 0.3254
Episode: 501/10000 (5.0100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3450s / 30.1780 s
agent0:                 episode reward: 0.2545,                 loss: nan
agent1:                 episode reward: -0.2545,                 loss: 0.3298
Episode: 521/10000 (5.2100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3481s / 31.5261 s
agent0:                 episode reward: 1.0391,                 loss: nan
agent1:                 episode reward: -1.0391,                 loss: 0.3285
Episode: 541/10000 (5.4100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3599s / 32.8860 s
agent0:                 episode reward: 0.9052,                 loss: nan
agent1:                 episode reward: -0.9052,                 loss: 0.3280
Episode: 561/10000 (5.6100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3746s / 34.2606 s
agent0:                 episode reward: 1.2556,                 loss: nan
agent1:                 episode reward: -1.2556,                 loss: 0.3257
Episode: 581/10000 (5.8100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3835s / 35.6441 s
agent0:                 episode reward: 0.9977,                 loss: nan
agent1:                 episode reward: -0.9977,                 loss: 0.3532
Episode: 601/10000 (6.0100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3597s / 37.0038 s
agent0:                 episode reward: 1.1212,                 loss: nan
agent1:                 episode reward: -1.1212,                 loss: 0.3532
Episode: 621/10000 (6.2100%),                 avg. length: 9.0,                last time consumption/overall running time: 1.3794s / 38.3832 s
agent0:                 episode reward: 0.5978,                 loss: nan
agent1:                 episode reward: -0.5978,                 loss: 0.3538
Traceback (most recent call last):
  File "exploit_arbitrary_mdp3.py", line 51, in <module>
    launch_rollout(parser_args.method, parser_args.load_id, parser_args.epi)
  File "exploit_arbitrary_mdp3.py", line 43, in launch_rollout
    rollout(env, model, args, load_id+f'_exploit_{epi}')
  File "/home/zihan/research/MARS/mars/rollout.py", line 21, in rollout
    rollout_normal(env, model, save_id, args)
  File "/home/zihan/research/MARS/mars/rollout.py", line 161, in rollout_normal
    loss = model.update(
  File "/home/zihan/research/MARS/mars/rl/agents/multiagent.py", line 281, in update
    loss = agent.update()
  File "/home/zihan/research/MARS/mars/rl/agents/dqn.py", line 141, in update
    self.optimizer.step()
  File "/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/optim/adam.py", line 153, in step
    maximize=group['maximize'])
  File "/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/optim/_functional.py", line 105, in adam
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt
