pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 1076.0,                last time consumption/overall running time: 9.6494s / 9.6494 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0048
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 907.65,                last time consumption/overall running time: 147.7587s / 157.4081 s
env0_first_0:                 episode reward: 7.8500,                 loss: 0.0023
env0_second_0:                 episode reward: -7.8500,                 loss: nan
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/21_0.
Episode: 41/10000 (0.4100%),                 avg. length: 1464.65,                last time consumption/overall running time: 541.6548s / 699.0629 s
env0_first_0:                 episode reward: -5.3500,                 loss: nan
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0028
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1378.2,                last time consumption/overall running time: 603.4601s / 1302.5230 s
env0_first_0:                 episode reward: -6.2500,                 loss: nan
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0027
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1311.3,                last time consumption/overall running time: 578.7039s / 1881.2269 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0030
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0025
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/72_1.
Episode: 101/10000 (1.0100%),                 avg. length: 1754.65,                last time consumption/overall running time: 779.0518s / 2660.2787 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0026
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1667.35,                last time consumption/overall running time: 742.9780s / 3403.2567 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1573.85,                last time consumption/overall running time: 702.7249s / 4105.9816 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0012
env0_second_0:                 episode reward: -6.0000,                 loss: 0.0023
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/136_0.
Episode: 161/10000 (1.6100%),                 avg. length: 1603.9,                last time consumption/overall running time: 716.4397s / 4822.4213 s
env0_first_0:                 episode reward: -2.6000,                 loss: nan
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0016
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1507.15,                last time consumption/overall running time: 672.9408s / 5495.3621 s
env0_first_0:                 episode reward: -4.5000,                 loss: nan
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0016
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1390.35,                last time consumption/overall running time: 620.5911s / 6115.9532 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0043
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0016
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Score delta: 16.6, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/190_1.
Episode: 221/10000 (2.2100%),                 avg. length: 1868.4,                last time consumption/overall running time: 834.6975s / 6950.6507 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0023
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1766.0,                last time consumption/overall running time: 788.4094s / 7739.0601 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1740.9,                last time consumption/overall running time: 777.2471s / 8516.3071 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1735.2,                last time consumption/overall running time: 775.2709s / 9291.5780 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.8000,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1660.45,                last time consumption/overall running time: 741.7375s / 10033.3155 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0014
env0_second_0:                 episode reward: -6.1500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1553.1,                last time consumption/overall running time: 693.4989s / 10726.8144 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1465.35,                last time consumption/overall running time: 654.1567s / 11380.9711 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0011
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1472.9,                last time consumption/overall running time: 657.9873s / 12038.9583 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0010
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1596.15,                last time consumption/overall running time: 714.0167s / 12752.9750 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0010
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1527.2,                last time consumption/overall running time: 682.4049s / 13435.3800 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0010
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0032
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/392_0.
Episode: 421/10000 (4.2100%),                 avg. length: 1491.45,                last time consumption/overall running time: 666.3753s / 14101.7553 s
env0_first_0:                 episode reward: -3.7000,                 loss: nan
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0019
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1579.7,                last time consumption/overall running time: 706.8618s / 14808.6171 s
env0_first_0:                 episode reward: -4.9000,                 loss: nan
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0017
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1395.95,                last time consumption/overall running time: 625.5029s / 15434.1200 s
env0_first_0:                 episode reward: -6.1500,                 loss: nan
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0015
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1485.75,                last time consumption/overall running time: 663.9326s / 16098.0526 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0040
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0014
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Score delta: 16.8, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/471_1.
Episode: 501/10000 (5.0100%),                 avg. length: 1917.45,                last time consumption/overall running time: 857.2322s / 16955.2847 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0028
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1735.0,                last time consumption/overall running time: 775.3996s / 17730.6843 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0021
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1658.2,                last time consumption/overall running time: 740.5731s / 18471.2574 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1566.45,                last time consumption/overall running time: 699.7676s / 19171.0251 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1803.2,                last time consumption/overall running time: 804.7495s / 19975.7746 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0012
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1880.85,                last time consumption/overall running time: 839.3930s / 20815.1676 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0015
env0_second_0:                 episode reward: -1.6500,                 loss: nan
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1683.05,                last time consumption/overall running time: 752.3630s / 21567.5306 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1667.65,                last time consumption/overall running time: 745.9748s / 22313.5054 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1454.65,                last time consumption/overall running time: 650.4967s / 22964.0021 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0013
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0015
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Score delta: 16.8, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/648_0.
Episode: 681/10000 (6.8100%),                 avg. length: 1617.9,                last time consumption/overall running time: 722.6408s / 23686.6429 s
env0_first_0:                 episode reward: -3.3500,                 loss: nan
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0016
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1646.85,                last time consumption/overall running time: 735.1506s / 24421.7936 s
env0_first_0:                 episode reward: -4.8000,                 loss: nan
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0015
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1442.8,                last time consumption/overall running time: 645.3412s / 25067.1347 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0028
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0015
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Score delta: 16.6, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/712_1.
Episode: 741/10000 (7.4100%),                 avg. length: 1864.6,                last time consumption/overall running time: 834.5252s / 25901.6599 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2100.45,                last time consumption/overall running time: 940.6398s / 26842.2997 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0015
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1904.2,                last time consumption/overall running time: 852.4891s / 27694.7888 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -1.8500,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1687.15,                last time consumption/overall running time: 753.8641s / 28448.6529 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1901.95,                last time consumption/overall running time: 850.0343s / 29298.6872 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0016
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1810.05,                last time consumption/overall running time: 808.5747s / 30107.2619 s
env0_first_0:                 episode reward: 3.6000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.6000,                 loss: nan
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1665.15,                last time consumption/overall running time: 743.6312s / 30850.8931 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1552.9,                last time consumption/overall running time: 694.6358s / 31545.5288 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1506.75,                last time consumption/overall running time: 673.7268s / 32219.2556 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0013
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1524.35,                last time consumption/overall running time: 681.2361s / 32900.4917 s
env0_first_0:                 episode reward: 6.2500,                 loss: 0.0011
env0_second_0:                 episode reward: -6.2500,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1512.8,                last time consumption/overall running time: 675.8922s / 33576.3839 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0012
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1660.9,                last time consumption/overall running time: 741.2105s / 34317.5944 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0013
env0_second_0:                 episode reward: -2.6000,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1868.8,                last time consumption/overall running time: 834.4486s / 35152.0430 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.4000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1429.6,                last time consumption/overall running time: 638.3859s / 35790.4289 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1605.15,                last time consumption/overall running time: 716.2016s / 36506.6305 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1610.05,                last time consumption/overall running time: 718.6872s / 37225.3177 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1535.9,                last time consumption/overall running time: 754.0923s / 37979.4100 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0013
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1494.0,                last time consumption/overall running time: 784.8771s / 38764.2870 s
env0_first_0:                 episode reward: 6.1000,                 loss: 0.0013
env0_second_0:                 episode reward: -6.1000,                 loss: nan
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1518.8,                last time consumption/overall running time: 801.5245s / 39565.8116 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.1500,                 loss: nan
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1601.3,                last time consumption/overall running time: 847.5381s / 40413.3497 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1518.3,                last time consumption/overall running time: 802.8390s / 41216.1887 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0013
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1477.2,                last time consumption/overall running time: 782.2917s / 41998.4804 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0011
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1543.05,                last time consumption/overall running time: 817.4334s / 42815.9138 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0012
env0_second_0:                 episode reward: -2.2000,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1539.45,                last time consumption/overall running time: 814.5532s / 43630.4670 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0012
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1531.55,                last time consumption/overall running time: 811.0593s / 44441.5262 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0012
env0_second_0:                 episode reward: -6.0500,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1645.5,                last time consumption/overall running time: 871.2480s / 45312.7743 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0013
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1644.65,                last time consumption/overall running time: 871.4697s / 46184.2439 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1572.9,                last time consumption/overall running time: 834.1052s / 47018.3491 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1449.5,                last time consumption/overall running time: 768.4978s / 47786.8469 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.0500,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1660.5,                last time consumption/overall running time: 878.6401s / 48665.4870 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1589.0,                last time consumption/overall running time: 841.6575s / 49507.1445 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -2.8500,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1810.85,                last time consumption/overall running time: 958.7015s / 50465.8460 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0018
env0_second_0:                 episode reward: -2.8500,                 loss: nan
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1783.95,                last time consumption/overall running time: 945.6896s / 51411.5356 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0022
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1625.55,                last time consumption/overall running time: 862.2398s / 52273.7753 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1648.75,                last time consumption/overall running time: 873.1967s / 53146.9720 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0018
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1768.9,                last time consumption/overall running time: 937.4780s / 54084.4500 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1489.2,                last time consumption/overall running time: 789.6476s / 54874.0976 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1642.85,                last time consumption/overall running time: 870.7391s / 55744.8367 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1613.05,                last time consumption/overall running time: 854.0192s / 56598.8560 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1581.85,                last time consumption/overall running time: 837.7552s / 57436.6112 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1637.05,                last time consumption/overall running time: 866.9477s / 58303.5589 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1543.35,                last time consumption/overall running time: 817.0832s / 59120.6421 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0017
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0025
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Score delta: 16.6, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/1547_0.
Episode: 1581/10000 (15.8100%),                 avg. length: 1547.6,                last time consumption/overall running time: 819.1737s / 59939.8158 s
env0_first_0:                 episode reward: -2.6000,                 loss: nan
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0020
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1572.35,                last time consumption/overall running time: 831.4894s / 60771.3052 s
env0_first_0:                 episode reward: -5.8500,                 loss: nan
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0018
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1619.9,                last time consumption/overall running time: 858.9994s / 61630.3047 s
env0_first_0:                 episode reward: -1.6500,                 loss: nan
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0017
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1655.3,                last time consumption/overall running time: 878.2746s / 62508.5792 s
env0_first_0:                 episode reward: -3.7500,                 loss: nan
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0018
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1384.6,                last time consumption/overall running time: 734.3767s / 63242.9559 s
env0_first_0:                 episode reward: -5.8500,                 loss: nan
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0016
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1700.45,                last time consumption/overall running time: 901.3422s / 64144.2981 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0019
env0_second_0:                 episode reward: -2.0500,                 loss: 0.0015
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Score delta: 16.6, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/1662_1.
Episode: 1701/10000 (17.0100%),                 avg. length: 1654.8,                last time consumption/overall running time: 877.5348s / 65021.8329 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.2000,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1764.55,                last time consumption/overall running time: 934.7586s / 65956.5915 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1448.25,                last time consumption/overall running time: 768.2504s / 66724.8419 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0013
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1587.5,                last time consumption/overall running time: 841.6184s / 67566.4603 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0012
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1612.95,                last time consumption/overall running time: 855.2076s / 68421.6680 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0012
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1530.85,                last time consumption/overall running time: 811.9783s / 69233.6463 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0012
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1612.5,                last time consumption/overall running time: 855.4177s / 70089.0640 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0012
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1609.15,                last time consumption/overall running time: 851.7715s / 70940.8355 s
env0_first_0:                 episode reward: 5.5000,                 loss: 0.0012
env0_second_0:                 episode reward: -5.5000,                 loss: nan
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1529.3,                last time consumption/overall running time: 811.8451s / 71752.6806 s
env0_first_0:                 episode reward: 5.7500,                 loss: 0.0013
env0_second_0:                 episode reward: -5.7500,                 loss: nan
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1642.05,                last time consumption/overall running time: 868.7110s / 72621.3917 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0012
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1691.7,                last time consumption/overall running time: 895.8184s / 73517.2101 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0012
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1808.35,                last time consumption/overall running time: 957.4703s / 74474.6803 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0016
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1876.35,                last time consumption/overall running time: 993.8626s / 75468.5429 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -2.2500,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1852.2,                last time consumption/overall running time: 980.7050s / 76449.2479 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0018
env0_second_0:                 episode reward: -1.3500,                 loss: nan
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1914.25,                last time consumption/overall running time: 1013.8374s / 77463.0853 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -2.1500,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1690.2,                last time consumption/overall running time: 896.0738s / 78359.1591 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1763.5,                last time consumption/overall running time: 934.9336s / 79294.0927 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1630.15,                last time consumption/overall running time: 864.2241s / 80158.3168 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1769.4,                last time consumption/overall running time: 936.5949s / 81094.9116 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0012
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1598.2,                last time consumption/overall running time: 846.8402s / 81941.7518 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0013
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1841.8,                last time consumption/overall running time: 976.0315s / 82917.7833 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0014
env0_second_0:                 episode reward: -2.6000,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1859.7,                last time consumption/overall running time: 986.3439s / 83904.1272 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0016
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1766.6,                last time consumption/overall running time: 936.1825s / 84840.3097 s
env0_first_0:                 episode reward: 3.0500,                 loss: 0.0016
env0_second_0:                 episode reward: -3.0500,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1920.95,                last time consumption/overall running time: 1016.9672s / 85857.2769 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0017
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1694.4,                last time consumption/overall running time: 897.1253s / 86754.4022 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1631.15,                last time consumption/overall running time: 863.9671s / 87618.3693 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1616.1,                last time consumption/overall running time: 856.3336s / 88474.7029 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1710.4,                last time consumption/overall running time: 906.8210s / 89381.5239 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1729.6,                last time consumption/overall running time: 917.1246s / 90298.6485 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1667.6,                last time consumption/overall running time: 884.3717s / 91183.0202 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.5500,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1799.4,                last time consumption/overall running time: 953.0438s / 92136.0639 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0018
env0_second_0:                 episode reward: -1.9000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1759.95,                last time consumption/overall running time: 933.6036s / 93069.6676 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0019
env0_second_0:                 episode reward: -3.4000,                 loss: nan
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1668.55,                last time consumption/overall running time: 885.3912s / 93955.0587 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1475.2,                last time consumption/overall running time: 783.0945s / 94738.1532 s
env0_first_0:                 episode reward: 6.5000,                 loss: 0.0015
env0_second_0:                 episode reward: -6.5000,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1619.35,                last time consumption/overall running time: 859.9084s / 95598.0616 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1650.6,                last time consumption/overall running time: 875.9521s / 96474.0137 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1655.45,                last time consumption/overall running time: 877.4382s / 97351.4519 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.8000,                 loss: nan
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1761.45,                last time consumption/overall running time: 931.6373s / 98283.0892 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1484.3,                last time consumption/overall running time: 785.5564s / 99068.6456 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1554.0,                last time consumption/overall running time: 821.6587s / 99890.3044 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1554.1,                last time consumption/overall running time: 820.5764s / 100710.8808 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.8000,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1555.55,                last time consumption/overall running time: 821.5116s / 101532.3924 s
env0_first_0:                 episode reward: 5.7500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.7500,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1484.95,                last time consumption/overall running time: 784.0933s / 102316.4857 s
env0_first_0:                 episode reward: 5.3000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.3000,                 loss: nan
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1555.2,                last time consumption/overall running time: 822.6654s / 103139.1511 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1586.15,                last time consumption/overall running time: 838.4955s / 103977.6466 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1605.15,                last time consumption/overall running time: 847.9657s / 104825.6122 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1458.35,                last time consumption/overall running time: 770.3793s / 105595.9915 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1502.95,                last time consumption/overall running time: 795.1986s / 106391.1901 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1480.0,                last time consumption/overall running time: 782.8689s / 107174.0590 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0014
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1423.25,                last time consumption/overall running time: 751.5020s / 107925.5610 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0013
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1503.4,                last time consumption/overall running time: 794.5343s / 108720.0953 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0013
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1540.2,                last time consumption/overall running time: 813.8648s / 109533.9601 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1690.85,                last time consumption/overall running time: 895.2242s / 110429.1844 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.4000,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1601.55,                last time consumption/overall running time: 847.0866s / 111276.2710 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0019
env0_second_0:                 episode reward: -2.3500,                 loss: nan
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1743.45,                last time consumption/overall running time: 922.4458s / 112198.7167 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0018
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1616.25,                last time consumption/overall running time: 854.6427s / 113053.3594 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1645.8,                last time consumption/overall running time: 870.1037s / 113923.4631 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1612.75,                last time consumption/overall running time: 853.0123s / 114776.4754 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1559.95,                last time consumption/overall running time: 825.2703s / 115601.7457 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1509.05,                last time consumption/overall running time: 797.8709s / 116399.6166 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0015
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1635.15,                last time consumption/overall running time: 864.0560s / 117263.6726 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1641.45,                last time consumption/overall running time: 867.8505s / 118131.5230 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0017
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1802.55,                last time consumption/overall running time: 952.1200s / 119083.6431 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0021
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1532.15,                last time consumption/overall running time: 810.0035s / 119893.6466 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0020
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1729.8,                last time consumption/overall running time: 914.3761s / 120808.0227 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1525.3,                last time consumption/overall running time: 806.0878s / 121614.1104 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1688.95,                last time consumption/overall running time: 892.5734s / 122506.6838 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0018
env0_second_0:                 episode reward: -2.5500,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1511.45,                last time consumption/overall running time: 799.5868s / 123306.2706 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1580.9,                last time consumption/overall running time: 835.5728s / 124141.8434 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1711.25,                last time consumption/overall running time: 904.6147s / 125046.4581 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1723.55,                last time consumption/overall running time: 909.4329s / 125955.8910 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1647.75,                last time consumption/overall running time: 871.8691s / 126827.7600 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0018
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1526.4,                last time consumption/overall running time: 807.3962s / 127635.1563 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0018
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1557.15,                last time consumption/overall running time: 823.2012s / 128458.3574 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1531.2,                last time consumption/overall running time: 809.2995s / 129267.6569 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0017
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1457.5,                last time consumption/overall running time: 771.7997s / 130039.4566 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1450.6,                last time consumption/overall running time: 767.9234s / 130807.3800 s
env0_first_0:                 episode reward: 2.7500,                 loss: 0.0013
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0016
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/3211_0.
Episode: 3241/10000 (32.4100%),                 avg. length: 1482.45,                last time consumption/overall running time: 784.1674s / 131591.5474 s
env0_first_0:                 episode reward: -6.2000,                 loss: nan
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0016
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1547.15,                last time consumption/overall running time: 818.3018s / 132409.8492 s
env0_first_0:                 episode reward: -5.4000,                 loss: nan
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0015
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1455.9,                last time consumption/overall running time: 769.7586s / 133179.6078 s
env0_first_0:                 episode reward: -6.1500,                 loss: nan
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0014
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1587.85,                last time consumption/overall running time: 839.2971s / 134018.9049 s
env0_first_0:                 episode reward: -5.3500,                 loss: nan
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0014
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1387.75,                last time consumption/overall running time: 734.0911s / 134752.9960 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0044
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0014
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/3318_1.
Episode: 3341/10000 (33.4100%),                 avg. length: 1949.8,                last time consumption/overall running time: 1031.4999s / 135784.4959 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0019
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1825.55,                last time consumption/overall running time: 965.2268s / 136749.7227 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0019
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1901.3,                last time consumption/overall running time: 1005.5975s / 137755.3202 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0018
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1951.75,                last time consumption/overall running time: 1032.0446s / 138787.3648 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0016
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1817.15,                last time consumption/overall running time: 959.0153s / 139746.3801 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0016
env0_second_0:                 episode reward: -1.6000,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1882.65,                last time consumption/overall running time: 994.7553s / 140741.1353 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -2.5500,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1766.0,                last time consumption/overall running time: 933.8813s / 141675.0167 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1814.25,                last time consumption/overall running time: 958.9774s / 142633.9940 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1989.35,                last time consumption/overall running time: 1051.3865s / 143685.3805 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0015
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1844.7,                last time consumption/overall running time: 958.9023s / 144644.2828 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.3000,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1689.2,                last time consumption/overall running time: 841.9060s / 145486.1888 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1876.3,                last time consumption/overall running time: 934.2242s / 146420.4130 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1696.25,                last time consumption/overall running time: 844.2246s / 147264.6376 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1853.85,                last time consumption/overall running time: 923.3030s / 148187.9406 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0016
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1778.2,                last time consumption/overall running time: 886.5585s / 149074.4991 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0018
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1647.8,                last time consumption/overall running time: 821.3892s / 149895.8884 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0018
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1768.75,                last time consumption/overall running time: 880.5831s / 150776.4715 s
env0_first_0:                 episode reward: 3.6000,                 loss: 0.0019
env0_second_0:                 episode reward: -3.6000,                 loss: nan
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1773.45,                last time consumption/overall running time: 847.3230s / 151623.7945 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0017
env0_second_0:                 episode reward: -2.6000,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1862.2,                last time consumption/overall running time: 867.2397s / 152491.0342 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0017
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1845.95,                last time consumption/overall running time: 859.4888s / 153350.5231 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0019
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1831.5,                last time consumption/overall running time: 852.9521s / 154203.4751 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0019
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1728.3,                last time consumption/overall running time: 804.5668s / 155008.0420 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1752.25,                last time consumption/overall running time: 816.3740s / 155824.4160 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0018
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1943.95,                last time consumption/overall running time: 905.3648s / 156729.7807 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0020
env0_second_0:                 episode reward: -2.2000,                 loss: nan
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1904.6,                last time consumption/overall running time: 887.0009s / 157616.7817 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0019
env0_second_0:                 episode reward: -2.0500,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1674.7,                last time consumption/overall running time: 780.6261s / 158397.4078 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0018
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1876.8,                last time consumption/overall running time: 873.8200s / 159271.2278 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0018
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1746.95,                last time consumption/overall running time: 814.1632s / 160085.3910 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0018
env0_second_0:                 episode reward: -3.1500,                 loss: nan
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1946.2,                last time consumption/overall running time: 857.0541s / 160942.4451 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0019
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1680.3,                last time consumption/overall running time: 728.6083s / 161671.0534 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0018
env0_second_0:                 episode reward: -0.8500,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1757.5,                last time consumption/overall running time: 760.3673s / 162431.4207 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0018
env0_second_0:                 episode reward: -1.4500,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1757.25,                last time consumption/overall running time: 760.8511s / 163192.2718 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0019
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1617.9,                last time consumption/overall running time: 701.4760s / 163893.7478 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1787.75,                last time consumption/overall running time: 775.6086s / 164669.3563 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0018
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1635.55,                last time consumption/overall running time: 709.4319s / 165378.7882 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1901.85,                last time consumption/overall running time: 824.7954s / 166203.5836 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0020
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1992.85,                last time consumption/overall running time: 863.6943s / 167067.2778 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0021
env0_second_0:                 episode reward: -2.0500,                 loss: nan
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1926.65,                last time consumption/overall running time: 834.8780s / 167902.1558 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0023
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1803.75,                last time consumption/overall running time: 781.2141s / 168683.3700 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0020
env0_second_0:                 episode reward: -2.2500,                 loss: nan
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1813.05,                last time consumption/overall running time: 785.1820s / 169468.5519 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0020
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1795.1,                last time consumption/overall running time: 778.0011s / 170246.5530 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0023
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1679.95,                last time consumption/overall running time: 728.4057s / 170974.9587 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0020
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1560.25,                last time consumption/overall running time: 676.3077s / 171651.2664 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1807.05,                last time consumption/overall running time: 782.7185s / 172433.9849 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0018
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1624.75,                last time consumption/overall running time: 704.1201s / 173138.1050 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1845.3,                last time consumption/overall running time: 800.4777s / 173938.5827 s
env0_first_0:                 episode reward: 3.1000,                 loss: 0.0019
env0_second_0:                 episode reward: -3.1000,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1713.85,                last time consumption/overall running time: 742.7514s / 174681.3342 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0018
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1798.0,                last time consumption/overall running time: 779.6200s / 175460.9541 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0017
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1667.5,                last time consumption/overall running time: 722.6210s / 176183.5751 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1795.5,                last time consumption/overall running time: 777.9544s / 176961.5295 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -2.5500,                 loss: nan
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1786.25,                last time consumption/overall running time: 773.7949s / 177735.3244 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1704.85,                last time consumption/overall running time: 737.4121s / 178472.7365 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0018
env0_second_0:                 episode reward: -2.8500,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1741.45,                last time consumption/overall running time: 753.1561s / 179225.8926 s
env0_first_0:                 episode reward: 3.1000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.1000,                 loss: nan
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1646.9,                last time consumption/overall running time: 713.0183s / 179938.9108 s
env0_first_0:                 episode reward: 3.1000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.1000,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1859.15,                last time consumption/overall running time: 805.5649s / 180744.4758 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -2.2000,                 loss: nan
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1783.95,                last time consumption/overall running time: 772.7699s / 181517.2457 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0019
env0_second_0:                 episode reward: -1.9000,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1833.85,                last time consumption/overall running time: 794.2167s / 182311.4624 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0019
env0_second_0:                 episode reward: -1.4500,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1844.0,                last time consumption/overall running time: 798.6181s / 183110.0805 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0019
env0_second_0:                 episode reward: -2.6000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1781.95,                last time consumption/overall running time: 771.4079s / 183881.4884 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0021
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1679.0,                last time consumption/overall running time: 727.3351s / 184608.8235 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0018
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1594.9,                last time consumption/overall running time: 691.3333s / 185300.1568 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0017
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1708.4,                last time consumption/overall running time: 739.6638s / 186039.8206 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1621.3,                last time consumption/overall running time: 701.5662s / 186741.3868 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1715.85,                last time consumption/overall running time: 741.9726s / 187483.3594 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1599.3,                last time consumption/overall running time: 691.3639s / 188174.7233 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1657.0,                last time consumption/overall running time: 716.6796s / 188891.4029 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1510.2,                last time consumption/overall running time: 654.2132s / 189545.6161 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.0500,                 loss: 0.0029
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/4656_0.
Episode: 4681/10000 (46.8100%),                 avg. length: 1677.35,                last time consumption/overall running time: 725.0192s / 190270.6353 s
env0_first_0:                 episode reward: -2.0000,                 loss: nan
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0017
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1436.15,                last time consumption/overall running time: 620.0589s / 190890.6942 s
env0_first_0:                 episode reward: -4.1000,                 loss: nan
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0017
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1829.0,                last time consumption/overall running time: 790.1169s / 191680.8110 s
env0_first_0:                 episode reward: -2.6500,                 loss: nan
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0016
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1464.2,                last time consumption/overall running time: 631.7311s / 192312.5422 s
env0_first_0:                 episode reward: -6.0000,                 loss: nan
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0014
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1493.75,                last time consumption/overall running time: 644.3868s / 192956.9290 s
env0_first_0:                 episode reward: -6.6500,                 loss: nan
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0014
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1563.45,                last time consumption/overall running time: 675.0176s / 193631.9466 s
env0_first_0:                 episode reward: -1.9500,                 loss: nan
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0015
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1556.55,                last time consumption/overall running time: 671.6975s / 194303.6441 s
env0_first_0:                 episode reward: -3.6000,                 loss: nan
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0017
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1526.3,                last time consumption/overall running time: 658.1551s / 194961.7992 s
env0_first_0:                 episode reward: -4.8500,                 loss: nan
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0018
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1642.8,                last time consumption/overall running time: 707.6953s / 195669.4945 s
env0_first_0:                 episode reward: -3.3500,                 loss: nan
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0016
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1470.95,                last time consumption/overall running time: 633.5673s / 196303.0618 s
env0_first_0:                 episode reward: -4.8000,                 loss: nan
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0016
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1684.05,                last time consumption/overall running time: 725.8469s / 197028.9086 s
env0_first_0:                 episode reward: -4.5500,                 loss: nan
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0016
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1744.45,                last time consumption/overall running time: 751.2652s / 197780.1738 s
env0_first_0:                 episode reward: -4.9000,                 loss: nan
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0017
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1706.4,                last time consumption/overall running time: 734.6607s / 198514.8344 s
env0_first_0:                 episode reward: -5.8500,                 loss: nan
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0014
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1491.5,                last time consumption/overall running time: 642.1341s / 199156.9685 s
env0_first_0:                 episode reward: -5.6500,                 loss: nan
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0013
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1672.4,                last time consumption/overall running time: 719.2673s / 199876.2358 s
env0_first_0:                 episode reward: -3.0000,                 loss: nan
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0014
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1396.15,                last time consumption/overall running time: 600.2704s / 200476.5062 s
env0_first_0:                 episode reward: -5.7500,                 loss: nan
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0014
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1748.9,                last time consumption/overall running time: 750.3123s / 201226.8186 s
env0_first_0:                 episode reward: -4.4000,                 loss: nan
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0015
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1459.15,                last time consumption/overall running time: 626.3123s / 201853.1309 s
env0_first_0:                 episode reward: -6.0500,                 loss: nan
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0015
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1456.35,                last time consumption/overall running time: 624.8531s / 202477.9840 s
env0_first_0:                 episode reward: -5.8000,                 loss: nan
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0014
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1659.05,                last time consumption/overall running time: 712.4504s / 203190.4344 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0024
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0014
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Score delta: 16.8, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_fictitious_selfplay2/5046_1.
Episode: 5081/10000 (50.8100%),                 avg. length: 1762.7,                last time consumption/overall running time: 757.9167s / 203948.3511 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0020
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1701.1,                last time consumption/overall running time: 729.9563s / 204678.3074 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0019
env0_second_0:                 episode reward: -2.3000,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1780.5,                last time consumption/overall running time: 763.8182s / 205442.1256 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0018
env0_second_0:                 episode reward: -2.3500,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1787.7,                last time consumption/overall running time: 766.9511s / 206209.0767 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0019
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1745.4,                last time consumption/overall running time: 749.6279s / 206958.7046 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0020
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1672.45,                last time consumption/overall running time: 719.1528s / 207677.8574 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0020
env0_second_0:                 episode reward: -1.3000,                 loss: nan
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1847.95,                last time consumption/overall running time: 793.2519s / 208471.1094 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0026
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1653.2,                last time consumption/overall running time: 709.3860s / 209180.4954 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0026
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1682.25,                last time consumption/overall running time: 722.4273s / 209902.9227 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0022
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1756.3,                last time consumption/overall running time: 753.3245s / 210656.2472 s
env0_first_0:                 episode reward: 5.3000,                 loss: 0.0021
env0_second_0:                 episode reward: -5.3000,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1646.1,                last time consumption/overall running time: 706.3186s / 211362.5658 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0022
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1662.45,                last time consumption/overall running time: 713.6176s / 212076.1833 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0023
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1892.1,                last time consumption/overall running time: 812.1126s / 212888.2959 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0026
env0_second_0:                 episode reward: -2.4000,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1756.45,                last time consumption/overall running time: 754.7930s / 213643.0889 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0023
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1742.55,                last time consumption/overall running time: 721.4054s / 214364.4943 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0031
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1751.75,                last time consumption/overall running time: 708.7875s / 215073.2818 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0023
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1835.6,                last time consumption/overall running time: 743.0182s / 215816.3000 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0021
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1864.45,                last time consumption/overall running time: 753.9897s / 216570.2898 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0020
env0_second_0:                 episode reward: -1.3500,                 loss: nan
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1740.85,                last time consumption/overall running time: 703.9034s / 217274.1931 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0021
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1869.4,                last time consumption/overall running time: 755.8209s / 218030.0140 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0020
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1759.85,                last time consumption/overall running time: 710.7887s / 218740.8027 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0022
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1617.9,                last time consumption/overall running time: 654.3710s / 219395.1737 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0022
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1836.35,                last time consumption/overall running time: 742.1150s / 220137.2887 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0020
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1611.6,                last time consumption/overall running time: 651.7333s / 220789.0220 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0019
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1712.5,                last time consumption/overall running time: 691.4815s / 221480.5036 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0019
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1780.8,                last time consumption/overall running time: 719.5065s / 222200.0101 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0020
env0_second_0:                 episode reward: -3.1500,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1904.25,                last time consumption/overall running time: 769.3471s / 222969.3572 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0021
env0_second_0:                 episode reward: -1.5500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1854.25,                last time consumption/overall running time: 748.6225s / 223717.9797 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0020
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1906.35,                last time consumption/overall running time: 769.7868s / 224487.7666 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0021
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1717.05,                last time consumption/overall running time: 693.2400s / 225181.0065 s
env0_first_0:                 episode reward: 2.7500,                 loss: 0.0021
env0_second_0:                 episode reward: -2.7500,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1671.45,                last time consumption/overall running time: 675.3177s / 225856.3242 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0021
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1735.5,                last time consumption/overall running time: 701.4604s / 226557.7846 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0021
env0_second_0:                 episode reward: -3.2000,                 loss: nan
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1771.95,                last time consumption/overall running time: 715.5318s / 227273.3164 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0021
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1719.45,                last time consumption/overall running time: 660.7427s / 227934.0591 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0022
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1716.95,                last time consumption/overall running time: 652.3566s / 228586.4157 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0022
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1631.15,                last time consumption/overall running time: 584.9097s / 229171.3255 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0021
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1680.15,                last time consumption/overall running time: 597.2772s / 229768.6027 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1923.05,                last time consumption/overall running time: 683.1328s / 230451.7355 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0020
env0_second_0:                 episode reward: -2.4000,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1815.7,                last time consumption/overall running time: 644.1710s / 231095.9065 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0020
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1762.85,                last time consumption/overall running time: 624.3527s / 231720.2592 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1689.3,                last time consumption/overall running time: 599.1193s / 232319.3785 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1693.4,                last time consumption/overall running time: 601.3911s / 232920.7696 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0020
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1625.9,                last time consumption/overall running time: 575.7282s / 233496.4979 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0019
env0_second_0:                 episode reward: -6.0500,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1642.25,                last time consumption/overall running time: 582.5736s / 234079.0715 s
env0_first_0:                 episode reward: 5.5000,                 loss: 0.0018
env0_second_0:                 episode reward: -5.5000,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1768.65,                last time consumption/overall running time: 627.8007s / 234706.8722 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1816.8,                last time consumption/overall running time: 645.2025s / 235352.0747 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0020
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 1724.5,                last time consumption/overall running time: 612.5502s / 235964.6249 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0018
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1843.85,                last time consumption/overall running time: 654.5937s / 236619.2186 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0020
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1742.45,                last time consumption/overall running time: 618.2023s / 237237.4209 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0021
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 1787.25,                last time consumption/overall running time: 634.6953s / 237872.1162 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0019
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1508.35,                last time consumption/overall running time: 535.5584s / 238407.6746 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0020
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1784.05,                last time consumption/overall running time: 633.2871s / 239040.9617 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0020
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1711.4,                last time consumption/overall running time: 607.7590s / 239648.7207 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0021
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1584.2,                last time consumption/overall running time: 562.1638s / 240210.8844 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0021
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1639.95,                last time consumption/overall running time: 581.5236s / 240792.4080 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0020
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1661.7,                last time consumption/overall running time: 589.6566s / 241382.0646 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1574.65,                last time consumption/overall running time: 559.1400s / 241941.2047 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0019
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1711.35,                last time consumption/overall running time: 607.1975s / 242548.4022 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.8000,                 loss: nan
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1649.1,                last time consumption/overall running time: 585.7154s / 243134.1176 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0021
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1617.65,                last time consumption/overall running time: 574.2041s / 243708.3217 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0021
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1730.05,                last time consumption/overall running time: 614.3045s / 244322.6262 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0020
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1649.7,                last time consumption/overall running time: 585.0173s / 244907.6435 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1788.1,                last time consumption/overall running time: 634.3593s / 245542.0028 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0020
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 1714.7,                last time consumption/overall running time: 608.6522s / 246150.6550 s
env0_first_0:                 episode reward: 3.4500,                 loss: 0.0021
env0_second_0:                 episode reward: -3.4500,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1680.0,                last time consumption/overall running time: 596.8884s / 246747.5434 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0022
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1561.85,                last time consumption/overall running time: 554.0013s / 247301.5448 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0021
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 1743.25,                last time consumption/overall running time: 617.9807s / 247919.5255 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0023
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1762.1,                last time consumption/overall running time: 626.1221s / 248545.6476 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0023
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1636.9,                last time consumption/overall running time: 581.4044s / 249127.0520 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0022
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1583.85,                last time consumption/overall running time: 561.9288s / 249688.9808 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0020
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1731.6,                last time consumption/overall running time: 614.8033s / 250303.7840 s
env0_first_0:                 episode reward: 5.3000,                 loss: 0.0020
env0_second_0:                 episode reward: -5.3000,                 loss: nan
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1611.25,                last time consumption/overall running time: 570.7864s / 250874.5704 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0021
env0_second_0:                 episode reward: -5.5500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1771.95,                last time consumption/overall running time: 628.8300s / 251503.4004 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0021
env0_second_0:                 episode reward: -3.4000,                 loss: nan
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1763.85,                last time consumption/overall running time: 626.2741s / 252129.6746 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0023
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1836.5,                last time consumption/overall running time: 651.1483s / 252780.8229 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0022
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1731.3,                last time consumption/overall running time: 614.0653s / 253394.8882 s
env0_first_0:                 episode reward: 3.4500,                 loss: 0.0022
env0_second_0:                 episode reward: -3.4500,                 loss: nan
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1750.4,                last time consumption/overall running time: 620.2878s / 254015.1760 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0021
env0_second_0:                 episode reward: -1.4000,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1904.05,                last time consumption/overall running time: 674.8512s / 254690.0273 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0024
env0_second_0:                 episode reward: -1.3000,                 loss: nan
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1694.95,                last time consumption/overall running time: 600.3898s / 255290.4171 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0024
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1750.25,                last time consumption/overall running time: 620.0728s / 255910.4899 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0024
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 1726.95,                last time consumption/overall running time: 612.0018s / 256522.4917 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0022
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1664.1,                last time consumption/overall running time: 591.0444s / 257113.5361 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0021
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1690.65,                last time consumption/overall running time: 599.2216s / 257712.7577 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0022
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1692.3,                last time consumption/overall running time: 600.0704s / 258312.8281 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0021
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1808.4,                last time consumption/overall running time: 640.3245s / 258953.1526 s
env0_first_0:                 episode reward: 2.0000,                 loss: 0.0022
env0_second_0:                 episode reward: -2.0000,                 loss: nan
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1702.55,                last time consumption/overall running time: 602.2359s / 259555.3884 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0024
env0_second_0:                 episode reward: -2.3000,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1825.85,                last time consumption/overall running time: 645.2442s / 260200.6327 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0022
env0_second_0:                 episode reward: -3.5500,                 loss: nan
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1832.3,                last time consumption/overall running time: 648.5558s / 260849.1884 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0024
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1751.25,                last time consumption/overall running time: 619.3674s / 261468.5559 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0025
env0_second_0:                 episode reward: -1.7000,                 loss: nan
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1754.75,                last time consumption/overall running time: 621.1655s / 262089.7214 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0024
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1741.95,                last time consumption/overall running time: 616.6467s / 262706.3680 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0023
env0_second_0:                 episode reward: -1.4000,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1736.6,                last time consumption/overall running time: 614.9646s / 263321.3327 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0023
env0_second_0:                 episode reward: -0.8000,                 loss: nan
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 1875.5,                last time consumption/overall running time: 663.3820s / 263984.7146 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0023
env0_second_0:                 episode reward: -1.4500,                 loss: nan
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1692.85,                last time consumption/overall running time: 598.5131s / 264583.2277 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0024
env0_second_0:                 episode reward: -1.7500,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1882.1,                last time consumption/overall running time: 666.7926s / 265250.0203 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0025
env0_second_0:                 episode reward: -1.7000,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1755.65,                last time consumption/overall running time: 621.6793s / 265871.6996 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0023
env0_second_0:                 episode reward: -2.1000,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1763.05,                last time consumption/overall running time: 623.2202s / 266494.9198 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0022
env0_second_0:                 episode reward: -4.0500,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 1843.55,                last time consumption/overall running time: 651.3989s / 267146.3187 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0023
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1896.2,                last time consumption/overall running time: 671.2499s / 267817.5686 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0025
env0_second_0:                 episode reward: -2.4000,                 loss: nan
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1911.35,                last time consumption/overall running time: 676.1042s / 268493.6728 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0022
env0_second_0:                 episode reward: -1.4000,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1806.6,                last time consumption/overall running time: 640.2822s / 269133.9550 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0022
env0_second_0:                 episode reward: -2.9500,                 loss: nan
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1752.65,                last time consumption/overall running time: 619.7947s / 269753.7497 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0022
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 1677.9,                last time consumption/overall running time: 594.6566s / 270348.4063 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0022
env0_second_0:                 episode reward: -5.5500,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 1632.45,                last time consumption/overall running time: 577.6458s / 270926.0521 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0020
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1763.95,                last time consumption/overall running time: 623.7311s / 271549.7832 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0019
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 1619.25,                last time consumption/overall running time: 572.2978s / 272122.0811 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0020
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1556.6,                last time consumption/overall running time: 551.3654s / 272673.4465 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0021
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1796.45,                last time consumption/overall running time: 635.3808s / 273308.8273 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0022
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1819.0,                last time consumption/overall running time: 643.4037s / 273952.2310 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0022
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1553.6,                last time consumption/overall running time: 550.2474s / 274502.4784 s
env0_first_0:                 episode reward: 3.4500,                 loss: 0.0023
env0_second_0:                 episode reward: -3.4500,                 loss: nan
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1753.95,                last time consumption/overall running time: 620.7127s / 275123.1911 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0023
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1747.85,                last time consumption/overall running time: 619.2160s / 275742.4071 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0022
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1787.1,                last time consumption/overall running time: 632.1665s / 276374.5736 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0023
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1698.65,                last time consumption/overall running time: 601.1969s / 276975.7705 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0022
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1828.4,                last time consumption/overall running time: 646.0114s / 277621.7818 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0026
env0_second_0:                 episode reward: -4.0500,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1741.45,                last time consumption/overall running time: 617.0332s / 278238.8150 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0023
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1842.15,                last time consumption/overall running time: 651.8318s / 278890.6468 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0022
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1782.3,                last time consumption/overall running time: 629.8519s / 279520.4987 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0023
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1889.5,                last time consumption/overall running time: 667.8064s / 280188.3052 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0024
env0_second_0:                 episode reward: -2.9500,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1734.8,                last time consumption/overall running time: 612.9678s / 280801.2729 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0024
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1752.9,                last time consumption/overall running time: 619.2306s / 281420.5035 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0024
env0_second_0:                 episode reward: -1.7500,                 loss: nan
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 1769.75,                last time consumption/overall running time: 625.3673s / 282045.8708 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0024
env0_second_0:                 episode reward: -3.2000,                 loss: nan
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1747.45,                last time consumption/overall running time: 616.9673s / 282662.8381 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0023
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1906.5,                last time consumption/overall running time: 673.1892s / 283336.0273 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0023
env0_second_0:                 episode reward: -2.9500,                 loss: nan
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1713.15,                last time consumption/overall running time: 604.9641s / 283940.9914 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0023
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1687.65,                last time consumption/overall running time: 596.2100s / 284537.2014 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0025
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1948.8,                last time consumption/overall running time: 689.0317s / 285226.2331 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0027
env0_second_0:                 episode reward: -2.2500,                 loss: nan
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1575.25,                last time consumption/overall running time: 557.3537s / 285783.5868 s
env0_first_0:                 episode reward: 3.3500,                 loss: 0.0027
env0_second_0:                 episode reward: -3.3500,                 loss: nan
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1734.9,                last time consumption/overall running time: 613.2829s / 286396.8697 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0025
env0_second_0:                 episode reward: -2.5500,                 loss: nan
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1839.8,                last time consumption/overall running time: 650.5524s / 287047.4221 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0024
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 1784.7,                last time consumption/overall running time: 629.7692s / 287677.1913 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0026
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1859.15,                last time consumption/overall running time: 656.3456s / 288333.5369 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0028
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1789.4,                last time consumption/overall running time: 632.3260s / 288965.8629 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0027
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan