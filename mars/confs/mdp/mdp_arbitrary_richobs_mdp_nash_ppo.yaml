env_args:
    env_name: arbitrary_mdp
    env_type: mdp
    num_envs: 1
    ram: True
    seed: random

agent_args:
  algorithm: NashPPO
  algorithm_spec:
    episodic_update: true
    gamma: 1.0
    lambda: 0.0  # standard TD
    eps_clip: 0.2
    K_epoch: 4
    GAE: false  # there is just no GAE in NashPPO

train_args:
  batch_size: 640
  max_episodes: 100000
  max_steps_per_episode: 10000
  train_start_frame: 0
  optimizer: adam
  learning_rate: 1e-4
  device: gpu
  update_itr: 1
  log_avg_window: 10
  log_interval: 10
  save_interval: 1000 # episode interval to save models
  multiprocess: false
  net_architecture:
    policy:
      hidden_dim_list:
      - 128
      - 128
      - 128
      hidden_activation: ReLU
      output_activation: Softmax
    value:
      hidden_dim_list:
      - 128
      - 128
      - 128
      hidden_activation: ReLU
      output_activation: false
  marl_method: nash_ppo
  marl_spec:
    {}
