pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_surround_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_surround_v1_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 1678.0,                last time consumption/overall running time: 24.1761s / 24.1761 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0091
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0103
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1355.45,                last time consumption/overall running time: 673.7375s / 697.9136 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0099
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0097
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1328.2,                last time consumption/overall running time: 957.2483s / 1655.1619 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0094
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0094
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1271.75,                last time consumption/overall running time: 994.6595s / 2649.8214 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0091
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0090
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1339.75,                last time consumption/overall running time: 1083.6513s / 3733.4727 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0088
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0086
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1211.1,                last time consumption/overall running time: 993.9956s / 4727.4683 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0082
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0084
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1250.7,                last time consumption/overall running time: 1032.9799s / 5760.4482 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0082
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0083
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1215.3,                last time consumption/overall running time: 999.7911s / 6760.2393 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0082
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0079
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1255.65,                last time consumption/overall running time: 1039.2798s / 7799.5191 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0078
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0077
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1287.95,                last time consumption/overall running time: 1069.1543s / 8868.6734 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0078
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0079
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1292.2,                last time consumption/overall running time: 1071.3583s / 9940.0317 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0076
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0076
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1213.85,                last time consumption/overall running time: 1003.2121s / 10943.2438 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0075
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0073
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1178.5,                last time consumption/overall running time: 971.7987s / 11915.0425 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0071
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0070
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1283.15,                last time consumption/overall running time: 1067.7167s / 12982.7592 s
env0_first_0:                 episode reward: -4.9000,                 loss: 0.0071
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0068
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1256.1,                last time consumption/overall running time: 1037.3487s / 14020.1080 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0069
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0066
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1129.1,                last time consumption/overall running time: 942.4313s / 14962.5392 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0069
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0062
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1088.15,                last time consumption/overall running time: 903.1416s / 15865.6809 s
env0_first_0:                 episode reward: -8.2500,                 loss: 0.0068
env0_second_0:                 episode reward: 8.2500,                 loss: 0.0060
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1099.7,                last time consumption/overall running time: 914.3080s / 16779.9889 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0060
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0056
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1164.95,                last time consumption/overall running time: 958.6830s / 17738.6719 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0059
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0054
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1163.35,                last time consumption/overall running time: 971.3669s / 18710.0388 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0058
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0053
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1159.35,                last time consumption/overall running time: 965.4419s / 19675.4807 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0058
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0054
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1222.9,                last time consumption/overall running time: 1020.1388s / 20695.6195 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0055
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0049
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1224.95,                last time consumption/overall running time: 1012.5974s / 21708.2169 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0055
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0053
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1109.9,                last time consumption/overall running time: 917.1042s / 22625.3211 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0056
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0051
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1085.05,                last time consumption/overall running time: 907.3396s / 23532.6607 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0047
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1233.8,                last time consumption/overall running time: 1032.0007s / 24564.6613 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0047
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0046
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1206.25,                last time consumption/overall running time: 998.8604s / 25563.5217 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0052
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0047
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1211.6,                last time consumption/overall running time: 1004.6483s / 26568.1699 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0051
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0049
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1155.6,                last time consumption/overall running time: 959.4010s / 27527.5709 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0054
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0051
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1156.25,                last time consumption/overall running time: 957.7488s / 28485.3197 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0053
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0052
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1114.6,                last time consumption/overall running time: 928.6896s / 29414.0093 s
env0_first_0:                 episode reward: -8.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 8.0500,                 loss: 0.0052
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1141.6,                last time consumption/overall running time: 947.4090s / 30361.4183 s
env0_first_0:                 episode reward: -8.1500,                 loss: 0.0050
env0_second_0:                 episode reward: 8.1500,                 loss: 0.0054
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1232.35,                last time consumption/overall running time: 1025.2199s / 31386.6382 s
env0_first_0:                 episode reward: -7.5500,                 loss: 0.0050
env0_second_0:                 episode reward: 7.5500,                 loss: 0.0054
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1179.15,                last time consumption/overall running time: 985.5550s / 32372.1932 s
env0_first_0:                 episode reward: -6.7000,                 loss: 0.0056
env0_second_0:                 episode reward: 6.7000,                 loss: 0.0056
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1236.4,                last time consumption/overall running time: 1027.5105s / 33399.7037 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0059
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0062
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1303.8,                last time consumption/overall running time: 1087.0209s / 34486.7246 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0059
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0061
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1242.1,                last time consumption/overall running time: 1033.1563s / 35519.8809 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0060
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0061
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1195.3,                last time consumption/overall running time: 992.7171s / 36512.5980 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0059
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0060
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1238.2,                last time consumption/overall running time: 1055.3266s / 37567.9246 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0055
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0061
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1170.45,                last time consumption/overall running time: 1130.8299s / 38698.7544 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0054
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0057
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1168.25,                last time consumption/overall running time: 1128.2045s / 39826.9589 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0048
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1203.1,                last time consumption/overall running time: 1158.5682s / 40985.5272 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0046
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0048
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1230.6,                last time consumption/overall running time: 1185.5673s / 42171.0945 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0043
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0047
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1186.05,                last time consumption/overall running time: 1144.3144s / 43315.4088 s
env0_first_0:                 episode reward: -8.2000,                 loss: 0.0045
env0_second_0:                 episode reward: 8.2000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1271.4,                last time consumption/overall running time: 1222.1044s / 44537.5133 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0047
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0048
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1304.05,                last time consumption/overall running time: 1248.0637s / 45785.5770 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0051
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0051
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1348.6,                last time consumption/overall running time: 1291.0759s / 47076.6529 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0055
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0058
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1247.25,                last time consumption/overall running time: 1190.0494s / 48266.7023 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0056
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0058
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1300.35,                last time consumption/overall running time: 1248.0613s / 49514.7636 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0056
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0056
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1267.9,                last time consumption/overall running time: 1219.1423s / 50733.9059 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0056
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0058
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1306.75,                last time consumption/overall running time: 1251.5560s / 51985.4619 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0058
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0055
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1289.9,                last time consumption/overall running time: 1234.6003s / 53220.0622 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0053
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1248.45,                last time consumption/overall running time: 1194.7326s / 54414.7947 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0052
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1259.1,                last time consumption/overall running time: 1208.6183s / 55623.4130 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0050
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1272.65,                last time consumption/overall running time: 1218.1841s / 56841.5971 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0048
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0049
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1247.7,                last time consumption/overall running time: 1198.2495s / 58039.8466 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0050
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1178.05,                last time consumption/overall running time: 1127.4101s / 59167.2567 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0047
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0049
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1225.9,                last time consumption/overall running time: 1183.6083s / 60350.8650 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0047
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0046
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1239.2,                last time consumption/overall running time: 1189.9617s / 61540.8267 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0044
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0049
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1204.55,                last time consumption/overall running time: 1156.0830s / 62696.9097 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0045
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0044
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1207.15,                last time consumption/overall running time: 1154.0590s / 63850.9687 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0047
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0045
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1251.55,                last time consumption/overall running time: 1200.3488s / 65051.3175 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0049
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0048
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1247.35,                last time consumption/overall running time: 1194.1924s / 66245.5099 s
env0_first_0:                 episode reward: -6.7000,                 loss: 0.0048
env0_second_0:                 episode reward: 6.7000,                 loss: 0.0046
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1240.5,                last time consumption/overall running time: 1188.1110s / 67433.6209 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0046
env0_second_0:                 episode reward: 8.0000,                 loss: 0.0044
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1264.95,                last time consumption/overall running time: 1212.1894s / 68645.8103 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0044
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0041
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1180.3,                last time consumption/overall running time: 1129.4010s / 69775.2113 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0043
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0041
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1344.55,                last time consumption/overall running time: 1296.2182s / 71071.4295 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0046
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0045
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1199.8,                last time consumption/overall running time: 1147.9889s / 72219.4184 s
env0_first_0:                 episode reward: -7.4500,                 loss: 0.0050
env0_second_0:                 episode reward: 7.4500,                 loss: 0.0053
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1228.6,                last time consumption/overall running time: 1177.4832s / 73396.9016 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0045
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0048
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1223.8,                last time consumption/overall running time: 1175.9018s / 74572.8034 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0044
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0044
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1265.55,                last time consumption/overall running time: 1212.0389s / 75784.8423 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0043
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0046
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1266.8,                last time consumption/overall running time: 1215.8443s / 77000.6866 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0046
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0047
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1248.35,                last time consumption/overall running time: 1193.8787s / 78194.5652 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0046
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0048
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1210.35,                last time consumption/overall running time: 1158.8861s / 79353.4513 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0046
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0050
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1225.4,                last time consumption/overall running time: 1175.4709s / 80528.9223 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1329.9,                last time consumption/overall running time: 1276.4759s / 81805.3981 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0049
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0047
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1319.5,                last time consumption/overall running time: 1260.3213s / 83065.7195 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0048
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0047
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1302.65,                last time consumption/overall running time: 1243.1543s / 84308.8737 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0045
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1240.15,                last time consumption/overall running time: 1184.7945s / 85493.6682 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.0047
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0043
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1227.05,                last time consumption/overall running time: 1177.0706s / 86670.7389 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0044
env0_second_0:                 episode reward: 7.6500,                 loss: 0.0041
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1228.15,                last time consumption/overall running time: 1175.5963s / 87846.3352 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0042
env0_second_0:                 episode reward: 8.0000,                 loss: 0.0041
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1221.4,                last time consumption/overall running time: 1170.6191s / 89016.9543 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0041
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0041
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1223.7,                last time consumption/overall running time: 1174.6371s / 90191.5914 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0042
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0041
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1269.05,                last time consumption/overall running time: 1213.9285s / 91405.5199 s
env0_first_0:                 episode reward: -6.7000,                 loss: 0.0044
env0_second_0:                 episode reward: 6.7000,                 loss: 0.0044
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1388.0,                last time consumption/overall running time: 1327.4506s / 92732.9706 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0050
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0048
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1388.85,                last time consumption/overall running time: 1322.1244s / 94055.0950 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0052
env0_second_0:                 episode reward: 6.3500,                 loss: 0.0053
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1303.7,                last time consumption/overall running time: 1239.4842s / 95294.5791 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1253.05,                last time consumption/overall running time: 1187.3155s / 96481.8946 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0048
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0049
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1229.9,                last time consumption/overall running time: 1162.3674s / 97644.2621 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 7.6500,                 loss: 0.0046
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1310.8,                last time consumption/overall running time: 1231.9249s / 98876.1870 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0041
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0045
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1351.95,                last time consumption/overall running time: 1270.0932s / 100146.2801 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0046
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1401.95,                last time consumption/overall running time: 1325.2805s / 101471.5606 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0052
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0056
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1314.95,                last time consumption/overall running time: 1242.8035s / 102714.3642 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0053
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1325.1,                last time consumption/overall running time: 1247.3133s / 103961.6775 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0048
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1384.85,                last time consumption/overall running time: 1310.1455s / 105271.8230 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0048
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1389.95,                last time consumption/overall running time: 1306.2588s / 106578.0818 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1335.05,                last time consumption/overall running time: 1264.7748s / 107842.8566 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0048
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0052
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1429.15,                last time consumption/overall running time: 1344.8180s / 109187.6746 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0049
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1458.25,                last time consumption/overall running time: 1368.7732s / 110556.4478 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0048
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1408.2,                last time consumption/overall running time: 1319.9011s / 111876.3489 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0052
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0053
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1259.45,                last time consumption/overall running time: 1183.5505s / 113059.8993 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0048
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0051
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1298.8,                last time consumption/overall running time: 1214.3289s / 114274.2282 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0045
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1321.75,                last time consumption/overall running time: 1239.4621s / 115513.6903 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0044
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1392.5,                last time consumption/overall running time: 1310.1136s / 116823.8039 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0044
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0048
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1318.95,                last time consumption/overall running time: 1236.4843s / 118060.2881 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0045
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0050
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1406.15,                last time consumption/overall running time: 1323.0057s / 119383.2939 s
env0_first_0:                 episode reward: -6.7000,                 loss: 0.0049
env0_second_0:                 episode reward: 6.7000,                 loss: 0.0051
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1360.65,                last time consumption/overall running time: 1278.7206s / 120662.0145 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0053
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1369.5,                last time consumption/overall running time: 1293.5023s / 121955.5168 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0052
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0054
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1477.55,                last time consumption/overall running time: 1392.9075s / 123348.4243 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0054
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1394.7,                last time consumption/overall running time: 1315.1935s / 124663.6178 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0050
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1331.35,                last time consumption/overall running time: 1247.4716s / 125911.0894 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0046
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1390.65,                last time consumption/overall running time: 1305.9292s / 127217.0186 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0049
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1354.55,                last time consumption/overall running time: 1274.5280s / 128491.5466 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0051
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1362.3,                last time consumption/overall running time: 1282.7435s / 129774.2902 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0053
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0054
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1330.3,                last time consumption/overall running time: 1254.1729s / 131028.4630 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0052
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0053
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1341.1,                last time consumption/overall running time: 1262.3344s / 132290.7975 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0050
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0052
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1311.8,                last time consumption/overall running time: 1236.6796s / 133527.4771 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0051
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1406.1,                last time consumption/overall running time: 1333.5789s / 134861.0560 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0054
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0049
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1264.3,                last time consumption/overall running time: 1187.1305s / 136048.1865 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0053
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0050
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1258.8,                last time consumption/overall running time: 1187.7446s / 137235.9311 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0052
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1434.95,                last time consumption/overall running time: 1353.1904s / 138589.1215 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0055
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1325.9,                last time consumption/overall running time: 1254.5424s / 139843.6639 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0058
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0055
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1419.05,                last time consumption/overall running time: 1343.4896s / 141187.1536 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0054
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1347.05,                last time consumption/overall running time: 1263.8965s / 142451.0500 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0053
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0048
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1412.75,                last time consumption/overall running time: 1336.5861s / 143787.6361 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0049
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0045
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1364.9,                last time consumption/overall running time: 1276.5549s / 145064.1910 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0050
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1425.35,                last time consumption/overall running time: 1318.5283s / 146382.7193 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0050
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1355.3,                last time consumption/overall running time: 1256.9120s / 147639.6313 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0051
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1337.45,                last time consumption/overall running time: 1236.0703s / 148875.7015 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0052
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1438.15,                last time consumption/overall running time: 1330.0539s / 150205.7555 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0050
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1323.65,                last time consumption/overall running time: 1228.0320s / 151433.7875 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0051
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0051
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1342.2,                last time consumption/overall running time: 1235.4425s / 152669.2301 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0046
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1470.8,                last time consumption/overall running time: 1355.4010s / 154024.6311 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0049
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1416.8,                last time consumption/overall running time: 1300.7527s / 155325.3838 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1265.15,                last time consumption/overall running time: 1172.2116s / 156497.5954 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0047
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0047
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1393.95,                last time consumption/overall running time: 1282.5777s / 157780.1731 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0045
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0044
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1275.35,                last time consumption/overall running time: 1174.7231s / 158954.8961 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0044
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0044
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1361.1,                last time consumption/overall running time: 1260.0671s / 160214.9632 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0046
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0045
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1326.9,                last time consumption/overall running time: 1209.3907s / 161424.3539 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0045
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0045
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1302.45,                last time consumption/overall running time: 1192.2461s / 162616.6000 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0046
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0047
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1375.0,                last time consumption/overall running time: 1262.2659s / 163878.8659 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0048
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0047
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1361.85,                last time consumption/overall running time: 1244.5379s / 165123.4038 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0049
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0048
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1408.8,                last time consumption/overall running time: 1285.0573s / 166408.4611 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.3500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1446.7,                last time consumption/overall running time: 1314.7731s / 167723.2342 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0048
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1415.15,                last time consumption/overall running time: 1294.2687s / 169017.5028 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0052
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1452.75,                last time consumption/overall running time: 1322.3813s / 170339.8841 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0055
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0055
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1450.15,                last time consumption/overall running time: 1321.1530s / 171661.0371 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0051
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0055
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1417.75,                last time consumption/overall running time: 1294.7615s / 172955.7986 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0054
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0054
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1529.35,                last time consumption/overall running time: 1400.2514s / 174356.0500 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0055
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0056
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1450.25,                last time consumption/overall running time: 1321.2621s / 175677.3121 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0054
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1441.7,                last time consumption/overall running time: 1323.2351s / 177000.5472 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1418.25,                last time consumption/overall running time: 1292.9220s / 178293.4692 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1347.65,                last time consumption/overall running time: 1216.4933s / 179509.9625 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0049
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1427.05,                last time consumption/overall running time: 1285.5578s / 180795.5203 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0047
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1333.9,                last time consumption/overall running time: 1206.7882s / 182002.3085 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0048
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1418.65,                last time consumption/overall running time: 1286.2216s / 183288.5301 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1407.7,                last time consumption/overall running time: 1280.1619s / 184568.6921 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0053
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1487.4,                last time consumption/overall running time: 1352.8189s / 185921.5110 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0051
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0051
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1449.0,                last time consumption/overall running time: 1298.2006s / 187219.7115 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0055
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0051
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1455.55,                last time consumption/overall running time: 1312.7171s / 188532.4286 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0051
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1391.15,                last time consumption/overall running time: 1251.6757s / 189784.1043 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0052
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1646.25,                last time consumption/overall running time: 1473.2828s / 191257.3871 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0057
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0054
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1587.85,                last time consumption/overall running time: 1412.1850s / 192669.5721 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0059
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0059
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1503.0,                last time consumption/overall running time: 1329.1439s / 193998.7161 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0062
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0060
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1548.55,                last time consumption/overall running time: 1364.3923s / 195363.1084 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.0060
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0059
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1523.1,                last time consumption/overall running time: 1341.1010s / 196704.2094 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0058
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0055
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1421.35,                last time consumption/overall running time: 1247.4294s / 197951.6388 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0055
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0054
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1478.4,                last time consumption/overall running time: 1295.7452s / 199247.3840 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0053
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0052
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1565.35,                last time consumption/overall running time: 1363.8074s / 200611.1914 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0056
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0055
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1513.2,                last time consumption/overall running time: 1295.8433s / 201907.0347 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0055
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0056
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1347.3,                last time consumption/overall running time: 1155.5964s / 203062.6311 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0054
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0055
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1417.8,                last time consumption/overall running time: 1218.6650s / 204281.2961 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0055
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0057
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1382.4,                last time consumption/overall running time: 1194.5147s / 205475.8108 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0052
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0056
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1459.15,                last time consumption/overall running time: 1254.0812s / 206729.8920 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0051
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0051
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1510.35,                last time consumption/overall running time: 1305.2384s / 208035.1304 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0052
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1376.9,                last time consumption/overall running time: 1193.3252s / 209228.4556 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0053
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0052
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1448.85,                last time consumption/overall running time: 1252.5930s / 210481.0486 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1438.0,                last time consumption/overall running time: 1246.7882s / 211727.8368 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0051
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1454.05,                last time consumption/overall running time: 1249.3755s / 212977.2124 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0050
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1514.5,                last time consumption/overall running time: 1301.8293s / 214279.0416 s
env0_first_0:                 episode reward: -4.9000,                 loss: 0.0053
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0051
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1452.6,                last time consumption/overall running time: 1237.4935s / 215516.5351 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0051
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1404.8,                last time consumption/overall running time: 1203.5313s / 216720.0664 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0049
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0049
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1381.8,                last time consumption/overall running time: 1171.5163s / 217891.5827 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0048
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0048
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1395.3,                last time consumption/overall running time: 1181.4131s / 219072.9958 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0046
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0048
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1376.3,                last time consumption/overall running time: 1174.5338s / 220247.5295 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0048
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1455.7,                last time consumption/overall running time: 1226.7047s / 221474.2342 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0047
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0048
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1465.1,                last time consumption/overall running time: 1236.6187s / 222710.8529 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0047
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0048
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1393.2,                last time consumption/overall running time: 1181.2485s / 223892.1013 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0049
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1351.1,                last time consumption/overall running time: 1149.1127s / 225041.2141 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0046
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0046
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1509.15,                last time consumption/overall running time: 1284.0297s / 226325.2437 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0047
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0048
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1348.15,                last time consumption/overall running time: 1145.0915s / 227470.3352 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0046
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0046
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1507.9,                last time consumption/overall running time: 1269.2086s / 228739.5438 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0048
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1372.85,                last time consumption/overall running time: 1151.0989s / 229890.6427 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0053
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1298.2,                last time consumption/overall running time: 1079.1663s / 230969.8089 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0051
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0053
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1447.65,                last time consumption/overall running time: 1205.3223s / 232175.1312 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0050
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0052
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1390.05,                last time consumption/overall running time: 1157.5887s / 233332.7199 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0054
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1456.4,                last time consumption/overall running time: 1209.6159s / 234542.3358 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1435.1,                last time consumption/overall running time: 1192.5711s / 235734.9069 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0052
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1423.75,                last time consumption/overall running time: 1193.9585s / 236928.8654 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0051
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1471.85,                last time consumption/overall running time: 1222.0972s / 238150.9626 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0052
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1547.75,                last time consumption/overall running time: 1293.0080s / 239443.9706 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0054
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0053
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1450.2,                last time consumption/overall running time: 1213.5607s / 240657.5313 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0054
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0053
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1592.4,                last time consumption/overall running time: 1325.3348s / 241982.8661 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0054
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0054
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1443.3,                last time consumption/overall running time: 1210.4704s / 243193.3365 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0053
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0052
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1479.3,                last time consumption/overall running time: 1235.4590s / 244428.7954 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0050
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0051
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1528.85,                last time consumption/overall running time: 1275.9843s / 245704.7797 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0050
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0051
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1464.05,                last time consumption/overall running time: 1222.3149s / 246927.0946 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0051
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1472.4,                last time consumption/overall running time: 1224.2074s / 248151.3020 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0053
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1491.8,                last time consumption/overall running time: 1236.4114s / 249387.7134 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0048
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1524.95,                last time consumption/overall running time: 1266.0795s / 250653.7929 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0052
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1538.4,                last time consumption/overall running time: 1283.6164s / 251937.4093 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0051
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1466.0,                last time consumption/overall running time: 1207.8304s / 253145.2397 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0051
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1546.05,                last time consumption/overall running time: 1276.7055s / 254421.9452 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0050
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0049
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1361.3,                last time consumption/overall running time: 1122.0655s / 255544.0108 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0051
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1560.55,                last time consumption/overall running time: 1290.0291s / 256834.0398 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0051
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1521.4,                last time consumption/overall running time: 1249.1020s / 258083.1418 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0055
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0055
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1568.25,                last time consumption/overall running time: 1289.7617s / 259372.9035 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0053
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1734.7,                last time consumption/overall running time: 1416.5733s / 260789.4769 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0053
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0055
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1697.35,                last time consumption/overall running time: 1390.3758s / 262179.8527 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0059
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0059
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1620.45,                last time consumption/overall running time: 1324.6166s / 263504.4693 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0062
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0060
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1572.2,                last time consumption/overall running time: 1286.4933s / 264790.9626 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0061
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0061
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1581.35,                last time consumption/overall running time: 1292.0734s / 266083.0360 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0056
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0056
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1424.05,                last time consumption/overall running time: 1164.6878s / 267247.7237 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1434.75,                last time consumption/overall running time: 1180.3757s / 268428.0994 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0047
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0046
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1374.25,                last time consumption/overall running time: 1128.3811s / 269556.4806 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0044
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0046
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1439.25,                last time consumption/overall running time: 1180.0814s / 270736.5620 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0045
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0045
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1412.9,                last time consumption/overall running time: 1150.4859s / 271887.0479 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0042
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0045
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1470.6,                last time consumption/overall running time: 1201.2238s / 273088.2717 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0043
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0045
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1552.0,                last time consumption/overall running time: 1261.8877s / 274350.1594 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0046
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0048
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1452.15,                last time consumption/overall running time: 1184.8210s / 275534.9804 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0048
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0047
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1490.6,                last time consumption/overall running time: 1209.6722s / 276744.6526 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1537.45,                last time consumption/overall running time: 1257.3357s / 278001.9883 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0046
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0047
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1386.35,                last time consumption/overall running time: 1130.0688s / 279132.0571 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1419.6,                last time consumption/overall running time: 1148.5918s / 280280.6490 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0045
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0045
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1481.85,                last time consumption/overall running time: 1196.3747s / 281477.0236 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0046
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0047
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1492.65,                last time consumption/overall running time: 1204.5591s / 282681.5827 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0047
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0046
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1496.3,                last time consumption/overall running time: 1209.0969s / 283890.6796 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0044
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0045
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1478.45,                last time consumption/overall running time: 1199.9120s / 285090.5916 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0044
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0045
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1515.2,                last time consumption/overall running time: 1222.6716s / 286313.2631 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0047
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1554.75,                last time consumption/overall running time: 1259.5047s / 287572.7678 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0047
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0048
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1593.15,                last time consumption/overall running time: 1297.5860s / 288870.3538 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1476.85,                last time consumption/overall running time: 1197.6608s / 290068.0147 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0045
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0045
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1389.4,                last time consumption/overall running time: 1123.6293s / 291191.6440 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0044
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0047
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1530.85,                last time consumption/overall running time: 1228.5659s / 292420.2098 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0043
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0046
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1565.75,                last time consumption/overall running time: 1259.3872s / 293679.5970 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0043
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1510.55,                last time consumption/overall running time: 1191.3919s / 294870.9889 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0047
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1539.35,                last time consumption/overall running time: 1215.0013s / 296085.9902 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0047
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0051
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1553.9,                last time consumption/overall running time: 1229.7352s / 297315.7255 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0046
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0048
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1571.6,                last time consumption/overall running time: 1244.0029s / 298559.7283 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0046
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1570.85,                last time consumption/overall running time: 1229.7193s / 299789.4476 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0047
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1428.75,                last time consumption/overall running time: 1129.1374s / 300918.5851 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0045
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1510.4,                last time consumption/overall running time: 1179.0877s / 302097.6727 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0044
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0047
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1524.55,                last time consumption/overall running time: 1194.4302s / 303292.1029 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0047
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1417.9,                last time consumption/overall running time: 1110.6410s / 304402.7439 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0049
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0051
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1462.75,                last time consumption/overall running time: 1131.9244s / 305534.6683 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0046
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0047
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1528.5,                last time consumption/overall running time: 1192.1652s / 306726.8335 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0044
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0047
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1682.95,                last time consumption/overall running time: 1310.2074s / 308037.0408 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0048
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0048
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1785.05,                last time consumption/overall running time: 1380.0201s / 309417.0609 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0049
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1560.75,                last time consumption/overall running time: 1198.9233s / 310615.9842 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0050
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1511.45,                last time consumption/overall running time: 1161.8014s / 311777.7856 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0051
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0050
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1587.7,                last time consumption/overall running time: 1227.7851s / 313005.5707 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0050
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0049
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1660.9,                last time consumption/overall running time: 1264.1114s / 314269.6821 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0052
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0049
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1684.85,                last time consumption/overall running time: 1280.5258s / 315550.2079 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0051
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1702.45,                last time consumption/overall running time: 1287.6883s / 316837.8962 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0053
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0050
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1643.35,                last time consumption/overall running time: 1228.4286s / 318066.3248 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0051
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0051
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1772.0,                last time consumption/overall running time: 1326.5903s / 319392.9151 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1628.05,                last time consumption/overall running time: 1218.1835s / 320611.0985 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0055
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0052
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1620.1,                last time consumption/overall running time: 1207.4771s / 321818.5757 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0054
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0051
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1706.0,                last time consumption/overall running time: 1278.8461s / 323097.4218 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0050
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0052
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1617.45,                last time consumption/overall running time: 1195.3059s / 324292.7277 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0054
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0053
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1656.15,                last time consumption/overall running time: 1221.4210s / 325514.1487 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0053
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0054
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1625.3,                last time consumption/overall running time: 1191.1663s / 326705.3150 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0053
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1668.6,                last time consumption/overall running time: 1224.0451s / 327929.3601 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0050
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1632.55,                last time consumption/overall running time: 1201.1775s / 329130.5375 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.0052
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0049
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1656.1,                last time consumption/overall running time: 1210.6217s / 330341.1592 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0056
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0056
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1737.6,                last time consumption/overall running time: 1278.9320s / 331620.0912 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0055
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0056
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1705.85,                last time consumption/overall running time: 1252.0064s / 332872.0977 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0055
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0056
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1857.35,                last time consumption/overall running time: 1354.2997s / 334226.3974 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0054
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0054
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1805.75,                last time consumption/overall running time: 1318.9794s / 335545.3768 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0051
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0053
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1733.15,                last time consumption/overall running time: 1257.3130s / 336802.6898 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.0053
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0056
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1603.0,                last time consumption/overall running time: 1160.9819s / 337963.6717 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.0053
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0053
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1731.1,                last time consumption/overall running time: 1262.4900s / 339226.1617 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0056
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1614.1,                last time consumption/overall running time: 1169.5426s / 340395.7042 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0052
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0056
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1509.5,                last time consumption/overall running time: 1081.1111s / 341476.8153 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0053
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1421.95,                last time consumption/overall running time: 1019.4434s / 342496.2587 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0049
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0054
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1436.85,                last time consumption/overall running time: 1042.0580s / 343538.3167 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0053
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0055
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1505.1,                last time consumption/overall running time: 1084.1860s / 344622.5027 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0055
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0054
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1652.0,                last time consumption/overall running time: 1182.7378s / 345805.2405 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0052
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0054
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1599.7,                last time consumption/overall running time: 1144.0459s / 346949.2864 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0055
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0058
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1593.45,                last time consumption/overall running time: 1137.8506s / 348087.1370 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0053
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0056
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1670.95,                last time consumption/overall running time: 1200.7250s / 349287.8620 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0053
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0053
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1719.95,                last time consumption/overall running time: 1226.4187s / 350514.2807 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0055
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0057
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1621.3,                last time consumption/overall running time: 1161.1342s / 351675.4149 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0054
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0057
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1548.2,                last time consumption/overall running time: 1097.1662s / 352772.5811 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0055
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1564.45,                last time consumption/overall running time: 1111.1424s / 353883.7235 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0051
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0054
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1665.55,                last time consumption/overall running time: 1192.2150s / 355075.9385 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.0049
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0051
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1667.3,                last time consumption/overall running time: 1186.6982s / 356262.6367 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0054
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1470.95,                last time consumption/overall running time: 1038.6039s / 357301.2406 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1576.8,                last time consumption/overall running time: 1114.4570s / 358415.6976 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0051
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1498.4,                last time consumption/overall running time: 1059.7469s / 359475.4444 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0051
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 1480.8,                last time consumption/overall running time: 1051.3246s / 360526.7690 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0047
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1606.2,                last time consumption/overall running time: 1141.5251s / 361668.2941 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0046
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0049
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1692.9,                last time consumption/overall running time: 1203.3270s / 362871.6210 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0053
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 1521.35,                last time consumption/overall running time: 1072.1876s / 363943.8086 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0055
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1573.1,                last time consumption/overall running time: 1102.4281s / 365046.2367 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0054
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0051
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1632.05,                last time consumption/overall running time: 1140.7612s / 366186.9979 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0051
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0057
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1575.0,                last time consumption/overall running time: 1106.7584s / 367293.7563 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0055
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0059
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1677.0,                last time consumption/overall running time: 1168.2598s / 368462.0161 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0059
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0060
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1666.05,                last time consumption/overall running time: 1155.0835s / 369617.0997 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0059
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0061
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1639.0,                last time consumption/overall running time: 1134.6789s / 370751.7786 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0055
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0057
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1503.55,                last time consumption/overall running time: 1054.9563s / 371806.7349 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0053
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0056
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1523.35,                last time consumption/overall running time: 1057.1158s / 372863.8507 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0053
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0054
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1522.25,                last time consumption/overall running time: 1051.8727s / 373915.7234 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0054
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1564.3,                last time consumption/overall running time: 1066.5296s / 374982.2530 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0051
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0055
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1618.5,                last time consumption/overall running time: 1099.3997s / 376081.6526 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0052
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0055
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1621.2,                last time consumption/overall running time: 1113.9234s / 377195.5761 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0054
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0057
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1583.7,                last time consumption/overall running time: 1081.6390s / 378277.2151 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0056
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 1510.7,                last time consumption/overall running time: 1032.7143s / 379309.9294 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0051
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0054
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1521.75,                last time consumption/overall running time: 1043.0617s / 380352.9911 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0052
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1553.95,                last time consumption/overall running time: 1062.5360s / 381415.5271 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 1664.55,                last time consumption/overall running time: 1143.2763s / 382558.8035 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0054
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0053
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1711.5,                last time consumption/overall running time: 1172.4059s / 383731.2094 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0055
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0057
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1692.55,                last time consumption/overall running time: 1159.7198s / 384890.9292 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0054
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0056
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1648.9,                last time consumption/overall running time: 1210.8075s / 386101.7367 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0055
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0056
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1612.6,                last time consumption/overall running time: 1268.8144s / 387370.5511 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0053
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1566.6,                last time consumption/overall running time: 1227.5550s / 388598.1061 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0051
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1628.0,                last time consumption/overall running time: 1285.4070s / 389883.5131 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0053
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0052
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1501.6,                last time consumption/overall running time: 1185.3813s / 391068.8944 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1490.45,                last time consumption/overall running time: 1168.3516s / 392237.2460 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0053
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1614.15,                last time consumption/overall running time: 1251.9171s / 393489.1631 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0056
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1506.0,                last time consumption/overall running time: 1172.4957s / 394661.6588 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0053
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1561.0,                last time consumption/overall running time: 1217.4824s / 395879.1413 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0054
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0055
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1695.65,                last time consumption/overall running time: 1329.4233s / 397208.5645 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0056
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0056
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1724.95,                last time consumption/overall running time: 1332.5673s / 398541.1319 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0057
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0056
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 1566.4,                last time consumption/overall running time: 1208.3452s / 399749.4770 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0058
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0055
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1612.45,                last time consumption/overall running time: 1247.7557s / 400997.2327 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0057
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0055
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1664.95,                last time consumption/overall running time: 1288.2316s / 402285.4643 s
env0_first_0:                 episode reward: -4.9000,                 loss: 0.0058
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0057
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1618.5,                last time consumption/overall running time: 1241.6686s / 403527.1329 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0056
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0055
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1660.85,                last time consumption/overall running time: 1280.1889s / 404807.3218 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0054
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0054
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1593.05,                last time consumption/overall running time: 1224.9856s / 406032.3074 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0055
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0055
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1515.2,                last time consumption/overall running time: 1166.1132s / 407198.4206 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0054
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0053
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1559.55,                last time consumption/overall running time: 1199.2781s / 408397.6987 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0054
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1687.5,                last time consumption/overall running time: 1298.1224s / 409695.8211 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0056
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0054
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1624.4,                last time consumption/overall running time: 1237.7900s / 410933.6111 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0056
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0058
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1563.9,                last time consumption/overall running time: 1181.7107s / 412115.3218 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0054
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1599.65,                last time consumption/overall running time: 1209.9800s / 413325.3018 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 1611.15,                last time consumption/overall running time: 1227.5820s / 414552.8839 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0054
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0051
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1607.3,                last time consumption/overall running time: 1215.0436s / 415767.9275 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0054
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1495.95,                last time consumption/overall running time: 1146.7603s / 416914.6878 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0053
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0053
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1575.75,                last time consumption/overall running time: 1197.4561s / 418112.1439 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0053
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1533.45,                last time consumption/overall running time: 1173.5816s / 419285.7256 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0052
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0051
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 1719.7,                last time consumption/overall running time: 1304.8394s / 420590.5649 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0052
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0051
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1559.85,                last time consumption/overall running time: 1181.8936s / 421772.4585 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1575.75,                last time consumption/overall running time: 1190.2991s / 422962.7576 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1442.85,                last time consumption/overall running time: 1078.4441s / 424041.2017 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0048
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1578.0,                last time consumption/overall running time: 1189.7126s / 425230.9143 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 1499.35,                last time consumption/overall running time: 1134.1925s / 426365.1068 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0053
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 1467.0,                last time consumption/overall running time: 1100.7493s / 427465.8561 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0047
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1609.25,                last time consumption/overall running time: 1197.9435s / 428663.7996 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 1632.25,                last time consumption/overall running time: 1219.9364s / 429883.7360 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0050
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0053
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1622.2,                last time consumption/overall running time: 1207.8151s / 431091.5511 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0051
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0052
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1666.1,                last time consumption/overall running time: 1253.1186s / 432344.6697 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0049
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1527.4,                last time consumption/overall running time: 1149.7579s / 433494.4277 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0049
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0048
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1616.1,                last time consumption/overall running time: 1205.5046s / 434699.9323 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0049
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0048
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1610.35,                last time consumption/overall running time: 1205.9010s / 435905.8333 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0048
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0050
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1413.6,                last time consumption/overall running time: 1062.2789s / 436968.1122 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.0048
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0050
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1492.7,                last time consumption/overall running time: 1105.3576s / 438073.4698 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0046
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1517.2,                last time consumption/overall running time: 1124.1698s / 439197.6396 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0047
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1560.35,                last time consumption/overall running time: 1160.3202s / 440357.9598 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0047
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1582.45,                last time consumption/overall running time: 1176.1618s / 441534.1216 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0051
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0051
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1665.4,                last time consumption/overall running time: 1222.1373s / 442756.2590 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0053
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0053
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1702.95,                last time consumption/overall running time: 1251.7104s / 444007.9694 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0050
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0050
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1535.2,                last time consumption/overall running time: 1127.6476s / 445135.6170 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0051
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1513.15,                last time consumption/overall running time: 1105.9471s / 446241.5640 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1455.1,                last time consumption/overall running time: 1055.0707s / 447296.6347 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0048
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0052
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 1585.75,                last time consumption/overall running time: 1132.1500s / 448428.7847 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.0050
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0052
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1632.6,                last time consumption/overall running time: 1142.8760s / 449571.6607 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0050
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1766.7,                last time consumption/overall running time: 1230.7336s / 450802.3942 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0051
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1712.45,                last time consumption/overall running time: 1197.5645s / 451999.9588 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0053
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1628.2,                last time consumption/overall running time: 1128.5587s / 453128.5174 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0048
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1637.0,                last time consumption/overall running time: 1143.7965s / 454272.3139 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0052
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0052
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1778.5,                last time consumption/overall running time: 1215.8460s / 455488.1599 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0053
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1672.7,                last time consumption/overall running time: 1120.9523s / 456609.1122 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0053
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0054
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1531.5,                last time consumption/overall running time: 1017.8243s / 457626.9365 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0051
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0050
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 1685.3,                last time consumption/overall running time: 1124.8667s / 458751.8032 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0052
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1659.65,                last time consumption/overall running time: 1112.8766s / 459864.6798 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1628.4,                last time consumption/overall running time: 1086.5879s / 460951.2677 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0052
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0051
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1851.15,                last time consumption/overall running time: 1237.6368s / 462188.9045 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0053
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0051
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1742.7,                last time consumption/overall running time: 1171.8664s / 463360.7709 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0053
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0052
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 1628.25,                last time consumption/overall running time: 1083.4156s / 464444.1865 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0053
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 1813.05,                last time consumption/overall running time: 1217.9048s / 465662.0913 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0051
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0053
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 1719.9,                last time consumption/overall running time: 1166.3198s / 466828.4111 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0054
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0053
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 1855.5,                last time consumption/overall running time: 1426.0235s / 468254.4346 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0053
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0053
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 1741.05,                last time consumption/overall running time: 1322.5070s / 469576.9416 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0051
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0050
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 1722.5,                last time consumption/overall running time: 1305.4181s / 470882.3597 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0052
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0051
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 1756.9,                last time consumption/overall running time: 1322.9193s / 472205.2790 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0055
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0054
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 1767.65,                last time consumption/overall running time: 1339.4132s / 473544.6923 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0054
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0053
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1716.35,                last time consumption/overall running time: 1297.4042s / 474842.0964 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0051
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0051
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1806.5,                last time consumption/overall running time: 1371.3486s / 476213.4450 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0051
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0052
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 1988.35,                last time consumption/overall running time: 1506.7567s / 477720.2017 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0053
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 1855.2,                last time consumption/overall running time: 1415.5225s / 479135.7242 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0054
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0051
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 1797.7,                last time consumption/overall running time: 1362.5512s / 480498.2754 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0052
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0052
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 1939.4,                last time consumption/overall running time: 1475.5880s / 481973.8633 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0054
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0054
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 1751.45,                last time consumption/overall running time: 1339.2405s / 483313.1038 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0060
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0060
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1769.3,                last time consumption/overall running time: 1342.8844s / 484655.9882 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0060
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0060
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1758.2,                last time consumption/overall running time: 1339.0473s / 485995.0355 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0060
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0059
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 1867.85,                last time consumption/overall running time: 1421.9795s / 487417.0150 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0062
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0061
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 1921.3,                last time consumption/overall running time: 1463.5979s / 488880.6128 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0060
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0060
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1822.45,                last time consumption/overall running time: 1384.2449s / 490264.8578 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0060
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0059
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1867.5,                last time consumption/overall running time: 1432.3701s / 491697.2279 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0062
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 1734.25,                last time consumption/overall running time: 1322.9873s / 493020.2151 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0066
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0066
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1645.95,                last time consumption/overall running time: 1255.4635s / 494275.6787 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0065
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0063
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 1828.6,                last time consumption/overall running time: 1391.9402s / 495667.6188 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0063
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0061
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1713.95,                last time consumption/overall running time: 1306.2788s / 496973.8977 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0059
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1655.5,                last time consumption/overall running time: 1265.3191s / 498239.2168 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0058
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0054
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 1540.4,                last time consumption/overall running time: 1167.3010s / 499406.5177 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0056
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0055
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 1696.0,                last time consumption/overall running time: 1288.0115s / 500694.5292 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0057
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0053
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 1707.5,                last time consumption/overall running time: 1312.7406s / 502007.2699 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.0054
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0053
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 1728.8,                last time consumption/overall running time: 1314.9220s / 503322.1919 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0054
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0052
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 1646.9,                last time consumption/overall running time: 1267.0753s / 504589.2672 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0055
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0052
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1767.1,                last time consumption/overall running time: 1352.1476s / 505941.4148 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0053
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0054
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 1782.45,                last time consumption/overall running time: 1350.9780s / 507292.3929 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0053
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1720.75,                last time consumption/overall running time: 1298.5610s / 508590.9538 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0050
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1904.9,                last time consumption/overall running time: 1813.5724s / 510404.5263 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0051
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0052
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1768.45,                last time consumption/overall running time: 1794.9058s / 512199.4320 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0051
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 1698.1,                last time consumption/overall running time: 1715.5491s / 513914.9811 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0052
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 1761.9,                last time consumption/overall running time: 1790.9749s / 515705.9560 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0056
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0056
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1799.55,                last time consumption/overall running time: 1803.9370s / 517509.8930 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0057
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0057
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 1661.75,                last time consumption/overall running time: 1678.7664s / 519188.6594 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0054
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0053
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 1640.55,                last time consumption/overall running time: 1632.5410s / 520821.2004 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0050
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 1740.25,                last time consumption/overall running time: 1723.1639s / 522544.3643 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0050
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 1748.8,                last time consumption/overall running time: 1728.7372s / 524273.1014 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0048
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0049
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1690.7,                last time consumption/overall running time: 1661.7483s / 525934.8497 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0049
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0047
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1761.2,                last time consumption/overall running time: 1728.4224s / 527663.2721 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0047
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 1696.75,                last time consumption/overall running time: 1671.9872s / 529335.2593 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0047
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0049
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1661.3,                last time consumption/overall running time: 1625.5352s / 530960.7945 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1640.0,                last time consumption/overall running time: 1619.8642s / 532580.6587 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0051
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1879.15,                last time consumption/overall running time: 1857.5216s / 534438.1803 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0050
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1815.6,                last time consumption/overall running time: 1785.4281s / 536223.6084 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.0049
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0050
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1692.4,                last time consumption/overall running time: 1662.8079s / 537886.4163 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0051
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0050
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1739.05,                last time consumption/overall running time: 1696.9573s / 539583.3736 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0052
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0050
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1589.7,                last time consumption/overall running time: 1560.6295s / 541144.0031 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0048
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1629.2,                last time consumption/overall running time: 1589.8791s / 542733.8822 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0050
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0051
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 1706.2,                last time consumption/overall running time: 1659.1724s / 544393.0546 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0047
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1656.2,                last time consumption/overall running time: 1592.8648s / 545985.9194 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 1614.7,                last time consumption/overall running time: 1550.3105s / 547536.2298 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0045
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1735.05,                last time consumption/overall running time: 1679.7950s / 549216.0249 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0048
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0049
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 1819.15,                last time consumption/overall running time: 1750.4578s / 550966.4827 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0050
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0051
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1845.0,                last time consumption/overall running time: 1779.8599s / 552746.3426 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0047
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1594.85,                last time consumption/overall running time: 1544.3906s / 554290.7331 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0046
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0044
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1768.25,                last time consumption/overall running time: 1709.6521s / 556000.3852 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.0048
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0046
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1697.3,                last time consumption/overall running time: 1635.2451s / 557635.6303 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0047
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1771.05,                last time consumption/overall running time: 1687.1850s / 559322.8153 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0049
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0045
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 1809.25,                last time consumption/overall running time: 1718.7177s / 561041.5330 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0048
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0047
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 1743.85,                last time consumption/overall running time: 1632.1928s / 562673.7258 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0048
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0045
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1695.35,                last time consumption/overall running time: 1579.4144s / 564253.1402 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0045
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 1658.15,                last time consumption/overall running time: 1544.0548s / 565797.1949 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0046
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0044
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 1609.4,                last time consumption/overall running time: 1491.6307s / 567288.8256 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0046
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0044
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 1624.3,                last time consumption/overall running time: 1505.8410s / 568794.6666 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0046
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0044
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1654.35,                last time consumption/overall running time: 1536.8802s / 570331.5469 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0045
env0_second_0:                 episode reward: 6.3500,                 loss: 0.0042
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1684.1,                last time consumption/overall running time: 1573.2685s / 571904.8154 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0046
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0044
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 1769.25,                last time consumption/overall running time: 1658.0662s / 573562.8816 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0047
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0049
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 1720.45,                last time consumption/overall running time: 1609.1813s / 575172.0629 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0046
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0046
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1668.15,                last time consumption/overall running time: 1552.7949s / 576724.8578 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0043
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0044
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1784.35,                last time consumption/overall running time: 1634.5153s / 578359.3731 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0042
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 1746.4,                last time consumption/overall running time: 1616.2552s / 579975.6283 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0045
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0043
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 1696.35,                last time consumption/overall running time: 1577.6202s / 581553.2485 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0044
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0045
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 1627.3,                last time consumption/overall running time: 1484.3563s / 583037.6048 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0042
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0041
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1725.65,                last time consumption/overall running time: 1560.6513s / 584598.2561 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0043
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0043
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1672.25,                last time consumption/overall running time: 1499.0876s / 586097.3437 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0041
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0041
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1731.15,                last time consumption/overall running time: 1551.6743s / 587649.0179 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0042
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0042
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1633.1,                last time consumption/overall running time: 1442.4048s / 589091.4227 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0042
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0044
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 1667.25,                last time consumption/overall running time: 1472.8955s / 590564.3182 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0039
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0041
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 1648.8,                last time consumption/overall running time: 1470.0964s / 592034.4146 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0042
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0040
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 1858.0,                last time consumption/overall running time: 1643.7597s / 593678.1743 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0042
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0041
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 1766.2,                last time consumption/overall running time: 1567.2033s / 595245.3775 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0042
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1717.7,                last time consumption/overall running time: 1526.0934s / 596771.4709 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0044
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0042
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 1720.65,                last time consumption/overall running time: 1513.3589s / 598284.8298 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0043
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0039
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 1754.45,                last time consumption/overall running time: 1544.3734s / 599829.2032 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0039
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0038
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1723.5,                last time consumption/overall running time: 1519.1668s / 601348.3700 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0042
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0038
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 1665.6,                last time consumption/overall running time: 1457.7826s / 602806.1526 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0040
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0038
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1773.95,                last time consumption/overall running time: 1552.6154s / 604358.7680 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0042
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0037
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 1737.85,                last time consumption/overall running time: 1509.7519s / 605868.5200 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0042
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0041
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 1796.7,                last time consumption/overall running time: 1567.8600s / 607436.3800 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0046
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0043
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1600.2,                last time consumption/overall running time: 1398.4360s / 608834.8160 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0048
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0044
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 1703.8,                last time consumption/overall running time: 1490.1396s / 610324.9556 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0052
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0046
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1715.5,                last time consumption/overall running time: 1494.2780s / 611819.2336 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0040
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 1735.15,                last time consumption/overall running time: 1511.6205s / 613330.8541 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0042
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0038
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 1642.95,                last time consumption/overall running time: 1428.8907s / 614759.7448 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0043Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
/home/zihan/research/MARS/mars/rl/agents/nash_dqn_exploiter.py:174: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  action = torch.LongTensor(action).to(self.device)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_second_0:                 episode reward: 5.5000,                 loss: 0.0040
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1706.45,                last time consumption/overall running time: 1460.5605s / 616220.3053 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0043
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0042
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 1612.6,                last time consumption/overall running time: 1372.2543s / 617592.5595 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0046
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0042
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1645.0,                last time consumption/overall running time: 1389.4946s / 618982.0541 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0046
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0043
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1620.65,                last time consumption/overall running time: 1364.2753s / 620346.3294 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0043
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0042
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 1708.95,                last time consumption/overall running time: 1419.6708s / 621766.0002 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0043
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0043
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 1615.5,                last time consumption/overall running time: 1361.2010s / 623127.2013 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0044
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0042
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 1673.2,                last time consumption/overall running time: 1395.3517s / 624522.5529 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0042
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0040
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1752.3,                last time consumption/overall running time: 1452.1099s / 625974.6628 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0042
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0039
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1634.85,                last time consumption/overall running time: 1372.6787s / 627347.3416 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0042
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0038
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1628.65,                last time consumption/overall running time: 1365.9384s / 628713.2800 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0039
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0038
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1727.2,                last time consumption/overall running time: 1421.6550s / 630134.9350 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0043
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0039
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
