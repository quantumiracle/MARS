pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=12, out_features=1024, bias=True)
      (1): Tanh()
      (2): Linear(in_features=1024, out_features=1024, bias=True)
      (3): Tanh()
      (4): Linear(in_features=1024, out_features=1024, bias=True)
      (5): Tanh()
      (6): Linear(in_features=1024, out_features=6, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 32, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [1024, 1024, 1024], 'hidden_activation': 'Tanh', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 4, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0524/slimevolley_SlimeVolley-v0_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0524/slimevolley_SlimeVolley-v0_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 458.0,                last time consumption/overall running time: 7.1789s / 7.1789 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0048
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 571.45,                last time consumption/overall running time: 130.1864s / 137.3653 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0071
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 619.25,                last time consumption/overall running time: 219.1017s / 356.4670 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0101
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 542.0,                last time consumption/overall running time: 196.6306s / 553.0976 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0116
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0111
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Score delta: 4.6, update the opponent.
Episode: 81/10000 (0.8100%),                 avg. length: 596.9,                last time consumption/overall running time: 218.7662s / 771.8638 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0111
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 602.3,                last time consumption/overall running time: 222.5084s / 994.3722 s
env0_first_0:                 episode reward: 0.3500,                 loss: nan
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0123
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 568.8,                last time consumption/overall running time: 211.2054s / 1205.5776 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0142
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 542.35,                last time consumption/overall running time: 204.2175s / 1409.7951 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0150
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 568.65,                last time consumption/overall running time: 215.1423s / 1624.9374 s
env0_first_0:                 episode reward: -0.4500,                 loss: nan
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0161
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 581.15,                last time consumption/overall running time: 219.3270s / 1844.2644 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0163
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 560.0,                last time consumption/overall running time: 211.8285s / 2056.0929 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0185
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 592.45,                last time consumption/overall running time: 223.7377s / 2279.8307 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0199
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 555.05,                last time consumption/overall running time: 209.3097s / 2489.1404 s
env0_first_0:                 episode reward: -0.8000,                 loss: nan
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0203
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 566.9,                last time consumption/overall running time: 212.2060s / 2701.3464 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0209
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0216
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 281/10000 (2.8100%),                 avg. length: 614.15,                last time consumption/overall running time: 230.1003s / 2931.4467 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0187
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 549.8,                last time consumption/overall running time: 206.9775s / 3138.4242 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0190
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 568.05,                last time consumption/overall running time: 214.5057s / 3352.9298 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0192
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 574.3,                last time consumption/overall running time: 216.9447s / 3569.8745 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0184
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 584.65,                last time consumption/overall running time: 219.6054s / 3789.4799 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0191
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 584.1,                last time consumption/overall running time: 220.6445s / 4010.1244 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0189
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 605.15,                last time consumption/overall running time: 227.7651s / 4237.8895 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0199
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 556.95,                last time consumption/overall running time: 210.4905s / 4448.3800 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0202
env0_second_0:                 episode reward: -1.4000,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 599.75,                last time consumption/overall running time: 226.1837s / 4674.5637 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0212
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 624.5,                last time consumption/overall running time: 235.4999s / 4910.0636 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0203
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 557.85,                last time consumption/overall running time: 210.8317s / 5120.8953 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0199
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 601.7,                last time consumption/overall running time: 228.0705s / 5348.9658 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0206
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0216
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 521/10000 (5.2100%),                 avg. length: 597.1,                last time consumption/overall running time: 226.1279s / 5575.0936 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0215
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 596.65,                last time consumption/overall running time: 225.3221s / 5800.4157 s
env0_first_0:                 episode reward: 0.0500,                 loss: nan
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0215
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 596.4,                last time consumption/overall running time: 224.8099s / 6025.2256 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0211
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 605.5,                last time consumption/overall running time: 228.0130s / 6253.2386 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0197
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 557.75,                last time consumption/overall running time: 210.1543s / 6463.3929 s
env0_first_0:                 episode reward: 0.5500,                 loss: nan
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0189
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 574.5,                last time consumption/overall running time: 216.9139s / 6680.3068 s
env0_first_0:                 episode reward: 0.3500,                 loss: nan
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0202
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 590.75,                last time consumption/overall running time: 221.9285s / 6902.2353 s
env0_first_0:                 episode reward: 0.8000,                 loss: nan
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0216
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 631.85,                last time consumption/overall running time: 237.3410s / 7139.5763 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0217
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 608.25,                last time consumption/overall running time: 229.4464s / 7369.0226 s
env0_first_0:                 episode reward: 0.5000,                 loss: nan
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0213
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 610.1,                last time consumption/overall running time: 230.2692s / 7599.2918 s
env0_first_0:                 episode reward: -0.1500,                 loss: nan
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0199
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 571.7,                last time consumption/overall running time: 214.8098s / 7814.1016 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0186
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0189
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 741/10000 (7.4100%),                 avg. length: 582.35,                last time consumption/overall running time: 219.0429s / 8033.1445 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0193
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0197
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 761/10000 (7.6100%),                 avg. length: 554.0,                last time consumption/overall running time: 208.2920s / 8241.4364 s
env0_first_0:                 episode reward: -1.0000,                 loss: nan
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0202
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 592.95,                last time consumption/overall running time: 222.7246s / 8464.1610 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0191
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 633.85,                last time consumption/overall running time: 238.4770s / 8702.6380 s
env0_first_0:                 episode reward: -0.2000,                 loss: nan
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0183
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 624.85,                last time consumption/overall running time: 234.8919s / 8937.5299 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0193
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0200
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 841/10000 (8.4100%),                 avg. length: 675.4,                last time consumption/overall running time: 254.3752s / 9191.9051 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0185
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 702.8,                last time consumption/overall running time: 264.3293s / 9456.2345 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0178
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 682.3,                last time consumption/overall running time: 257.0769s / 9713.3113 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0162
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 743.85,                last time consumption/overall running time: 280.1151s / 9993.4264 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0156
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 743.5,                last time consumption/overall running time: 279.4060s / 10272.8324 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0149
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 727.0,                last time consumption/overall running time: 274.2985s / 10547.1309 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0143
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0172
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 961/10000 (9.6100%),                 avg. length: 720.65,                last time consumption/overall running time: 271.1832s / 10818.3141 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0157
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 724.95,                last time consumption/overall running time: 272.4417s / 11090.7559 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0145
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 739.0,                last time consumption/overall running time: 278.1277s / 11368.8836 s
env0_first_0:                 episode reward: -0.6500,                 loss: nan
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0139
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 768.3,                last time consumption/overall running time: 288.3390s / 11657.2226 s
env0_first_0:                 episode reward: -1.1500,                 loss: nan
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0139
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 803.25,                last time consumption/overall running time: 302.2960s / 11959.5187 s
env0_first_0:                 episode reward: -1.4000,                 loss: nan
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0135
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1050.45,                last time consumption/overall running time: 396.0516s / 12355.5702 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0133
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0132
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 1081/10000 (10.8100%),                 avg. length: 1177.05,                last time consumption/overall running time: 442.2977s / 12797.8679 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0108
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1342.1,                last time consumption/overall running time: 503.3630s / 13301.2309 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0091
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1380.3,                last time consumption/overall running time: 518.3484s / 13819.5792 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0073
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1323.9,                last time consumption/overall running time: 497.1613s / 14316.7406 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0069
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1546.75,                last time consumption/overall running time: 580.8089s / 14897.5495 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1301.2,                last time consumption/overall running time: 488.7728s / 15386.3223 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0055
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0093
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Score delta: 4.2, update the opponent.
Episode: 1201/10000 (12.0100%),                 avg. length: 1330.65,                last time consumption/overall running time: 499.4534s / 15885.7757 s
env0_first_0:                 episode reward: 3.8500,                 loss: nan
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0075
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1331.35,                last time consumption/overall running time: 499.6915s / 16385.4673 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0056
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1399.15,                last time consumption/overall running time: 525.0033s / 16910.4706 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0047
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1525.25,                last time consumption/overall running time: 572.9361s / 17483.4067 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0050
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1604.4,                last time consumption/overall running time: 601.7821s / 18085.1888 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0048
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2182.5,                last time consumption/overall running time: 817.8383s / 18903.0270 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0056
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 2305.55,                last time consumption/overall running time: 864.9956s / 19768.0226 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0051
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 2621.9,                last time consumption/overall running time: 982.9263s / 20750.9489 s
env0_first_0:                 episode reward: 2.1000,                 loss: nan
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1947.9,                last time consumption/overall running time: 731.2490s / 21482.1979 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0037
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2166.55,                last time consumption/overall running time: 814.3834s / 22296.5813 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0034
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1250.05,                last time consumption/overall running time: 469.8487s / 22766.4300 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1609.4,                last time consumption/overall running time: 602.7166s / 23369.1466 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0051
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2044.75,                last time consumption/overall running time: 766.4344s / 24135.5810 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1394.8,                last time consumption/overall running time: 523.1622s / 24658.7432 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0038
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1832.9,                last time consumption/overall running time: 688.3189s / 25347.0621 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1939.45,                last time consumption/overall running time: 727.9119s / 26074.9741 s
env0_first_0:                 episode reward: 3.6500,                 loss: nan
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2177.1,                last time consumption/overall running time: 817.4427s / 26892.4168 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0041
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1804.0,                last time consumption/overall running time: 678.5417s / 27570.9585 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1454.15,                last time consumption/overall running time: 545.2034s / 28116.1619 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1680.6,                last time consumption/overall running time: 630.0510s / 28746.2129 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1534.2,                last time consumption/overall running time: 575.5705s / 29321.7834 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1377.85,                last time consumption/overall running time: 518.0968s / 29839.8802 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0046
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1837.95,                last time consumption/overall running time: 690.5195s / 30530.3997 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1819.25,                last time consumption/overall running time: 683.1332s / 31213.5329 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1902.95,                last time consumption/overall running time: 714.0081s / 31927.5410 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0044
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 2092.25,                last time consumption/overall running time: 786.0376s / 32713.5786 s
env0_first_0:                 episode reward: 2.2000,                 loss: nan
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0036
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1602.1,                last time consumption/overall running time: 599.3304s / 33312.9090 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0048
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 2015.7,                last time consumption/overall running time: 754.3792s / 34067.2882 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0053
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1558.25,                last time consumption/overall running time: 586.5677s / 34653.8558 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1738.8,                last time consumption/overall running time: 654.1026s / 35307.9584 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1578.45,                last time consumption/overall running time: 592.9386s / 35900.8970 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2077.35,                last time consumption/overall running time: 779.8307s / 36680.7277 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0036
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1275.1,                last time consumption/overall running time: 478.0701s / 37158.7977 s
env0_first_0:                 episode reward: 4.0500,                 loss: nan
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0041
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1861.25,                last time consumption/overall running time: 801.5038s / 37960.3015 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0050
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 2068.75,                last time consumption/overall running time: 926.3254s / 38886.6269 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0035
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1255.0,                last time consumption/overall running time: 561.9158s / 39448.5427 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1227.6,                last time consumption/overall running time: 550.4602s / 39999.0030 s
env0_first_0:                 episode reward: 4.0000,                 loss: nan
env0_second_0:                 episode reward: -4.0000,                 loss: 0.0051
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1953.3,                last time consumption/overall running time: 875.1264s / 40874.1294 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0047
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1940.15,                last time consumption/overall running time: 870.2006s / 41744.3300 s
env0_first_0:                 episode reward: 2.1500,                 loss: nan
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0037
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1693.8,                last time consumption/overall running time: 759.2029s / 42503.5329 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0037
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1474.95,                last time consumption/overall running time: 661.7326s / 43165.2655 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 2435.55,                last time consumption/overall running time: 1093.0291s / 44258.2946 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0038
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1356.3,                last time consumption/overall running time: 607.7756s / 44866.0702 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0035
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1615.85,                last time consumption/overall running time: 724.5212s / 45590.5914 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 2099.7,                last time consumption/overall running time: 944.8872s / 46535.4786 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0039
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1412.75,                last time consumption/overall running time: 634.3646s / 47169.8432 s
env0_first_0:                 episode reward: 4.1000,                 loss: nan
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 2356.55,                last time consumption/overall running time: 1058.9704s / 48228.8135 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1768.75,                last time consumption/overall running time: 795.3907s / 49024.2043 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1933.9,                last time consumption/overall running time: 869.1453s / 49893.3495 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1342.6,                last time consumption/overall running time: 602.6728s / 50496.0223 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1909.7,                last time consumption/overall running time: 858.9442s / 51354.9665 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0047
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1576.7,                last time consumption/overall running time: 707.2604s / 52062.2268 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0041
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1969.45,                last time consumption/overall running time: 885.5472s / 52947.7740 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1784.75,                last time consumption/overall running time: 800.1739s / 53747.9479 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0039
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1574.85,                last time consumption/overall running time: 707.6556s / 54455.6035 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1773.5,                last time consumption/overall running time: 796.4003s / 55252.0038 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1768.1,                last time consumption/overall running time: 793.5918s / 56045.5956 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1978.65,                last time consumption/overall running time: 888.8421s / 56934.4377 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0035
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1382.5,                last time consumption/overall running time: 620.6434s / 57555.0810 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 2029.75,                last time consumption/overall running time: 913.4428s / 58468.5238 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1634.75,                last time consumption/overall running time: 735.1725s / 59203.6963 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1930.8,                last time consumption/overall running time: 867.0660s / 60070.7623 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1828.4,                last time consumption/overall running time: 822.0641s / 60892.8264 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0038
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1682.75,                last time consumption/overall running time: 756.4809s / 61649.3074 s
env0_first_0:                 episode reward: 3.9500,                 loss: nan
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1814.15,                last time consumption/overall running time: 814.8876s / 62464.1950 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1870.75,                last time consumption/overall running time: 839.1036s / 63303.2985 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0038
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1523.6,                last time consumption/overall running time: 684.4332s / 63987.7318 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0041
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1913.4,                last time consumption/overall running time: 861.1541s / 64848.8859 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2018.6,                last time consumption/overall running time: 908.9951s / 65757.8810 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0036
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1863.4,                last time consumption/overall running time: 836.9464s / 66594.8274 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0035
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1369.15,                last time consumption/overall running time: 617.0223s / 67211.8497 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1741.75,                last time consumption/overall running time: 782.4473s / 67994.2970 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 2314.6,                last time consumption/overall running time: 1040.2221s / 69034.5191 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0041
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1959.75,                last time consumption/overall running time: 883.2763s / 69917.7954 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0036
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 2254.1,                last time consumption/overall running time: 1015.5491s / 70933.3445 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 2015.45,                last time consumption/overall running time: 904.6741s / 71838.0186 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0039
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 2098.9,                last time consumption/overall running time: 942.4589s / 72780.4775 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1799.55,                last time consumption/overall running time: 806.6197s / 73587.0972 s
env0_first_0:                 episode reward: 2.3500,                 loss: nan
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0039
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 2001.6,                last time consumption/overall running time: 899.9999s / 74487.0971 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1942.5,                last time consumption/overall running time: 873.1567s / 75360.2538 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0041
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1683.6,                last time consumption/overall running time: 757.0368s / 76117.2906 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2154.4,                last time consumption/overall running time: 967.2746s / 77084.5652 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1652.1,                last time consumption/overall running time: 741.7238s / 77826.2890 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0041
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1472.85,                last time consumption/overall running time: 661.6357s / 78487.9246 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0050
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 2442.05,                last time consumption/overall running time: 1097.0446s / 79584.9692 s
env0_first_0:                 episode reward: 1.5000,                 loss: nan
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0054
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1767.75,                last time consumption/overall running time: 795.1836s / 80380.1529 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0038
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 2429.95,                last time consumption/overall running time: 1091.7485s / 81471.9013 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2324.55,                last time consumption/overall running time: 1045.3103s / 82517.2117 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0037
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 2032.7,                last time consumption/overall running time: 915.0103s / 83432.2219 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1965.4,                last time consumption/overall running time: 882.2031s / 84314.4250 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 2306.85,                last time consumption/overall running time: 1034.6581s / 85349.0831 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0041
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2056.55,                last time consumption/overall running time: 923.5472s / 86272.6303 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 2177.85,                last time consumption/overall running time: 977.9780s / 87250.6083 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0035
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 2370.45,                last time consumption/overall running time: 1065.8064s / 88316.4147 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 2312.0,                last time consumption/overall running time: 1041.0096s / 89357.4243 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0031
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1920.25,                last time consumption/overall running time: 862.2762s / 90219.7005 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0034
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1749.05,                last time consumption/overall running time: 785.1502s / 91004.8507 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 2238.35,                last time consumption/overall running time: 1006.1564s / 92011.0070 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0042
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 2030.05,                last time consumption/overall running time: 913.9201s / 92924.9271 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0036
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1941.7,                last time consumption/overall running time: 874.2579s / 93799.1850 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2042.55,                last time consumption/overall running time: 919.7540s / 94718.9390 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 2046.75,                last time consumption/overall running time: 920.9289s / 95639.8679 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 2334.95,                last time consumption/overall running time: 1049.9771s / 96689.8450 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1637.55,                last time consumption/overall running time: 737.1739s / 97427.0189 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1859.25,                last time consumption/overall running time: 836.3109s / 98263.3298 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 2130.65,                last time consumption/overall running time: 955.8534s / 99219.1832 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0039
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1799.45,                last time consumption/overall running time: 808.4018s / 100027.5849 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 2082.65,                last time consumption/overall running time: 933.3969s / 100960.9818 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0048
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 2071.15,                last time consumption/overall running time: 928.6064s / 101889.5882 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 2003.35,                last time consumption/overall running time: 899.7994s / 102789.3876 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1557.95,                last time consumption/overall running time: 699.3466s / 103488.7342 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1955.1,                last time consumption/overall running time: 878.7351s / 104367.4693 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1700.3,                last time consumption/overall running time: 763.1388s / 105130.6081 s
env0_first_0:                 episode reward: 3.8500,                 loss: nan
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 2009.7,                last time consumption/overall running time: 901.7749s / 106032.3830 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1489.75,                last time consumption/overall running time: 669.2793s / 106701.6623 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1907.05,                last time consumption/overall running time: 856.5585s / 107558.2209 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0048
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 2331.55,                last time consumption/overall running time: 1047.0990s / 108605.3199 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0037
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1535.25,                last time consumption/overall running time: 690.1629s / 109295.4827 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 2009.1,                last time consumption/overall running time: 902.9150s / 110198.3977 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1839.0,                last time consumption/overall running time: 825.3828s / 111023.7806 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 2279.35,                last time consumption/overall running time: 1023.9494s / 112047.7299 s
env0_first_0:                 episode reward: 2.3000,                 loss: nan
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1535.8,                last time consumption/overall running time: 688.9871s / 112736.7171 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1456.15,                last time consumption/overall running time: 654.6307s / 113391.3478 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0054
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 2122.95,                last time consumption/overall running time: 952.8005s / 114344.1483 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0048
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1977.6,                last time consumption/overall running time: 888.0828s / 115232.2311 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0038
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1700.0,                last time consumption/overall running time: 763.1831s / 115995.4142 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1729.95,                last time consumption/overall running time: 776.2833s / 116771.6975 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 2106.25,                last time consumption/overall running time: 945.4222s / 117717.1196 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0047
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2150.2,                last time consumption/overall running time: 964.8623s / 118681.9820 s
env0_first_0:                 episode reward: 2.4000,                 loss: nan
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0049
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2063.65,                last time consumption/overall running time: 927.0909s / 119609.0729 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2171.15,                last time consumption/overall running time: 974.0213s / 120583.0943 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1633.25,                last time consumption/overall running time: 732.6270s / 121315.7213 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0050
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2110.3,                last time consumption/overall running time: 945.8114s / 122261.5327 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0057
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1997.15,                last time consumption/overall running time: 895.6193s / 123157.1521 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1472.05,                last time consumption/overall running time: 660.9337s / 123818.0857 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1562.8,                last time consumption/overall running time: 701.5561s / 124519.6418 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0051
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 2001.95,                last time consumption/overall running time: 898.1371s / 125417.7789 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0050
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1718.35,                last time consumption/overall running time: 771.4240s / 126189.2029 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0051
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1766.2,                last time consumption/overall running time: 793.1674s / 126982.3703 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0050
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1670.35,                last time consumption/overall running time: 751.7488s / 127734.1191 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1927.7,                last time consumption/overall running time: 865.6404s / 128599.7595 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0051
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1764.85,                last time consumption/overall running time: 792.7804s / 129392.5399 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0047
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1562.2,                last time consumption/overall running time: 703.2088s / 130095.7488 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0048
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 2082.85,                last time consumption/overall running time: 936.6414s / 131032.3902 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0052
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 2041.0,                last time consumption/overall running time: 916.0875s / 131948.4777 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1835.15,                last time consumption/overall running time: 825.8958s / 132774.3735 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1680.55,                last time consumption/overall running time: 754.2063s / 133528.5798 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0048
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1654.2,                last time consumption/overall running time: 743.8368s / 134272.4166 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0048
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 2292.65,                last time consumption/overall running time: 1029.7874s / 135302.2040 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0049
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1855.3,                last time consumption/overall running time: 836.0640s / 136138.2680 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0038
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 2079.15,                last time consumption/overall running time: 932.9768s / 137071.2448 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 2020.25,                last time consumption/overall running time: 908.1449s / 137979.3897 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0041
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 2479.15,                last time consumption/overall running time: 1112.9309s / 139092.3206 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 2085.15,                last time consumption/overall running time: 936.3009s / 140028.6215 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2377.95,                last time consumption/overall running time: 1070.0234s / 141098.6449 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1764.85,                last time consumption/overall running time: 792.6982s / 141891.3431 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1995.15,                last time consumption/overall running time: 896.6209s / 142787.9640 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 2063.75,                last time consumption/overall running time: 927.8271s / 143715.7911 s
env0_first_0:                 episode reward: 2.3500,                 loss: nan
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1789.5,                last time consumption/overall running time: 804.8923s / 144520.6834 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0046
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1949.9,                last time consumption/overall running time: 876.1312s / 145396.8146 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0051
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 2129.7,                last time consumption/overall running time: 957.1054s / 146353.9200 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1408.2,                last time consumption/overall running time: 631.2903s / 146985.2103 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1673.0,                last time consumption/overall running time: 752.4460s / 147737.6563 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0052
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 2270.05,                last time consumption/overall running time: 1019.1917s / 148756.8480 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 2005.65,                last time consumption/overall running time: 897.6153s / 149654.4633 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1795.2,                last time consumption/overall running time: 805.5042s / 150459.9675 s
env0_first_0:                 episode reward: 3.9000,                 loss: nan
env0_second_0:                 episode reward: -3.9000,                 loss: 0.0044
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1817.2,                last time consumption/overall running time: 817.8161s / 151277.7836 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0046
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1874.35,                last time consumption/overall running time: 842.0334s / 152119.8170 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1854.45,                last time consumption/overall running time: 831.9056s / 152951.7226 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0044
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 2188.05,                last time consumption/overall running time: 981.3298s / 153933.0524 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 2133.6,                last time consumption/overall running time: 955.8938s / 154888.9462 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 2106.15,                last time consumption/overall running time: 944.1786s / 155833.1247 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0039
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1803.5,                last time consumption/overall running time: 809.9355s / 156643.0602 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 2399.95,                last time consumption/overall running time: 1075.7738s / 157718.8340 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1533.85,                last time consumption/overall running time: 688.7872s / 158407.6213 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 2305.35,                last time consumption/overall running time: 1033.4592s / 159441.0805 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1143.6,                last time consumption/overall running time: 513.4845s / 159954.5650 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 2061.5,                last time consumption/overall running time: 925.7350s / 160880.3000 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0053
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 2601.45,                last time consumption/overall running time: 1166.9181s / 162047.2181 s
env0_first_0:                 episode reward: 2.4000,                 loss: nan
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1973.95,                last time consumption/overall running time: 884.4428s / 162931.6609 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 2484.1,                last time consumption/overall running time: 1115.7951s / 164047.4560 s
env0_first_0:                 episode reward: 2.3000,                 loss: nan
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1879.0,                last time consumption/overall running time: 841.3582s / 164888.8142 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 2066.0,                last time consumption/overall running time: 924.0805s / 165812.8947 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0047
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 2116.8,                last time consumption/overall running time: 947.5757s / 166760.4704 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1712.5,                last time consumption/overall running time: 767.8079s / 167528.2783 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1580.0,                last time consumption/overall running time: 708.3181s / 168236.5965 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0050
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 2470.4,                last time consumption/overall running time: 1106.6301s / 169343.2265 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1545.5,                last time consumption/overall running time: 693.6201s / 170036.8467 s
env0_first_0:                 episode reward: 3.6500,                 loss: nan
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1994.8,                last time consumption/overall running time: 892.4881s / 170929.3348 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0049
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1742.2,                last time consumption/overall running time: 780.3483s / 171709.6831 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1827.15,                last time consumption/overall running time: 818.6201s / 172528.3032 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0046
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 2056.3,                last time consumption/overall running time: 920.9853s / 173449.2885 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1725.3,                last time consumption/overall running time: 771.0640s / 174220.3524 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1880.85,                last time consumption/overall running time: 841.8741s / 175062.2265 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0049
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 2234.55,                last time consumption/overall running time: 1000.7904s / 176063.0169 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1766.0,                last time consumption/overall running time: 790.8160s / 176853.8328 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1872.1,                last time consumption/overall running time: 839.0813s / 177692.9141 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 2056.1,                last time consumption/overall running time: 917.5827s / 178610.4968 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 2241.55,                last time consumption/overall running time: 1003.7715s / 179614.2683 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1829.2,                last time consumption/overall running time: 817.9626s / 180432.2308 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1912.8,                last time consumption/overall running time: 854.5978s / 181286.8286 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1699.6,                last time consumption/overall running time: 760.4234s / 182047.2519 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 2058.05,                last time consumption/overall running time: 921.2851s / 182968.5371 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1476.2,                last time consumption/overall running time: 660.4941s / 183629.0312 s
env0_first_0:                 episode reward: 4.0000,                 loss: nan
env0_second_0:                 episode reward: -4.0000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 2158.35,                last time consumption/overall running time: 966.6004s / 184595.6316 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 2505.25,                last time consumption/overall running time: 1121.2184s / 185716.8500 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0043
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1653.15,                last time consumption/overall running time: 738.7719s / 186455.6219 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1317.35,                last time consumption/overall running time: 589.0719s / 187044.6937 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0055
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 2430.45,                last time consumption/overall running time: 1086.7469s / 188131.4406 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0050
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1679.75,                last time consumption/overall running time: 750.8079s / 188882.2485 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0039
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1662.2,                last time consumption/overall running time: 744.0103s / 189626.2588 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 2294.1,                last time consumption/overall running time: 1022.1599s / 190648.4187 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1692.6,                last time consumption/overall running time: 754.9546s / 191403.3733 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 2238.9,                last time consumption/overall running time: 997.9880s / 192401.3613 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1772.9,                last time consumption/overall running time: 791.1196s / 193192.4809 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 2193.0,                last time consumption/overall running time: 979.5622s / 194172.0431 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 2082.2,                last time consumption/overall running time: 929.3856s / 195101.4287 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0038
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1909.0,                last time consumption/overall running time: 851.8623s / 195953.2911 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 2273.9,                last time consumption/overall running time: 1016.0280s / 196969.3191 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0045
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1878.5,                last time consumption/overall running time: 839.6313s / 197808.9505 s
env0_first_0:                 episode reward: 2.4000,                 loss: nan
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 2300.75,                last time consumption/overall running time: 1027.0010s / 198835.9515 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 2389.4,                last time consumption/overall running time: 1066.5829s / 199902.5344 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1794.3,                last time consumption/overall running time: 799.0451s / 200701.5796 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 2082.4,                last time consumption/overall running time: 928.1145s / 201629.6940 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1466.35,                last time consumption/overall running time: 653.5915s / 202283.2855 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1773.9,                last time consumption/overall running time: 790.5112s / 203073.7967 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0050
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 2348.75,                last time consumption/overall running time: 1047.8106s / 204121.6074 s
env0_first_0:                 episode reward: 2.2000,                 loss: nan
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 2019.75,                last time consumption/overall running time: 898.9278s / 205020.5351 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1929.85,                last time consumption/overall running time: 860.2669s / 205880.8020 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1655.65,                last time consumption/overall running time: 738.0300s / 206618.8320 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 2234.1,                last time consumption/overall running time: 995.1604s / 207613.9924 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0048
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1992.6,                last time consumption/overall running time: 887.4991s / 208501.4914 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1842.7,                last time consumption/overall running time: 822.0571s / 209323.5485 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0048
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 2367.35,                last time consumption/overall running time: 1055.0072s / 210378.5557 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 2011.5,                last time consumption/overall running time: 896.9104s / 211275.4660 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0034
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1533.65,                last time consumption/overall running time: 683.9571s / 211959.4232 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 2457.4,                last time consumption/overall running time: 1096.6831s / 213056.1062 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1577.3,                last time consumption/overall running time: 703.4243s / 213759.5305 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0037
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1431.95,                last time consumption/overall running time: 639.4096s / 214398.9401 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 2111.45,                last time consumption/overall running time: 942.5460s / 215341.4862 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 2244.15,                last time consumption/overall running time: 1000.5791s / 216342.0653 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1896.45,                last time consumption/overall running time: 846.6794s / 217188.7447 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1939.05,                last time consumption/overall running time: 862.4015s / 218051.1462 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0047
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 2156.6,                last time consumption/overall running time: 957.9501s / 219009.0963 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1919.85,                last time consumption/overall running time: 853.7555s / 219862.8517 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 2183.9,                last time consumption/overall running time: 972.4514s / 220835.3031 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1740.25,                last time consumption/overall running time: 774.8837s / 221610.1869 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 2084.65,                last time consumption/overall running time: 928.9661s / 222539.1529 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 2120.45,                last time consumption/overall running time: 943.9409s / 223483.0938 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 2124.35,                last time consumption/overall running time: 944.4995s / 224427.5933 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 2051.05,                last time consumption/overall running time: 912.3466s / 225339.9399 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 2369.85,                last time consumption/overall running time: 1053.2909s / 226393.2308 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1680.45,                last time consumption/overall running time: 747.2062s / 227140.4370 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0038
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1701.0,                last time consumption/overall running time: 756.6819s / 227897.1190 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 2103.6,                last time consumption/overall running time: 936.0728s / 228833.1917 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 2148.05,                last time consumption/overall running time: 954.1454s / 229787.3372 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 2123.75,                last time consumption/overall running time: 943.1189s / 230730.4561 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0039
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 2044.55,                last time consumption/overall running time: 908.7991s / 231639.2553 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1494.2,                last time consumption/overall running time: 663.9178s / 232303.1731 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1789.05,                last time consumption/overall running time: 795.0552s / 233098.2283 s
env0_first_0:                 episode reward: 3.8500,                 loss: nan
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0051
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 2671.95,                last time consumption/overall running time: 1186.1589s / 234284.3872 s
env0_first_0:                 episode reward: 2.1500,                 loss: nan
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1791.0,                last time consumption/overall running time: 795.5089s / 235079.8961 s
env0_first_0:                 episode reward: 3.6500,                 loss: nan
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0037
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1423.4,                last time consumption/overall running time: 632.1090s / 235712.0051 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1758.8,                last time consumption/overall running time: 781.3094s / 236493.3145 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0052
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1858.15,                last time consumption/overall running time: 825.8370s / 237319.1514 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0047
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 2042.4,                last time consumption/overall running time: 906.2172s / 238225.3687 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 2046.7,                last time consumption/overall running time: 909.2545s / 239134.6231 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1967.9,                last time consumption/overall running time: 874.9743s / 240009.5974 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1817.45,                last time consumption/overall running time: 807.6658s / 240817.2633 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 2053.4,                last time consumption/overall running time: 914.3901s / 241731.6534 s
env0_first_0:                 episode reward: 2.0500,                 loss: nan
env0_second_0:                 episode reward: -2.0500,                 loss: 0.0047
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1922.05,                last time consumption/overall running time: 854.1587s / 242585.8120 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 2268.3,                last time consumption/overall running time: 1007.8896s / 243593.7017 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0041
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1474.05,                last time consumption/overall running time: 653.3872s / 244247.0889 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 2282.8,                last time consumption/overall running time: 1016.2335s / 245263.3223 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0049
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 2201.8,                last time consumption/overall running time: 979.8158s / 246243.1382 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1769.85,                last time consumption/overall running time: 786.3530s / 247029.4912 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1776.7,                last time consumption/overall running time: 791.4009s / 247820.8921 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1708.7,                last time consumption/overall running time: 759.5123s / 248580.4044 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 2093.2,                last time consumption/overall running time: 932.7130s / 249513.1174 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1913.3,                last time consumption/overall running time: 851.4888s / 250364.6062 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0051
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1895.0,                last time consumption/overall running time: 842.4320s / 251207.0382 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 2335.9,                last time consumption/overall running time: 1040.5680s / 252247.6062 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1831.05,                last time consumption/overall running time: 812.8080s / 253060.4142 s
env0_first_0:                 episode reward: 4.1500,                 loss: nan
env0_second_0:                 episode reward: -4.1500,                 loss: 0.0041
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 2115.6,                last time consumption/overall running time: 938.8888s / 253999.3031 s
env0_first_0:                 episode reward: 2.3500,                 loss: nan
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 2207.2,                last time consumption/overall running time: 981.4288s / 254980.7318 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 2291.95,                last time consumption/overall running time: 1016.1251s / 255996.8569 s
env0_first_0:                 episode reward: 1.5000,                 loss: nan
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0048
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 2155.95,                last time consumption/overall running time: 959.3198s / 256956.1767 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0037
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1897.4,                last time consumption/overall running time: 842.8792s / 257799.0560 s
env0_first_0:                 episode reward: 4.0500,                 loss: nan
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 2230.2,                last time consumption/overall running time: 992.0564s / 258791.1124 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1641.75,                last time consumption/overall running time: 729.0147s / 259520.1270 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 2247.45,                last time consumption/overall running time: 998.5570s / 260518.6841 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0043
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 2282.9,                last time consumption/overall running time: 1013.5630s / 261532.2471 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1777.1,                last time consumption/overall running time: 787.5309s / 262319.7780 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1820.85,                last time consumption/overall running time: 806.5975s / 263126.3755 s
env0_first_0:                 episode reward: 3.8500,                 loss: nan
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0046
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1764.1,                last time consumption/overall running time: 782.4126s / 263908.7881 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1867.5,                last time consumption/overall running time: 827.6863s / 264736.4744 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0046
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 2242.25,                last time consumption/overall running time: 996.0271s / 265732.5015 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0051
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 2441.1,                last time consumption/overall running time: 1083.6584s / 266816.1598 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0050
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1879.95,                last time consumption/overall running time: 832.2637s / 267648.4235 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 2407.05,                last time consumption/overall running time: 1067.4104s / 268715.8339 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1949.35,                last time consumption/overall running time: 864.6619s / 269580.4958 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0039
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1885.1,                last time consumption/overall running time: 836.3714s / 270416.8672 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1704.5,                last time consumption/overall running time: 756.7636s / 271173.6307 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1763.6,                last time consumption/overall running time: 783.0401s / 271956.6709 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0046
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 2005.4,                last time consumption/overall running time: 890.2062s / 272846.8771 s
env0_first_0:                 episode reward: 3.6500,                 loss: nan
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1937.15,                last time consumption/overall running time: 859.9495s / 273706.8266 s
env0_first_0:                 episode reward: 2.4000,                 loss: nan
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 2340.5,                last time consumption/overall running time: 1037.7513s / 274744.5778 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1894.95,                last time consumption/overall running time: 841.1076s / 275585.6855 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1754.35,                last time consumption/overall running time: 778.2240s / 276363.9094 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0046
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1805.45,                last time consumption/overall running time: 801.3025s / 277165.2120 s
env0_first_0:                 episode reward: 3.7000,                 loss: nan
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1781.15,                last time consumption/overall running time: 790.5432s / 277955.7552 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1959.2,                last time consumption/overall running time: 869.4662s / 278825.2213 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0039
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 2132.05,                last time consumption/overall running time: 944.9863s / 279770.2076 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 2010.55,                last time consumption/overall running time: 893.0813s / 280663.2889 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 2155.05,                last time consumption/overall running time: 956.3968s / 281619.6857 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 2651.95,                last time consumption/overall running time: 1177.4544s / 282797.1401 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0039
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1708.55,                last time consumption/overall running time: 757.7382s / 283554.8784 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0036
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 2110.05,                last time consumption/overall running time: 936.3992s / 284491.2776 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1867.95,                last time consumption/overall running time: 828.2170s / 285319.4946 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 2133.6,                last time consumption/overall running time: 947.1911s / 286266.6857 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 2237.35,                last time consumption/overall running time: 992.5800s / 287259.2657 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1765.35,                last time consumption/overall running time: 782.7111s / 288041.9767 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0039
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 2212.25,                last time consumption/overall running time: 980.5168s / 289022.4935 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1877.8,                last time consumption/overall running time: 808.9033s / 289831.3968 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 2139.2,                last time consumption/overall running time: 897.7063s / 290729.1031 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0039
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 2185.8,                last time consumption/overall running time: 915.7720s / 291644.8751 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0041
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 2299.7,                last time consumption/overall running time: 963.8091s / 292608.6842 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1769.95,                last time consumption/overall running time: 741.4540s / 293350.1383 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1934.5,                last time consumption/overall running time: 810.4329s / 294160.5711 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 1780.35,                last time consumption/overall running time: 747.3175s / 294907.8887 s
env0_first_0:                 episode reward: 3.6500,                 loss: nan
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 2247.55,                last time consumption/overall running time: 942.6957s / 295850.5844 s
env0_first_0:                 episode reward: 3.6500,                 loss: nan
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0041
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 1727.7,                last time consumption/overall running time: 725.1930s / 296575.7774 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 1919.8,                last time consumption/overall running time: 804.6500s / 297380.4274 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 1750.4,                last time consumption/overall running time: 733.8210s / 298114.2484 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 1899.1,                last time consumption/overall running time: 749.7006s / 298863.9489 s
env0_first_0:                 episode reward: 3.5500,                 loss: nan
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 2162.15,                last time consumption/overall running time: 853.6227s / 299717.5716 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 2202.2,                last time consumption/overall running time: 868.5321s / 300586.1037 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0037
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1590.8,                last time consumption/overall running time: 628.3152s / 301214.4188 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0037
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1716.05,                last time consumption/overall running time: 678.0658s / 301892.4846 s
env0_first_0:                 episode reward: 3.9000,                 loss: nan
env0_second_0:                 episode reward: -3.9000,                 loss: 0.0045
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 1904.95,                last time consumption/overall running time: 753.0947s / 302645.5793 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 1697.15,                last time consumption/overall running time: 670.7215s / 303316.3009 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 2196.55,                last time consumption/overall running time: 868.4585s / 304184.7594 s
env0_first_0:                 episode reward: 2.4500,                 loss: nan
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 1644.25,                last time consumption/overall running time: 650.4985s / 304835.2578 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 2450.85,                last time consumption/overall running time: 966.6718s / 305801.9296 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1427.55,                last time consumption/overall running time: 562.0477s / 306363.9773 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1484.0,                last time consumption/overall running time: 585.5037s / 306949.4811 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0052
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 2496.4,                last time consumption/overall running time: 982.9234s / 307932.4044 s
env0_first_0:                 episode reward: 1.5500,                 loss: nan
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0054
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 2360.15,                last time consumption/overall running time: 930.6054s / 308863.0098 s
env0_first_0:                 episode reward: 2.0000,                 loss: nan
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1872.75,                last time consumption/overall running time: 738.9630s / 309601.9728 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0049
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 2009.9,                last time consumption/overall running time: 765.6746s / 310367.6474 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0055
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 2106.55,                last time consumption/overall running time: 781.2196s / 311148.8671 s
env0_first_0:                 episode reward: 1.9000,                 loss: nan
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0047
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 2119.25,                last time consumption/overall running time: 784.6967s / 311933.5638 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 2000.55,                last time consumption/overall running time: 741.6964s / 312675.2602 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1781.95,                last time consumption/overall running time: 659.4719s / 313334.7321 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1783.05,                last time consumption/overall running time: 660.4124s / 313995.1445 s
env0_first_0:                 episode reward: 2.2000,                 loss: nan
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 2057.75,                last time consumption/overall running time: 763.9808s / 314759.1253 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 2065.1,                last time consumption/overall running time: 767.1594s / 315526.2847 s
env0_first_0:                 episode reward: 3.9000,                 loss: nan
env0_second_0:                 episode reward: -3.9000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 1687.2,                last time consumption/overall running time: 625.4487s / 316151.7333 s
env0_first_0:                 episode reward: 3.8000,                 loss: nan
env0_second_0:                 episode reward: -3.8000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 2154.9,                last time consumption/overall running time: 752.6139s / 316904.3472 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0043
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 2113.05,                last time consumption/overall running time: 721.6680s / 317626.0153 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0038
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1543.2,                last time consumption/overall running time: 498.8900s / 318124.9053 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 2518.25,                last time consumption/overall running time: 815.3322s / 318940.2375 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0040
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1897.15,                last time consumption/overall running time: 612.9322s / 319553.1697 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0035
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1971.4,                last time consumption/overall running time: 639.5821s / 320192.7518 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 2337.35,                last time consumption/overall running time: 754.9555s / 320947.7073 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0041
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 2090.35,                last time consumption/overall running time: 677.5407s / 321625.2480 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0037
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 2266.65,                last time consumption/overall running time: 732.8288s / 322358.0769 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0037
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1534.9,                last time consumption/overall running time: 496.6802s / 322854.7571 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 1841.75,                last time consumption/overall running time: 594.2870s / 323449.0442 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0048
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 2527.75,                last time consumption/overall running time: 817.6394s / 324266.6835 s
env0_first_0:                 episode reward: 2.3500,                 loss: nan
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 2308.85,                last time consumption/overall running time: 745.0405s / 325011.7240 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0035
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 2130.7,                last time consumption/overall running time: 688.9708s / 325700.6948 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0039
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1966.4,                last time consumption/overall running time: 634.1495s / 326334.8443 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1987.85,                last time consumption/overall running time: 642.8140s / 326977.6583 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0047
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 2347.65,                last time consumption/overall running time: 757.9034s / 327735.5617 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0039
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 2288.8,                last time consumption/overall running time: 739.9534s / 328475.5151 s
env0_first_0:                 episode reward: 2.3500,                 loss: nan
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0041
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 2012.3,                last time consumption/overall running time: 648.9245s / 329124.4396 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1895.25,                last time consumption/overall running time: 611.8533s / 329736.2928 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 2061.15,                last time consumption/overall running time: 665.1517s / 330401.4446 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1930.1,                last time consumption/overall running time: 623.6764s / 331025.1210 s
env0_first_0:                 episode reward: 2.3000,                 loss: nan
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 2408.8,                last time consumption/overall running time: 778.5435s / 331803.6645 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1966.2,                last time consumption/overall running time: 635.8419s / 332439.5064 s
env0_first_0:                 episode reward: 2.4000,                 loss: nan
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 2450.7,                last time consumption/overall running time: 792.1753s / 333231.6816 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 2012.05,                last time consumption/overall running time: 644.4508s / 333876.1324 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0046
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 2295.75,                last time consumption/overall running time: 685.3526s / 334561.4850 s
env0_first_0:                 episode reward: 2.3500,                 loss: nan
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 2148.35,                last time consumption/overall running time: 640.6238s / 335202.1088 s
env0_first_0:                 episode reward: 2.6500,                 loss: nan
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0038
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 2156.75,                last time consumption/overall running time: 642.9805s / 335845.0893 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 1691.8,                last time consumption/overall running time: 505.5648s / 336350.6541 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 2517.9,                last time consumption/overall running time: 751.9492s / 337102.6033 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1909.85,                last time consumption/overall running time: 569.9880s / 337672.5913 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 2280.0,                last time consumption/overall running time: 681.5991s / 338354.1904 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 2220.85,                last time consumption/overall running time: 663.5410s / 339017.7314 s
env0_first_0:                 episode reward: 2.1500,                 loss: nan
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 2222.35,                last time consumption/overall running time: 663.8577s / 339681.5891 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0043
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 2292.95,                last time consumption/overall running time: 685.5740s / 340367.1630 s
env0_first_0:                 episode reward: 3.1000,                 loss: nan
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 2016.45,                last time consumption/overall running time: 602.8411s / 340970.0041 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0044
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 2166.0,                last time consumption/overall running time: 646.9591s / 341616.9632 s
env0_first_0:                 episode reward: 2.7500,                 loss: nan
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 2070.6,                last time consumption/overall running time: 618.9290s / 342235.8921 s
env0_first_0:                 episode reward: 2.4000,                 loss: nan
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 2222.3,                last time consumption/overall running time: 663.1164s / 342899.0086 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0049
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 2451.55,                last time consumption/overall running time: 730.7117s / 343629.7203 s
env0_first_0:                 episode reward: 2.2500,                 loss: nan
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0046
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1819.65,                last time consumption/overall running time: 542.6537s / 344172.3740 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 2212.9,                last time consumption/overall running time: 659.6377s / 344832.0117 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0046
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 2329.3,                last time consumption/overall running time: 620.4488s / 345452.4605 s
env0_first_0:                 episode reward: 1.8000,                 loss: nan
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 2402.25,                last time consumption/overall running time: 640.6124s / 346093.0729 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0043
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1884.5,                last time consumption/overall running time: 502.7096s / 346595.7825 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1996.45,                last time consumption/overall running time: 531.9123s / 347127.6948 s
env0_first_0:                 episode reward: 2.6000,                 loss: nan
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 2001.55,                last time consumption/overall running time: 534.6171s / 347662.3119 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 2004.3,                last time consumption/overall running time: 535.2728s / 348197.5847 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 2074.75,                last time consumption/overall running time: 553.9893s / 348751.5740 s
env0_first_0:                 episode reward: 2.9000,                 loss: nan
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0042
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1735.05,                last time consumption/overall running time: 464.2573s / 349215.8313 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0044
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1893.45,                last time consumption/overall running time: 505.0628s / 349720.8942 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0044
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1833.8,                last time consumption/overall running time: 489.2449s / 350210.1390 s
env0_first_0:                 episode reward: 2.5000,                 loss: nan
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0047
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1966.2,                last time consumption/overall running time: 527.0158s / 350737.1548 s
env0_first_0:                 episode reward: 3.2500,                 loss: nan
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0047
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 2051.0,                last time consumption/overall running time: 547.5163s / 351284.6711 s
env0_first_0:                 episode reward: 2.9500,                 loss: nan
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0045
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 2198.25,                last time consumption/overall running time: 586.4799s / 351871.1510 s
env0_first_0:                 episode reward: 2.5500,                 loss: nan
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 2314.85,                last time consumption/overall running time: 616.8130s / 352487.9641 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0044
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 2213.65,                last time consumption/overall running time: 591.8873s / 353079.8514 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1924.05,                last time consumption/overall running time: 512.4646s / 353592.3160 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 2450.9,                last time consumption/overall running time: 652.8305s / 354245.1465 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0044
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 2344.85,                last time consumption/overall running time: 624.0870s / 354869.2335 s
env0_first_0:                 episode reward: 2.4000,                 loss: nan
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1886.95,                last time consumption/overall running time: 503.3453s / 355372.5788 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 2061.35,                last time consumption/overall running time: 550.3311s / 355922.9100 s
env0_first_0:                 episode reward: 3.3000,                 loss: nan
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1989.75,                last time consumption/overall running time: 530.7289s / 356453.6389 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 2025.35,                last time consumption/overall running time: 539.9238s / 356993.5627 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0040
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 2358.85,                last time consumption/overall running time: 628.7921s / 357622.3548 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0036
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1564.1,                last time consumption/overall running time: 417.0023s / 358039.3571 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0039
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 2287.65,                last time consumption/overall running time: 611.7420s / 358651.0991 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 2036.05,                last time consumption/overall running time: 542.2742s / 359193.3733 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0043
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 2162.25,                last time consumption/overall running time: 577.0260s / 359770.3993 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0042
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 2352.15,                last time consumption/overall running time: 626.9477s / 360397.3470 s
env0_first_0:                 episode reward: 3.1500,                 loss: nan
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 2016.5,                last time consumption/overall running time: 537.5364s / 360934.8834 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0042
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 2099.6,                last time consumption/overall running time: 560.9522s / 361495.8356 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 2078.8,                last time consumption/overall running time: 556.8147s / 362052.6503 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0041
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1931.4,                last time consumption/overall running time: 514.3554s / 362567.0057 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 2292.45,                last time consumption/overall running time: 611.8791s / 363178.8848 s
env0_first_0:                 episode reward: 2.7500,                 loss: nanLoad SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_second_0:                 episode reward: -2.7500,                 loss: 0.0040
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 2073.3,                last time consumption/overall running time: 554.2756s / 363733.1604 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0038
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 2288.9,                last time consumption/overall running time: 612.1880s / 364345.3484 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0038
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1603.15,                last time consumption/overall running time: 428.0195s / 364773.3679 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0039
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 2182.7,                last time consumption/overall running time: 582.1762s / 365355.5441 s
env0_first_0:                 episode reward: 3.3500,                 loss: nan
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 2234.15,                last time consumption/overall running time: 596.5337s / 365952.0777 s
env0_first_0:                 episode reward: 3.4000,                 loss: nan
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0040
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1970.8,                last time consumption/overall running time: 525.9808s / 366478.0586 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0041
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
