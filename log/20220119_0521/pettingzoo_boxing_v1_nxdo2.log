pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 80, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_boxing_v1_nxdo2.
boxing_v1 pettingzoo
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
random seed: 11
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 17.7500s / 17.7500 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0003
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1272.7,                last time consumption/overall running time: 301.8446s / 319.5947 s
env0_first_0:                 episode reward: 65.9500,                 loss: 0.0313
env0_second_0:                 episode reward: -65.9500,                 loss: nan
env1_first_0:                 episode reward: 68.9500,                 loss: nan
env1_second_0:                 episode reward: -68.9500,                 loss: nan
Score delta: 178.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/21_0.
Episode: 41/10000 (0.4100%),                 avg. length: 1637.9,                last time consumption/overall running time: 719.4074s / 1039.0020 s
env0_first_0:                 episode reward: -18.4500,                 loss: nan
env0_second_0:                 episode reward: 18.4500,                 loss: 0.0200
env1_first_0:                 episode reward: -14.0500,                 loss: nan
env1_second_0:                 episode reward: 14.0500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1625.65,                last time consumption/overall running time: 768.6905s / 1807.6925 s
env0_first_0:                 episode reward: -32.3000,                 loss: 0.0511
env0_second_0:                 episode reward: 32.3000,                 loss: 0.0566
env1_first_0:                 episode reward: -29.9000,                 loss: nan
env1_second_0:                 episode reward: 29.9000,                 loss: nan
Score delta: 86.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/45_1.
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 810.8063s / 2618.4988 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0394
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1680.65,                last time consumption/overall running time: 763.5166s / 3382.0155 s
env0_first_0:                 episode reward: 11.6000,                 loss: 0.0072
env0_second_0:                 episode reward: -11.6000,                 loss: nan
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1246.35,                last time consumption/overall running time: 573.4577s / 3955.4731 s
env0_first_0:                 episode reward: 45.7000,                 loss: 0.0183
env0_second_0:                 episode reward: -45.7000,                 loss: 0.0682
env1_first_0:                 episode reward: 50.5500,                 loss: nan
env1_second_0:                 episode reward: -50.5500,                 loss: nan
Score delta: 83.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/105_0.
Episode: 141/10000 (1.4100%),                 avg. length: 1769.2,                last time consumption/overall running time: 875.2530s / 4830.7261 s
env0_first_0:                 episode reward: -40.5500,                 loss: 0.0268
env0_second_0:                 episode reward: 40.5500,                 loss: 0.0630
env1_first_0:                 episode reward: -33.6500,                 loss: nan
env1_second_0:                 episode reward: 33.6500,                 loss: nan
Score delta: 80.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/135_1.
Episode: 161/10000 (1.6100%),                 avg. length: 1775.45,                last time consumption/overall running time: 800.1715s / 5630.8976 s
env0_first_0:                 episode reward: 19.3500,                 loss: 0.0487
env0_second_0:                 episode reward: -19.3500,                 loss: nan
env1_first_0:                 episode reward: 16.1500,                 loss: nan
env1_second_0:                 episode reward: -16.1500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1524.35,                last time consumption/overall running time: 763.2565s / 6394.1541 s
env0_first_0:                 episode reward: 35.8000,                 loss: 0.0494
env0_second_0:                 episode reward: -35.8000,                 loss: 0.0423
env1_first_0:                 episode reward: 39.7500,                 loss: nan
env1_second_0:                 episode reward: -39.7500,                 loss: nan
Score delta: 86.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/175_0.
Episode: 201/10000 (2.0100%),                 avg. length: 1741.9,                last time consumption/overall running time: 903.2585s / 7297.4126 s
env0_first_0:                 episode reward: -15.8000,                 loss: 0.0544
env0_second_0:                 episode reward: 15.8000,                 loss: 0.0393
env1_first_0:                 episode reward: -22.3500,                 loss: nan
env1_second_0:                 episode reward: 22.3500,                 loss: nan
Score delta: 83.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/198_1.
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 803.3868s / 8100.7994 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0452
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1776.7,                last time consumption/overall running time: 912.9203s / 9013.7197 s
env0_first_0:                 episode reward: 32.6000,                 loss: 0.0403
env0_second_0:                 episode reward: -32.6000,                 loss: 0.0429
env1_first_0:                 episode reward: 30.2500,                 loss: nan
env1_second_0:                 episode reward: -30.2500,                 loss: nan
Score delta: 81.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/239_0.
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 801.3452s / 9815.0649 s
env0_first_0:                 episode reward: -10.2500,                 loss: nan
env0_second_0:                 episode reward: 10.2500,                 loss: 0.0445
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1545.6,                last time consumption/overall running time: 851.6606s / 10666.7255 s
env0_first_0:                 episode reward: 23.0000,                 loss: 0.0454
env0_second_0:                 episode reward: -23.0000,                 loss: 0.0455
env1_first_0:                 episode reward: 13.0500,                 loss: nan
env1_second_0:                 episode reward: -13.0500,                 loss: nan
Score delta: 86.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/263_1.
Episode: 301/10000 (3.0100%),                 avg. length: 1249.25,                last time consumption/overall running time: 658.7003s / 11325.4258 s
env0_first_0:                 episode reward: -28.3000,                 loss: 0.0624
env0_second_0:                 episode reward: 28.3000,                 loss: 0.0463
env1_first_0:                 episode reward: -28.4000,                 loss: nan
env1_second_0:                 episode reward: 28.4000,                 loss: nan
Score delta: 124.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/284_0.
Episode: 321/10000 (3.2100%),                 avg. length: 1074.25,                last time consumption/overall running time: 680.2750s / 12005.7008 s
env0_first_0:                 episode reward: 11.2500,                 loss: 0.0785
env0_second_0:                 episode reward: -11.2500,                 loss: 0.0501
env1_first_0:                 episode reward: 13.1000,                 loss: nan
env1_second_0:                 episode reward: -13.1000,                 loss: nan
Score delta: 151.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/305_1.
Episode: 341/10000 (3.4100%),                 avg. length: 1300.5,                last time consumption/overall running time: 756.7808s / 12762.4816 s
env0_first_0:                 episode reward: -27.5500,                 loss: 0.1195
env0_second_0:                 episode reward: 27.5500,                 loss: 0.0498
env1_first_0:                 episode reward: -27.5500,                 loss: nan
env1_second_0:                 episode reward: 27.5500,                 loss: nan
Score delta: 80.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/326_0.
Episode: 361/10000 (3.6100%),                 avg. length: 1076.7,                last time consumption/overall running time: 720.4393s / 13482.9209 s
env0_first_0:                 episode reward: 7.6000,                 loss: 0.1366
env0_second_0:                 episode reward: -7.6000,                 loss: 0.0598
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Score delta: 143.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/347_1.
Episode: 381/10000 (3.8100%),                 avg. length: 765.25,                last time consumption/overall running time: 523.0823s / 14006.0032 s
env0_first_0:                 episode reward: 13.6500,                 loss: 0.1895
env0_second_0:                 episode reward: -13.6500,                 loss: 0.0618
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Score delta: 120.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/368_0.
Episode: 401/10000 (4.0100%),                 avg. length: 1121.75,                last time consumption/overall running time: 777.1472s / 14783.1504 s
env0_first_0:                 episode reward: -37.2000,                 loss: 0.2123
env0_second_0:                 episode reward: 37.2000,                 loss: 0.0717
env1_first_0:                 episode reward: -56.5500,                 loss: nan
env1_second_0:                 episode reward: 56.5500,                 loss: nan
Score delta: 131.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/389_1.
Episode: 421/10000 (4.2100%),                 avg. length: 608.65,                last time consumption/overall running time: 366.8910s / 15150.0414 s
env0_first_0:                 episode reward: 61.9500,                 loss: 0.2329
env0_second_0:                 episode reward: -61.9500,                 loss: 0.0895
env1_first_0:                 episode reward: 60.6500,                 loss: nan
env1_second_0:                 episode reward: -60.6500,                 loss: nan
Score delta: 102.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/411_0.
Episode: 441/10000 (4.4100%),                 avg. length: 613.35,                last time consumption/overall running time: 586.6289s / 15736.6703 s
env0_first_0:                 episode reward: 13.0000,                 loss: 0.2389
env0_second_0:                 episode reward: -13.0000,                 loss: 0.1212
env1_first_0:                 episode reward: -11.5000,                 loss: nan
env1_second_0:                 episode reward: 11.5000,                 loss: nan
Score delta: 104.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/439_1.
Episode: 461/10000 (4.6100%),                 avg. length: 509.2,                last time consumption/overall running time: 345.4399s / 16082.1102 s
env0_first_0:                 episode reward: 35.3000,                 loss: 0.2509
env0_second_0:                 episode reward: -35.3000,                 loss: 0.1651
env1_first_0:                 episode reward: 29.8500,                 loss: nan
env1_second_0:                 episode reward: -29.8500,                 loss: nan
Score delta: 94.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/460_0.
Episode: 481/10000 (4.8100%),                 avg. length: 488.05,                last time consumption/overall running time: 222.0208s / 16304.1309 s
env0_first_0:                 episode reward: 46.3000,                 loss: nan
env0_second_0:                 episode reward: -46.3000,                 loss: 0.1937
env1_first_0:                 episode reward: 25.7500,                 loss: nan
env1_second_0:                 episode reward: -25.7500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 634.35,                last time consumption/overall running time: 287.6131s / 16591.7441 s
env0_first_0:                 episode reward: 40.1000,                 loss: nan
env0_second_0:                 episode reward: -40.1000,                 loss: 0.2626
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 681.3,                last time consumption/overall running time: 652.5960s / 17244.3401 s
env0_first_0:                 episode reward: -44.7000,                 loss: 0.2965
env0_second_0:                 episode reward: 44.7000,                 loss: 0.2984
env1_first_0:                 episode reward: -52.5000,                 loss: nan
env1_second_0:                 episode reward: 52.5000,                 loss: nan
Score delta: 92.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/512_1.
Episode: 541/10000 (5.4100%),                 avg. length: 595.45,                last time consumption/overall running time: 521.9857s / 17766.3258 s
env0_first_0:                 episode reward: 27.4000,                 loss: 0.3205
env0_second_0:                 episode reward: -27.4000,                 loss: nan
env1_first_0:                 episode reward: 51.3000,                 loss: nan
env1_second_0:                 episode reward: -51.3000,                 loss: nan
Score delta: 87.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/541_0.
Episode: 561/10000 (5.6100%),                 avg. length: 819.0,                last time consumption/overall running time: 366.9909s / 18133.3167 s
env0_first_0:                 episode reward: -14.6000,                 loss: nan
env0_second_0:                 episode reward: 14.6000,                 loss: 0.3446
env1_first_0:                 episode reward: -29.3000,                 loss: nan
env1_second_0:                 episode reward: 29.3000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 537.3,                last time consumption/overall running time: 626.9579s / 18760.2747 s
env0_first_0:                 episode reward: -78.0000,                 loss: 0.2429
env0_second_0:                 episode reward: 78.0000,                 loss: 0.3663
env1_first_0:                 episode reward: -77.0000,                 loss: nan
env1_second_0:                 episode reward: 77.0000,                 loss: nan
Score delta: 87.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/566_1.
Episode: 601/10000 (6.0100%),                 avg. length: 1142.45,                last time consumption/overall running time: 511.5837s / 19271.8583 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.2919
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: -11.0500,                 loss: nan
env1_second_0:                 episode reward: 11.0500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 484.25,                last time consumption/overall running time: 448.0672s / 19719.9256 s
env0_first_0:                 episode reward: -20.7000,                 loss: 0.2865
env0_second_0:                 episode reward: 20.7000,                 loss: 0.3641
env1_first_0:                 episode reward: -29.9000,                 loss: nan
env1_second_0:                 episode reward: 29.9000,                 loss: nan
Score delta: 93.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/605_0.
Episode: 641/10000 (6.4100%),                 avg. length: 321.5,                last time consumption/overall running time: 573.9149s / 20293.8405 s
env0_first_0:                 episode reward: -91.9000,                 loss: 0.2237
env0_second_0:                 episode reward: 91.9000,                 loss: 0.3491
env1_first_0:                 episode reward: -81.9000,                 loss: nan
env1_second_0:                 episode reward: 81.9000,                 loss: nan
Score delta: 81.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/626_1.
Episode: 661/10000 (6.6100%),                 avg. length: 277.25,                last time consumption/overall running time: 125.9280s / 20419.7684 s
env0_first_0:                 episode reward: -79.1000,                 loss: 0.2185
env0_second_0:                 episode reward: 79.1000,                 loss: nan
env1_first_0:                 episode reward: -81.4000,                 loss: nan
env1_second_0:                 episode reward: 81.4000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 695.65,                last time consumption/overall running time: 312.0776s / 20731.8460 s
env0_first_0:                 episode reward: -12.1500,                 loss: 0.2226
env0_second_0:                 episode reward: 12.1500,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 540.0,                last time consumption/overall running time: 559.0002s / 21290.8462 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.2189
env0_second_0:                 episode reward: -2.1500,                 loss: 0.3519
env1_first_0:                 episode reward: -14.6500,                 loss: nan
env1_second_0:                 episode reward: 14.6500,                 loss: nan
Score delta: 82.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/684_0.
Episode: 721/10000 (7.2100%),                 avg. length: 913.05,                last time consumption/overall running time: 874.6922s / 22165.5384 s
env0_first_0:                 episode reward: 12.4500,                 loss: 0.2053
env0_second_0:                 episode reward: -12.4500,                 loss: 0.3552
env1_first_0:                 episode reward: 10.7000,                 loss: nan
env1_second_0:                 episode reward: -10.7000,                 loss: nan
Score delta: 97.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/705_1.
Episode: 741/10000 (7.4100%),                 avg. length: 773.4,                last time consumption/overall running time: 659.1654s / 22824.7037 s
env0_first_0:                 episode reward: -29.2500,                 loss: 0.2091
env0_second_0:                 episode reward: 29.2500,                 loss: 0.3565
env1_first_0:                 episode reward: -47.5500,                 loss: nan
env1_second_0:                 episode reward: 47.5500,                 loss: nan
Score delta: 89.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/729_0.
Episode: 761/10000 (7.6100%),                 avg. length: 437.95,                last time consumption/overall running time: 699.1781s / 23523.8818 s
env0_first_0:                 episode reward: -66.5500,                 loss: 0.0690
env0_second_0:                 episode reward: 66.5500,                 loss: 0.3475
env1_first_0:                 episode reward: -62.2000,                 loss: nan
env1_second_0:                 episode reward: 62.2000,                 loss: nan
Score delta: 107.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/754_1.
Episode: 781/10000 (7.8100%),                 avg. length: 293.0,                last time consumption/overall running time: 133.8820s / 23657.7638 s
env0_first_0:                 episode reward: -89.7000,                 loss: 0.0686
env0_second_0:                 episode reward: 89.7000,                 loss: nan
env1_first_0:                 episode reward: -91.8500,                 loss: nan
env1_second_0:                 episode reward: 91.8500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 302.9,                last time consumption/overall running time: 137.3667s / 23795.1306 s
env0_first_0:                 episode reward: -92.1000,                 loss: 0.0959
env0_second_0:                 episode reward: 92.1000,                 loss: nan
env1_first_0:                 episode reward: -89.6500,                 loss: nan
env1_second_0:                 episode reward: 89.6500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 311.35,                last time consumption/overall running time: 140.1981s / 23935.3287 s
env0_first_0:                 episode reward: -91.3000,                 loss: 0.1300
env0_second_0:                 episode reward: 91.3000,                 loss: nan
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 847.9,                last time consumption/overall running time: 378.1482s / 24313.4769 s
env0_first_0:                 episode reward: -54.6000,                 loss: 0.1834
env0_second_0:                 episode reward: 54.6000,                 loss: nan
env1_first_0:                 episode reward: -49.8500,                 loss: nan
env1_second_0:                 episode reward: 49.8500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1237.45,                last time consumption/overall running time: 556.0111s / 24869.4880 s
env0_first_0:                 episode reward: -11.1500,                 loss: 0.2526
env0_second_0:                 episode reward: 11.1500,                 loss: nan
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 770.25,                last time consumption/overall running time: 725.5930s / 25595.0810 s
env0_first_0:                 episode reward: 14.0500,                 loss: 0.2838
env0_second_0:                 episode reward: -14.0500,                 loss: 0.3316
env1_first_0:                 episode reward: 12.0500,                 loss: nan
env1_second_0:                 episode reward: -12.0500,                 loss: nan
Score delta: 82.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/874_0.
Episode: 901/10000 (9.0100%),                 avg. length: 518.05,                last time consumption/overall running time: 234.5335s / 25829.6146 s
env0_first_0:                 episode reward: -15.6500,                 loss: nan
env0_second_0:                 episode reward: 15.6500,                 loss: 0.3539
env1_first_0:                 episode reward: -9.3000,                 loss: nan
env1_second_0:                 episode reward: 9.3000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 371.5,                last time consumption/overall running time: 169.5876s / 25999.2022 s
env0_first_0:                 episode reward: 2.2000,                 loss: nan
env0_second_0:                 episode reward: -2.2000,                 loss: 0.3980
env1_first_0:                 episode reward: 11.5500,                 loss: nan
env1_second_0:                 episode reward: -11.5500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 491.45,                last time consumption/overall running time: 764.9240s / 26764.1262 s
env0_first_0:                 episode reward: -0.6000,                 loss: nan
env0_second_0:                 episode reward: 0.6000,                 loss: 0.3963
env1_first_0:                 episode reward: -23.3500,                 loss: nan
env1_second_0:                 episode reward: 23.3500,                 loss: nan
Score delta: 95.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/941_1.
Episode: 961/10000 (9.6100%),                 avg. length: 720.7,                last time consumption/overall running time: 323.6411s / 27087.7673 s
env0_first_0:                 episode reward: -19.7000,                 loss: 0.3181
env0_second_0:                 episode reward: 19.7000,                 loss: nan
env1_first_0:                 episode reward: -26.3000,                 loss: nan
env1_second_0:                 episode reward: 26.3000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 625.3,                last time consumption/overall running time: 282.3056s / 27370.0729 s
env0_first_0:                 episode reward: -39.9500,                 loss: 0.3816
env0_second_0:                 episode reward: 39.9500,                 loss: nan
env1_first_0:                 episode reward: -31.1500,                 loss: nan
env1_second_0:                 episode reward: 31.1500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 495.1,                last time consumption/overall running time: 224.3223s / 27594.3952 s
env0_first_0:                 episode reward: -43.1500,                 loss: 0.4031
env0_second_0:                 episode reward: 43.1500,                 loss: nan
env1_first_0:                 episode reward: -46.0000,                 loss: nan
env1_second_0:                 episode reward: 46.0000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 651.55,                last time consumption/overall running time: 292.0685s / 27886.4637 s
env0_first_0:                 episode reward: -32.8500,                 loss: 0.3720
env0_second_0:                 episode reward: 32.8500,                 loss: nan
env1_first_0:                 episode reward: -39.4000,                 loss: nan
env1_second_0:                 episode reward: 39.4000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 882.3,                last time consumption/overall running time: 394.4619s / 28280.9256 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.3609
env0_second_0:                 episode reward: 3.2500,                 loss: nan
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1066.8,                last time consumption/overall running time: 977.3029s / 29258.2285 s
env0_first_0:                 episode reward: 30.1000,                 loss: 0.3116
env0_second_0:                 episode reward: -30.1000,                 loss: 0.4105
env1_first_0:                 episode reward: 20.5500,                 loss: nan
env1_second_0:                 episode reward: -20.5500,                 loss: nan
Score delta: 89.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/1058_0.
Episode: 1081/10000 (10.8100%),                 avg. length: 444.7,                last time consumption/overall running time: 784.6159s / 30042.8444 s
env0_first_0:                 episode reward: -30.6000,                 loss: nan
env0_second_0:                 episode reward: 30.6000,                 loss: 0.4346
env1_first_0:                 episode reward: -45.5500,                 loss: nan
env1_second_0:                 episode reward: 45.5500,                 loss: nan
Score delta: 82.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/1081_1.
Episode: 1101/10000 (11.0100%),                 avg. length: 734.6,                last time consumption/overall running time: 330.4579s / 30373.3023 s
env0_first_0:                 episode reward: 8.5500,                 loss: 0.2935
env0_second_0:                 episode reward: -8.5500,                 loss: nan
env1_first_0:                 episode reward: -22.2500,                 loss: nan
env1_second_0:                 episode reward: 22.2500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 534.25,                last time consumption/overall running time: 240.8657s / 30614.1679 s
env0_first_0:                 episode reward: -23.5000,                 loss: 0.3178
env0_second_0:                 episode reward: 23.5000,                 loss: nan
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 507.2,                last time consumption/overall running time: 228.9851s / 30843.1530 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.3759
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 36.0500,                 loss: nan
env1_second_0:                 episode reward: -36.0500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 512.35,                last time consumption/overall running time: 520.5540s / 31363.7070 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.3840
env0_second_0:                 episode reward: -0.9000,                 loss: 0.4580
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Score delta: 83.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/1151_0.
Episode: 1181/10000 (11.8100%),                 avg. length: 600.9,                last time consumption/overall running time: 886.8301s / 32250.5371 s
env0_first_0:                 episode reward: -30.6500,                 loss: 0.4285
env0_second_0:                 episode reward: 30.6500,                 loss: 0.4579
env1_first_0:                 episode reward: -47.5000,                 loss: nan
env1_second_0:                 episode reward: 47.5000,                 loss: nan
Score delta: 89.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/1173_1.
Episode: 1201/10000 (12.0100%),                 avg. length: 484.15,                last time consumption/overall running time: 218.5461s / 32469.0832 s
env0_first_0:                 episode reward: -54.3000,                 loss: 0.3768
env0_second_0:                 episode reward: 54.3000,                 loss: nan
env1_first_0:                 episode reward: -56.5000,                 loss: nan
env1_second_0:                 episode reward: 56.5000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 516.05,                last time consumption/overall running time: 232.2720s / 32701.3552 s
env0_first_0:                 episode reward: -51.4500,                 loss: 0.4293
env0_second_0:                 episode reward: 51.4500,                 loss: nan
env1_first_0:                 episode reward: -33.9000,                 loss: nan
env1_second_0:                 episode reward: 33.9000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 478.0,                last time consumption/overall running time: 214.3174s / 32915.6726 s
env0_first_0:                 episode reward: -22.0500,                 loss: 0.4740
env0_second_0:                 episode reward: 22.0500,                 loss: nan
env1_first_0:                 episode reward: -30.6500,                 loss: nan
env1_second_0:                 episode reward: 30.6500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 550.9,                last time consumption/overall running time: 248.5157s / 33164.1883 s
env0_first_0:                 episode reward: 10.4000,                 loss: 0.4855
env0_second_0:                 episode reward: -10.4000,                 loss: nan
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 811.95,                last time consumption/overall running time: 776.7814s / 33940.9697 s
env0_first_0:                 episode reward: 53.1500,                 loss: 0.4988
env0_second_0:                 episode reward: -53.1500,                 loss: 0.3442
env1_first_0:                 episode reward: 37.7000,                 loss: nan
env1_second_0:                 episode reward: -37.7000,                 loss: nan
Score delta: 84.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/1264_0.
Episode: 1301/10000 (13.0100%),                 avg. length: 628.25,                last time consumption/overall running time: 285.0365s / 34226.0062 s
env0_first_0:                 episode reward: 62.0000,                 loss: nan
env0_second_0:                 episode reward: -62.0000,                 loss: 0.3205
env1_first_0:                 episode reward: 44.8500,                 loss: nan
env1_second_0:                 episode reward: -44.8500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 504.8,                last time consumption/overall running time: 229.5313s / 34455.5375 s
env0_first_0:                 episode reward: 31.5500,                 loss: nan
env0_second_0:                 episode reward: -31.5500,                 loss: 0.3618
env1_first_0:                 episode reward: 19.4500,                 loss: nan
env1_second_0:                 episode reward: -19.4500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 679.55,                last time consumption/overall running time: 308.6321s / 34764.1696 s
env0_first_0:                 episode reward: -6.0500,                 loss: nan
env0_second_0:                 episode reward: 6.0500,                 loss: 0.3728
env1_first_0:                 episode reward: 10.0500,                 loss: nan
env1_second_0:                 episode reward: -10.0500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 829.2,                last time consumption/overall running time: 374.8174s / 35138.9870 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.4167
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 544.75,                last time consumption/overall running time: 899.7382s / 36038.7251 s
env0_first_0:                 episode reward: -49.0500,                 loss: 0.4373
env0_second_0:                 episode reward: 49.0500,                 loss: 0.4362
env1_first_0:                 episode reward: -42.1500,                 loss: nan
env1_second_0:                 episode reward: 42.1500,                 loss: nan
Score delta: 95.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/1372_1.
Episode: 1401/10000 (14.0100%),                 avg. length: 691.15,                last time consumption/overall running time: 312.5036s / 36351.2287 s
env0_first_0:                 episode reward: -45.9000,                 loss: 0.4221
env0_second_0:                 episode reward: 45.9000,                 loss: nan
env1_first_0:                 episode reward: -15.2500,                 loss: nan
env1_second_0:                 episode reward: 15.2500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 508.85,                last time consumption/overall running time: 229.0895s / 36580.3183 s
env0_first_0:                 episode reward: -46.7000,                 loss: 0.3701
env0_second_0:                 episode reward: 46.7000,                 loss: nan
env1_first_0:                 episode reward: -49.4000,                 loss: nan
env1_second_0:                 episode reward: 49.4000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 539.8,                last time consumption/overall running time: 243.9480s / 36824.2663 s
env0_first_0:                 episode reward: -57.8500,                 loss: 0.3690
env0_second_0:                 episode reward: 57.8500,                 loss: nan
env1_first_0:                 episode reward: -32.6000,                 loss: nan
env1_second_0:                 episode reward: 32.6000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 408.05,                last time consumption/overall running time: 184.7246s / 37008.9909 s
env0_first_0:                 episode reward: -39.6000,                 loss: 0.3509
env0_second_0:                 episode reward: 39.6000,                 loss: nan
env1_first_0:                 episode reward: -31.1500,                 loss: nan
env1_second_0:                 episode reward: 31.1500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 567.25,                last time consumption/overall running time: 693.1967s / 37702.1876 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.3697
env0_second_0:                 episode reward: 6.8500,                 loss: 0.4452
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Score delta: 85.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/1477_0.
Episode: 1501/10000 (15.0100%),                 avg. length: 303.9,                last time consumption/overall running time: 990.3524s / 38692.5400 s
env0_first_0:                 episode reward: -82.1000,                 loss: 0.1278
env0_second_0:                 episode reward: 82.1000,                 loss: 0.4315
env1_first_0:                 episode reward: -80.1000,                 loss: nan
env1_second_0:                 episode reward: 80.1000,                 loss: nan
Score delta: 155.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/1498_1.
Episode: 1521/10000 (15.2100%),                 avg. length: 242.25,                last time consumption/overall running time: 131.3930s / 38823.9330 s
env0_first_0:                 episode reward: -94.1500,                 loss: 0.0877
env0_second_0:                 episode reward: 94.1500,                 loss: nan
env1_first_0:                 episode reward: -98.3000,                 loss: nan
env1_second_0:                 episode reward: 98.3000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 325.15,                last time consumption/overall running time: 175.5705s / 38999.5036 s
env0_first_0:                 episode reward: -91.5000,                 loss: 0.0957
env0_second_0:                 episode reward: 91.5000,                 loss: nan
env1_first_0:                 episode reward: -87.6000,                 loss: nan
env1_second_0:                 episode reward: 87.6000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 321.35,                last time consumption/overall running time: 171.3774s / 39170.8809 s
env0_first_0:                 episode reward: -94.3000,                 loss: 0.1324
env0_second_0:                 episode reward: 94.3000,                 loss: nan
env1_first_0:                 episode reward: -93.2000,                 loss: nan
env1_second_0:                 episode reward: 93.2000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 947.8,                last time consumption/overall running time: 505.5388s / 39676.4197 s
env0_first_0:                 episode reward: -50.1500,                 loss: 0.1689
env0_second_0:                 episode reward: 50.1500,                 loss: nan
env1_first_0:                 episode reward: -53.8000,                 loss: nan
env1_second_0:                 episode reward: 53.8000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1017.1,                last time consumption/overall running time: 543.9801s / 40220.3998 s
env0_first_0:                 episode reward: -42.0000,                 loss: 0.1375
env0_second_0:                 episode reward: 42.0000,                 loss: nan
env1_first_0:                 episode reward: -49.4500,                 loss: nan
env1_second_0:                 episode reward: 49.4500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 459.3,                last time consumption/overall running time: 246.3044s / 40466.7043 s
env0_first_0:                 episode reward: -80.9000,                 loss: 0.1074
env0_second_0:                 episode reward: 80.9000,                 loss: nan
env1_first_0:                 episode reward: -70.5000,                 loss: nan
env1_second_0:                 episode reward: 70.5000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1295.45,                last time consumption/overall running time: 689.3480s / 41156.0523 s
env0_first_0:                 episode reward: -25.7000,                 loss: 0.0854
env0_second_0:                 episode reward: 25.7000,                 loss: nan
env1_first_0:                 episode reward: -30.5000,                 loss: nan
env1_second_0:                 episode reward: 30.5000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1634.25,                last time consumption/overall running time: 869.6473s / 42025.6996 s
env0_first_0:                 episode reward: -9.7000,                 loss: 0.0595
env0_second_0:                 episode reward: 9.7000,                 loss: nan
env1_first_0:                 episode reward: -10.6500,                 loss: nan
env1_second_0:                 episode reward: 10.6500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1721.8,                last time consumption/overall running time: 916.8702s / 42942.5697 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0137
env0_second_0:                 episode reward: 12.5000,                 loss: nan
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1354.65,                last time consumption/overall running time: 722.7473s / 43665.3170 s
env0_first_0:                 episode reward: -25.3000,                 loss: 0.0110
env0_second_0:                 episode reward: 25.3000,                 loss: nan
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1489.65,                last time consumption/overall running time: 794.6892s / 44460.0062 s
env0_first_0:                 episode reward: -15.1000,                 loss: 0.0144
env0_second_0:                 episode reward: 15.1000,                 loss: nan
env1_first_0:                 episode reward: -19.6500,                 loss: nan
env1_second_0:                 episode reward: 19.6500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 881.55,                last time consumption/overall running time: 471.9189s / 44931.9251 s
env0_first_0:                 episode reward: -37.8500,                 loss: 0.0198
env0_second_0:                 episode reward: 37.8500,                 loss: nan
env1_first_0:                 episode reward: -56.2000,                 loss: nan
env1_second_0:                 episode reward: 56.2000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1064.0,                last time consumption/overall running time: 566.1501s / 45498.0751 s
env0_first_0:                 episode reward: -50.8500,                 loss: 0.0446
env0_second_0:                 episode reward: 50.8500,                 loss: nan
env1_first_0:                 episode reward: -33.4500,                 loss: nan
env1_second_0:                 episode reward: 33.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1387.55,                last time consumption/overall running time: 739.6040s / 46237.6791 s
env0_first_0:                 episode reward: -27.9000,                 loss: 0.0537
env0_second_0:                 episode reward: 27.9000,                 loss: nan
env1_first_0:                 episode reward: -20.0000,                 loss: nan
env1_second_0:                 episode reward: 20.0000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1479.9,                last time consumption/overall running time: 787.3919s / 47025.0711 s
env0_first_0:                 episode reward: -32.3500,                 loss: 0.0347
env0_second_0:                 episode reward: 32.3500,                 loss: nan
env1_first_0:                 episode reward: -19.4000,                 loss: nan
env1_second_0:                 episode reward: 19.4000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1321.05,                last time consumption/overall running time: 697.5795s / 47722.6505 s
env0_first_0:                 episode reward: -40.8000,                 loss: 0.0320
env0_second_0:                 episode reward: 40.8000,                 loss: nan
env1_first_0:                 episode reward: -25.8500,                 loss: nan
env1_second_0:                 episode reward: 25.8500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1136.7,                last time consumption/overall running time: 605.9330s / 48328.5836 s
env0_first_0:                 episode reward: -38.2500,                 loss: 0.0466
env0_second_0:                 episode reward: 38.2500,                 loss: nan
env1_first_0:                 episode reward: -37.6000,                 loss: nan
env1_second_0:                 episode reward: 37.6000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1473.05,                last time consumption/overall running time: 781.5485s / 49110.1320 s
env0_first_0:                 episode reward: -29.5500,                 loss: 0.0516
env0_second_0:                 episode reward: 29.5500,                 loss: nan
env1_first_0:                 episode reward: -14.2500,                 loss: nan
env1_second_0:                 episode reward: 14.2500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1569.6,                last time consumption/overall running time: 837.9456s / 49948.0777 s
env0_first_0:                 episode reward: -17.4000,                 loss: 0.0317
env0_second_0:                 episode reward: 17.4000,                 loss: nan
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1372.3,                last time consumption/overall running time: 730.7742s / 50678.8519 s
env0_first_0:                 episode reward: -15.3000,                 loss: 0.0188
env0_second_0:                 episode reward: 15.3000,                 loss: nan
env1_first_0:                 episode reward: -25.0000,                 loss: nan
env1_second_0:                 episode reward: 25.0000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1179.0,                last time consumption/overall running time: 626.6789s / 51305.5308 s
env0_first_0:                 episode reward: -40.2000,                 loss: 0.0263
env0_second_0:                 episode reward: 40.2000,                 loss: nan
env1_first_0:                 episode reward: -35.1500,                 loss: nan
env1_second_0:                 episode reward: 35.1500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1316.1,                last time consumption/overall running time: 696.0616s / 52001.5925 s
env0_first_0:                 episode reward: -48.9000,                 loss: 0.0396
env0_second_0:                 episode reward: 48.9000,                 loss: nan
env1_first_0:                 episode reward: -41.3000,                 loss: nan
env1_second_0:                 episode reward: 41.3000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1628.6,                last time consumption/overall running time: 865.8600s / 52867.4525 s
env0_first_0:                 episode reward: -9.3000,                 loss: 0.0417
env0_second_0:                 episode reward: 9.3000,                 loss: nan
env1_first_0:                 episode reward: -10.6000,                 loss: nan
env1_second_0:                 episode reward: 10.6000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1531.45,                last time consumption/overall running time: 813.2790s / 53680.7315 s
env0_first_0:                 episode reward: 8.6000,                 loss: 0.0299
env0_second_0:                 episode reward: -8.6000,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1332.25,                last time consumption/overall running time: 706.0985s / 54386.8300 s
env0_first_0:                 episode reward: 13.3000,                 loss: 0.0371
env0_second_0:                 episode reward: -13.3000,                 loss: nan
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1388.75,                last time consumption/overall running time: 1525.0706s / 55911.9007 s
env0_first_0:                 episode reward: 22.3500,                 loss: 0.0372
env0_second_0:                 episode reward: -22.3500,                 loss: nan
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Score delta: 96.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2021_0.
Episode: 2041/10000 (20.4100%),                 avg. length: 304.15,                last time consumption/overall running time: 163.9479s / 56075.8485 s
env0_first_0:                 episode reward: -89.5500,                 loss: nan
env0_second_0:                 episode reward: 89.5500,                 loss: 0.4039
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 586.55,                last time consumption/overall running time: 1174.9502s / 57250.7988 s
env0_first_0:                 episode reward: -75.2000,                 loss: 0.0270
env0_second_0:                 episode reward: 75.2000,                 loss: 0.3729
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Score delta: 169.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2042_1.
Episode: 2081/10000 (20.8100%),                 avg. length: 989.0,                last time consumption/overall running time: 524.7076s / 57775.5064 s
env0_first_0:                 episode reward: -43.9000,                 loss: 0.0286
env0_second_0:                 episode reward: 43.9000,                 loss: nan
env1_first_0:                 episode reward: -30.1000,                 loss: nan
env1_second_0:                 episode reward: 30.1000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1088.05,                last time consumption/overall running time: 575.2707s / 58350.7771 s
env0_first_0:                 episode reward: 8.5000,                 loss: 0.0469
env0_second_0:                 episode reward: -8.5000,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 483.3,                last time consumption/overall running time: 1041.7045s / 59392.4816 s
env0_first_0:                 episode reward: -67.0000,                 loss: 0.0507
env0_second_0:                 episode reward: 67.0000,                 loss: 0.4024
env1_first_0:                 episode reward: -71.2000,                 loss: nan
env1_second_0:                 episode reward: 71.2000,                 loss: nan
Score delta: 113.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2104_0.
Episode: 2141/10000 (21.4100%),                 avg. length: 458.25,                last time consumption/overall running time: 1147.3389s / 60539.8205 s
env0_first_0:                 episode reward: -71.9000,                 loss: 0.0463
env0_second_0:                 episode reward: 71.9000,                 loss: 0.4255
env1_first_0:                 episode reward: -82.0000,                 loss: nan
env1_second_0:                 episode reward: 82.0000,                 loss: nan
Score delta: 159.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2125_1.
Episode: 2161/10000 (21.6100%),                 avg. length: 836.45,                last time consumption/overall running time: 445.0746s / 60984.8951 s
env0_first_0:                 episode reward: -39.0500,                 loss: 0.0331
env0_second_0:                 episode reward: 39.0500,                 loss: nan
env1_first_0:                 episode reward: -35.6500,                 loss: nan
env1_second_0:                 episode reward: 35.6500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 998.55,                last time consumption/overall running time: 1216.6738s / 62201.5689 s
env0_first_0:                 episode reward: 10.9000,                 loss: 0.0446
env0_second_0:                 episode reward: -10.9000,                 loss: 0.4313
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Score delta: 81.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2179_0.
Episode: 2201/10000 (22.0100%),                 avg. length: 260.25,                last time consumption/overall running time: 1104.4721s / 63306.0410 s
env0_first_0:                 episode reward: -93.6000,                 loss: 0.1031
env0_second_0:                 episode reward: 93.6000,                 loss: 0.4430
env1_first_0:                 episode reward: -94.6500,                 loss: nan
env1_second_0:                 episode reward: 94.6500,                 loss: nan
Score delta: 182.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2200_1.
Episode: 2221/10000 (22.2100%),                 avg. length: 343.15,                last time consumption/overall running time: 185.4631s / 63491.5041 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.0422
env0_second_0:                 episode reward: 93.2000,                 loss: nan
env1_first_0:                 episode reward: -78.9000,                 loss: nan
env1_second_0:                 episode reward: 78.9000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 936.45,                last time consumption/overall running time: 502.4438s / 63993.9479 s
env0_first_0:                 episode reward: -30.3000,                 loss: 0.0361
env0_second_0:                 episode reward: 30.3000,                 loss: nan
env1_first_0:                 episode reward: -32.8000,                 loss: nan
env1_second_0:                 episode reward: 32.8000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 914.05,                last time consumption/overall running time: 488.6677s / 64482.6156 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0596
env0_second_0:                 episode reward: 7.9000,                 loss: nan
env1_first_0:                 episode reward: -14.7000,                 loss: nan
env1_second_0:                 episode reward: 14.7000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1062.65,                last time consumption/overall running time: 1468.5107s / 65951.1264 s
env0_first_0:                 episode reward: 23.3000,                 loss: 0.0688
env0_second_0:                 episode reward: -23.3000,                 loss: 0.4471
env1_first_0:                 episode reward: 22.1500,                 loss: nan
env1_second_0:                 episode reward: -22.1500,                 loss: nan
Score delta: 114.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2276_0.
Episode: 2301/10000 (23.0100%),                 avg. length: 644.9,                last time consumption/overall running time: 1343.9352s / 67295.0615 s
env0_first_0:                 episode reward: -41.2000,                 loss: 0.2369
env0_second_0:                 episode reward: 41.2000,                 loss: 0.4316
env1_first_0:                 episode reward: -44.2000,                 loss: nan
env1_second_0:                 episode reward: 44.2000,                 loss: nan
Score delta: 84.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2299_1.
Episode: 2321/10000 (23.2100%),                 avg. length: 456.0,                last time consumption/overall running time: 243.0874s / 67538.1489 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.1660
env0_second_0:                 episode reward: 85.6500,                 loss: nan
env1_first_0:                 episode reward: -74.3500,                 loss: nan
env1_second_0:                 episode reward: 74.3500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 471.0,                last time consumption/overall running time: 251.1853s / 67789.3343 s
env0_first_0:                 episode reward: -67.5500,                 loss: 0.1578
env0_second_0:                 episode reward: 67.5500,                 loss: nan
env1_first_0:                 episode reward: -68.4000,                 loss: nan
env1_second_0:                 episode reward: 68.4000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 759.6,                last time consumption/overall running time: 407.0210s / 68196.3553 s
env0_first_0:                 episode reward: -51.6000,                 loss: 0.1931
env0_second_0:                 episode reward: 51.6000,                 loss: nan
env1_first_0:                 episode reward: -55.6500,                 loss: nan
env1_second_0:                 episode reward: 55.6500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 833.75,                last time consumption/overall running time: 445.8153s / 68642.1706 s
env0_first_0:                 episode reward: -52.1000,                 loss: 0.2228
env0_second_0:                 episode reward: 52.1000,                 loss: nan
env1_first_0:                 episode reward: -43.6500,                 loss: nan
env1_second_0:                 episode reward: 43.6500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1115.35,                last time consumption/overall running time: 594.7044s / 69236.8751 s
env0_first_0:                 episode reward: -32.7500,                 loss: 0.1945
env0_second_0:                 episode reward: 32.7500,                 loss: nan
env1_first_0:                 episode reward: -34.8500,                 loss: nan
env1_second_0:                 episode reward: 34.8500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1160.1,                last time consumption/overall running time: 615.4147s / 69852.2898 s
env0_first_0:                 episode reward: -24.9000,                 loss: 0.1142
env0_second_0:                 episode reward: 24.9000,                 loss: nan
env1_first_0:                 episode reward: -21.3000,                 loss: nan
env1_second_0:                 episode reward: 21.3000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1108.35,                last time consumption/overall running time: 591.6354s / 70443.9252 s
env0_first_0:                 episode reward: -25.3000,                 loss: 0.0985
env0_second_0:                 episode reward: 25.3000,                 loss: nan
env1_first_0:                 episode reward: -34.8500,                 loss: nan
env1_second_0:                 episode reward: 34.8500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1278.95,                last time consumption/overall running time: 680.5358s / 71124.4609 s
env0_first_0:                 episode reward: -16.2500,                 loss: 0.0947
env0_second_0:                 episode reward: 16.2500,                 loss: nan
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1381.4,                last time consumption/overall running time: 1667.8126s / 72792.2735 s
env0_first_0:                 episode reward: 15.2500,                 loss: 0.0988
env0_second_0:                 episode reward: -15.2500,                 loss: 0.4675
env1_first_0:                 episode reward: 13.0000,                 loss: nan
env1_second_0:                 episode reward: -13.0000,                 loss: nan
Score delta: 89.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2476_0.
Episode: 2501/10000 (25.0100%),                 avg. length: 620.45,                last time consumption/overall running time: 1378.5413s / 74170.8148 s
env0_first_0:                 episode reward: -46.4500,                 loss: 0.2970
env0_second_0:                 episode reward: 46.4500,                 loss: 0.4931
env1_first_0:                 episode reward: -42.3000,                 loss: nan
env1_second_0:                 episode reward: 42.3000,                 loss: nan
Score delta: 112.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2497_1.
Episode: 2521/10000 (25.2100%),                 avg. length: 642.15,                last time consumption/overall running time: 339.0781s / 74509.8929 s
env0_first_0:                 episode reward: -63.7000,                 loss: 0.1180
env0_second_0:                 episode reward: 63.7000,                 loss: nan
env1_first_0:                 episode reward: -65.2500,                 loss: nan
env1_second_0:                 episode reward: 65.2500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 608.7,                last time consumption/overall running time: 325.7985s / 74835.6914 s
env0_first_0:                 episode reward: -66.0000,                 loss: 0.1106
env0_second_0:                 episode reward: 66.0000,                 loss: nan
env1_first_0:                 episode reward: -64.7000,                 loss: nan
env1_second_0:                 episode reward: 64.7000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 751.6,                last time consumption/overall running time: 401.8744s / 75237.5657 s
env0_first_0:                 episode reward: -45.3000,                 loss: 0.1470
env0_second_0:                 episode reward: 45.3000,                 loss: nan
env1_first_0:                 episode reward: -72.4500,                 loss: nan
env1_second_0:                 episode reward: 72.4500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1079.8,                last time consumption/overall running time: 576.4242s / 75813.9900 s
env0_first_0:                 episode reward: -45.4000,                 loss: 0.1786
env0_second_0:                 episode reward: 45.4000,                 loss: nan
env1_first_0:                 episode reward: -42.7500,                 loss: nan
env1_second_0:                 episode reward: 42.7500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 749.5,                last time consumption/overall running time: 400.1684s / 76214.1584 s
env0_first_0:                 episode reward: -51.4500,                 loss: 0.1756
env0_second_0:                 episode reward: 51.4500,                 loss: nan
env1_first_0:                 episode reward: -53.9500,                 loss: nan
env1_second_0:                 episode reward: 53.9500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1006.15,                last time consumption/overall running time: 536.4623s / 76750.6207 s
env0_first_0:                 episode reward: -34.7500,                 loss: 0.1435
env0_second_0:                 episode reward: 34.7500,                 loss: nan
env1_first_0:                 episode reward: -26.3500,                 loss: nan
env1_second_0:                 episode reward: 26.3500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1151.55,                last time consumption/overall running time: 608.4119s / 77359.0326 s
env0_first_0:                 episode reward: -23.7500,                 loss: 0.1763
env0_second_0:                 episode reward: 23.7500,                 loss: nan
env1_first_0:                 episode reward: -23.2500,                 loss: nan
env1_second_0:                 episode reward: 23.2500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 838.7,                last time consumption/overall running time: 445.9270s / 77804.9596 s
env0_first_0:                 episode reward: -43.1500,                 loss: 0.1715
env0_second_0:                 episode reward: 43.1500,                 loss: nan
env1_first_0:                 episode reward: -54.0000,                 loss: nan
env1_second_0:                 episode reward: 54.0000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 765.9,                last time consumption/overall running time: 408.1846s / 78213.1442 s
env0_first_0:                 episode reward: -17.3000,                 loss: 0.2005
env0_second_0:                 episode reward: 17.3000,                 loss: nan
env1_first_0:                 episode reward: -48.8500,                 loss: nan
env1_second_0:                 episode reward: 48.8500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1199.7,                last time consumption/overall running time: 634.7103s / 78847.8545 s
env0_first_0:                 episode reward: -12.2500,                 loss: 0.2188
env0_second_0:                 episode reward: 12.2500,                 loss: nan
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1104.5,                last time consumption/overall running time: 588.0111s / 79435.8656 s
env0_first_0:                 episode reward: -21.3000,                 loss: 0.2015
env0_second_0:                 episode reward: 21.3000,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 830.55,                last time consumption/overall running time: 436.3968s / 79872.2624 s
env0_first_0:                 episode reward: -15.5000,                 loss: 0.1649
env0_second_0:                 episode reward: 15.5000,                 loss: nan
env1_first_0:                 episode reward: -14.0500,                 loss: nan
env1_second_0:                 episode reward: 14.0500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 596.35,                last time consumption/overall running time: 317.4906s / 80189.7531 s
env0_first_0:                 episode reward: -23.7500,                 loss: 0.2088
env0_second_0:                 episode reward: 23.7500,                 loss: nan
env1_first_0:                 episode reward: -44.9000,                 loss: nan
env1_second_0:                 episode reward: 44.9000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1005.0,                last time consumption/overall running time: 537.5066s / 80727.2597 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.2603
env0_second_0:                 episode reward: 7.8000,                 loss: nan
env1_first_0:                 episode reward: -36.8500,                 loss: nan
env1_second_0:                 episode reward: 36.8500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 803.3,                last time consumption/overall running time: 428.7178s / 81155.9775 s
env0_first_0:                 episode reward: 12.4000,                 loss: 0.2460
env0_second_0:                 episode reward: -12.4000,                 loss: nan
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 471.85,                last time consumption/overall running time: 250.9096s / 81406.8871 s
env0_first_0:                 episode reward: -23.5000,                 loss: 0.2746
env0_second_0:                 episode reward: 23.5000,                 loss: nan
env1_first_0:                 episode reward: -28.5500,                 loss: nan
env1_second_0:                 episode reward: 28.5500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 628.0,                last time consumption/overall running time: 333.8096s / 81740.6967 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.2784
env0_second_0:                 episode reward: -2.4000,                 loss: nan
env1_first_0:                 episode reward: -10.7000,                 loss: nan
env1_second_0:                 episode reward: 10.7000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 596.65,                last time consumption/overall running time: 1250.3423s / 82991.0390 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.3091
env0_second_0:                 episode reward: 12.0500,                 loss: 0.4961
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Score delta: 93.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2857_0.
Episode: 2881/10000 (28.8100%),                 avg. length: 373.65,                last time consumption/overall running time: 1300.5732s / 84291.6122 s
env0_first_0:                 episode reward: -42.9000,                 loss: nan
env0_second_0:                 episode reward: 42.9000,                 loss: 0.4641
env1_first_0:                 episode reward: -58.0000,                 loss: nan
env1_second_0:                 episode reward: 58.0000,                 loss: nan
Score delta: 105.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/2881_1.
Episode: 2901/10000 (29.0100%),                 avg. length: 438.65,                last time consumption/overall running time: 231.9982s / 84523.6104 s
env0_first_0:                 episode reward: -82.6500,                 loss: 0.4334
env0_second_0:                 episode reward: 82.6500,                 loss: nan
env1_first_0:                 episode reward: -79.3500,                 loss: nan
env1_second_0:                 episode reward: 79.3500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 434.35,                last time consumption/overall running time: 228.9974s / 84752.6078 s
env0_first_0:                 episode reward: -85.0500,                 loss: 0.4133
env0_second_0:                 episode reward: 85.0500,                 loss: nan
env1_first_0:                 episode reward: -87.5000,                 loss: nan
env1_second_0:                 episode reward: 87.5000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 333.15,                last time consumption/overall running time: 175.4888s / 84928.0966 s
env0_first_0:                 episode reward: -79.5000,                 loss: 0.4108
env0_second_0:                 episode reward: 79.5000,                 loss: nan
env1_first_0:                 episode reward: -76.7000,                 loss: nan
env1_second_0:                 episode reward: 76.7000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 439.15,                last time consumption/overall running time: 230.5243s / 85158.6209 s
env0_first_0:                 episode reward: -65.7500,                 loss: 0.4737
env0_second_0:                 episode reward: 65.7500,                 loss: nan
env1_first_0:                 episode reward: -63.3000,                 loss: nan
env1_second_0:                 episode reward: 63.3000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 687.4,                last time consumption/overall running time: 363.4234s / 85522.0443 s
env0_first_0:                 episode reward: -59.8500,                 loss: 0.4518
env0_second_0:                 episode reward: 59.8500,                 loss: nan
env1_first_0:                 episode reward: -57.7500,                 loss: nan
env1_second_0:                 episode reward: 57.7500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 564.0,                last time consumption/overall running time: 300.1360s / 85822.1803 s
env0_first_0:                 episode reward: -67.5000,                 loss: 0.4413
env0_second_0:                 episode reward: 67.5000,                 loss: nan
env1_first_0:                 episode reward: -62.4000,                 loss: nan
env1_second_0:                 episode reward: 62.4000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 589.05,                last time consumption/overall running time: 313.8376s / 86136.0179 s
env0_first_0:                 episode reward: -46.0000,                 loss: 0.4146
env0_second_0:                 episode reward: 46.0000,                 loss: nan
env1_first_0:                 episode reward: -58.9500,                 loss: nan
env1_second_0:                 episode reward: 58.9500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 553.75,                last time consumption/overall running time: 297.7156s / 86433.7335 s
env0_first_0:                 episode reward: -42.5000,                 loss: 0.4115
env0_second_0:                 episode reward: 42.5000,                 loss: nan
env1_first_0:                 episode reward: -72.6500,                 loss: nan
env1_second_0:                 episode reward: 72.6500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 519.85,                last time consumption/overall running time: 279.1874s / 86712.9210 s
env0_first_0:                 episode reward: -69.0000,                 loss: 0.4034
env0_second_0:                 episode reward: 69.0000,                 loss: nan
env1_first_0:                 episode reward: -49.7500,                 loss: nan
env1_second_0:                 episode reward: 49.7500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 603.6,                last time consumption/overall running time: 322.7371s / 87035.6581 s
env0_first_0:                 episode reward: -62.5000,                 loss: 0.4128
env0_second_0:                 episode reward: 62.5000,                 loss: nan
env1_first_0:                 episode reward: -52.6500,                 loss: nan
env1_second_0:                 episode reward: 52.6500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 596.05,                last time consumption/overall running time: 317.6804s / 87353.3384 s
env0_first_0:                 episode reward: -64.1000,                 loss: 0.4606
env0_second_0:                 episode reward: 64.1000,                 loss: nan
env1_first_0:                 episode reward: -62.5500,                 loss: nan
env1_second_0:                 episode reward: 62.5500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 751.65,                last time consumption/overall running time: 401.4508s / 87754.7892 s
env0_first_0:                 episode reward: -58.5000,                 loss: 0.4394
env0_second_0:                 episode reward: 58.5000,                 loss: nan
env1_first_0:                 episode reward: -41.3500,                 loss: nan
env1_second_0:                 episode reward: 41.3500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 641.35,                last time consumption/overall running time: 343.1439s / 88097.9331 s
env0_first_0:                 episode reward: -45.8500,                 loss: 0.4056
env0_second_0:                 episode reward: 45.8500,                 loss: nan
env1_first_0:                 episode reward: -52.7000,                 loss: nan
env1_second_0:                 episode reward: 52.7000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 619.2,                last time consumption/overall running time: 330.9396s / 88428.8728 s
env0_first_0:                 episode reward: -47.2000,                 loss: 0.3813
env0_second_0:                 episode reward: 47.2000,                 loss: nan
env1_first_0:                 episode reward: -48.0500,                 loss: nan
env1_second_0:                 episode reward: 48.0500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 541.95,                last time consumption/overall running time: 288.8790s / 88717.7518 s
env0_first_0:                 episode reward: -18.4500,                 loss: 0.4343
env0_second_0:                 episode reward: 18.4500,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 518.45,                last time consumption/overall running time: 276.2643s / 88994.0162 s
env0_first_0:                 episode reward: -19.7000,                 loss: 0.4614
env0_second_0:                 episode reward: 19.7000,                 loss: nan
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 549.0,                last time consumption/overall running time: 292.6695s / 89286.6857 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.5113
env0_second_0:                 episode reward: 4.9500,                 loss: nan
env1_first_0:                 episode reward: -15.6500,                 loss: nan
env1_second_0:                 episode reward: 15.6500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 608.4,                last time consumption/overall running time: 324.0887s / 89610.7743 s
env0_first_0:                 episode reward: -16.0500,                 loss: 0.5828
env0_second_0:                 episode reward: 16.0500,                 loss: nan
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 431.0,                last time consumption/overall running time: 230.2058s / 89840.9802 s
env0_first_0:                 episode reward: -21.8000,                 loss: 0.5932
env0_second_0:                 episode reward: 21.8000,                 loss: nan
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 376.25,                last time consumption/overall running time: 201.5424s / 90042.5225 s
env0_first_0:                 episode reward: -35.2000,                 loss: 0.6302
env0_second_0:                 episode reward: 35.2000,                 loss: nan
env1_first_0:                 episode reward: -31.7000,                 loss: nan
env1_second_0:                 episode reward: 31.7000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1170.05,                last time consumption/overall running time: 620.4051s / 90662.9276 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.5314
env0_second_0:                 episode reward: 12.2000,                 loss: nan
env1_first_0:                 episode reward: -20.8000,                 loss: nan
env1_second_0:                 episode reward: 20.8000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 825.2,                last time consumption/overall running time: 435.4320s / 91098.3596 s
env0_first_0:                 episode reward: -30.7500,                 loss: 0.3874
env0_second_0:                 episode reward: 30.7500,                 loss: nan
env1_first_0:                 episode reward: -45.8500,                 loss: nan
env1_second_0:                 episode reward: 45.8500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 855.65,                last time consumption/overall running time: 450.1425s / 91548.5021 s
env0_first_0:                 episode reward: 8.5500,                 loss: 0.2603
env0_second_0:                 episode reward: -8.5500,                 loss: nan
env1_first_0:                 episode reward: -11.8500,                 loss: nan
env1_second_0:                 episode reward: 11.8500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 321.45,                last time consumption/overall running time: 171.1001s / 91719.6022 s
env0_first_0:                 episode reward: -23.0500,                 loss: 0.3173
env0_second_0:                 episode reward: 23.0500,                 loss: nan
env1_first_0:                 episode reward: 14.1000,                 loss: nan
env1_second_0:                 episode reward: -14.1000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 349.3,                last time consumption/overall running time: 187.4187s / 91907.0210 s
env0_first_0:                 episode reward: 11.5500,                 loss: 0.4527
env0_second_0:                 episode reward: -11.5500,                 loss: nan
env1_first_0:                 episode reward: 10.5000,                 loss: nan
env1_second_0:                 episode reward: -10.5000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 489.2,                last time consumption/overall running time: 678.6554s / 92585.6763 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.4987
env0_second_0:                 episode reward: 1.6500,                 loss: 0.4808
env1_first_0:                 episode reward: 18.9500,                 loss: nan
env1_second_0:                 episode reward: -18.9500,                 loss: nan
Score delta: 90.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/3389_0.
Episode: 3421/10000 (34.2100%),                 avg. length: 511.7,                last time consumption/overall running time: 1424.3863s / 94010.0626 s
env0_first_0:                 episode reward: -61.7500,                 loss: 0.2728
env0_second_0:                 episode reward: 61.7500,                 loss: 0.4813
env1_first_0:                 episode reward: -33.7500,                 loss: nan
env1_second_0:                 episode reward: 33.7500,                 loss: nan
Score delta: 95.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/3410_1.
Episode: 3441/10000 (34.4100%),                 avg. length: 322.75,                last time consumption/overall running time: 174.5128s / 94184.5754 s
env0_first_0:                 episode reward: -80.8500,                 loss: 0.2786
env0_second_0:                 episode reward: 80.8500,                 loss: nan
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 426.55,                last time consumption/overall running time: 229.7775s / 94414.3529 s
env0_first_0:                 episode reward: -79.1500,                 loss: 0.3385
env0_second_0:                 episode reward: 79.1500,                 loss: nan
env1_first_0:                 episode reward: -51.9000,                 loss: nan
env1_second_0:                 episode reward: 51.9000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1041.15,                last time consumption/overall running time: 556.5041s / 94970.8570 s
env0_first_0:                 episode reward: -28.8500,                 loss: 0.3824
env0_second_0:                 episode reward: 28.8500,                 loss: nan
env1_first_0:                 episode reward: -45.2500,                 loss: nan
env1_second_0:                 episode reward: 45.2500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 690.7,                last time consumption/overall running time: 368.7670s / 95339.6240 s
env0_first_0:                 episode reward: -48.4000,                 loss: 0.2914
env0_second_0:                 episode reward: 48.4000,                 loss: nan
env1_first_0:                 episode reward: -13.0500,                 loss: nan
env1_second_0:                 episode reward: 13.0500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 916.25,                last time consumption/overall running time: 486.2720s / 95825.8961 s
env0_first_0:                 episode reward: -26.5500,                 loss: 0.2230
env0_second_0:                 episode reward: 26.5500,                 loss: nan
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 991.2,                last time consumption/overall running time: 524.4823s / 96350.3784 s
env0_first_0:                 episode reward: -22.0000,                 loss: 0.2182
env0_second_0:                 episode reward: 22.0000,                 loss: nan
env1_first_0:                 episode reward: -38.1000,                 loss: nan
env1_second_0:                 episode reward: 38.1000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 377.85,                last time consumption/overall running time: 201.4825s / 96551.8609 s
env0_first_0:                 episode reward: -16.6500,                 loss: 0.2754
env0_second_0:                 episode reward: 16.6500,                 loss: nan
env1_first_0:                 episode reward: -20.0000,                 loss: nan
env1_second_0:                 episode reward: 20.0000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 474.25,                last time consumption/overall running time: 656.4792s / 97208.3401 s
env0_first_0:                 episode reward: 67.9500,                 loss: 0.3503
env0_second_0:                 episode reward: -67.9500,                 loss: 0.5145
env1_first_0:                 episode reward: 59.5000,                 loss: nan
env1_second_0:                 episode reward: -59.5000,                 loss: nan
Score delta: 85.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/3564_0.
Episode: 3601/10000 (36.0100%),                 avg. length: 698.6,                last time consumption/overall running time: 366.1899s / 97574.5300 s
env0_first_0:                 episode reward: 52.4000,                 loss: nan
env0_second_0:                 episode reward: -52.4000,                 loss: 0.4050
env1_first_0:                 episode reward: 64.6000,                 loss: nan
env1_second_0:                 episode reward: -64.6000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 445.05,                last time consumption/overall running time: 234.0089s / 97808.5389 s
env0_first_0:                 episode reward: 30.9500,                 loss: nan
env0_second_0:                 episode reward: -30.9500,                 loss: 0.4132
env1_first_0:                 episode reward: 51.1500,                 loss: nan
env1_second_0:                 episode reward: -51.1500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 327.5,                last time consumption/overall running time: 171.9389s / 97980.4778 s
env0_first_0:                 episode reward: 10.2000,                 loss: nan
env0_second_0:                 episode reward: -10.2000,                 loss: 0.4574
env1_first_0:                 episode reward: -10.7500,                 loss: nan
env1_second_0:                 episode reward: 10.7500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 313.65,                last time consumption/overall running time: 164.5734s / 98145.0512 s
env0_first_0:                 episode reward: 36.4500,                 loss: nan
env0_second_0:                 episode reward: -36.4500,                 loss: 0.5090
env1_first_0:                 episode reward: 23.9500,                 loss: nan
env1_second_0:                 episode reward: -23.9500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 312.65,                last time consumption/overall running time: 1341.3992s / 99486.4504 s
env0_first_0:                 episode reward: -9.8000,                 loss: nan
env0_second_0:                 episode reward: 9.8000,                 loss: 0.5441
env1_first_0:                 episode reward: -29.1500,                 loss: nan
env1_second_0:                 episode reward: 29.1500,                 loss: nan
Score delta: 94.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/3681_1.
Episode: 3701/10000 (37.0100%),                 avg. length: 248.75,                last time consumption/overall running time: 132.8224s / 99619.2728 s
env0_first_0:                 episode reward: -57.0000,                 loss: 0.3565
env0_second_0:                 episode reward: 57.0000,                 loss: nan
env1_first_0:                 episode reward: -61.7500,                 loss: nan
env1_second_0:                 episode reward: 61.7500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 253.15,                last time consumption/overall running time: 135.5701s / 99754.8429 s
env0_first_0:                 episode reward: -74.6000,                 loss: 0.3621
env0_second_0:                 episode reward: 74.6000,                 loss: nan
env1_first_0:                 episode reward: -55.6500,                 loss: nan
env1_second_0:                 episode reward: 55.6500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 330.45,                last time consumption/overall running time: 586.6156s / 100341.4585 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.4159
env0_second_0:                 episode reward: -1.3500,                 loss: 0.5694
env1_first_0:                 episode reward: -20.1000,                 loss: nan
env1_second_0:                 episode reward: 20.1000,                 loss: nan
Score delta: 84.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/3735_0.
Episode: 3761/10000 (37.6100%),                 avg. length: 354.2,                last time consumption/overall running time: 188.8965s / 100530.3550 s
env0_first_0:                 episode reward: -14.2000,                 loss: nan
env0_second_0:                 episode reward: 14.2000,                 loss: 0.6426
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 362.3,                last time consumption/overall running time: 1421.2705s / 101951.6255 s
env0_first_0:                 episode reward: -26.0000,                 loss: 0.5845
env0_second_0:                 episode reward: 26.0000,                 loss: 0.7344
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Score delta: 95.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/3778_1.
Episode: 3801/10000 (38.0100%),                 avg. length: 419.4,                last time consumption/overall running time: 224.8516s / 102176.4771 s
env0_first_0:                 episode reward: -20.6000,                 loss: 0.6220
env0_second_0:                 episode reward: 20.6000,                 loss: nan
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 358.8,                last time consumption/overall running time: 193.0400s / 102369.5171 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.7425
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 379.85,                last time consumption/overall running time: 204.5612s / 102574.0783 s
env0_first_0:                 episode reward: 9.6500,                 loss: 0.9179
env0_second_0:                 episode reward: -9.6500,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 391.1,                last time consumption/overall running time: 210.4166s / 102784.4949 s
env0_first_0:                 episode reward: -1.3000,                 loss: 1.1180
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: 28.7500,                 loss: nan
env1_second_0:                 episode reward: -28.7500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 308.45,                last time consumption/overall running time: 165.2845s / 102949.7794 s
env0_first_0:                 episode reward: 10.8500,                 loss: 1.1696
env0_second_0:                 episode reward: -10.8500,                 loss: nan
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 503.9,                last time consumption/overall running time: 269.1066s / 103218.8860 s
env0_first_0:                 episode reward: 7.7000,                 loss: 1.1989
env0_second_0:                 episode reward: -7.7000,                 loss: nan
env1_first_0:                 episode reward: 31.9000,                 loss: nan
env1_second_0:                 episode reward: -31.9000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 711.65,                last time consumption/overall running time: 380.7736s / 103599.6595 s
env0_first_0:                 episode reward: 18.7500,                 loss: 1.1095
env0_second_0:                 episode reward: -18.7500,                 loss: nan
env1_first_0:                 episode reward: 27.0500,                 loss: nan
env1_second_0:                 episode reward: -27.0500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 940.25,                last time consumption/overall running time: 1646.8373s / 105246.4968 s
env0_first_0:                 episode reward: -8.9500,                 loss: 0.9219
env0_second_0:                 episode reward: 8.9500,                 loss: 0.8089
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Score delta: 86.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/3930_0.
Episode: 3961/10000 (39.6100%),                 avg. length: 553.3,                last time consumption/overall running time: 1575.6039s / 106822.1008 s
env0_first_0:                 episode reward: -46.0000,                 loss: 0.1401
env0_second_0:                 episode reward: 46.0000,                 loss: 0.7468
env1_first_0:                 episode reward: -45.3000,                 loss: nan
env1_second_0:                 episode reward: 45.3000,                 loss: nan
Score delta: 105.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/3957_1.
Episode: 3981/10000 (39.8100%),                 avg. length: 641.2,                last time consumption/overall running time: 339.4667s / 107161.5675 s
env0_first_0:                 episode reward: -63.4500,                 loss: 0.1381
env0_second_0:                 episode reward: 63.4500,                 loss: nan
env1_first_0:                 episode reward: -58.9500,                 loss: nan
env1_second_0:                 episode reward: 58.9500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 346.15,                last time consumption/overall running time: 185.1347s / 107346.7021 s
env0_first_0:                 episode reward: -80.2000,                 loss: 0.1935
env0_second_0:                 episode reward: 80.2000,                 loss: nan
env1_first_0:                 episode reward: -86.8500,                 loss: nan
env1_second_0:                 episode reward: 86.8500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 522.9,                last time consumption/overall running time: 276.2390s / 107622.9412 s
env0_first_0:                 episode reward: -70.7000,                 loss: 0.2489
env0_second_0:                 episode reward: 70.7000,                 loss: nan
env1_first_0:                 episode reward: -70.5500,                 loss: nan
env1_second_0:                 episode reward: 70.5500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 824.9,                last time consumption/overall running time: 435.0496s / 108057.9908 s
env0_first_0:                 episode reward: -38.4000,                 loss: 0.3119
env0_second_0:                 episode reward: 38.4000,                 loss: nan
env1_first_0:                 episode reward: -43.5500,                 loss: nan
env1_second_0:                 episode reward: 43.5500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1644.4,                last time consumption/overall running time: 874.1828s / 108932.1736 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.2984
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1628.35,                last time consumption/overall running time: 862.2699s / 109794.4435 s
env0_first_0:                 episode reward: -10.0000,                 loss: 0.1404
env0_second_0:                 episode reward: 10.0000,                 loss: nan
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1499.5,                last time consumption/overall running time: 793.2658s / 110587.7093 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0380
env0_second_0:                 episode reward: 6.3500,                 loss: nan
env1_first_0:                 episode reward: -12.3000,                 loss: nan
env1_second_0:                 episode reward: 12.3000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1087.3,                last time consumption/overall running time: 573.5383s / 111161.2476 s
env0_first_0:                 episode reward: -24.8000,                 loss: 0.0909
env0_second_0:                 episode reward: 24.8000,                 loss: nan
env1_first_0:                 episode reward: -25.2000,                 loss: nan
env1_second_0:                 episode reward: 25.2000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1534.15,                last time consumption/overall running time: 803.7729s / 111965.0205 s
env0_first_0:                 episode reward: -9.6000,                 loss: 0.0925
env0_second_0:                 episode reward: 9.6000,                 loss: nan
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1441.2,                last time consumption/overall running time: 759.1249s / 112724.1455 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.0521
env0_second_0:                 episode reward: 11.4500,                 loss: nan
env1_first_0:                 episode reward: -10.5500,                 loss: nan
env1_second_0:                 episode reward: 10.5500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 957.2,                last time consumption/overall running time: 506.3761s / 113230.5216 s
env0_first_0:                 episode reward: -43.8500,                 loss: 0.0340
env0_second_0:                 episode reward: 43.8500,                 loss: nan
env1_first_0:                 episode reward: -52.6000,                 loss: nan
env1_second_0:                 episode reward: 52.6000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.2248s / 114172.7463 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0270
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1089.3,                last time consumption/overall running time: 579.4331s / 114752.1794 s
env0_first_0:                 episode reward: -36.0500,                 loss: 0.0275
env0_second_0:                 episode reward: 36.0500,                 loss: nan
env1_first_0:                 episode reward: -41.0500,                 loss: nan
env1_second_0:                 episode reward: 41.0500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 385.3,                last time consumption/overall running time: 206.2146s / 114958.3940 s
env0_first_0:                 episode reward: -74.1500,                 loss: 0.0735
env0_second_0:                 episode reward: 74.1500,                 loss: nan
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 983.25,                last time consumption/overall running time: 524.8296s / 115483.2236 s
env0_first_0:                 episode reward: -38.2500,                 loss: 0.1216
env0_second_0:                 episode reward: 38.2500,                 loss: nan
env1_first_0:                 episode reward: -46.2000,                 loss: nan
env1_second_0:                 episode reward: 46.2000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1301.0,                last time consumption/overall running time: 694.3015s / 116177.5251 s
env0_first_0:                 episode reward: -23.4500,                 loss: 0.1703
env0_second_0:                 episode reward: 23.4500,                 loss: nan
env1_first_0:                 episode reward: -23.5000,                 loss: nan
env1_second_0:                 episode reward: 23.5000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1156.25,                last time consumption/overall running time: 611.4995s / 116789.0246 s
env0_first_0:                 episode reward: -31.7500,                 loss: 0.1045
env0_second_0:                 episode reward: 31.7500,                 loss: nan
env1_first_0:                 episode reward: -17.0000,                 loss: nan
env1_second_0:                 episode reward: 17.0000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 633.3,                last time consumption/overall running time: 337.6802s / 117126.7048 s
env0_first_0:                 episode reward: -47.0000,                 loss: 0.1516
env0_second_0:                 episode reward: 47.0000,                 loss: nan
env1_first_0:                 episode reward: -51.7000,                 loss: nan
env1_second_0:                 episode reward: 51.7000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 633.05,                last time consumption/overall running time: 334.5472s / 117461.2521 s
env0_first_0:                 episode reward: -34.4000,                 loss: 0.2187
env0_second_0:                 episode reward: 34.4000,                 loss: nan
env1_first_0:                 episode reward: -32.3500,                 loss: nan
env1_second_0:                 episode reward: 32.3500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 820.05,                last time consumption/overall running time: 427.7964s / 117889.0484 s
env0_first_0:                 episode reward: -15.9500,                 loss: 0.2978
env0_second_0:                 episode reward: 15.9500,                 loss: nan
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 727.15,                last time consumption/overall running time: 376.7334s / 118265.7818 s
env0_first_0:                 episode reward: -28.8000,                 loss: 0.3013
env0_second_0:                 episode reward: 28.8000,                 loss: nan
env1_first_0:                 episode reward: -26.3000,                 loss: nan
env1_second_0:                 episode reward: 26.3000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 684.65,                last time consumption/overall running time: 356.2093s / 118621.9911 s
env0_first_0:                 episode reward: -26.7500,                 loss: 0.3090
env0_second_0:                 episode reward: 26.7500,                 loss: nan
env1_first_0:                 episode reward: -34.4500,                 loss: nan
env1_second_0:                 episode reward: 34.4500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 836.85,                last time consumption/overall running time: 439.3452s / 119061.3363 s
env0_first_0:                 episode reward: 8.9000,                 loss: 0.3234
env0_second_0:                 episode reward: -8.9000,                 loss: nan
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1184.8,                last time consumption/overall running time: 630.0774s / 119691.4137 s
env0_first_0:                 episode reward: -9.0500,                 loss: 0.3000
env0_second_0:                 episode reward: 9.0500,                 loss: nan
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 690.85,                last time consumption/overall running time: 368.8423s / 120060.2560 s
env0_first_0:                 episode reward: -26.2000,                 loss: 0.2774
env0_second_0:                 episode reward: 26.2000,                 loss: nan
env1_first_0:                 episode reward: -41.1000,                 loss: nan
env1_second_0:                 episode reward: 41.1000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1158.0,                last time consumption/overall running time: 615.4595s / 120675.7155 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.2743
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: -17.6500,                 loss: nan
env1_second_0:                 episode reward: 17.6500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1069.8,                last time consumption/overall running time: 567.3778s / 121243.0933 s
env0_first_0:                 episode reward: 9.7500,                 loss: 0.2563
env0_second_0:                 episode reward: -9.7500,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 607.3,                last time consumption/overall running time: 322.5441s / 121565.6374 s
env0_first_0:                 episode reward: -42.8000,                 loss: 0.2365
env0_second_0:                 episode reward: 42.8000,                 loss: nan
env1_first_0:                 episode reward: -37.7000,                 loss: nan
env1_second_0:                 episode reward: 37.7000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 540.85,                last time consumption/overall running time: 285.7702s / 121851.4077 s
env0_first_0:                 episode reward: -35.4000,                 loss: 0.2340
env0_second_0:                 episode reward: 35.4000,                 loss: nan
env1_first_0:                 episode reward: -30.1000,                 loss: nan
env1_second_0:                 episode reward: 30.1000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 705.9,                last time consumption/overall running time: 372.3595s / 122223.7672 s
env0_first_0:                 episode reward: -13.9000,                 loss: 0.2939
env0_second_0:                 episode reward: 13.9000,                 loss: nan
env1_first_0:                 episode reward: -17.6500,                 loss: nan
env1_second_0:                 episode reward: 17.6500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 679.9,                last time consumption/overall running time: 360.4348s / 122584.2020 s
env0_first_0:                 episode reward: -21.4500,                 loss: 0.3604
env0_second_0:                 episode reward: 21.4500,                 loss: nan
env1_first_0:                 episode reward: -38.3500,                 loss: nan
env1_second_0:                 episode reward: 38.3500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1116.05,                last time consumption/overall running time: 592.0798s / 123176.2818 s
env0_first_0:                 episode reward: 7.5000,                 loss: 0.3253
env0_second_0:                 episode reward: -7.5000,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 948.5,                last time consumption/overall running time: 504.5623s / 123680.8440 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.2263
env0_second_0:                 episode reward: 5.6500,                 loss: nan
env1_first_0:                 episode reward: -12.9500,                 loss: nan
env1_second_0:                 episode reward: 12.9500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 609.5,                last time consumption/overall running time: 325.1502s / 124005.9942 s
env0_first_0:                 episode reward: -36.5500,                 loss: 0.2023
env0_second_0:                 episode reward: 36.5500,                 loss: nan
env1_first_0:                 episode reward: -32.3500,                 loss: nan
env1_second_0:                 episode reward: 32.3500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1173.85,                last time consumption/overall running time: 622.0034s / 124627.9976 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.2530
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1361.75,                last time consumption/overall running time: 704.6513s / 125332.6489 s
env0_first_0:                 episode reward: -14.5000,                 loss: 0.1711
env0_second_0:                 episode reward: 14.5000,                 loss: nan
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1164.4,                last time consumption/overall running time: 601.9636s / 125934.6125 s
env0_first_0:                 episode reward: -11.0500,                 loss: 0.0817
env0_second_0:                 episode reward: 11.0500,                 loss: nan
env1_first_0:                 episode reward: -31.2500,                 loss: nan
env1_second_0:                 episode reward: 31.2500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 989.65,                last time consumption/overall running time: 515.2560s / 126449.8685 s
env0_first_0:                 episode reward: -31.0500,                 loss: 0.0853
env0_second_0:                 episode reward: 31.0500,                 loss: nan
env1_first_0:                 episode reward: -32.8000,                 loss: nan
env1_second_0:                 episode reward: 32.8000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 454.5,                last time consumption/overall running time: 241.9319s / 126691.8004 s
env0_first_0:                 episode reward: -53.5500,                 loss: 0.1428
env0_second_0:                 episode reward: 53.5500,                 loss: nan
env1_first_0:                 episode reward: -47.0500,                 loss: nan
env1_second_0:                 episode reward: 47.0500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 459.6,                last time consumption/overall running time: 245.6052s / 126937.4055 s
env0_first_0:                 episode reward: -47.2500,                 loss: 0.1870
env0_second_0:                 episode reward: 47.2500,                 loss: nan
env1_first_0:                 episode reward: -39.4000,                 loss: nan
env1_second_0:                 episode reward: 39.4000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 532.05,                last time consumption/overall running time: 282.8384s / 127220.2439 s
env0_first_0:                 episode reward: -19.8500,                 loss: 0.2225
env0_second_0:                 episode reward: 19.8500,                 loss: nan
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 566.4,                last time consumption/overall running time: 299.4687s / 127519.7127 s
env0_first_0:                 episode reward: -29.2500,                 loss: 0.2826
env0_second_0:                 episode reward: 29.2500,                 loss: nan
env1_first_0:                 episode reward: -23.4500,                 loss: nan
env1_second_0:                 episode reward: 23.4500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 591.2,                last time consumption/overall running time: 311.0031s / 127830.7158 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.3256
env0_second_0:                 episode reward: 4.6500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 713.6,                last time consumption/overall running time: 375.6043s / 128206.3200 s
env0_first_0:                 episode reward: 16.1500,                 loss: 0.3244
env0_second_0:                 episode reward: -16.1500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 481.05,                last time consumption/overall running time: 254.2934s / 128460.6134 s
env0_first_0:                 episode reward: -35.3000,                 loss: 0.3307
env0_second_0:                 episode reward: 35.3000,                 loss: nan
env1_first_0:                 episode reward: -48.3000,                 loss: nan
env1_second_0:                 episode reward: 48.3000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 576.05,                last time consumption/overall running time: 303.3284s / 128763.9418 s
env0_first_0:                 episode reward: -16.3000,                 loss: 0.3245
env0_second_0:                 episode reward: 16.3000,                 loss: nan
env1_first_0:                 episode reward: -10.1500,                 loss: nan
env1_second_0:                 episode reward: 10.1500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 524.7,                last time consumption/overall running time: 276.8285s / 129040.7702 s
env0_first_0:                 episode reward: -17.2000,                 loss: 0.3328
env0_second_0:                 episode reward: 17.2000,                 loss: nan
env1_first_0:                 episode reward: -26.9000,                 loss: nan
env1_second_0:                 episode reward: 26.9000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 586.55,                last time consumption/overall running time: 309.3382s / 129350.1084 s
env0_first_0:                 episode reward: 22.1000,                 loss: 0.3520
env0_second_0:                 episode reward: -22.1000,                 loss: nan
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 788.2,                last time consumption/overall running time: 1184.4326s / 130534.5410 s
env0_first_0:                 episode reward: -29.7500,                 loss: 0.3483
env0_second_0:                 episode reward: 29.7500,                 loss: 0.6115
env1_first_0:                 episode reward: -35.6500,                 loss: nan
env1_second_0:                 episode reward: 35.6500,                 loss: nan
Score delta: 91.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/4925_0.
Episode: 4961/10000 (49.6100%),                 avg. length: 243.55,                last time consumption/overall running time: 1454.3343s / 131988.8753 s
env0_first_0:                 episode reward: -87.1500,                 loss: 0.8637
env0_second_0:                 episode reward: 87.1500,                 loss: 0.4919
env1_first_0:                 episode reward: -94.8000,                 loss: nan
env1_second_0:                 episode reward: 94.8000,                 loss: nan
Score delta: 104.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/4946_1.
Episode: 4981/10000 (49.8100%),                 avg. length: 267.5,                last time consumption/overall running time: 140.2409s / 132129.1162 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.6176
env0_second_0:                 episode reward: 90.4000,                 loss: nan
env1_first_0:                 episode reward: -87.1500,                 loss: nan
env1_second_0:                 episode reward: 87.1500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 249.0,                last time consumption/overall running time: 131.5988s / 132260.7150 s
env0_first_0:                 episode reward: -90.8500,                 loss: 0.5135
env0_second_0:                 episode reward: 90.8500,                 loss: nan
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 239.1,                last time consumption/overall running time: 126.1515s / 132386.8665 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.4914
env0_second_0:                 episode reward: 93.3500,                 loss: nan
env1_first_0:                 episode reward: -96.2000,                 loss: nan
env1_second_0:                 episode reward: 96.2000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 382.3,                last time consumption/overall running time: 199.4046s / 132586.2711 s
env0_first_0:                 episode reward: -77.1500,                 loss: 0.5093
env0_second_0:                 episode reward: 77.1500,                 loss: nan
env1_first_0:                 episode reward: -76.2000,                 loss: nan
env1_second_0:                 episode reward: 76.2000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 382.05,                last time consumption/overall running time: 198.5373s / 132784.8085 s
env0_first_0:                 episode reward: -79.6000,                 loss: 0.5335
env0_second_0:                 episode reward: 79.6000,                 loss: nan
env1_first_0:                 episode reward: -83.4000,                 loss: nan
env1_second_0:                 episode reward: 83.4000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 341.25,                last time consumption/overall running time: 177.0265s / 132961.8350 s
env0_first_0:                 episode reward: -77.2500,                 loss: 0.5519
env0_second_0:                 episode reward: 77.2500,                 loss: nan
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 350.9,                last time consumption/overall running time: 183.0524s / 133144.8874 s
env0_first_0:                 episode reward: -69.6000,                 loss: 0.5985
env0_second_0:                 episode reward: 69.6000,                 loss: nan
env1_first_0:                 episode reward: -76.9500,                 loss: nan
env1_second_0:                 episode reward: 76.9500,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 811.7,                last time consumption/overall running time: 424.3631s / 133569.2504 s
env0_first_0:                 episode reward: -40.2000,                 loss: 0.6080
env0_second_0:                 episode reward: 40.2000,                 loss: nan
env1_first_0:                 episode reward: -54.9500,                 loss: nan
env1_second_0:                 episode reward: 54.9500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 416.0,                last time consumption/overall running time: 219.5301s / 133788.7805 s
env0_first_0:                 episode reward: -63.3000,                 loss: 0.5034
env0_second_0:                 episode reward: 63.3000,                 loss: nan
env1_first_0:                 episode reward: -57.1500,                 loss: nan
env1_second_0:                 episode reward: 57.1500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 622.2,                last time consumption/overall running time: 327.2715s / 134116.0520 s
env0_first_0:                 episode reward: -54.8000,                 loss: 0.4787
env0_second_0:                 episode reward: 54.8000,                 loss: nan
env1_first_0:                 episode reward: -65.9000,                 loss: nan
env1_second_0:                 episode reward: 65.9000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1207.15,                last time consumption/overall running time: 636.6257s / 134752.6777 s
env0_first_0:                 episode reward: -29.1500,                 loss: 0.3309
env0_second_0:                 episode reward: 29.1500,                 loss: nan
env1_first_0:                 episode reward: -32.4500,                 loss: nan
env1_second_0:                 episode reward: 32.4500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1592.9,                last time consumption/overall running time: 840.0043s / 135592.6820 s
env0_first_0:                 episode reward: -11.5500,                 loss: 0.1251
env0_second_0:                 episode reward: 11.5500,                 loss: nan
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1673.55,                last time consumption/overall running time: 884.4588s / 136477.1408 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0555
env0_second_0:                 episode reward: 7.0500,                 loss: nan
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 761.25,                last time consumption/overall running time: 402.3344s / 136879.4752 s
env0_first_0:                 episode reward: -58.3000,                 loss: 0.0756
env0_second_0:                 episode reward: 58.3000,                 loss: nan
env1_first_0:                 episode reward: -61.1500,                 loss: nan
env1_second_0:                 episode reward: 61.1500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 793.9,                last time consumption/overall running time: 421.8180s / 137301.2932 s
env0_first_0:                 episode reward: -55.6500,                 loss: 0.1241
env0_second_0:                 episode reward: 55.6500,                 loss: nan
env1_first_0:                 episode reward: -57.2000,                 loss: nan
env1_second_0:                 episode reward: 57.2000,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1557.2,                last time consumption/overall running time: 826.2322s / 138127.5254 s
env0_first_0:                 episode reward: -19.5500,                 loss: 0.1283
env0_second_0:                 episode reward: 19.5500,                 loss: nan
env1_first_0:                 episode reward: -22.3500,                 loss: nan
env1_second_0:                 episode reward: 22.3500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1486.4,                last time consumption/overall running time: 780.0538s / 138907.5792 s
env0_first_0:                 episode reward: -19.2500,                 loss: 0.0319
env0_second_0:                 episode reward: 19.2500,                 loss: nan
env1_first_0:                 episode reward: -20.0500,                 loss: nan
env1_second_0:                 episode reward: 20.0500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 293.35,                last time consumption/overall running time: 156.1614s / 139063.7406 s
env0_first_0:                 episode reward: -82.3000,                 loss: 0.0702
env0_second_0:                 episode reward: 82.3000,                 loss: nan
env1_first_0:                 episode reward: -95.8500,                 loss: nan
env1_second_0:                 episode reward: 95.8500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 302.0,                last time consumption/overall running time: 159.4177s / 139223.1584 s
env0_first_0:                 episode reward: -88.3000,                 loss: 0.1218
env0_second_0:                 episode reward: 88.3000,                 loss: nan
env1_first_0:                 episode reward: -95.3500,                 loss: nan
env1_second_0:                 episode reward: 95.3500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 842.85,                last time consumption/overall running time: 439.6303s / 139662.7886 s
env0_first_0:                 episode reward: -62.0500,                 loss: 0.1685
env0_second_0:                 episode reward: 62.0500,                 loss: nan
env1_first_0:                 episode reward: -54.1500,                 loss: nan
env1_second_0:                 episode reward: 54.1500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 294.25,                last time consumption/overall running time: 153.9607s / 139816.7493 s
env0_first_0:                 episode reward: -90.5500,                 loss: 0.2088
env0_second_0:                 episode reward: 90.5500,                 loss: nan
env1_first_0:                 episode reward: -92.6000,                 loss: nan
env1_second_0:                 episode reward: 92.6000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 347.35,                last time consumption/overall running time: 182.4966s / 139999.2459 s
env0_first_0:                 episode reward: -88.4500,                 loss: 0.2532
env0_second_0:                 episode reward: 88.4500,                 loss: nan
env1_first_0:                 episode reward: -80.7000,                 loss: nan
env1_second_0:                 episode reward: 80.7000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 416.25,                last time consumption/overall running time: 218.2866s / 140217.5325 s
env0_first_0:                 episode reward: -70.9000,                 loss: 0.3040
env0_second_0:                 episode reward: 70.9000,                 loss: nan
env1_first_0:                 episode reward: -69.4500,                 loss: nan
env1_second_0:                 episode reward: 69.4500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 390.75,                last time consumption/overall running time: 206.3663s / 140423.8988 s
env0_first_0:                 episode reward: -69.3500,                 loss: 0.3112
env0_second_0:                 episode reward: 69.3500,                 loss: nan
env1_first_0:                 episode reward: -86.7000,                 loss: nan
env1_second_0:                 episode reward: 86.7000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 333.4,                last time consumption/overall running time: 176.5762s / 140600.4750 s
env0_first_0:                 episode reward: -70.7500,                 loss: 0.3322
env0_second_0:                 episode reward: 70.7500,                 loss: nan
env1_first_0:                 episode reward: -83.4500,                 loss: nan
env1_second_0:                 episode reward: 83.4500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 429.1,                last time consumption/overall running time: 227.3698s / 140827.8448 s
env0_first_0:                 episode reward: -64.3500,                 loss: 0.3631
env0_second_0:                 episode reward: 64.3500,                 loss: nan
env1_first_0:                 episode reward: -78.9500,                 loss: nan
env1_second_0:                 episode reward: 78.9500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 488.45,                last time consumption/overall running time: 258.0981s / 141085.9429 s
env0_first_0:                 episode reward: -63.3500,                 loss: 0.4029
env0_second_0:                 episode reward: 63.3500,                 loss: nan
env1_first_0:                 episode reward: -57.3000,                 loss: nan
env1_second_0:                 episode reward: 57.3000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 442.35,                last time consumption/overall running time: 232.9432s / 141318.8861 s
env0_first_0:                 episode reward: -81.2000,                 loss: 0.4376
env0_second_0:                 episode reward: 81.2000,                 loss: nan
env1_first_0:                 episode reward: -58.2000,                 loss: nan
env1_second_0:                 episode reward: 58.2000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 335.5,                last time consumption/overall running time: 176.6721s / 141495.5582 s
env0_first_0:                 episode reward: -81.2500,                 loss: 0.4818
env0_second_0:                 episode reward: 81.2500,                 loss: nan
env1_first_0:                 episode reward: -77.5000,                 loss: nan
env1_second_0:                 episode reward: 77.5000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 391.55,                last time consumption/overall running time: 206.8532s / 141702.4113 s
env0_first_0:                 episode reward: -73.1500,                 loss: 0.5168
env0_second_0:                 episode reward: 73.1500,                 loss: nan
env1_first_0:                 episode reward: -62.1500,                 loss: nan
env1_second_0:                 episode reward: 62.1500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 411.25,                last time consumption/overall running time: 217.4502s / 141919.8615 s
env0_first_0:                 episode reward: -69.8500,                 loss: 0.5327
env0_second_0:                 episode reward: 69.8500,                 loss: nan
env1_first_0:                 episode reward: -75.2500,                 loss: nan
env1_second_0:                 episode reward: 75.2500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 521.5,                last time consumption/overall running time: 273.7746s / 142193.6361 s
env0_first_0:                 episode reward: -58.2000,                 loss: 0.5227
env0_second_0:                 episode reward: 58.2000,                 loss: nan
env1_first_0:                 episode reward: -65.3000,                 loss: nan
env1_second_0:                 episode reward: 65.3000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1107.9,                last time consumption/overall running time: 577.7680s / 142771.4041 s
env0_first_0:                 episode reward: -40.8000,                 loss: 0.3952
env0_second_0:                 episode reward: 40.8000,                 loss: nan
env1_first_0:                 episode reward: -37.1500,                 loss: nan
env1_second_0:                 episode reward: 37.1500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1708.95,                last time consumption/overall running time: 895.5444s / 143666.9485 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.1094
env0_second_0:                 episode reward: 7.0500,                 loss: nan
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1160.85,                last time consumption/overall running time: 612.9952s / 144279.9437 s
env0_first_0:                 episode reward: -44.9000,                 loss: 0.0211
env0_second_0:                 episode reward: 44.9000,                 loss: nan
env1_first_0:                 episode reward: -41.4000,                 loss: nan
env1_second_0:                 episode reward: 41.4000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 791.65,                last time consumption/overall running time: 419.3153s / 144699.2590 s
env0_first_0:                 episode reward: -56.8000,                 loss: 0.0589
env0_second_0:                 episode reward: 56.8000,                 loss: nan
env1_first_0:                 episode reward: -57.5500,                 loss: nan
env1_second_0:                 episode reward: 57.5500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1394.85,                last time consumption/overall running time: 736.3582s / 145435.6172 s
env0_first_0:                 episode reward: -18.2500,                 loss: 0.0684
env0_second_0:                 episode reward: 18.2500,                 loss: nan
env1_first_0:                 episode reward: -15.8000,                 loss: nan
env1_second_0:                 episode reward: 15.8000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 524.8,                last time consumption/overall running time: 277.2725s / 145712.8897 s
env0_first_0:                 episode reward: -68.6000,                 loss: 0.0879
env0_second_0:                 episode reward: 68.6000,                 loss: nan
env1_first_0:                 episode reward: -62.3000,                 loss: nan
env1_second_0:                 episode reward: 62.3000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 328.1,                last time consumption/overall running time: 174.2616s / 145887.1513 s
env0_first_0:                 episode reward: -84.1500,                 loss: 0.1685
env0_second_0:                 episode reward: 84.1500,                 loss: nan
env1_first_0:                 episode reward: -81.5000,                 loss: nan
env1_second_0:                 episode reward: 81.5000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 297.2,                last time consumption/overall running time: 157.4484s / 146044.5997 s
env0_first_0:                 episode reward: -84.0000,                 loss: 0.2059
env0_second_0:                 episode reward: 84.0000,                 loss: nan
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 494.95,                last time consumption/overall running time: 260.5860s / 146305.1857 s
env0_first_0:                 episode reward: -59.3000,                 loss: 0.2712
env0_second_0:                 episode reward: 59.3000,                 loss: nan
env1_first_0:                 episode reward: -64.5000,                 loss: nan
env1_second_0:                 episode reward: 64.5000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 469.85,                last time consumption/overall running time: 245.8691s / 146551.0548 s
env0_first_0:                 episode reward: -72.6500,                 loss: 0.3590
env0_second_0:                 episode reward: 72.6500,                 loss: nan
env1_first_0:                 episode reward: -69.1000,                 loss: nan
env1_second_0:                 episode reward: 69.1000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 338.55,                last time consumption/overall running time: 177.5401s / 146728.5948 s
env0_first_0:                 episode reward: -88.5000,                 loss: 0.4009
env0_second_0:                 episode reward: 88.5000,                 loss: nan
env1_first_0:                 episode reward: -75.5000,                 loss: nan
env1_second_0:                 episode reward: 75.5000,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 989.8,                last time consumption/overall running time: 518.0130s / 147246.6078 s
env0_first_0:                 episode reward: -41.0000,                 loss: 0.4757
env0_second_0:                 episode reward: 41.0000,                 loss: nan
env1_first_0:                 episode reward: -39.7000,                 loss: nan
env1_second_0:                 episode reward: 39.7000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 793.6,                last time consumption/overall running time: 415.6921s / 147662.3000 s
env0_first_0:                 episode reward: -57.4500,                 loss: 0.3013
env0_second_0:                 episode reward: 57.4500,                 loss: nan
env1_first_0:                 episode reward: -49.6000,                 loss: nan
env1_second_0:                 episode reward: 49.6000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 336.0,                last time consumption/overall running time: 177.4699s / 147839.7699 s
env0_first_0:                 episode reward: -76.4500,                 loss: 0.3023
env0_second_0:                 episode reward: 76.4500,                 loss: nan
env1_first_0:                 episode reward: -82.9000,                 loss: nan
env1_second_0:                 episode reward: 82.9000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 310.45,                last time consumption/overall running time: 163.6027s / 148003.3725 s
env0_first_0:                 episode reward: -68.3500,                 loss: 0.3087
env0_second_0:                 episode reward: 68.3500,                 loss: nan
env1_first_0:                 episode reward: -83.3000,                 loss: nan
env1_second_0:                 episode reward: 83.3000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 599.35,                last time consumption/overall running time: 315.2559s / 148318.6284 s
env0_first_0:                 episode reward: -63.6000,                 loss: 0.2542
env0_second_0:                 episode reward: 63.6000,                 loss: nan
env1_first_0:                 episode reward: -58.6000,                 loss: nan
env1_second_0:                 episode reward: 58.6000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 457.6,                last time consumption/overall running time: 242.6820s / 148561.3105 s
env0_first_0:                 episode reward: -53.1500,                 loss: 0.3035
env0_second_0:                 episode reward: 53.1500,                 loss: nan
env1_first_0:                 episode reward: -63.5500,                 loss: nan
env1_second_0:                 episode reward: 63.5500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 927.65,                last time consumption/overall running time: 490.3087s / 149051.6191 s
env0_first_0:                 episode reward: -52.4000,                 loss: 0.3602
env0_second_0:                 episode reward: 52.4000,                 loss: nan
env1_first_0:                 episode reward: -51.0000,                 loss: nan
env1_second_0:                 episode reward: 51.0000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 397.25,                last time consumption/overall running time: 211.6785s / 149263.2977 s
env0_first_0:                 episode reward: -84.2000,                 loss: 0.2856
env0_second_0:                 episode reward: 84.2000,                 loss: nan
env1_first_0:                 episode reward: -73.7500,                 loss: nan
env1_second_0:                 episode reward: 73.7500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 417.3,                last time consumption/overall running time: 220.8157s / 149484.1134 s
env0_first_0:                 episode reward: -73.7000,                 loss: 0.2779
env0_second_0:                 episode reward: 73.7000,                 loss: nan
env1_first_0:                 episode reward: -65.5500,                 loss: nan
env1_second_0:                 episode reward: 65.5500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 344.8,                last time consumption/overall running time: 182.3013s / 149666.4147 s
env0_first_0:                 episode reward: -91.0000,                 loss: 0.3098
env0_second_0:                 episode reward: 91.0000,                 loss: nan
env1_first_0:                 episode reward: -78.5000,                 loss: nan
env1_second_0:                 episode reward: 78.5000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 465.9,                last time consumption/overall running time: 245.5825s / 149911.9972 s
env0_first_0:                 episode reward: -64.3500,                 loss: 0.3497
env0_second_0:                 episode reward: 64.3500,                 loss: nan
env1_first_0:                 episode reward: -76.3000,                 loss: nan
env1_second_0:                 episode reward: 76.3000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 421.05,                last time consumption/overall running time: 221.7814s / 150133.7787 s
env0_first_0:                 episode reward: -63.7000,                 loss: 0.3735
env0_second_0:                 episode reward: 63.7000,                 loss: nan
env1_first_0:                 episode reward: -67.2000,                 loss: nan
env1_second_0:                 episode reward: 67.2000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 453.2,                last time consumption/overall running time: 240.0627s / 150373.8414 s
env0_first_0:                 episode reward: -76.6000,                 loss: 0.4045
env0_second_0:                 episode reward: 76.6000,                 loss: nan
env1_first_0:                 episode reward: -51.5000,                 loss: nan
env1_second_0:                 episode reward: 51.5000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 475.6,                last time consumption/overall running time: 252.6478s / 150626.4891 s
env0_first_0:                 episode reward: -51.5500,                 loss: 0.4448
env0_second_0:                 episode reward: 51.5500,                 loss: nan
env1_first_0:                 episode reward: -61.7000,                 loss: nan
env1_second_0:                 episode reward: 61.7000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 429.65,                last time consumption/overall running time: 227.4822s / 150853.9713 s
env0_first_0:                 episode reward: -68.0500,                 loss: 0.4385
env0_second_0:                 episode reward: 68.0500,                 loss: nan
env1_first_0:                 episode reward: -45.6500,                 loss: nan
env1_second_0:                 episode reward: 45.6500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 385.8,                last time consumption/overall running time: 204.2703s / 151058.2416 s
env0_first_0:                 episode reward: -74.6500,                 loss: 0.4583
env0_second_0:                 episode reward: 74.6500,                 loss: nan
env1_first_0:                 episode reward: -49.6000,                 loss: nan
env1_second_0:                 episode reward: 49.6000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 475.7,                last time consumption/overall running time: 250.2881s / 151308.5297 s
env0_first_0:                 episode reward: -66.7500,                 loss: 0.4779
env0_second_0:                 episode reward: 66.7500,                 loss: nan
env1_first_0:                 episode reward: -54.7000,                 loss: nan
env1_second_0:                 episode reward: 54.7000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 464.95,                last time consumption/overall running time: 244.7076s / 151553.2373 s
env0_first_0:                 episode reward: -48.4500,                 loss: 0.5101
env0_second_0:                 episode reward: 48.4500,                 loss: nan
env1_first_0:                 episode reward: -70.8000,                 loss: nan
env1_second_0:                 episode reward: 70.8000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 479.35,                last time consumption/overall running time: 252.7920s / 151806.0294 s
env0_first_0:                 episode reward: -55.1000,                 loss: 0.4708
env0_second_0:                 episode reward: 55.1000,                 loss: nan
env1_first_0:                 episode reward: -50.4500,                 loss: nan
env1_second_0:                 episode reward: 50.4500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 466.1,                last time consumption/overall running time: 246.8115s / 152052.8409 s
env0_first_0:                 episode reward: -49.8000,                 loss: 0.4487
env0_second_0:                 episode reward: 49.8000,                 loss: nan
env1_first_0:                 episode reward: -46.0500,                 loss: nan
env1_second_0:                 episode reward: 46.0500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 523.35,                last time consumption/overall running time: 276.1076s / 152328.9485 s
env0_first_0:                 episode reward: -45.8500,                 loss: 0.4762
env0_second_0:                 episode reward: 45.8500,                 loss: nan
env1_first_0:                 episode reward: -57.4500,                 loss: nan
env1_second_0:                 episode reward: 57.4500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 496.1,                last time consumption/overall running time: 262.7392s / 152591.6877 s
env0_first_0:                 episode reward: -50.2500,                 loss: 0.4543
env0_second_0:                 episode reward: 50.2500,                 loss: nan
env1_first_0:                 episode reward: -60.5500,                 loss: nan
env1_second_0:                 episode reward: 60.5500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 407.85,                last time consumption/overall running time: 216.4309s / 152808.1185 s
env0_first_0:                 episode reward: -63.9000,                 loss: 0.4607
env0_second_0:                 episode reward: 63.9000,                 loss: nan
env1_first_0:                 episode reward: -55.8000,                 loss: nan
env1_second_0:                 episode reward: 55.8000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 401.1,                last time consumption/overall running time: 213.1996s / 153021.3181 s
env0_first_0:                 episode reward: -49.8000,                 loss: 0.4727
env0_second_0:                 episode reward: 49.8000,                 loss: nan
env1_first_0:                 episode reward: -68.9000,                 loss: nan
env1_second_0:                 episode reward: 68.9000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 405.3,                last time consumption/overall running time: 216.1373s / 153237.4554 s
env0_first_0:                 episode reward: -44.5500,                 loss: 0.4782
env0_second_0:                 episode reward: 44.5500,                 loss: nan
env1_first_0:                 episode reward: -63.3000,                 loss: nan
env1_second_0:                 episode reward: 63.3000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 299.05,                last time consumption/overall running time: 160.3685s / 153397.8239 s
env0_first_0:                 episode reward: -73.8500,                 loss: 0.4515
env0_second_0:                 episode reward: 73.8500,                 loss: nan
env1_first_0:                 episode reward: -67.2000,                 loss: nan
env1_second_0:                 episode reward: 67.2000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 309.3,                last time consumption/overall running time: 165.1591s / 153562.9830 s
env0_first_0:                 episode reward: -60.2000,                 loss: 0.4552
env0_second_0:                 episode reward: 60.2000,                 loss: nan
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 435.55,                last time consumption/overall running time: 229.2747s / 153792.2576 s
env0_first_0:                 episode reward: -47.4000,                 loss: 0.4705
env0_second_0:                 episode reward: 47.4000,                 loss: nan
env1_first_0:                 episode reward: -65.8500,                 loss: nan
env1_second_0:                 episode reward: 65.8500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 585.85,                last time consumption/overall running time: 305.0016s / 154097.2592 s
env0_first_0:                 episode reward: -53.0500,                 loss: 0.5255
env0_second_0:                 episode reward: 53.0500,                 loss: nan
env1_first_0:                 episode reward: -50.5500,                 loss: nan
env1_second_0:                 episode reward: 50.5500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 508.6,                last time consumption/overall running time: 266.7740s / 154364.0332 s
env0_first_0:                 episode reward: -62.9500,                 loss: 0.4536
env0_second_0:                 episode reward: 62.9500,                 loss: nan
env1_first_0:                 episode reward: -60.0000,                 loss: nan
env1_second_0:                 episode reward: 60.0000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 376.0,                last time consumption/overall running time: 196.7679s / 154560.8011 s
env0_first_0:                 episode reward: -60.5000,                 loss: 0.4903
env0_second_0:                 episode reward: 60.5000,                 loss: nan
env1_first_0:                 episode reward: -69.9000,                 loss: nan
env1_second_0:                 episode reward: 69.9000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 383.0,                last time consumption/overall running time: 200.3229s / 154761.1240 s
env0_first_0:                 episode reward: -50.4000,                 loss: 0.4901
env0_second_0:                 episode reward: 50.4000,                 loss: nan
env1_first_0:                 episode reward: -65.0500,                 loss: nan
env1_second_0:                 episode reward: 65.0500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 500.7,                last time consumption/overall running time: 261.5152s / 155022.6392 s
env0_first_0:                 episode reward: -57.7500,                 loss: 0.4199
env0_second_0:                 episode reward: 57.7500,                 loss: nan
env1_first_0:                 episode reward: -58.5500,                 loss: nan
env1_second_0:                 episode reward: 58.5500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 370.8,                last time consumption/overall running time: 194.1172s / 155216.7564 s
env0_first_0:                 episode reward: -68.3500,                 loss: 0.4407
env0_second_0:                 episode reward: 68.3500,                 loss: nan
env1_first_0:                 episode reward: -58.9500,                 loss: nan
env1_second_0:                 episode reward: 58.9500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 509.75,                last time consumption/overall running time: 265.8358s / 155482.5923 s
env0_first_0:                 episode reward: -45.5500,                 loss: 0.4770
env0_second_0:                 episode reward: 45.5500,                 loss: nan
env1_first_0:                 episode reward: -52.6500,                 loss: nan
env1_second_0:                 episode reward: 52.6500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 548.4,                last time consumption/overall running time: 286.1663s / 155768.7586 s
env0_first_0:                 episode reward: -47.0500,                 loss: 0.5107
env0_second_0:                 episode reward: 47.0500,                 loss: nan
env1_first_0:                 episode reward: -63.0000,                 loss: nan
env1_second_0:                 episode reward: 63.0000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 346.6,                last time consumption/overall running time: 182.6743s / 155951.4329 s
env0_first_0:                 episode reward: -76.0000,                 loss: 0.4714
env0_second_0:                 episode reward: 76.0000,                 loss: nan
env1_first_0:                 episode reward: -44.1000,                 loss: nan
env1_second_0:                 episode reward: 44.1000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 389.65,                last time consumption/overall running time: 205.9313s / 156157.3642 s
env0_first_0:                 episode reward: -56.8000,                 loss: 0.5648
env0_second_0:                 episode reward: 56.8000,                 loss: nan
env1_first_0:                 episode reward: -64.6000,                 loss: nan
env1_second_0:                 episode reward: 64.6000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 372.85,                last time consumption/overall running time: 198.4817s / 156355.8459 s
env0_first_0:                 episode reward: -69.3500,                 loss: 0.5506
env0_second_0:                 episode reward: 69.3500,                 loss: nan
env1_first_0:                 episode reward: -57.8000,                 loss: nan
env1_second_0:                 episode reward: 57.8000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 369.55,                last time consumption/overall running time: 196.7448s / 156552.5907 s
env0_first_0:                 episode reward: -59.5000,                 loss: 0.5119
env0_second_0:                 episode reward: 59.5000,                 loss: nan
env1_first_0:                 episode reward: -68.4000,                 loss: nan
env1_second_0:                 episode reward: 68.4000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 380.6,                last time consumption/overall running time: 200.4130s / 156753.0037 s
env0_first_0:                 episode reward: -68.9500,                 loss: 0.5075
env0_second_0:                 episode reward: 68.9500,                 loss: nan
env1_first_0:                 episode reward: -55.8500,                 loss: nan
env1_second_0:                 episode reward: 55.8500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 369.4,                last time consumption/overall running time: 194.0987s / 156947.1024 s
env0_first_0:                 episode reward: -67.7500,                 loss: 0.5191
env0_second_0:                 episode reward: 67.7500,                 loss: nan
env1_first_0:                 episode reward: -54.8500,                 loss: nan
env1_second_0:                 episode reward: 54.8500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 655.75,                last time consumption/overall running time: 343.8914s / 157290.9938 s
env0_first_0:                 episode reward: -41.2000,                 loss: 0.5424
env0_second_0:                 episode reward: 41.2000,                 loss: nan
env1_first_0:                 episode reward: -37.0500,                 loss: nan
env1_second_0:                 episode reward: 37.0500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 501.05,                last time consumption/overall running time: 265.8510s / 157556.8448 s
env0_first_0:                 episode reward: -41.4500,                 loss: 0.5074
env0_second_0:                 episode reward: 41.4500,                 loss: nan
env1_first_0:                 episode reward: -60.2000,                 loss: nan
env1_second_0:                 episode reward: 60.2000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 430.8,                last time consumption/overall running time: 230.9236s / 157787.7684 s
env0_first_0:                 episode reward: -38.8500,                 loss: 0.4830
env0_second_0:                 episode reward: 38.8500,                 loss: nan
env1_first_0:                 episode reward: -51.7000,                 loss: nan
env1_second_0:                 episode reward: 51.7000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 471.8,                last time consumption/overall running time: 252.6629s / 158040.4314 s
env0_first_0:                 episode reward: -45.8000,                 loss: 0.4732
env0_second_0:                 episode reward: 45.8000,                 loss: nan
env1_first_0:                 episode reward: -45.9500,                 loss: nan
env1_second_0:                 episode reward: 45.9500,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 463.6,                last time consumption/overall running time: 245.4030s / 158285.8343 s
env0_first_0:                 episode reward: -34.8000,                 loss: 0.4433
env0_second_0:                 episode reward: 34.8000,                 loss: nan
env1_first_0:                 episode reward: -60.7500,                 loss: nan
env1_second_0:                 episode reward: 60.7500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 415.2,                last time consumption/overall running time: 219.4191s / 158505.2534 s
env0_first_0:                 episode reward: -51.3000,                 loss: 0.4492
env0_second_0:                 episode reward: 51.3000,                 loss: nan
env1_first_0:                 episode reward: -64.6500,                 loss: nan
env1_second_0:                 episode reward: 64.6500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 361.55,                last time consumption/overall running time: 190.8070s / 158696.0604 s
env0_first_0:                 episode reward: -73.3000,                 loss: 0.4757
env0_second_0:                 episode reward: 73.3000,                 loss: nan
env1_first_0:                 episode reward: -50.7000,                 loss: nan
env1_second_0:                 episode reward: 50.7000,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 443.25,                last time consumption/overall running time: 234.0462s / 158930.1066 s
env0_first_0:                 episode reward: -54.8500,                 loss: 0.5251
env0_second_0:                 episode reward: 54.8500,                 loss: nan
env1_first_0:                 episode reward: -53.4500,                 loss: nan
env1_second_0:                 episode reward: 53.4500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 488.1,                last time consumption/overall running time: 256.7800s / 159186.8867 s
env0_first_0:                 episode reward: -44.8500,                 loss: 0.5287
env0_second_0:                 episode reward: 44.8500,                 loss: nan
env1_first_0:                 episode reward: -37.6000,                 loss: nan
env1_second_0:                 episode reward: 37.6000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 488.75,                last time consumption/overall running time: 256.3911s / 159443.2778 s
env0_first_0:                 episode reward: -45.7000,                 loss: 0.5195
env0_second_0:                 episode reward: 45.7000,                 loss: nan
env1_first_0:                 episode reward: -58.7000,                 loss: nan
env1_second_0:                 episode reward: 58.7000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 399.7,                last time consumption/overall running time: 211.6491s / 159654.9269 s
env0_first_0:                 episode reward: -71.8000,                 loss: 0.4923
env0_second_0:                 episode reward: 71.8000,                 loss: nan
env1_first_0:                 episode reward: -59.8500,                 loss: nan
env1_second_0:                 episode reward: 59.8500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 387.75,                last time consumption/overall running time: 205.7615s / 159860.6885 s
env0_first_0:                 episode reward: -68.7500,                 loss: 0.4930
env0_second_0:                 episode reward: 68.7500,                 loss: nan
env1_first_0:                 episode reward: -53.2500,                 loss: nan
env1_second_0:                 episode reward: 53.2500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 512.85,                last time consumption/overall running time: 271.8928s / 160132.5813 s
env0_first_0:                 episode reward: -42.1500,                 loss: 0.4720
env0_second_0:                 episode reward: 42.1500,                 loss: nan
env1_first_0:                 episode reward: -44.4500,                 loss: nan
env1_second_0:                 episode reward: 44.4500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 448.9,                last time consumption/overall running time: 238.9927s / 160371.5740 s
env0_first_0:                 episode reward: -59.0000,                 loss: 0.4854
env0_second_0:                 episode reward: 59.0000,                 loss: nan
env1_first_0:                 episode reward: -48.6500,                 loss: nan
env1_second_0:                 episode reward: 48.6500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 518.0,                last time consumption/overall running time: 275.4293s / 160647.0033 s
env0_first_0:                 episode reward: -49.6000,                 loss: 0.4654
env0_second_0:                 episode reward: 49.6000,                 loss: nan
env1_first_0:                 episode reward: -55.0500,                 loss: nan
env1_second_0:                 episode reward: 55.0500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 496.4,                last time consumption/overall running time: 263.2850s / 160910.2884 s
env0_first_0:                 episode reward: -61.6500,                 loss: 0.4991
env0_second_0:                 episode reward: 61.6500,                 loss: nan
env1_first_0:                 episode reward: -39.8000,                 loss: nan
env1_second_0:                 episode reward: 39.8000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 613.9,                last time consumption/overall running time: 323.7656s / 161234.0539 s
env0_first_0:                 episode reward: -41.9500,                 loss: 0.5045
env0_second_0:                 episode reward: 41.9500,                 loss: nan
env1_first_0:                 episode reward: -34.2500,                 loss: nan
env1_second_0:                 episode reward: 34.2500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 661.45,                last time consumption/overall running time: 346.2434s / 161580.2973 s
env0_first_0:                 episode reward: -31.2000,                 loss: 0.4636
env0_second_0:                 episode reward: 31.2000,                 loss: nan
env1_first_0:                 episode reward: -33.0500,                 loss: nan
env1_second_0:                 episode reward: 33.0500,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 557.05,                last time consumption/overall running time: 289.4061s / 161869.7034 s
env0_first_0:                 episode reward: -46.1000,                 loss: 0.4574
env0_second_0:                 episode reward: 46.1000,                 loss: nan
env1_first_0:                 episode reward: -37.3000,                 loss: nan
env1_second_0:                 episode reward: 37.3000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 640.55,                last time consumption/overall running time: 329.3695s / 162199.0729 s
env0_first_0:                 episode reward: -33.8500,                 loss: 0.4425
env0_second_0:                 episode reward: 33.8500,                 loss: nan
env1_first_0:                 episode reward: -49.5500,                 loss: nan
env1_second_0:                 episode reward: 49.5500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 515.75,                last time consumption/overall running time: 265.8430s / 162464.9159 s
env0_first_0:                 episode reward: -41.2000,                 loss: 0.4306
env0_second_0:                 episode reward: 41.2000,                 loss: nan
env1_first_0:                 episode reward: -50.6000,                 loss: nan
env1_second_0:                 episode reward: 50.6000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 488.9,                last time consumption/overall running time: 250.7992s / 162715.7151 s
env0_first_0:                 episode reward: -39.5000,                 loss: 0.4298
env0_second_0:                 episode reward: 39.5000,                 loss: nan
env1_first_0:                 episode reward: -44.2000,                 loss: nan
env1_second_0:                 episode reward: 44.2000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 457.25,                last time consumption/overall running time: 235.2482s / 162950.9634 s
env0_first_0:                 episode reward: -54.1000,                 loss: 0.4736
env0_second_0:                 episode reward: 54.1000,                 loss: nan
env1_first_0:                 episode reward: -43.2000,                 loss: nan
env1_second_0:                 episode reward: 43.2000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 481.85,                last time consumption/overall running time: 251.2951s / 163202.2585 s
env0_first_0:                 episode reward: -40.9000,                 loss: 0.4834
env0_second_0:                 episode reward: 40.9000,                 loss: nan
env1_first_0:                 episode reward: -53.1500,                 loss: nan
env1_second_0:                 episode reward: 53.1500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 885.55,                last time consumption/overall running time: 463.6292s / 163665.8877 s
env0_first_0:                 episode reward: -14.1500,                 loss: 0.4766
env0_second_0:                 episode reward: 14.1500,                 loss: nan
env1_first_0:                 episode reward: -19.2500,                 loss: nan
env1_second_0:                 episode reward: 19.2500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 549.85,                last time consumption/overall running time: 290.7605s / 163956.6482 s
env0_first_0:                 episode reward: -36.9000,                 loss: 0.3581
env0_second_0:                 episode reward: 36.9000,                 loss: nan
env1_first_0:                 episode reward: -27.2000,                 loss: nan
env1_second_0:                 episode reward: 27.2000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 567.1,                last time consumption/overall running time: 300.2785s / 164256.9267 s
env0_first_0:                 episode reward: -34.5000,                 loss: 0.3438
env0_second_0:                 episode reward: 34.5000,                 loss: nan
env1_first_0:                 episode reward: -20.3000,                 loss: nan
env1_second_0:                 episode reward: 20.3000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 510.85,                last time consumption/overall running time: 269.7463s / 164526.6730 s
env0_first_0:                 episode reward: -46.0000,                 loss: 0.3252
env0_second_0:                 episode reward: 46.0000,                 loss: nan
env1_first_0:                 episode reward: -29.0500,                 loss: nan
env1_second_0:                 episode reward: 29.0500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 502.5,                last time consumption/overall running time: 267.7521s / 164794.4250 s
env0_first_0:                 episode reward: -33.4000,                 loss: 0.3302
env0_second_0:                 episode reward: 33.4000,                 loss: nan
env1_first_0:                 episode reward: -36.5500,                 loss: nan
env1_second_0:                 episode reward: 36.5500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 516.65,                last time consumption/overall running time: 274.1486s / 165068.5736 s
env0_first_0:                 episode reward: -26.2500,                 loss: 0.3856
env0_second_0:                 episode reward: 26.2500,                 loss: nan
env1_first_0:                 episode reward: -45.5000,                 loss: nan
env1_second_0:                 episode reward: 45.5000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 480.95,                last time consumption/overall running time: 255.4256s / 165323.9992 s
env0_first_0:                 episode reward: -47.4000,                 loss: 0.4387
env0_second_0:                 episode reward: 47.4000,                 loss: nan
env1_first_0:                 episode reward: -24.1000,                 loss: nan
env1_second_0:                 episode reward: 24.1000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 520.5,                last time consumption/overall running time: 276.1034s / 165600.1026 s
env0_first_0:                 episode reward: -35.5000,                 loss: 0.4173
env0_second_0:                 episode reward: 35.5000,                 loss: nan
env1_first_0:                 episode reward: -37.2500,                 loss: nan
env1_second_0:                 episode reward: 37.2500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 480.05,                last time consumption/overall running time: 255.0229s / 165855.1255 s
env0_first_0:                 episode reward: -42.8000,                 loss: 0.4112
env0_second_0:                 episode reward: 42.8000,                 loss: nan
env1_first_0:                 episode reward: -33.3500,                 loss: nan
env1_second_0:                 episode reward: 33.3500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 493.15,                last time consumption/overall running time: 263.0859s / 166118.2114 s
env0_first_0:                 episode reward: -44.6500,                 loss: 0.4060
env0_second_0:                 episode reward: 44.6500,                 loss: nan
env1_first_0:                 episode reward: -26.2500,                 loss: nan
env1_second_0:                 episode reward: 26.2500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 478.1,                last time consumption/overall running time: 255.7623s / 166373.9737 s
env0_first_0:                 episode reward: -34.7000,                 loss: 0.4089
env0_second_0:                 episode reward: 34.7000,                 loss: nan
env1_first_0:                 episode reward: -41.5000,                 loss: nan
env1_second_0:                 episode reward: 41.5000,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 632.65,                last time consumption/overall running time: 338.0258s / 166711.9995 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.4069
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -18.7500,                 loss: nan
env1_second_0:                 episode reward: 18.7500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 664.95,                last time consumption/overall running time: 351.9978s / 167063.9973 s
env0_first_0:                 episode reward: -31.7000,                 loss: 0.3826
env0_second_0:                 episode reward: 31.7000,                 loss: nan
env1_first_0:                 episode reward: -29.3000,                 loss: nan
env1_second_0:                 episode reward: 29.3000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 465.8,                last time consumption/overall running time: 248.7443s / 167312.7416 s
env0_first_0:                 episode reward: -34.6000,                 loss: 0.4029
env0_second_0:                 episode reward: 34.6000,                 loss: nan
env1_first_0:                 episode reward: -45.7500,                 loss: nan
env1_second_0:                 episode reward: 45.7500,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 394.45,                last time consumption/overall running time: 210.9116s / 167523.6532 s
env0_first_0:                 episode reward: -40.8000,                 loss: 0.4228
env0_second_0:                 episode reward: 40.8000,                 loss: nan
env1_first_0:                 episode reward: -46.4500,                 loss: nan
env1_second_0:                 episode reward: 46.4500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 512.3,                last time consumption/overall running time: 270.5184s / 167794.1716 s
env0_first_0:                 episode reward: -39.6500,                 loss: 0.4454
env0_second_0:                 episode reward: 39.6500,                 loss: nan
env1_first_0:                 episode reward: -42.6000,                 loss: nan
env1_second_0:                 episode reward: 42.6000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 449.35,                last time consumption/overall running time: 238.2453s / 168032.4169 s
env0_first_0:                 episode reward: -59.7000,                 loss: 0.4471
env0_second_0:                 episode reward: 59.7000,                 loss: nan
env1_first_0:                 episode reward: -36.3500,                 loss: nan
env1_second_0:                 episode reward: 36.3500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 521.35,                last time consumption/overall running time: 274.7231s / 168307.1401 s
env0_first_0:                 episode reward: -16.6000,                 loss: 0.4670
env0_second_0:                 episode reward: 16.6000,                 loss: nan
env1_first_0:                 episode reward: -37.6000,                 loss: nan
env1_second_0:                 episode reward: 37.6000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 464.8,                last time consumption/overall running time: 245.3768s / 168552.5169 s
env0_first_0:                 episode reward: -40.8000,                 loss: 0.4829
env0_second_0:                 episode reward: 40.8000,                 loss: nan
env1_first_0:                 episode reward: -43.1000,                 loss: nan
env1_second_0:                 episode reward: 43.1000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 447.85,                last time consumption/overall running time: 236.4152s / 168788.9320 s
env0_first_0:                 episode reward: -56.5500,                 loss: 0.4668
env0_second_0:                 episode reward: 56.5500,                 loss: nan
env1_first_0:                 episode reward: -44.0000,                 loss: nan
env1_second_0:                 episode reward: 44.0000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 498.0,                last time consumption/overall running time: 261.4466s / 169050.3787 s
env0_first_0:                 episode reward: -40.8500,                 loss: 0.4754
env0_second_0:                 episode reward: 40.8500,                 loss: nan
env1_first_0:                 episode reward: -43.0000,                 loss: nan
env1_second_0:                 episode reward: 43.0000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 513.5,                last time consumption/overall running time: 270.5876s / 169320.9663 s
env0_first_0:                 episode reward: -28.9500,                 loss: 0.4514
env0_second_0:                 episode reward: 28.9500,                 loss: nan
env1_first_0:                 episode reward: -43.8500,                 loss: nan
env1_second_0:                 episode reward: 43.8500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 827.9,                last time consumption/overall running time: 433.9331s / 169754.8994 s
env0_first_0:                 episode reward: -11.5000,                 loss: 0.4280
env0_second_0:                 episode reward: 11.5000,                 loss: nan
env1_first_0:                 episode reward: -20.3500,                 loss: nan
env1_second_0:                 episode reward: 20.3500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 542.85,                last time consumption/overall running time: 283.6681s / 170038.5674 s
env0_first_0:                 episode reward: -49.0500,                 loss: 0.3901
env0_second_0:                 episode reward: 49.0500,                 loss: nan
env1_first_0:                 episode reward: -47.9500,                 loss: nan
env1_second_0:                 episode reward: 47.9500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 609.9,                last time consumption/overall running time: 319.0874s / 170357.6548 s
env0_first_0:                 episode reward: -48.1500,                 loss: 0.3884
env0_second_0:                 episode reward: 48.1500,                 loss: nan
env1_first_0:                 episode reward: -34.1000,                 loss: nan
env1_second_0:                 episode reward: 34.1000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 446.9,                last time consumption/overall running time: 235.9797s / 170593.6345 s
env0_first_0:                 episode reward: -58.2500,                 loss: 0.3835
env0_second_0:                 episode reward: 58.2500,                 loss: nan
env1_first_0:                 episode reward: -43.0500,                 loss: nan
env1_second_0:                 episode reward: 43.0500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 612.1,                last time consumption/overall running time: 319.9518s / 170913.5863 s
env0_first_0:                 episode reward: -41.3000,                 loss: 0.4005
env0_second_0:                 episode reward: 41.3000,                 loss: nan
env1_first_0:                 episode reward: -37.9500,                 loss: nan
env1_second_0:                 episode reward: 37.9500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 592.05,                last time consumption/overall running time: 308.8994s / 171222.4857 s
env0_first_0:                 episode reward: -16.6500,                 loss: 0.4128
env0_second_0:                 episode reward: 16.6500,                 loss: nan
env1_first_0:                 episode reward: -15.2000,                 loss: nan
env1_second_0:                 episode reward: 15.2000,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 485.7,                last time consumption/overall running time: 254.5364s / 171477.0221 s
env0_first_0:                 episode reward: -51.3500,                 loss: 0.4012
env0_second_0:                 episode reward: 51.3500,                 loss: nan
env1_first_0:                 episode reward: -30.2000,                 loss: nan
env1_second_0:                 episode reward: 30.2000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 543.0,                last time consumption/overall running time: 285.3710s / 171762.3931 s
env0_first_0:                 episode reward: -26.3500,                 loss: 0.4394
env0_second_0:                 episode reward: 26.3500,                 loss: nan
env1_first_0:                 episode reward: -24.5500,                 loss: nan
env1_second_0:                 episode reward: 24.5500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 898.75,                last time consumption/overall running time: 470.2062s / 172232.5993 s
env0_first_0:                 episode reward: -43.8500,                 loss: 0.3666
env0_second_0:                 episode reward: 43.8500,                 loss: nan
env1_first_0:                 episode reward: -19.0000,                 loss: nan
env1_second_0:                 episode reward: 19.0000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 444.0,                last time consumption/overall running time: 234.2721s / 172466.8714 s
env0_first_0:                 episode reward: -34.1500,                 loss: 0.3475
env0_second_0:                 episode reward: 34.1500,                 loss: nan
env1_first_0:                 episode reward: -45.3500,                 loss: nan
env1_second_0:                 episode reward: 45.3500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 505.15,                last time consumption/overall running time: 265.7395s / 172732.6109 s
env0_first_0:                 episode reward: -36.9000,                 loss: 0.3345
env0_second_0:                 episode reward: 36.9000,                 loss: nan
env1_first_0:                 episode reward: -44.7000,                 loss: nan
env1_second_0:                 episode reward: 44.7000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 579.8,                last time consumption/overall running time: 304.1401s / 173036.7510 s
env0_first_0:                 episode reward: -11.5000,                 loss: 0.3011
env0_second_0:                 episode reward: 11.5000,                 loss: nan
env1_first_0:                 episode reward: -30.5500,                 loss: nan
env1_second_0:                 episode reward: 30.5500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 513.45,                last time consumption/overall running time: 270.8541s / 173307.6051 s
env0_first_0:                 episode reward: -28.3500,                 loss: 0.3035
env0_second_0:                 episode reward: 28.3500,                 loss: nan
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 556.8,                last time consumption/overall running time: 296.3399s / 173603.9450 s
env0_first_0:                 episode reward: -40.9000,                 loss: 0.3493
env0_second_0:                 episode reward: 40.9000,                 loss: nan
env1_first_0:                 episode reward: -30.9000,                 loss: nan
env1_second_0:                 episode reward: 30.9000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 359.9,                last time consumption/overall running time: 192.4052s / 173796.3503 s
env0_first_0:                 episode reward: -64.0000,                 loss: 0.3915
env0_second_0:                 episode reward: 64.0000,                 loss: nan
env1_first_0:                 episode reward: -33.0500,                 loss: nan
env1_second_0:                 episode reward: 33.0500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 517.7,                last time consumption/overall running time: 275.4035s / 174071.7537 s
env0_first_0:                 episode reward: -36.6000,                 loss: 0.4094
env0_second_0:                 episode reward: 36.6000,                 loss: nan
env1_first_0:                 episode reward: -15.7500,                 loss: nan
env1_second_0:                 episode reward: 15.7500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 624.0,                last time consumption/overall running time: 331.4988s / 174403.2525 s
env0_first_0:                 episode reward: -14.7500,                 loss: 0.4285
env0_second_0:                 episode reward: 14.7500,                 loss: nan
env1_first_0:                 episode reward: -41.3500,                 loss: nan
env1_second_0:                 episode reward: 41.3500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 490.25,                last time consumption/overall running time: 260.9148s / 174664.1673 s
env0_first_0:                 episode reward: -41.2000,                 loss: 0.4245
env0_second_0:                 episode reward: 41.2000,                 loss: nan
env1_first_0:                 episode reward: -44.4000,                 loss: nan
env1_second_0:                 episode reward: 44.4000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 557.15,                last time consumption/overall running time: 296.9617s / 174961.1290 s
env0_first_0:                 episode reward: -48.9000,                 loss: 0.4924
env0_second_0:                 episode reward: 48.9000,                 loss: nan
env1_first_0:                 episode reward: -31.4500,                 loss: nan
env1_second_0:                 episode reward: 31.4500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 756.75,                last time consumption/overall running time: 401.9620s / 175363.0910 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.4498
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 699.45,                last time consumption/overall running time: 371.4689s / 175734.5600 s
env0_first_0:                 episode reward: -30.2500,                 loss: 0.4168
env0_second_0:                 episode reward: 30.2500,                 loss: nan
env1_first_0:                 episode reward: -24.5500,                 loss: nan
env1_second_0:                 episode reward: 24.5500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 395.35,                last time consumption/overall running time: 211.1302s / 175945.6901 s
env0_first_0:                 episode reward: -47.6000,                 loss: 0.3931
env0_second_0:                 episode reward: 47.6000,                 loss: nan
env1_first_0:                 episode reward: -37.9000,                 loss: nan
env1_second_0:                 episode reward: 37.9000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 581.1,                last time consumption/overall running time: 309.5515s / 176255.2416 s
env0_first_0:                 episode reward: -35.7000,                 loss: 0.3423
env0_second_0:                 episode reward: 35.7000,                 loss: nan
env1_first_0:                 episode reward: -36.6000,                 loss: nan
env1_second_0:                 episode reward: 36.6000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 503.0,                last time consumption/overall running time: 267.5284s / 176522.7700 s
env0_first_0:                 episode reward: -45.2000,                 loss: 0.3621
env0_second_0:                 episode reward: 45.2000,                 loss: nan
env1_first_0:                 episode reward: -42.1000,                 loss: nan
env1_second_0:                 episode reward: 42.1000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 423.35,                last time consumption/overall running time: 224.4583s / 176747.2282 s
env0_first_0:                 episode reward: -42.1500,                 loss: 0.4728
env0_second_0:                 episode reward: 42.1500,                 loss: nan
env1_first_0:                 episode reward: -53.7500,                 loss: nan
env1_second_0:                 episode reward: 53.7500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 415.5,                last time consumption/overall running time: 220.3281s / 176967.5563 s
env0_first_0:                 episode reward: -31.3000,                 loss: 0.4894
env0_second_0:                 episode reward: 31.3000,                 loss: nan
env1_first_0:                 episode reward: -32.3000,                 loss: nan
env1_second_0:                 episode reward: 32.3000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 529.4,                last time consumption/overall running time: 278.6881s / 177246.2444 s
env0_first_0:                 episode reward: -48.5000,                 loss: 0.5003
env0_second_0:                 episode reward: 48.5000,                 loss: nan
env1_first_0:                 episode reward: -46.6000,                 loss: nan
env1_second_0:                 episode reward: 46.6000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 452.35,                last time consumption/overall running time: 237.2032s / 177483.4476 s
env0_first_0:                 episode reward: -42.1500,                 loss: 0.5195
env0_second_0:                 episode reward: 42.1500,                 loss: nan
env1_first_0:                 episode reward: -37.6000,                 loss: nan
env1_second_0:                 episode reward: 37.6000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 596.15,                last time consumption/overall running time: 311.7959s / 177795.2435 s
env0_first_0:                 episode reward: -28.6500,                 loss: 0.4951
env0_second_0:                 episode reward: 28.6500,                 loss: nan
env1_first_0:                 episode reward: -40.0500,                 loss: nan
env1_second_0:                 episode reward: 40.0500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 533.7,                last time consumption/overall running time: 278.3056s / 178073.5491 s
env0_first_0:                 episode reward: -40.7500,                 loss: 0.4511
env0_second_0:                 episode reward: 40.7500,                 loss: nan
env1_first_0:                 episode reward: -30.7500,                 loss: nan
env1_second_0:                 episode reward: 30.7500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 549.3,                last time consumption/overall running time: 285.7942s / 178359.3433 s
env0_first_0:                 episode reward: -49.9000,                 loss: 0.4741
env0_second_0:                 episode reward: 49.9000,                 loss: nan
env1_first_0:                 episode reward: -21.3500,                 loss: nan
env1_second_0:                 episode reward: 21.3500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 534.0,                last time consumption/overall running time: 277.7492s / 178637.0924 s
env0_first_0:                 episode reward: -28.4000,                 loss: 0.4980
env0_second_0:                 episode reward: 28.4000,                 loss: nan
env1_first_0:                 episode reward: -29.0500,                 loss: nan
env1_second_0:                 episode reward: 29.0500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 606.15,                last time consumption/overall running time: 315.4753s / 178952.5678 s
env0_first_0:                 episode reward: -27.2500,                 loss: 0.5028
env0_second_0:                 episode reward: 27.2500,                 loss: nan
env1_first_0:                 episode reward: -24.6500,                 loss: nan
env1_second_0:                 episode reward: 24.6500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 627.95,                last time consumption/overall running time: 327.1067s / 179279.6744 s
env0_first_0:                 episode reward: -28.2500,                 loss: 0.4485
env0_second_0:                 episode reward: 28.2500,                 loss: nan
env1_first_0:                 episode reward: -29.5000,                 loss: nan
env1_second_0:                 episode reward: 29.5000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 449.3,                last time consumption/overall running time: 236.2502s / 179515.9247 s
env0_first_0:                 episode reward: -39.8000,                 loss: 0.4333
env0_second_0:                 episode reward: 39.8000,                 loss: nan
env1_first_0:                 episode reward: -54.2000,                 loss: nan
env1_second_0:                 episode reward: 54.2000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 465.8,                last time consumption/overall running time: 245.7636s / 179761.6883 s
env0_first_0:                 episode reward: -38.6000,                 loss: 0.4462
env0_second_0:                 episode reward: 38.6000,                 loss: nan
env1_first_0:                 episode reward: -49.3000,                 loss: nan
env1_second_0:                 episode reward: 49.3000,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 556.3,                last time consumption/overall running time: 293.7900s / 180055.4783 s
env0_first_0:                 episode reward: -37.9500,                 loss: 0.4749
env0_second_0:                 episode reward: 37.9500,                 loss: nan
env1_first_0:                 episode reward: -40.4000,                 loss: nan
env1_second_0:                 episode reward: 40.4000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 422.7,                last time consumption/overall running time: 223.1778s / 180278.6561 s
env0_first_0:                 episode reward: -45.8000,                 loss: 0.5111
env0_second_0:                 episode reward: 45.8000,                 loss: nan
env1_first_0:                 episode reward: -46.9000,                 loss: nan
env1_second_0:                 episode reward: 46.9000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 384.15,                last time consumption/overall running time: 203.0569s / 180481.7130 s
env0_first_0:                 episode reward: -48.0000,                 loss: 0.5267
env0_second_0:                 episode reward: 48.0000,                 loss: nan
env1_first_0:                 episode reward: -50.9500,                 loss: nan
env1_second_0:                 episode reward: 50.9500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 369.95,                last time consumption/overall running time: 196.3739s / 180678.0868 s
env0_first_0:                 episode reward: -65.7500,                 loss: 0.6070
env0_second_0:                 episode reward: 65.7500,                 loss: nan
env1_first_0:                 episode reward: -70.5500,                 loss: nan
env1_second_0:                 episode reward: 70.5500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 502.5,                last time consumption/overall running time: 264.8872s / 180942.9741 s
env0_first_0:                 episode reward: -52.6000,                 loss: 0.6065
env0_second_0:                 episode reward: 52.6000,                 loss: nan
env1_first_0:                 episode reward: -48.9000,                 loss: nan
env1_second_0:                 episode reward: 48.9000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 562.3,                last time consumption/overall running time: 295.2961s / 181238.2702 s
env0_first_0:                 episode reward: -63.9000,                 loss: 0.5273
env0_second_0:                 episode reward: 63.9000,                 loss: nan
env1_first_0:                 episode reward: -37.5000,                 loss: nan
env1_second_0:                 episode reward: 37.5000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 428.5,                last time consumption/overall running time: 225.6852s / 181463.9554 s
env0_first_0:                 episode reward: -20.6500,                 loss: 0.5202
env0_second_0:                 episode reward: 20.6500,                 loss: nan
env1_first_0:                 episode reward: -56.8000,                 loss: nan
env1_second_0:                 episode reward: 56.8000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 446.85,                last time consumption/overall running time: 235.1512s / 181699.1066 s
env0_first_0:                 episode reward: -45.1500,                 loss: 0.5517
env0_second_0:                 episode reward: 45.1500,                 loss: nan
env1_first_0:                 episode reward: -35.8000,                 loss: nan
env1_second_0:                 episode reward: 35.8000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 595.75,                last time consumption/overall running time: 313.1157s / 182012.2223 s
env0_first_0:                 episode reward: -28.8000,                 loss: 0.5125
env0_second_0:                 episode reward: 28.8000,                 loss: nan
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 433.5,                last time consumption/overall running time: 229.4492s / 182241.6715 s
env0_first_0:                 episode reward: -18.0500,                 loss: 0.5009
env0_second_0:                 episode reward: 18.0500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 525.4,                last time consumption/overall running time: 277.7190s / 182519.3905 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.5451
env0_second_0:                 episode reward: -2.0500,                 loss: nan
env1_first_0:                 episode reward: -11.8000,                 loss: nan
env1_second_0:                 episode reward: 11.8000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 507.45,                last time consumption/overall running time: 889.1538s / 183408.5443 s
env0_first_0:                 episode reward: 25.8500,                 loss: 0.4920
env0_second_0:                 episode reward: -25.8500,                 loss: nan
env1_first_0:                 episode reward: 17.0000,                 loss: nan
env1_second_0:                 episode reward: -17.0000,                 loss: nan
Score delta: 88.2, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/8541_0.
Episode: 8561/10000 (85.6100%),                 avg. length: 454.35,                last time consumption/overall running time: 242.6299s / 183651.1743 s
env0_first_0:                 episode reward: -60.1000,                 loss: nan
env0_second_0:                 episode reward: 60.1000,                 loss: 0.4732
env1_first_0:                 episode reward: -61.9500,                 loss: nan
env1_second_0:                 episode reward: 61.9500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 226.75,                last time consumption/overall running time: 1490.3846s / 185141.5589 s
env0_first_0:                 episode reward: -97.5500,                 loss: 0.1039
env0_second_0:                 episode reward: 97.5500,                 loss: 0.4285
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Score delta: 85.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/8562_1.
Episode: 8601/10000 (86.0100%),                 avg. length: 261.35,                last time consumption/overall running time: 138.5836s / 185280.1425 s
env0_first_0:                 episode reward: -95.8000,                 loss: 0.1283
env0_second_0:                 episode reward: 95.8000,                 loss: nan
env1_first_0:                 episode reward: -91.9000,                 loss: nan
env1_second_0:                 episode reward: 91.9000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 459.2,                last time consumption/overall running time: 241.8053s / 185521.9478 s
env0_first_0:                 episode reward: -80.2500,                 loss: 0.1773
env0_second_0:                 episode reward: 80.2500,                 loss: nan
env1_first_0:                 episode reward: -80.0000,                 loss: nan
env1_second_0:                 episode reward: 80.0000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1169.65,                last time consumption/overall running time: 599.6535s / 186121.6013 s
env0_first_0:                 episode reward: -35.4500,                 loss: 0.3837
env0_second_0:                 episode reward: 35.4500,                 loss: nan
env1_first_0:                 episode reward: -38.8000,                 loss: nan
env1_second_0:                 episode reward: 38.8000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1321.0,                last time consumption/overall running time: 654.0193s / 186775.6206 s
env0_first_0:                 episode reward: -29.1000,                 loss: 0.1637
env0_second_0:                 episode reward: 29.1000,                 loss: nan
env1_first_0:                 episode reward: -29.6500,                 loss: nan
env1_second_0:                 episode reward: 29.6500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 1706.7,                last time consumption/overall running time: 851.3519s / 187626.9724 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0325
env0_second_0:                 episode reward: 5.2000,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1318.55,                last time consumption/overall running time: 665.7072s / 188292.6797 s
env0_first_0:                 episode reward: -30.0500,                 loss: 0.0259
env0_second_0:                 episode reward: 30.0500,                 loss: nan
env1_first_0:                 episode reward: -26.8500,                 loss: nan
env1_second_0:                 episode reward: 26.8500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1549.8,                last time consumption/overall running time: 783.2604s / 189075.9401 s
env0_first_0:                 episode reward: -10.2000,                 loss: 0.0235
env0_second_0:                 episode reward: 10.2000,                 loss: nan
env1_first_0:                 episode reward: -14.7000,                 loss: nan
env1_second_0:                 episode reward: 14.7000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1326.05,                last time consumption/overall running time: 670.6974s / 189746.6375 s
env0_first_0:                 episode reward: -21.5000,                 loss: 0.0177
env0_second_0:                 episode reward: 21.5000,                 loss: nan
env1_first_0:                 episode reward: -30.5500,                 loss: nan
env1_second_0:                 episode reward: 30.5500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1245.8,                last time consumption/overall running time: 629.6177s / 190376.2551 s
env0_first_0:                 episode reward: -35.2000,                 loss: 0.0347
env0_second_0:                 episode reward: 35.2000,                 loss: nan
env1_first_0:                 episode reward: -27.4500,                 loss: nan
env1_second_0:                 episode reward: 27.4500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1317.2,                last time consumption/overall running time: 660.7326s / 191036.9877 s
env0_first_0:                 episode reward: -24.8000,                 loss: 0.0275
env0_second_0:                 episode reward: 24.8000,                 loss: nan
env1_first_0:                 episode reward: -29.9500,                 loss: nan
env1_second_0:                 episode reward: 29.9500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1105.6,                last time consumption/overall running time: 554.5822s / 191591.5699 s
env0_first_0:                 episode reward: -36.0000,                 loss: 0.0323
env0_second_0:                 episode reward: 36.0000,                 loss: nan
env1_first_0:                 episode reward: -38.1000,                 loss: nan
env1_second_0:                 episode reward: 38.1000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1160.2,                last time consumption/overall running time: 577.7112s / 192169.2810 s
env0_first_0:                 episode reward: -40.0500,                 loss: 0.0459
env0_second_0:                 episode reward: 40.0500,                 loss: nan
env1_first_0:                 episode reward: -39.3000,                 loss: nan
env1_second_0:                 episode reward: 39.3000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1162.6,                last time consumption/overall running time: 573.7551s / 192743.0361 s
env0_first_0:                 episode reward: -37.5000,                 loss: 0.0339
env0_second_0:                 episode reward: 37.5000,                 loss: nan
env1_first_0:                 episode reward: -34.3000,                 loss: nan
env1_second_0:                 episode reward: 34.3000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 1472.0,                last time consumption/overall running time: 724.5869s / 193467.6230 s
env0_first_0:                 episode reward: -19.7500,                 loss: 0.0268
env0_second_0:                 episode reward: 19.7500,                 loss: nan
env1_first_0:                 episode reward: -20.8500,                 loss: nan
env1_second_0:                 episode reward: 20.8500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1470.6,                last time consumption/overall running time: 725.8441s / 194193.4671 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0269
env0_second_0:                 episode reward: 14.1000,                 loss: nan
env1_first_0:                 episode reward: -20.2500,                 loss: nan
env1_second_0:                 episode reward: 20.2500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 1252.8,                last time consumption/overall running time: 619.2669s / 194812.7340 s
env0_first_0:                 episode reward: -30.2500,                 loss: 0.0207
env0_second_0:                 episode reward: 30.2500,                 loss: nan
env1_first_0:                 episode reward: -24.1000,                 loss: nan
env1_second_0:                 episode reward: 24.1000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1646.65,                last time consumption/overall running time: 817.8981s / 195630.6322 s
env0_first_0:                 episode reward: 24.7000,                 loss: 0.0216
env0_second_0:                 episode reward: -24.7000,                 loss: nan
env1_first_0:                 episode reward: 29.7500,                 loss: nan
env1_second_0:                 episode reward: -29.7500,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 597.95,                last time consumption/overall running time: 1086.8417s / 196717.4739 s
env0_first_0:                 episode reward: -53.4500,                 loss: 0.0319
env0_second_0:                 episode reward: 53.4500,                 loss: 0.3696
env1_first_0:                 episode reward: -46.0500,                 loss: nan
env1_second_0:                 episode reward: 46.0500,                 loss: nan
Score delta: 87.8, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/8922_0.
Episode: 8961/10000 (89.6100%),                 avg. length: 600.25,                last time consumption/overall running time: 1642.0948s / 198359.5687 s
env0_first_0:                 episode reward: -70.3500,                 loss: 0.0666
env0_second_0:                 episode reward: 70.3500,                 loss: 0.3537
env1_first_0:                 episode reward: -69.4000,                 loss: nan
env1_second_0:                 episode reward: 69.4000,                 loss: nan
Score delta: 97.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/8943_1.
Episode: 8981/10000 (89.8100%),                 avg. length: 1353.15,                last time consumption/overall running time: 1862.7445s / 200222.3132 s
env0_first_0:                 episode reward: 15.4000,                 loss: 0.0787
env0_second_0:                 episode reward: -15.4000,                 loss: 0.3524
env1_first_0:                 episode reward: 16.6500,                 loss: nan
env1_second_0:                 episode reward: -16.6500,                 loss: nan
Score delta: 86.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/8976_0.
Episode: 9001/10000 (90.0100%),                 avg. length: 570.3,                last time consumption/overall running time: 1647.6102s / 201869.9233 s
env0_first_0:                 episode reward: -60.8500,                 loss: 0.0750
env0_second_0:                 episode reward: 60.8500,                 loss: 0.3460
env1_first_0:                 episode reward: -59.5000,                 loss: nan
env1_second_0:                 episode reward: 59.5000,                 loss: nan
Score delta: 87.0, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/8998_1.
Episode: 9021/10000 (90.2100%),                 avg. length: 302.3,                last time consumption/overall running time: 150.7696s / 202020.6929 s
env0_first_0:                 episode reward: -92.7000,                 loss: 0.0504
env0_second_0:                 episode reward: 92.7000,                 loss: nan
env1_first_0:                 episode reward: -91.9000,                 loss: nan
env1_second_0:                 episode reward: 91.9000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 530.3,                last time consumption/overall running time: 262.3233s / 202283.0162 s
env0_first_0:                 episode reward: -79.5500,                 loss: 0.0517
env0_second_0:                 episode reward: 79.5500,                 loss: nan
env1_first_0:                 episode reward: -79.7000,                 loss: nan
env1_second_0:                 episode reward: 79.7000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 566.35,                last time consumption/overall running time: 278.9371s / 202561.9534 s
env0_first_0:                 episode reward: -29.0000,                 loss: 0.0728
env0_second_0:                 episode reward: 29.0000,                 loss: nan
env1_first_0:                 episode reward: -50.8500,                 loss: nan
env1_second_0:                 episode reward: 50.8500,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 442.65,                last time consumption/overall running time: 1511.9643s / 204073.9177 s
env0_first_0:                 episode reward: 18.3500,                 loss: 0.0937
env0_second_0:                 episode reward: -18.3500,                 loss: 0.4196
env1_first_0:                 episode reward: 47.0500,                 loss: nan
env1_second_0:                 episode reward: -47.0500,                 loss: nan
Score delta: 101.4, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/9073_0.
Episode: 9101/10000 (91.0100%),                 avg. length: 500.5,                last time consumption/overall running time: 248.6470s / 204322.5647 s
env0_first_0:                 episode reward: -6.1500,                 loss: nan
env0_second_0:                 episode reward: 6.1500,                 loss: 0.4229
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 261.55,                last time consumption/overall running time: 1547.4281s / 205869.9928 s
env0_first_0:                 episode reward: -73.8000,                 loss: 0.6687
env0_second_0:                 episode reward: 73.8000,                 loss: 0.4794
env1_first_0:                 episode reward: -79.8500,                 loss: nan
env1_second_0:                 episode reward: 79.8500,                 loss: nan
Score delta: 111.6, save the model to .//data/model/20220119_0521/pettingzoo_boxing_v1_nxdo2/9108_1.
Episode: 9141/10000 (91.4100%),                 avg. length: 319.2,                last time consumption/overall running time: 160.2369s / 206030.2297 s
env0_first_0:                 episode reward: -86.6000,                 loss: 0.6845
env0_second_0:                 episode reward: 86.6000,                 loss: nan
env1_first_0:                 episode reward: -88.3500,                 loss: nan
env1_second_0:                 episode reward: 88.3500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 229.7,                last time consumption/overall running time: 115.8782s / 206146.1078 s
env0_first_0:                 episode reward: -98.1500,                 loss: 0.8756
env0_second_0:                 episode reward: 98.1500,                 loss: nan
env1_first_0:                 episode reward: -87.6500,                 loss: nan
env1_second_0:                 episode reward: 87.6500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 282.45,                last time consumption/overall running time: 142.5378s / 206288.6456 s
env0_first_0:                 episode reward: -90.0000,                 loss: 0.9363
env0_second_0:                 episode reward: 90.0000,                 loss: nan
env1_first_0:                 episode reward: -92.8000,                 loss: nan
env1_second_0:                 episode reward: 92.8000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 641.75,                last time consumption/overall running time: 321.9721s / 206610.6177 s
env0_first_0:                 episode reward: -61.5500,                 loss: 1.9737
env0_second_0:                 episode reward: 61.5500,                 loss: nan
env1_first_0:                 episode reward: -72.4000,                 loss: nan
env1_second_0:                 episode reward: 72.4000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 334.0,                last time consumption/overall running time: 168.7040s / 206779.3217 s
env0_first_0:                 episode reward: -90.5000,                 loss: 4.7187
env0_second_0:                 episode reward: 90.5000,                 loss: nan
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 278.6,                last time consumption/overall running time: 141.4359s / 206920.7577 s
env0_first_0:                 episode reward: -94.1500,                 loss: 4.6460
env0_second_0:                 episode reward: 94.1500,                 loss: nan
env1_first_0:                 episode reward: -82.4500,                 loss: nan
env1_second_0:                 episode reward: 82.4500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 243.15,                last time consumption/overall running time: 123.7538s / 207044.5115 s
env0_first_0:                 episode reward: -94.9500,                 loss: 3.1063
env0_second_0:                 episode reward: 94.9500,                 loss: nan
env1_first_0:                 episode reward: -91.4000,                 loss: nan
env1_second_0:                 episode reward: 91.4000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 252.15,                last time consumption/overall running time: 128.1522s / 207172.6637 s
env0_first_0:                 episode reward: -89.4000,                 loss: 0.7525
env0_second_0:                 episode reward: 89.4000,                 loss: nan
env1_first_0:                 episode reward: -88.4500,                 loss: nan
env1_second_0:                 episode reward: 88.4500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 292.9,                last time consumption/overall running time: 148.2203s / 207320.8839 s
env0_first_0:                 episode reward: -87.9000,                 loss: 0.6749
env0_second_0:                 episode reward: 87.9000,                 loss: nan
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 375.95,                last time consumption/overall running time: 189.2207s / 207510.1046 s
env0_first_0:                 episode reward: -86.8500,                 loss: 0.6302
env0_second_0:                 episode reward: 86.8500,                 loss: nan
env1_first_0:                 episode reward: -75.7500,                 loss: nan
env1_second_0:                 episode reward: 75.7500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 518.2,                last time consumption/overall running time: 259.7463s / 207769.8509 s
env0_first_0:                 episode reward: -60.9000,                 loss: 0.6408
env0_second_0:                 episode reward: 60.9000,                 loss: nan
env1_first_0:                 episode reward: -68.7500,                 loss: nan
env1_second_0:                 episode reward: 68.7500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 410.6,                last time consumption/overall running time: 206.8299s / 207976.6808 s
env0_first_0:                 episode reward: -75.2000,                 loss: 0.6004
env0_second_0:                 episode reward: 75.2000,                 loss: nan
env1_first_0:                 episode reward: -75.0500,                 loss: nan
env1_second_0:                 episode reward: 75.0500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 748.7,                last time consumption/overall running time: 374.6439s / 208351.3247 s
env0_first_0:                 episode reward: -62.6000,                 loss: 0.5429
env0_second_0:                 episode reward: 62.6000,                 loss: nan
env1_first_0:                 episode reward: -48.7000,                 loss: nan
env1_second_0:                 episode reward: 48.7000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 616.45,                last time consumption/overall running time: 307.0222s / 208658.3469 s
env0_first_0:                 episode reward: -49.9000,                 loss: 0.4020
env0_second_0:                 episode reward: 49.9000,                 loss: nan
env1_first_0:                 episode reward: -68.5500,                 loss: nan
env1_second_0:                 episode reward: 68.5500,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1182.4,                last time consumption/overall running time: 585.3448s / 209243.6917 s
env0_first_0:                 episode reward: -20.4000,                 loss: 0.3015
env0_second_0:                 episode reward: 20.4000,                 loss: nan
env1_first_0:                 episode reward: -23.5000,                 loss: nan
env1_second_0:                 episode reward: 23.5000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 946.95,                last time consumption/overall running time: 468.9717s / 209712.6635 s
env0_first_0:                 episode reward: -37.0500,                 loss: 0.2316
env0_second_0:                 episode reward: 37.0500,                 loss: nan
env1_first_0:                 episode reward: -39.5000,                 loss: nan
env1_second_0:                 episode reward: 39.5000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 615.05,                last time consumption/overall running time: 306.4407s / 210019.1042 s
env0_first_0:                 episode reward: -60.6500,                 loss: 0.1960
env0_second_0:                 episode reward: 60.6500,                 loss: nan
env1_first_0:                 episode reward: -57.9500,                 loss: nan
env1_second_0:                 episode reward: 57.9500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 477.95,                last time consumption/overall running time: 237.5553s / 210256.6595 s
env0_first_0:                 episode reward: -69.2000,                 loss: 0.2310
env0_second_0:                 episode reward: 69.2000,                 loss: nan
env1_first_0:                 episode reward: -76.5500,                 loss: nan
env1_second_0:                 episode reward: 76.5500,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 598.25,                last time consumption/overall running time: 295.3956s / 210552.0550 s
env0_first_0:                 episode reward: -42.1500,                 loss: 0.2572
env0_second_0:                 episode reward: 42.1500,                 loss: nan
env1_first_0:                 episode reward: -60.4500,                 loss: nan
env1_second_0:                 episode reward: 60.4500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 531.4,                last time consumption/overall running time: 262.7314s / 210814.7865 s
env0_first_0:                 episode reward: -54.1500,                 loss: 0.2869
env0_second_0:                 episode reward: 54.1500,                 loss: nan
env1_first_0:                 episode reward: -67.0500,                 loss: nan
env1_second_0:                 episode reward: 67.0500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 656.8,                last time consumption/overall running time: 325.2391s / 211140.0256 s
env0_first_0:                 episode reward: -47.3500,                 loss: 0.3184
env0_second_0:                 episode reward: 47.3500,                 loss: nan
env1_first_0:                 episode reward: -51.2500,                 loss: nan
env1_second_0:                 episode reward: 51.2500,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 694.2,                last time consumption/overall running time: 347.3979s / 211487.4235 s
env0_first_0:                 episode reward: -50.4000,                 loss: 0.2935
env0_second_0:                 episode reward: 50.4000,                 loss: nan
env1_first_0:                 episode reward: -67.4000,                 loss: nan
env1_second_0:                 episode reward: 67.4000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 774.0,                last time consumption/overall running time: 384.8360s / 211872.2595 s
env0_first_0:                 episode reward: -46.1000,                 loss: 0.2890
env0_second_0:                 episode reward: 46.1000,                 loss: nan
env1_first_0:                 episode reward: -35.5000,                 loss: nan
env1_second_0:                 episode reward: 35.5000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 651.35,                last time consumption/overall running time: 324.4343s / 212196.6938 s
env0_first_0:                 episode reward: -60.6500,                 loss: 0.2647
env0_second_0:                 episode reward: 60.6500,                 loss: nan
env1_first_0:                 episode reward: -47.3000,                 loss: nan
env1_second_0:                 episode reward: 47.3000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 567.05,                last time consumption/overall running time: 283.0979s / 212479.7917 s
env0_first_0:                 episode reward: -53.1000,                 loss: 0.2426
env0_second_0:                 episode reward: 53.1000,                 loss: nan
env1_first_0:                 episode reward: -45.4000,                 loss: nan
env1_second_0:                 episode reward: 45.4000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 411.3,                last time consumption/overall running time: 205.6919s / 212685.4836 s
env0_first_0:                 episode reward: -71.9000,                 loss: 0.2525
env0_second_0:                 episode reward: 71.9000,                 loss: nan
env1_first_0:                 episode reward: -67.2000,                 loss: nan
env1_second_0:                 episode reward: 67.2000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 428.25,                last time consumption/overall running time: 214.4604s / 212899.9440 s
env0_first_0:                 episode reward: -75.7500,                 loss: 0.2767
env0_second_0:                 episode reward: 75.7500,                 loss: nan
env1_first_0:                 episode reward: -60.7500,                 loss: nan
env1_second_0:                 episode reward: 60.7500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 425.3,                last time consumption/overall running time: 213.9446s / 213113.8886 s
env0_first_0:                 episode reward: -68.6000,                 loss: 0.2917
env0_second_0:                 episode reward: 68.6000,                 loss: nan
env1_first_0:                 episode reward: -61.6500,                 loss: nan
env1_second_0:                 episode reward: 61.6500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 403.1,                last time consumption/overall running time: 202.9932s / 213316.8818 s
env0_first_0:                 episode reward: -68.7500,                 loss: 0.3370
env0_second_0:                 episode reward: 68.7500,                 loss: nan
env1_first_0:                 episode reward: -63.0000,                 loss: nan
env1_second_0:                 episode reward: 63.0000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 525.15,                last time consumption/overall running time: 265.4191s / 213582.3010 s
env0_first_0:                 episode reward: -35.6000,                 loss: 0.3662
env0_second_0:                 episode reward: 35.6000,                 loss: nan
env1_first_0:                 episode reward: -68.8500,                 loss: nan
env1_second_0:                 episode reward: 68.8500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 563.2,                last time consumption/overall running time: 284.6590s / 213866.9599 s
env0_first_0:                 episode reward: -53.2000,                 loss: 0.3666
env0_second_0:                 episode reward: 53.2000,                 loss: nan
env1_first_0:                 episode reward: -44.6500,                 loss: nan
env1_second_0:                 episode reward: 44.6500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 687.15,                last time consumption/overall running time: 346.2073s / 214213.1672 s
env0_first_0:                 episode reward: -57.2000,                 loss: 0.3207
env0_second_0:                 episode reward: 57.2000,                 loss: nan
env1_first_0:                 episode reward: -37.1500,                 loss: nan
env1_second_0:                 episode reward: 37.1500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 490.0,                last time consumption/overall running time: 245.8517s / 214459.0188 s
env0_first_0:                 episode reward: -67.0500,                 loss: 0.2984
env0_second_0:                 episode reward: 67.0500,                 loss: nanLoad boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env1_first_0:                 episode reward: -39.3500,                 loss: nan
env1_second_0:                 episode reward: 39.3500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 433.6,                last time consumption/overall running time: 217.7539s / 214676.7727 s
env0_first_0:                 episode reward: -58.3000,                 loss: 0.2856
env0_second_0:                 episode reward: 58.3000,                 loss: nan
env1_first_0:                 episode reward: -45.8500,                 loss: nan
env1_second_0:                 episode reward: 45.8500,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 507.05,                last time consumption/overall running time: 254.2013s / 214930.9740 s
env0_first_0:                 episode reward: -48.0000,                 loss: 0.2865
env0_second_0:                 episode reward: 48.0000,                 loss: nan
env1_first_0:                 episode reward: -52.3000,                 loss: nan
env1_second_0:                 episode reward: 52.3000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 436.4,                last time consumption/overall running time: 220.0326s / 215151.0066 s
env0_first_0:                 episode reward: -38.5500,                 loss: 0.2992
env0_second_0:                 episode reward: 38.5500,                 loss: nan
env1_first_0:                 episode reward: -46.6000,                 loss: nan
env1_second_0:                 episode reward: 46.6000,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 494.9,                last time consumption/overall running time: 249.6147s / 215400.6213 s
env0_first_0:                 episode reward: -52.2000,                 loss: 0.3435
env0_second_0:                 episode reward: 52.2000,                 loss: nan
env1_first_0:                 episode reward: -41.0000,                 loss: nan
env1_second_0:                 episode reward: 41.0000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 581.95,                last time consumption/overall running time: 291.6345s / 215692.2558 s
env0_first_0:                 episode reward: -33.1000,                 loss: 0.3693
env0_second_0:                 episode reward: 33.1000,                 loss: nan
env1_first_0:                 episode reward: -23.0500,                 loss: nan
env1_second_0:                 episode reward: 23.0500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 531.2,                last time consumption/overall running time: 266.5322s / 215958.7880 s
env0_first_0:                 episode reward: -39.4000,                 loss: 0.4025
env0_second_0:                 episode reward: 39.4000,                 loss: nan
env1_first_0:                 episode reward: -38.4000,                 loss: nan
env1_second_0:                 episode reward: 38.4000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 497.15,                last time consumption/overall running time: 248.0474s / 216206.8354 s
env0_first_0:                 episode reward: -40.2500,                 loss: 0.3966
env0_second_0:                 episode reward: 40.2500,                 loss: nan
env1_first_0:                 episode reward: -35.0500,                 loss: nan
env1_second_0:                 episode reward: 35.0500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 424.8,                last time consumption/overall running time: 210.5462s / 216417.3816 s
env0_first_0:                 episode reward: -55.1000,                 loss: 0.4152
env0_second_0:                 episode reward: 55.1000,                 loss: nan
env1_first_0:                 episode reward: -53.3500,                 loss: nan
env1_second_0:                 episode reward: 53.3500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 592.5,                last time consumption/overall running time: 292.5036s / 216709.8852 s
env0_first_0:                 episode reward: -30.8000,                 loss: 0.4188
env0_second_0:                 episode reward: 30.8000,                 loss: nan
env1_first_0:                 episode reward: -41.9500,                 loss: nan
env1_second_0:                 episode reward: 41.9500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 653.0,                last time consumption/overall running time: 322.4910s / 217032.3762 s
env0_first_0:                 episode reward: -19.6500,                 loss: 0.4143
env0_second_0:                 episode reward: 19.6500,                 loss: nan
env1_first_0:                 episode reward: -34.3000,                 loss: nan
env1_second_0:                 episode reward: 34.3000,                 loss: nan
