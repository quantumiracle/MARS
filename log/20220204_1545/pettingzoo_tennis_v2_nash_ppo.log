pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220204_1545/pettingzoo_tennis_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220204_1545/pettingzoo_tennis_v2_nash_ppo.
Episode: 1/30000 (0.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 50.6952s / 50.6952 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.0147
env0_second_0:                 episode reward: 8.0000,                 loss: -0.0101
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 7068.05,                last time consumption/overall running time: 709.5889s / 760.2841 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0414
env0_second_0:                 episode reward: -3.6500,                 loss: -0.0399
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 4803.15,                last time consumption/overall running time: 442.0271s / 1202.3111 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0434
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0289
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 3948.2,                last time consumption/overall running time: 372.8307s / 1575.1419 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.0689
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0585
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 3621.95,                last time consumption/overall running time: 354.9613s / 1930.1032 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.1009
env0_second_0:                 episode reward: 2.7500,                 loss: -0.0881
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 5241.3,                last time consumption/overall running time: 508.3430s / 2438.4462 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0993
env0_second_0:                 episode reward: -9.1500,                 loss: -0.0972
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 5577.15,                last time consumption/overall running time: 561.0975s / 2999.5437 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1417
env0_second_0:                 episode reward: -5.1000,                 loss: -0.1228
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 6984.55,                last time consumption/overall running time: 687.8726s / 3687.4163 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1913
env0_second_0:                 episode reward: -3.8500,                 loss: -0.1764
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 8537.3,                last time consumption/overall running time: 826.8851s / 4514.3014 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.1871
env0_second_0:                 episode reward: -4.9000,                 loss: -0.1648
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 9412.4,                last time consumption/overall running time: 915.0379s / 5429.3394 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2217
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2102
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 9578.0,                last time consumption/overall running time: 950.0032s / 6379.3425 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.2051
env0_second_0:                 episode reward: 5.4000,                 loss: -0.1880
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 9712.5,                last time consumption/overall running time: 975.9982s / 7355.3407 s
env0_first_0:                 episode reward: 19.5000,                 loss: -0.1939
env0_second_0:                 episode reward: -19.5000,                 loss: -0.1813
env1_first_0:                 episode reward: 17.1000,                 loss: nan
env1_second_0:                 episode reward: -17.1000,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 9991.5,                last time consumption/overall running time: 996.9783s / 8352.3190 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.2332
env0_second_0:                 episode reward: -2.9500,                 loss: -0.2192
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 9189.25,                last time consumption/overall running time: 1049.3294s / 9401.6484 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.2251
env0_second_0:                 episode reward: -7.3000,                 loss: -0.2076
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 9698.95,                last time consumption/overall running time: 1099.5975s / 10501.2459 s
env0_first_0:                 episode reward: 8.3500,                 loss: -0.2372
env0_second_0:                 episode reward: -8.3500,                 loss: -0.2265
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 9597.7,                last time consumption/overall running time: 1079.9706s / 11581.2165 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.2302
env0_second_0:                 episode reward: 3.8500,                 loss: -0.2207
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 9992.05,                last time consumption/overall running time: 1044.8826s / 12626.0991 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2410
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2311
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 9998.25,                last time consumption/overall running time: 1187.6799s / 13813.7790 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.2231
env0_second_0:                 episode reward: 5.2500,                 loss: -0.2151
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 9479.2,                last time consumption/overall running time: 1129.4693s / 14943.2483 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.2023
env0_second_0:                 episode reward: -7.0000,                 loss: -0.1930
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1192.8364s / 16136.0847 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2181
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2131
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 9742.3,                last time consumption/overall running time: 1169.0972s / 17305.1819 s
env0_first_0:                 episode reward: -17.1000,                 loss: -0.1482
env0_second_0:                 episode reward: 17.1000,                 loss: -0.1158
env1_first_0:                 episode reward: -25.6000,                 loss: nan
env1_second_0:                 episode reward: 25.6000,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 9720.35,                last time consumption/overall running time: 1149.4440s / 18454.6259 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.2119
env0_second_0:                 episode reward: 6.7500,                 loss: -0.2056
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 9643.25,                last time consumption/overall running time: 1105.0028s / 19559.6287 s
env0_first_0:                 episode reward: 22.7000,                 loss: -0.1627
env0_second_0:                 episode reward: -22.7000,                 loss: -0.1481
env1_first_0:                 episode reward: 10.3500,                 loss: nan
env1_second_0:                 episode reward: -10.3500,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 9865.5,                last time consumption/overall running time: 1128.1361s / 20687.7648 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.2279
env0_second_0:                 episode reward: -4.0500,                 loss: -0.2214
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 9978.65,                last time consumption/overall running time: 1090.4899s / 21778.2547 s
env0_first_0:                 episode reward: -13.9000,                 loss: -0.2097
env0_second_0:                 episode reward: 13.9000,                 loss: -0.1982
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 992.7450s / 22770.9997 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1680
env0_second_0:                 episode reward: -5.2500,                 loss: -0.1446
env1_first_0:                 episode reward: 14.5500,                 loss: nan
env1_second_0:                 episode reward: -14.5500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1033.0872s / 23804.0869 s
env0_first_0:                 episode reward: 16.7500,                 loss: -0.1644
env0_second_0:                 episode reward: -16.7500,                 loss: -0.1650
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.2825s / 24911.3694 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.2190
env0_second_0:                 episode reward: 3.6000,                 loss: -0.2093
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1183.9711s / 26095.3405 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.2278
env0_second_0:                 episode reward: -8.6000,                 loss: -0.2175
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1154.7694s / 27250.1100 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.1978
env0_second_0:                 episode reward: 2.6500,                 loss: -0.1906
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 9925.4,                last time consumption/overall running time: 1130.7861s / 28380.8961 s
env0_first_0:                 episode reward: 14.8000,                 loss: -0.2177
env0_second_0:                 episode reward: -14.8000,                 loss: -0.2079
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1118.8024s / 29499.6984 s
env0_first_0:                 episode reward: 8.0000,                 loss: -0.2545
env0_second_0:                 episode reward: -8.0000,                 loss: -0.2457
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1165.4414s / 30665.1399 s
env0_first_0:                 episode reward: 8.9000,                 loss: -0.2331
env0_second_0:                 episode reward: -8.9000,                 loss: -0.2169
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1166.4520s / 31831.5918 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2309
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2262
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1200.2904s / 33031.8823 s
env0_first_0:                 episode reward: 23.7500,                 loss: -0.1853
env0_second_0:                 episode reward: -23.7500,                 loss: -0.1817
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 9864.5,                last time consumption/overall running time: 1124.8249s / 34156.7072 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.2365
env0_second_0:                 episode reward: -6.4500,                 loss: -0.2302
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.4385s / 35264.1457 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.2133
env0_second_0:                 episode reward: -7.7500,                 loss: -0.2050
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 741/30000 (2.4700%),                 avg. length: 9843.6,                last time consumption/overall running time: 1138.7224s / 36402.8680 s
env0_first_0:                 episode reward: 10.6000,                 loss: -0.2034
env0_second_0:                 episode reward: -10.6000,                 loss: -0.1951
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 761/30000 (2.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1187.7087s / 37590.5768 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.2381
env0_second_0:                 episode reward: -4.1500,                 loss: -0.2259
env1_first_0:                 episode reward: -10.7500,                 loss: nan
env1_second_0:                 episode reward: 10.7500,                 loss: nan
Episode: 781/30000 (2.6033%),                 avg. length: 9879.4,                last time consumption/overall running time: 1164.8529s / 38755.4296 s
env0_first_0:                 episode reward: -18.8000,                 loss: -0.2281
env0_second_0:                 episode reward: 18.8000,                 loss: -0.2145
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 801/30000 (2.6700%),                 avg. length: 9843.15,                last time consumption/overall running time: 1145.0861s / 39900.5157 s
env0_first_0:                 episode reward: -13.3500,                 loss: -0.1768
env0_second_0:                 episode reward: 13.3500,                 loss: -0.1614
env1_first_0:                 episode reward: -29.3500,                 loss: nan
env1_second_0:                 episode reward: 29.3500,                 loss: nan
Episode: 821/30000 (2.7367%),                 avg. length: 9785.35,                last time consumption/overall running time: 1153.6222s / 41054.1379 s
env0_first_0:                 episode reward: -23.6500,                 loss: -0.2044
env0_second_0:                 episode reward: 23.6500,                 loss: -0.1887
env1_first_0:                 episode reward: -20.9000,                 loss: nan
env1_second_0:                 episode reward: 20.9000,                 loss: nan
Episode: 841/30000 (2.8033%),                 avg. length: 9699.8,                last time consumption/overall running time: 1148.4483s / 42202.5861 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2206
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2053
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 861/30000 (2.8700%),                 avg. length: 9965.2,                last time consumption/overall running time: 1105.2799s / 43307.8660 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.2232
env0_second_0:                 episode reward: 7.5500,                 loss: -0.2112
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 881/30000 (2.9367%),                 avg. length: 9918.8,                last time consumption/overall running time: 1063.3327s / 44371.1987 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.2100
env0_second_0:                 episode reward: 3.3000,                 loss: -0.1906
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 901/30000 (3.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.0268s / 45425.2255 s
env0_first_0:                 episode reward: -8.7500,                 loss: -0.2421
env0_second_0:                 episode reward: 8.7500,                 loss: -0.2253
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 921/30000 (3.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.0907s / 46478.3162 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2397
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2173
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 941/30000 (3.1367%),                 avg. length: 9814.9,                last time consumption/overall running time: 1159.4273s / 47637.7435 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2087
env0_second_0:                 episode reward: -1.7500,                 loss: -0.1981
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 961/30000 (3.2033%),                 avg. length: 9800.8,                last time consumption/overall running time: 1175.5117s / 48813.2553 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1818
env0_second_0:                 episode reward: -2.2500,                 loss: -0.1708
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 981/30000 (3.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1223.3230s / 50036.5783 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.2164
env0_second_0:                 episode reward: -6.2500,                 loss: -0.2060
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1001/30000 (3.3367%),                 avg. length: 9990.35,                last time consumption/overall running time: 1209.9587s / 51246.5370 s
env0_first_0:                 episode reward: -12.7000,                 loss: -0.2119
env0_second_0:                 episode reward: 12.7000,                 loss: -0.2045
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 1021/30000 (3.4033%),                 avg. length: 9899.45,                last time consumption/overall running time: 1200.8395s / 52447.3765 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.2337
env0_second_0:                 episode reward: 6.5000,                 loss: -0.2226
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1041/30000 (3.4700%),                 avg. length: 9962.2,                last time consumption/overall running time: 1250.6632s / 53698.0397 s
env0_first_0:                 episode reward: -18.0000,                 loss: -0.1703
env0_second_0:                 episode reward: 18.0000,                 loss: -0.1538
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1061/30000 (3.5367%),                 avg. length: 9602.85,                last time consumption/overall running time: 1188.0668s / 54886.1064 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.2396
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2319
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1081/30000 (3.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1254.1320s / 56140.2384 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2281
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2210
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1101/30000 (3.6700%),                 avg. length: 9858.3,                last time consumption/overall running time: 1246.9509s / 57387.1893 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2069
env0_second_0:                 episode reward: -3.8000,                 loss: -0.1932
env1_first_0:                 episode reward: -20.5500,                 loss: nan
env1_second_0:                 episode reward: 20.5500,                 loss: nan
Episode: 1121/30000 (3.7367%),                 avg. length: 9973.6,                last time consumption/overall running time: 1256.0898s / 58643.2791 s
env0_first_0:                 episode reward: -3.6000,                 loss: -0.2409
env0_second_0:                 episode reward: 3.6000,                 loss: -0.2307
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 1141/30000 (3.8033%),                 avg. length: 9890.85,                last time consumption/overall running time: 1232.4963s / 59875.7755 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2231
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2106
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 1161/30000 (3.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1214.9391s / 61090.7145 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.2254
env0_second_0:                 episode reward: 3.2500,                 loss: -0.2170
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 1181/30000 (3.9367%),                 avg. length: 9546.15,                last time consumption/overall running time: 1184.5633s / 62275.2778 s
env0_first_0:                 episode reward: -11.2500,                 loss: -0.2000
env0_second_0:                 episode reward: 11.2500,                 loss: -0.1898
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Episode: 1201/30000 (4.0033%),                 avg. length: 9940.9,                last time consumption/overall running time: 1200.7389s / 63476.0167 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.2549
env0_second_0:                 episode reward: 5.3000,                 loss: -0.2435
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 1221/30000 (4.0700%),                 avg. length: 9986.3,                last time consumption/overall running time: 1224.8913s / 64700.9080 s
env0_first_0:                 episode reward: -7.1500,                 loss: -0.2591
env0_second_0:                 episode reward: 7.1500,                 loss: -0.2476
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 1241/30000 (4.1367%),                 avg. length: 9975.4,                last time consumption/overall running time: 1248.0092s / 65948.9172 s
env0_first_0:                 episode reward: -10.4500,                 loss: -0.2414
env0_second_0:                 episode reward: 10.4500,                 loss: -0.2314
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 1261/30000 (4.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1237.9014s / 67186.8186 s
env0_first_0:                 episode reward: -8.5500,                 loss: -0.2808
env0_second_0:                 episode reward: 8.5500,                 loss: -0.2710
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 1281/30000 (4.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1218.2842s / 68405.1028 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2752
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2656
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 1301/30000 (4.3367%),                 avg. length: 9901.0,                last time consumption/overall running time: 1205.6882s / 69610.7910 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2511
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2354
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1321/30000 (4.4033%),                 avg. length: 9848.0,                last time consumption/overall running time: 1215.1922s / 70825.9832 s
env0_first_0:                 episode reward: 15.1500,                 loss: -0.2415
env0_second_0:                 episode reward: -15.1500,                 loss: -0.2293
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 1341/30000 (4.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.0149s / 72077.9981 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2799
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2653
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1361/30000 (4.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.1970s / 73330.1951 s
env0_first_0:                 episode reward: 8.1500,                 loss: -0.2770
env0_second_0:                 episode reward: -8.1500,                 loss: -0.2615
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 1381/30000 (4.6033%),                 avg. length: 9951.25,                last time consumption/overall running time: 1237.9864s / 74568.1815 s
env0_first_0:                 episode reward: -7.2000,                 loss: -0.2578
env0_second_0:                 episode reward: 7.2000,                 loss: -0.2443
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1401/30000 (4.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1214.5068s / 75782.6883 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.2454
env0_second_0:                 episode reward: -4.7500,                 loss: -0.2300
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 1421/30000 (4.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1254.5478s / 77037.2361 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.2595
env0_second_0:                 episode reward: -3.6500,                 loss: -0.2433
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1441/30000 (4.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1262.8686s / 78300.1047 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.2513
env0_second_0:                 episode reward: -6.6500,                 loss: -0.2348
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 1461/30000 (4.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.8612s / 79552.9658 s
env0_first_0:                 episode reward: 13.9000,                 loss: -0.2264
env0_second_0:                 episode reward: -13.9000,                 loss: -0.2047
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 1481/30000 (4.9367%),                 avg. length: 9936.0,                last time consumption/overall running time: 1237.7004s / 80790.6662 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.2367
env0_second_0:                 episode reward: -6.6500,                 loss: -0.2220
env1_first_0:                 episode reward: 10.9000,                 loss: nan
env1_second_0:                 episode reward: -10.9000,                 loss: nan
Episode: 1501/30000 (5.0033%),                 avg. length: 9922.7,                last time consumption/overall running time: 1196.9077s / 81987.5739 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.2370
env0_second_0:                 episode reward: -7.2500,                 loss: -0.2203
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1521/30000 (5.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1235.4201s / 83222.9940 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.2321
env0_second_0:                 episode reward: -6.5500,                 loss: -0.2133
env1_first_0:                 episode reward: 18.1500,                 loss: nan
env1_second_0:                 episode reward: -18.1500,                 loss: nan
Episode: 1541/30000 (5.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1259.8791s / 84482.8731 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1937
env0_second_0:                 episode reward: -2.3000,                 loss: -0.1757
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1561/30000 (5.2033%),                 avg. length: 9891.25,                last time consumption/overall running time: 1201.3576s / 85684.2307 s
env0_first_0:                 episode reward: 11.7500,                 loss: -0.2073
env0_second_0:                 episode reward: -11.7500,                 loss: -0.1916
env1_first_0:                 episode reward: 12.9500,                 loss: nan
env1_second_0:                 episode reward: -12.9500,                 loss: nan
Episode: 1581/30000 (5.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1233.1406s / 86917.3712 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.2407
env0_second_0:                 episode reward: -7.6500,                 loss: -0.2244
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1601/30000 (5.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.0197s / 88155.3909 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.2287
env0_second_0:                 episode reward: -10.0000,                 loss: -0.2170
env1_first_0:                 episode reward: 11.5000,                 loss: nan
env1_second_0:                 episode reward: -11.5000,                 loss: nan
Episode: 1621/30000 (5.4033%),                 avg. length: 9965.8,                last time consumption/overall running time: 1230.0212s / 89385.4121 s
env0_first_0:                 episode reward: 14.0500,                 loss: -0.2261
env0_second_0:                 episode reward: -14.0500,                 loss: -0.2130
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 1641/30000 (5.4700%),                 avg. length: 9750.7,                last time consumption/overall running time: 1233.2730s / 90618.6851 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.2263
env0_second_0:                 episode reward: -5.7000,                 loss: -0.2182
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Episode: 1661/30000 (5.5367%),                 avg. length: 9993.5,                last time consumption/overall running time: 1232.8377s / 91851.5229 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.2477
env0_second_0:                 episode reward: -8.7000,                 loss: -0.2293
env1_first_0:                 episode reward: 11.2500,                 loss: nan
env1_second_0:                 episode reward: -11.2500,                 loss: nan
Episode: 1681/30000 (5.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1252.1746s / 93103.6975 s
env0_first_0:                 episode reward: 11.6500,                 loss: -0.2596
env0_second_0:                 episode reward: -11.6500,                 loss: -0.2438
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 1701/30000 (5.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1236.5532s / 94340.2507 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.2262
env0_second_0:                 episode reward: -5.8500,                 loss: -0.2131
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 1721/30000 (5.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1249.4639s / 95589.7146 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2291
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2169
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 1741/30000 (5.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1243.9071s / 96833.6217 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.2912
env0_second_0:                 episode reward: -5.3000,                 loss: -0.2794
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 1761/30000 (5.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1296.2737s / 98129.8955 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.2465
env0_second_0:                 episode reward: -5.9000,                 loss: -0.2254
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 1781/30000 (5.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1280.5842s / 99410.4796 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.2936
env0_second_0:                 episode reward: -4.2000,                 loss: -0.2847
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1801/30000 (6.0033%),                 avg. length: 9685.05,                last time consumption/overall running time: 1267.6881s / 100678.1678 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.3033
env0_second_0:                 episode reward: -3.7500,                 loss: -0.2915
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1821/30000 (6.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1315.4338s / 101993.6015 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.3106
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2964
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1841/30000 (6.1367%),                 avg. length: 9994.75,                last time consumption/overall running time: 1241.4387s / 103235.0402 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.2824
env0_second_0:                 episode reward: -8.8500,                 loss: -0.2750
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 1861/30000 (6.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1196.9412s / 104431.9814 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2759
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2636
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 1881/30000 (6.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1227.8979s / 105659.8793 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2890
env0_second_0:                 episode reward: -3.8000,                 loss: -0.2759
env1_first_0:                 episode reward: 10.1500,                 loss: nan
env1_second_0:                 episode reward: -10.1500,                 loss: nan
Episode: 1901/30000 (6.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1243.8467s / 106903.7261 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.2691
env0_second_0:                 episode reward: -5.4000,                 loss: -0.2549
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1921/30000 (6.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1258.2760s / 108162.0021 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3147
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2967
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 1941/30000 (6.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.2019s / 109400.2039 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2908
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2733
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1961/30000 (6.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1238.8508s / 110639.0548 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3162
env0_second_0:                 episode reward: 0.2500,                 loss: -0.3006
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1981/30000 (6.6033%),                 avg. length: 9975.75,                last time consumption/overall running time: 1249.1988s / 111888.2535 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.2849
env0_second_0:                 episode reward: -3.2500,                 loss: -0.2633
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 2001/30000 (6.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1209.4351s / 113097.6886 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.3046
env0_second_0:                 episode reward: -3.8000,                 loss: -0.2852
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2021/30000 (6.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1235.4299s / 114333.1186 s
env0_first_0:                 episode reward: 9.1000,                 loss: -0.2865
env0_second_0:                 episode reward: -9.1000,                 loss: -0.2731
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 2041/30000 (6.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1253.6498s / 115586.7684 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.3093
env0_second_0:                 episode reward: -4.0000,                 loss: -0.2918
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 2061/30000 (6.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1232.3432s / 116819.1115 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.3074
env0_second_0:                 episode reward: -4.3000,                 loss: -0.2875
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 2081/30000 (6.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1244.4261s / 118063.5376 s
env0_first_0:                 episode reward: 7.7000,                 loss: -0.3128
env0_second_0:                 episode reward: -7.7000,                 loss: -0.2984
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 2101/30000 (7.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1266.3696s / 119329.9072 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.3131
env0_second_0:                 episode reward: -6.1000,                 loss: -0.2955
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 2121/30000 (7.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1266.6419s / 120596.5491 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.2956
env0_second_0:                 episode reward: -6.1500,                 loss: -0.2745
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 2141/30000 (7.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1282.6727s / 121879.2218 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3068
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2919
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2161/30000 (7.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1278.4794s / 123157.7012 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3052
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2919
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 2181/30000 (7.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1282.2454s / 124439.9466 s
env0_first_0:                 episode reward: -12.3000,                 loss: -0.2771
env0_second_0:                 episode reward: 12.3000,                 loss: -0.2560
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 2201/30000 (7.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1274.7873s / 125714.7339 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2901
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2734
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2221/30000 (7.4033%),                 avg. length: 9989.7,                last time consumption/overall running time: 1265.1716s / 126979.9055 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.2755
env0_second_0:                 episode reward: 3.2000,                 loss: -0.2582
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2241/30000 (7.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1266.4146s / 128246.3201 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.2771
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2600
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2261/30000 (7.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1240.7818s / 129487.1018 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.2843
env0_second_0:                 episode reward: 2.7500,                 loss: -0.2659
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 2281/30000 (7.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.6608s / 130511.7627 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3050
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2918
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 2301/30000 (7.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1058.7506s / 131570.5133 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2987
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2795
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2321/30000 (7.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 999.8943s / 132570.4076 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3029
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2825
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 2341/30000 (7.8033%),                 avg. length: 9998.05,                last time consumption/overall running time: 1025.1849s / 133595.5925 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2927
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2723
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 2361/30000 (7.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1013.6455s / 134609.2380 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3069
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2870
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 2381/30000 (7.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.2348s / 135636.4728 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3134
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2912
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2401/30000 (8.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.8972s / 136659.3700 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.3005
env0_second_0:                 episode reward: 6.8000,                 loss: -0.2759
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2421/30000 (8.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1016.8002s / 137676.1702 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3164
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2927
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 2441/30000 (8.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1028.9923s / 138705.1625 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3084
env0_second_0:                 episode reward: 1.1000,                 loss: -0.2855
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 2461/30000 (8.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.0687s / 139729.2312 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.2964
env0_second_0:                 episode reward: 8.3500,                 loss: -0.2637
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2481/30000 (8.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 999.8995s / 140729.1307 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3150
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2890
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2501/30000 (8.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1014.4140s / 141743.5447 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.3199
env0_second_0:                 episode reward: -4.4500,                 loss: -0.2988
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 2521/30000 (8.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1009.6620s / 142753.2067 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3243
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2955
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2541/30000 (8.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1025.3609s / 143778.5676 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.3180
env0_second_0:                 episode reward: -6.4000,                 loss: -0.2751
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2561/30000 (8.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1048.4429s / 144827.0105 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3229
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2975
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2581/30000 (8.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.4152s / 145831.4257 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3284
env0_second_0:                 episode reward: -2.0000,                 loss: -0.3002
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 2601/30000 (8.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1067.9387s / 146899.3644 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.3262
env0_second_0:                 episode reward: 0.3500,                 loss: -0.2994
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2621/30000 (8.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.8847s / 147937.2490 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2919
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2526
env1_first_0:                 episode reward: -14.6000,                 loss: nan
env1_second_0:                 episode reward: 14.6000,                 loss: nan
Episode: 2641/30000 (8.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1062.9003s / 149000.1493 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3197
env0_second_0:                 episode reward: -2.5500,                 loss: -0.2862
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2661/30000 (8.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1073.8172s / 150073.9666 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2863
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2161
env1_first_0:                 episode reward: -10.0500,                 loss: nan
env1_second_0:                 episode reward: 10.0500,                 loss: nan
Episode: 2681/30000 (8.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1077.9289s / 151151.8955 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3201
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2807
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 2701/30000 (9.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1077.3771s / 152229.2727 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.3364
env0_second_0:                 episode reward: -4.4500,                 loss: -0.2955
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 2721/30000 (9.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1084.7054s / 153313.9780 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.3088
env0_second_0:                 episode reward: 5.0500,                 loss: -0.2660
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 2741/30000 (9.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1078.7969s / 154392.7749 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3440
env0_second_0:                 episode reward: 0.8500,                 loss: -0.3195
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2761/30000 (9.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1084.1018s / 155476.8767 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.3279
env0_second_0:                 episode reward: -3.6000,                 loss: -0.2937
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 2781/30000 (9.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1123.8894s / 156600.7662 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.3378
env0_second_0:                 episode reward: -4.0000,                 loss: -0.3056
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 2801/30000 (9.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1111.5092s / 157712.2754 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3418
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3147
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 2821/30000 (9.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1082.4775s / 158794.7528 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3356
env0_second_0:                 episode reward: -3.5500,                 loss: -0.3088
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2841/30000 (9.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1138.4820s / 159933.2349 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3304
env0_second_0:                 episode reward: -1.9000,                 loss: -0.3010
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 2861/30000 (9.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.7232s / 161059.9580 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.3313
env0_second_0:                 episode reward: -3.7500,                 loss: -0.2943
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2881/30000 (9.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1106.9721s / 162166.9302 s
env0_first_0:                 episode reward: -6.8000,                 loss: -0.3055
env0_second_0:                 episode reward: 6.8000,                 loss: -0.2585
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 2901/30000 (9.6700%),                 avg. length: 9955.35,                last time consumption/overall running time: 1084.9809s / 163251.9111 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.3272
env0_second_0:                 episode reward: -4.7000,                 loss: -0.2925
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 2921/30000 (9.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.3047s / 164371.2158 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.3382
env0_second_0:                 episode reward: -3.5000,                 loss: -0.3017
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2941/30000 (9.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1145.0442s / 165516.2599 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3194
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2834
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 2961/30000 (9.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.3187s / 166638.5786 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3310
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2884
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2981/30000 (9.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1101.9045s / 167740.4831 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.3257
env0_second_0:                 episode reward: 2.7500,                 loss: -0.2881
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3001/30000 (10.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1151.1418s / 168891.6248 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.3263
env0_second_0:                 episode reward: -4.5000,                 loss: -0.2918
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 3021/30000 (10.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1166.3237s / 170057.9485 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3411
env0_second_0:                 episode reward: 0.1000,                 loss: -0.3070
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3041/30000 (10.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1148.4561s / 171206.4047 s
env0_first_0:                 episode reward: -27.9000,                 loss: -0.2370
env0_second_0:                 episode reward: 27.9000,                 loss: -0.1813
env1_first_0:                 episode reward: -31.9000,                 loss: nan
env1_second_0:                 episode reward: 31.9000,                 loss: nan
Episode: 3061/30000 (10.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1155.4859s / 172361.8906 s
env0_first_0:                 episode reward: -19.5500,                 loss: -0.2969
env0_second_0:                 episode reward: 19.5500,                 loss: -0.2484
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 3081/30000 (10.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1136.1495s / 173498.0401 s
env0_first_0:                 episode reward: -36.7000,                 loss: -0.2991
env0_second_0:                 episode reward: 36.7000,                 loss: -0.2599
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3101/30000 (10.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.5483s / 174624.5884 s
env0_first_0:                 episode reward: -13.8000,                 loss: -0.2635
env0_second_0:                 episode reward: 13.8000,                 loss: -0.2102
env1_first_0:                 episode reward: -35.4500,                 loss: nan
env1_second_0:                 episode reward: 35.4500,                 loss: nan
Episode: 3121/30000 (10.4033%),                 avg. length: 9936.55,                last time consumption/overall running time: 1127.7196s / 175752.3079 s
env0_first_0:                 episode reward: -12.4500,                 loss: -0.2906
env0_second_0:                 episode reward: 12.4500,                 loss: -0.2426
env1_first_0:                 episode reward: -19.4000,                 loss: nan
env1_second_0:                 episode reward: 19.4000,                 loss: nan
Episode: 3141/30000 (10.4700%),                 avg. length: 9949.1,                last time consumption/overall running time: 1132.1458s / 176884.4537 s
env0_first_0:                 episode reward: -29.3000,                 loss: -0.2605
env0_second_0:                 episode reward: 29.3000,                 loss: -0.2051
env1_first_0:                 episode reward: -13.7500,                 loss: nan
env1_second_0:                 episode reward: 13.7500,                 loss: nan
Episode: 3161/30000 (10.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1116.6502s / 178001.1039 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3278
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2925
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3181/30000 (10.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1110.9368s / 179112.0408 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.3385
env0_second_0:                 episode reward: -0.4000,                 loss: -0.3026
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3201/30000 (10.6700%),                 avg. length: 9877.35,                last time consumption/overall running time: 1105.6796s / 180217.7203 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.3161
env0_second_0:                 episode reward: 7.7500,                 loss: -0.2807
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3221/30000 (10.7367%),                 avg. length: 9130.1,                last time consumption/overall running time: 1017.2860s / 181235.0063 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.2895
env0_second_0:                 episode reward: -7.3000,                 loss: -0.2227
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 3241/30000 (10.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.1903s / 182366.1966 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.3006
env0_second_0:                 episode reward: -2.1000,                 loss: -0.2649
env1_first_0:                 episode reward: -17.9500,                 loss: nan
env1_second_0:                 episode reward: 17.9500,                 loss: nan
Episode: 3261/30000 (10.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1161.8434s / 183528.0400 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3175
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2719
env1_first_0:                 episode reward: -17.0000,                 loss: nan
env1_second_0:                 episode reward: 17.0000,                 loss: nan
Episode: 3281/30000 (10.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1132.8135s / 184660.8534 s
env0_first_0:                 episode reward: -5.7000,                 loss: -0.3332
env0_second_0:                 episode reward: 5.7000,                 loss: -0.2931
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 3301/30000 (11.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1151.0730s / 185811.9264 s
env0_first_0:                 episode reward: -18.7000,                 loss: -0.3096
env0_second_0:                 episode reward: 18.7000,                 loss: -0.2460
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 3321/30000 (11.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.7585s / 186954.6849 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3525
env0_second_0:                 episode reward: -0.6000,                 loss: -0.3173
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3341/30000 (11.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.5905s / 188076.2754 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3397
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2833
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3361/30000 (11.2033%),                 avg. length: 9925.4,                last time consumption/overall running time: 1123.0952s / 189199.3706 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3283
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2782
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3381/30000 (11.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1129.1762s / 190328.5468 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.3417
env0_second_0:                 episode reward: -3.0000,                 loss: -0.2997
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 3401/30000 (11.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1150.3376s / 191478.8844 s
env0_first_0:                 episode reward: -10.6500,                 loss: -0.3078
env0_second_0:                 episode reward: 10.6500,                 loss: -0.2574
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 3421/30000 (11.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1130.3412s / 192609.2256 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3204
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2787
env1_first_0:                 episode reward: -11.3000,                 loss: nan
env1_second_0:                 episode reward: 11.3000,                 loss: nan
Episode: 3441/30000 (11.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.4287s / 193733.6543 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3228
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2802
env1_first_0:                 episode reward: -11.5000,                 loss: nan
env1_second_0:                 episode reward: 11.5000,                 loss: nan
Episode: 3461/30000 (11.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.4770s / 194853.1313 s
env0_first_0:                 episode reward: -11.3000,                 loss: -0.3165
env0_second_0:                 episode reward: 11.3000,                 loss: -0.2765
env1_first_0:                 episode reward: -9.4500,                 loss: nan
env1_second_0:                 episode reward: 9.4500,                 loss: nan
Episode: 3481/30000 (11.6033%),                 avg. length: 9950.55,                last time consumption/overall running time: 1113.9512s / 195967.0825 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3135
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2761
env1_first_0:                 episode reward: -21.4500,                 loss: nan
env1_second_0:                 episode reward: 21.4500,                 loss: nan
Episode: 3501/30000 (11.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1136.4712s / 197103.5536 s
env0_first_0:                 episode reward: -18.2000,                 loss: -0.3034
env0_second_0:                 episode reward: 18.2000,                 loss: -0.2534
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3521/30000 (11.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1140.3296s / 198243.8832 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.3245
env0_second_0:                 episode reward: 9.2000,                 loss: -0.2683
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 3541/30000 (11.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1120.9390s / 199364.8222 s
env0_first_0:                 episode reward: -20.5500,                 loss: -0.2979
env0_second_0:                 episode reward: 20.5500,                 loss: -0.2398
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3561/30000 (11.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.7713s / 200496.5936 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.3167
env0_second_0:                 episode reward: -2.2000,                 loss: -0.2720
env1_first_0:                 episode reward: -15.1500,                 loss: nan
env1_second_0:                 episode reward: 15.1500,                 loss: nan
Episode: 3581/30000 (11.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1106.1808s / 201602.7743 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3467
env0_second_0:                 episode reward: -0.5500,                 loss: -0.3049
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 3601/30000 (12.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1140.8318s / 202743.6062 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3534
env0_second_0:                 episode reward: -1.8000,                 loss: -0.3134
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3621/30000 (12.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1120.7160s / 203864.3221 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2934
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2239
env1_first_0:                 episode reward: -15.7000,                 loss: nan
env1_second_0:                 episode reward: 15.7000,                 loss: nan
Episode: 3641/30000 (12.1367%),                 avg. length: 9950.9,                last time consumption/overall running time: 1113.1306s / 204977.4527 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.3108
env0_second_0:                 episode reward: -6.7500,                 loss: -0.2650
env1_first_0:                 episode reward: -14.4000,                 loss: nan
env1_second_0:                 episode reward: 14.4000,                 loss: nan
Episode: 3661/30000 (12.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.3073s / 206108.7600 s
env0_first_0:                 episode reward: -6.3500,                 loss: -0.3363
env0_second_0:                 episode reward: 6.3500,                 loss: -0.2962
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3681/30000 (12.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.3846s / 207235.1445 s
env0_first_0:                 episode reward: -15.3500,                 loss: -0.3185
env0_second_0:                 episode reward: 15.3500,                 loss: -0.2600
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 3701/30000 (12.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.6735s / 208350.8180 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3485
env0_second_0:                 episode reward: -2.7500,                 loss: -0.3068
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3721/30000 (12.4033%),                 avg. length: 9950.6,                last time consumption/overall running time: 1127.1327s / 209477.9507 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3489
env0_second_0:                 episode reward: -2.1500,                 loss: -0.3062
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3741/30000 (12.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.5098s / 210597.4605 s
env0_first_0:                 episode reward: -8.0000,                 loss: -0.3311
env0_second_0:                 episode reward: 8.0000,                 loss: -0.2769
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3761/30000 (12.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.1780s / 211719.6385 s
env0_first_0:                 episode reward: -10.9000,                 loss: -0.3330
env0_second_0:                 episode reward: 10.9000,                 loss: -0.2867
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 3781/30000 (12.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1099.1486s / 212818.7871 s
env0_first_0:                 episode reward: -25.3000,                 loss: -0.2838
env0_second_0:                 episode reward: 25.3000,                 loss: -0.2105
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 3801/30000 (12.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1110.4783s / 213929.2654 s
env0_first_0:                 episode reward: -30.5500,                 loss: -0.3084
env0_second_0:                 episode reward: 30.5500,                 loss: -0.2619
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3821/30000 (12.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1102.8653s / 215032.1307 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3179
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2653
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 3841/30000 (12.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.6069s / 216156.7376 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.3223
env0_second_0:                 episode reward: -3.8500,                 loss: -0.2665
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 3861/30000 (12.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.3143s / 217262.0519 s
env0_first_0:                 episode reward: -10.0500,                 loss: -0.3391
env0_second_0:                 episode reward: 10.0500,                 loss: -0.3013
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3881/30000 (12.9367%),                 avg. length: 9884.75,                last time consumption/overall running time: 1095.7625s / 218357.8144 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3497
env0_second_0:                 episode reward: -0.2500,                 loss: -0.3098
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3901/30000 (13.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1127.1931s / 219485.0075 s
env0_first_0:                 episode reward: -4.3000,                 loss: -0.3481
env0_second_0:                 episode reward: 4.3000,                 loss: -0.3069
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 3921/30000 (13.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.3883s / 220607.3958 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3487
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2994
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3941/30000 (13.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.5096s / 221733.9053 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3415
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2924
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 3961/30000 (13.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1156.9291s / 222890.8344 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3237
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2653
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 3981/30000 (13.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1130.9601s / 224021.7946 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.3519
env0_second_0:                 episode reward: -3.5000,                 loss: -0.2936
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4001/30000 (13.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.7820s / 225137.5765 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3559
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2993
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 4021/30000 (13.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1130.3208s / 226267.8973 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3421
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2925
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 4041/30000 (13.4700%),                 avg. length: 9939.1,                last time consumption/overall running time: 1134.3772s / 227402.2746 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3496
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2992
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4061/30000 (13.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1133.7573s / 228536.0318 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3620
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3059
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4081/30000 (13.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1096.0866s / 229632.1184 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3495
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2988
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 4101/30000 (13.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1118.0347s / 230750.1531 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3478
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2944
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4121/30000 (13.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.4014s / 231874.5545 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3468
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2965
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4141/30000 (13.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1111.5486s / 232986.1031 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3456
env0_second_0:                 episode reward: 0.0000,                 loss: -0.3001
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 4161/30000 (13.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.0944s / 234107.1975 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3471
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2934
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4181/30000 (13.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1117.1986s / 235224.3961 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3497
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2925
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4201/30000 (14.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1100.8439s / 236325.2401 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3441
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2881
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4221/30000 (14.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1112.3104s / 237437.5504 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3484
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2950
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4241/30000 (14.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.7806s / 238562.3310 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3559
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2986
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4261/30000 (14.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1123.9135s / 239686.2445 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3500
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2833
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4281/30000 (14.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.1526s / 240801.3970 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3528
env0_second_0:                 episode reward: -2.0500,                 loss: -0.2954
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 4301/30000 (14.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1123.9032s / 241925.3002 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3435
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2867
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4321/30000 (14.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1134.4029s / 243059.7031 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3396
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2856
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4341/30000 (14.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.2971s / 244186.0002 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3464
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2909
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4361/30000 (14.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.3793s / 245312.3795 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.3487
env0_second_0:                 episode reward: -0.8000,                 loss: -0.2836
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4381/30000 (14.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.5563s / 246404.9358 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3483
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2941
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4401/30000 (14.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.9160s / 247527.8518 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3446
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2795
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4421/30000 (14.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1099.7415s / 248627.5933 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3471
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2713
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4441/30000 (14.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.1930s / 249732.7863 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3378
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2631
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4461/30000 (14.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1088.4542s / 250821.2405 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.3337
env0_second_0:                 episode reward: -9.1500,                 loss: -0.2692
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4481/30000 (14.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1103.4619s / 251924.7024 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3415
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2588
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4501/30000 (15.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.6681s / 253050.3704 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.3499
env0_second_0:                 episode reward: -2.2000,                 loss: -0.2683
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4521/30000 (15.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1167.8910s / 254218.2615 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.3461
env0_second_0:                 episode reward: 1.7500,                 loss: -0.2866
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4541/30000 (15.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1152.7103s / 255370.9717 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3444
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2843
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 4561/30000 (15.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.9203s / 256495.8921 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3405
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0867
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 4581/30000 (15.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1123.2532s / 257619.1453 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3290
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2254
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 4601/30000 (15.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.5324s / 258744.6777 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3310
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2636
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4621/30000 (15.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1174.7849s / 259919.4626 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3147
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2278
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 4641/30000 (15.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.3010s / 261061.7637 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.3382
env0_second_0:                 episode reward: 1.4500,                 loss: -0.2590
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 4661/30000 (15.5367%),                 avg. length: 9867.25,                last time consumption/overall running time: 1123.4297s / 262185.1934 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3231
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2391
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4681/30000 (15.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1130.5237s / 263315.7171 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3534
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2963
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4701/30000 (15.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.1686s / 264439.8857 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3311
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2701
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4721/30000 (15.7367%),                 avg. length: 9919.9,                last time consumption/overall running time: 1095.9173s / 265535.8030 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3360
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2620
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 4741/30000 (15.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.0705s / 266660.8735 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3383
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2805
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4761/30000 (15.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1152.1510s / 267813.0245 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3554
env0_second_0:                 episode reward: -0.9000,                 loss: -0.3001
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4781/30000 (15.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1150.3540s / 268963.3785 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3498
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2877
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 4801/30000 (16.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1141.3549s / 270104.7334 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3260
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2649
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4821/30000 (16.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1153.3146s / 271258.0480 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3418
env0_second_0:                 episode reward: -1.2500,                 loss: -0.2685
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4841/30000 (16.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1145.7935s / 272403.8415 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3273
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2550
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4861/30000 (16.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1128.4084s / 273532.2499 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3432
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2780
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 4881/30000 (16.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.4296s / 274657.6795 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3219
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2511
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 4901/30000 (16.3367%),                 avg. length: 9996.1,                last time consumption/overall running time: 1139.5349s / 275797.2144 s
env0_first_0:                 episode reward: 9.7500,                 loss: -0.3305
env0_second_0:                 episode reward: -9.7500,                 loss: -0.1114
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 4921/30000 (16.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.1294s / 276918.3438 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.2934
env0_second_0:                 episode reward: 0.9000,                 loss: -0.1442
env1_first_0:                 episode reward: -27.5500,                 loss: nan
env1_second_0:                 episode reward: 27.5500,                 loss: nan
Episode: 4941/30000 (16.4700%),                 avg. length: 9842.85,                last time consumption/overall running time: 1100.1539s / 278018.4978 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3396
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2676
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4961/30000 (16.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1114.8213s / 279133.3191 s
env0_first_0:                 episode reward: -10.5000,                 loss: -0.3413
env0_second_0:                 episode reward: 10.5000,                 loss: -0.2728
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 4981/30000 (16.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1127.6113s / 280260.9304 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3509
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2689
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5001/30000 (16.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1165.8845s / 281426.8148 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3391
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2608
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5021/30000 (16.7367%),                 avg. length: 9995.05,                last time consumption/overall running time: 1114.5148s / 282541.3297 s
env0_first_0:                 episode reward: -7.6500,                 loss: -0.3251
env0_second_0:                 episode reward: 7.6500,                 loss: -0.2563
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5041/30000 (16.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1116.3315s / 283657.6611 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.3639
env0_second_0:                 episode reward: 2.0500,                 loss: -0.3026
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5061/30000 (16.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1117.4096s / 284775.0707 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.3232
env0_second_0:                 episode reward: 2.0500,                 loss: -0.2283
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5081/30000 (16.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1095.2693s / 285870.3400 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3447
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2754
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 5101/30000 (17.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1127.1668s / 286997.5069 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3522
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2933
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5121/30000 (17.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.6755s / 288120.1824 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.3471
env0_second_0:                 episode reward: 0.9000,                 loss: -0.2811
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5141/30000 (17.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.1757s / 289225.3581 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3102
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2101
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 5161/30000 (17.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1120.5288s / 290345.8868 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.3347
env0_second_0:                 episode reward: 1.6500,                 loss: -0.2484
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5181/30000 (17.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1150.3721s / 291496.2589 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.3296
env0_second_0:                 episode reward: 2.2500,                 loss: -0.2202
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5201/30000 (17.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1095.9132s / 292592.1721 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3275
env0_second_0:                 episode reward: 0.7000,                 loss: -0.2605
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 5221/30000 (17.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1097.5330s / 293689.7052 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.3330
env0_second_0:                 episode reward: 2.0500,                 loss: -0.2682
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 5241/30000 (17.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1106.9913s / 294796.6964 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.3302
env0_second_0:                 episode reward: 1.7500,                 loss: -0.2381
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 5261/30000 (17.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1159.0027s / 295955.6992 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.3156
env0_second_0:                 episode reward: 2.9500,                 loss: -0.2438
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 5281/30000 (17.6033%),                 avg. length: 9926.8,                last time consumption/overall running time: 1128.8242s / 297084.5234 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3367
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2652
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 5301/30000 (17.6700%),                 avg. length: 9571.45,                last time consumption/overall running time: 1064.7384s / 298149.2618 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0394
env0_second_0:                 episode reward: -2.3500,                 loss: 0.8324
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 5321/30000 (17.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.9369s / 299255.1987 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.3456
env0_second_0:                 episode reward: -5.7500,                 loss: -0.0376
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5341/30000 (17.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1104.4720s / 300359.6708 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.3495
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0191
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5361/30000 (17.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.4066s / 301452.0774 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.3524
env0_second_0:                 episode reward: 3.7500,                 loss: -0.1444
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 5381/30000 (17.9367%),                 avg. length: 9914.35,                last time consumption/overall running time: 1143.1361s / 302595.2135 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.3403
env0_second_0:                 episode reward: 1.1500,                 loss: -0.1387
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 5401/30000 (18.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1099.5662s / 303694.7797 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3533
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2353
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 5421/30000 (18.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.4247s / 304810.2044 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3449
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2399
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 5441/30000 (18.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.7996s / 305933.0039 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.3558
env0_second_0:                 episode reward: 1.7500,                 loss: -0.2391
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 5461/30000 (18.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1162.7844s / 307095.7884 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3462
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2365
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 5481/30000 (18.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1090.7452s / 308186.5336 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3202
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2262
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5501/30000 (18.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1072.8357s / 309259.3692 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3283
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2508
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 5521/30000 (18.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.3637s / 310374.7329 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3468
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2926
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5541/30000 (18.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1156.5916s / 311531.3246 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.3520
env0_second_0:                 episode reward: 1.1500,                 loss: -0.2727
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5561/30000 (18.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1113.0603s / 312644.3849 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3643
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2879
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 5581/30000 (18.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.3972s / 313786.7821 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3280
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1826
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 5601/30000 (18.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1118.1995s / 314904.9816 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3367
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2793
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 5621/30000 (18.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1136.1391s / 316041.1207 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3343
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2549
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 5641/30000 (18.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1080.5127s / 317121.6334 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3546
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2826
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5661/30000 (18.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1089.8881s / 318211.5215 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.3495
env0_second_0:                 episode reward: 0.9000,                 loss: -0.2762
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 5681/30000 (18.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1091.2573s / 319302.7788 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3510
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2729
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 5701/30000 (19.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1166.5615s / 320469.3403 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3569
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2396
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 5721/30000 (19.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.7476s / 321575.0879 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3516
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2341
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 5741/30000 (19.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.2615s / 322682.3494 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3434
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3814
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 5761/30000 (19.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1098.7394s / 323781.0888 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3346
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1065
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 5781/30000 (19.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1108.7863s / 324889.8751 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3379
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1462
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 5801/30000 (19.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1192.2805s / 326082.1556 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3331
env0_second_0:                 episode reward: 1.1000,                 loss: -0.2117
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5821/30000 (19.4033%),                 avg. length: 9823.55,                last time consumption/overall running time: 1103.3384s / 327185.4940 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3256
env0_second_0:                 episode reward: -1.8500,                 loss: -0.1970
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5841/30000 (19.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.6458s / 328311.1398 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3221
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2288
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 5861/30000 (19.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1174.9213s / 329486.0611 s
env0_first_0:                 episode reward: -13.7000,                 loss: -0.2437
env0_second_0:                 episode reward: 13.7000,                 loss: -0.1321
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 5881/30000 (19.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1095.6611s / 330581.7222 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3398
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2496
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 5901/30000 (19.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1095.9210s / 331677.6432 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3304
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2442
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5921/30000 (19.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1136.8908s / 332814.5340 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3203
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1268
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 5941/30000 (19.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1114.9124s / 333929.4464 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.3534
env0_second_0:                 episode reward: -2.9000,                 loss: -0.2522
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5961/30000 (19.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1154.4878s / 335083.9342 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3745
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3074
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5981/30000 (19.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1141.7860s / 336225.7202 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3553
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2738
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6001/30000 (20.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1218.8679s / 337444.5882 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3362
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1719
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 6021/30000 (20.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1203.3630s / 338647.9511 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.3280
env0_second_0:                 episode reward: 2.4500,                 loss: -0.1841
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 6041/30000 (20.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1147.4212s / 339795.3724 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.3452
env0_second_0:                 episode reward: 1.8500,                 loss: -0.0949
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 6061/30000 (20.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1169.9197s / 340965.2921 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3416
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1453
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 6081/30000 (20.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1168.5776s / 342133.8697 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.2954
env0_second_0:                 episode reward: 1.5000,                 loss: -0.1226
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 6101/30000 (20.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.9703s / 343255.8400 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3676
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2953
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 6121/30000 (20.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.2659s / 344375.1059 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.3703
env0_second_0:                 episode reward: -0.4000,                 loss: -0.3191
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 6141/30000 (20.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1214.4164s / 345589.5223 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3645
env0_second_0:                 episode reward: 0.2000,                 loss: -0.3141
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6161/30000 (20.5367%),                 avg. length: 9683.6,                last time consumption/overall running time: 1128.0886s / 346717.6110 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.2771
env0_second_0:                 episode reward: 2.2000,                 loss: -0.1988
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 6181/30000 (20.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1186.4918s / 347904.1028 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.3268
env0_second_0:                 episode reward: -4.1000,                 loss: -0.2370
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 6201/30000 (20.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1230.2216s / 349134.3244 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3512
env0_second_0:                 episode reward: -2.3000,                 loss: -0.2775
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6221/30000 (20.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1163.2536s / 350297.5781 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.3436
env0_second_0:                 episode reward: 1.5500,                 loss: -0.2558
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 6241/30000 (20.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1246.0687s / 351543.6468 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3475
env0_second_0:                 episode reward: 1.1000,                 loss: -0.2530
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 6261/30000 (20.8700%),                 avg. length: 9649.3,                last time consumption/overall running time: 1152.1875s / 352695.8343 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.3351
env0_second_0:                 episode reward: -4.2500,                 loss: -0.2641
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6281/30000 (20.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1149.4751s / 353845.3094 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3482
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2852
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6301/30000 (21.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1159.6416s / 355004.9510 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3250
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2481
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 6321/30000 (21.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1175.0216s / 356179.9726 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3335
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2473
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 6341/30000 (21.1367%),                 avg. length: 9869.85,                last time consumption/overall running time: 1186.2035s / 357366.1761 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3255
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2220
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 6361/30000 (21.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1302.5036s / 358668.6797 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.3142
env0_second_0:                 episode reward: 1.6000,                 loss: -0.2267
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 6381/30000 (21.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1161.8829s / 359830.5626 s
env0_first_0:                 episode reward: -11.8500,                 loss: -0.3234
env0_second_0:                 episode reward: 11.8500,                 loss: -0.2455
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 6401/30000 (21.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1112.8565s / 360943.4191 s
env0_first_0:                 episode reward: -8.8000,                 loss: -0.3069
env0_second_0:                 episode reward: 8.8000,                 loss: -0.0418
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6421/30000 (21.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1282.5158s / 362225.9349 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3367
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2852
env1_first_0:                 episode reward: -8.0500,                 loss: nan
env1_second_0:                 episode reward: 8.0500,                 loss: nan
Episode: 6441/30000 (21.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1239.6890s / 363465.6239 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3646
env0_second_0:                 episode reward: -0.1000,                 loss: -0.3122
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 6461/30000 (21.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1209.4697s / 364675.0936 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3566
env0_second_0:                 episode reward: -0.9500,                 loss: -0.3075
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 6481/30000 (21.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1086.0625s / 365761.1562 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.3777
env0_second_0:                 episode reward: 0.3500,                 loss: -0.3362
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6501/30000 (21.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1168.9895s / 366930.1457 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.3700
env0_second_0:                 episode reward: 2.1500,                 loss: -0.3309
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 6521/30000 (21.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1204.0529s / 368134.1986 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3161
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2596
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 6541/30000 (21.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1192.0158s / 369326.2144 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3782
env0_second_0:                 episode reward: -0.6500,                 loss: -0.3317
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 6561/30000 (21.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1236.1042s / 370562.3186 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.3566
env0_second_0:                 episode reward: -2.8000,                 loss: -0.3008
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 6581/30000 (21.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1167.6499s / 371729.9685 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.3687
env0_second_0:                 episode reward: 2.4500,                 loss: -0.3208
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 6601/30000 (22.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1224.1551s / 372954.1236 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3527
env0_second_0:                 episode reward: -2.2500,                 loss: -0.3015
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6621/30000 (22.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1188.3945s / 374142.5181 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3423
env0_second_0:                 episode reward: 0.3000,                 loss: -0.2714
env1_first_0:                 episode reward: 11.7000,                 loss: nan
env1_second_0:                 episode reward: -11.7000,                 loss: nan
Episode: 6641/30000 (22.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.3241s / 375234.8422 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3623
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2910
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 6661/30000 (22.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1157.7104s / 376392.5527 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.3498
env0_second_0:                 episode reward: 2.0500,                 loss: -0.2784
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 6681/30000 (22.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1124.2157s / 377516.7684 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.3465
env0_second_0:                 episode reward: 2.8500,                 loss: -0.2766
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 6701/30000 (22.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1135.7914s / 378652.5598 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.3328
env0_second_0:                 episode reward: 1.4500,                 loss: -0.2394
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 6721/30000 (22.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1068.2548s / 379720.8146 s
env0_first_0:                 episode reward: -9.6500,                 loss: -0.2787
env0_second_0:                 episode reward: 9.6500,                 loss: -0.1552
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6741/30000 (22.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1160.4485s / 380881.2631 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.3336
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2222
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6761/30000 (22.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1166.8623s / 382048.1255 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.3315
env0_second_0:                 episode reward: 1.7000,                 loss: -0.2447
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 6781/30000 (22.6033%),                 avg. length: 8498.4,                last time consumption/overall running time: 934.5141s / 382982.6395 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.2217
env0_second_0:                 episode reward: -7.0000,                 loss: -0.0603
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 6801/30000 (22.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1083.5735s / 384066.2130 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.3567
env0_second_0:                 episode reward: -5.2500,                 loss: -0.2996
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 6821/30000 (22.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1101.7780s / 385167.9910 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.3467
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2763
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 6841/30000 (22.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 997.9736s / 386165.9646 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3712
env0_second_0:                 episode reward: -0.9000,                 loss: -0.3219
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 6861/30000 (22.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1015.5432s / 387181.5078 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.3545
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2859
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 6881/30000 (22.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.7721s / 388186.2799 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3549
env0_second_0:                 episode reward: -1.2500,                 loss: -0.2820
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 6901/30000 (23.0033%),                 avg. length: 9726.8,                last time consumption/overall running time: 1034.7705s / 389221.0504 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.2329
env0_second_0:                 episode reward: -5.4500,                 loss: -0.1404
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 6921/30000 (23.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1021.1398s / 390242.1902 s
env0_first_0:                 episode reward: -9.5000,                 loss: -0.3304
env0_second_0:                 episode reward: 9.5000,                 loss: -0.2366
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6941/30000 (23.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1094.3802s / 391336.5704 s
env0_first_0:                 episode reward: -40.4000,                 loss: -0.2099
env0_second_0:                 episode reward: 40.4000,                 loss: -0.0998
env1_first_0:                 episode reward: -31.2500,                 loss: nan
env1_second_0:                 episode reward: 31.2500,                 loss: nan
Episode: 6961/30000 (23.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1098.1193s / 392434.6896 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.3313
env0_second_0:                 episode reward: 8.3000,                 loss: -0.2529
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 6981/30000 (23.2700%),                 avg. length: 9901.75,                last time consumption/overall running time: 992.5427s / 393427.2323 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3316
env0_second_0:                 episode reward: -0.8500,                 loss: -0.2216
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 7001/30000 (23.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1066.3585s / 394493.5908 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.3237
env0_second_0:                 episode reward: 1.6500,                 loss: -0.2478
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 7021/30000 (23.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1012.2197s / 395505.8105 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3379
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2456
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 7041/30000 (23.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1044.0740s / 396549.8845 s
env0_first_0:                 episode reward: -18.4500,                 loss: -0.2360
env0_second_0:                 episode reward: 18.4500,                 loss: -0.1295
env1_first_0:                 episode reward: -47.9500,                 loss: nan
env1_second_0:                 episode reward: 47.9500,                 loss: nan
Episode: 7061/30000 (23.5367%),                 avg. length: 9875.6,                last time consumption/overall running time: 955.3551s / 397505.2396 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.3408
env0_second_0:                 episode reward: -7.7500,                 loss: -0.2657
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 7081/30000 (23.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 959.0785s / 398464.3181 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3683
env0_second_0:                 episode reward: -1.0000,                 loss: -0.3068
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 7101/30000 (23.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1014.1085s / 399478.4266 s
env0_first_0:                 episode reward: -15.9000,                 loss: -0.3191
env0_second_0:                 episode reward: 15.9000,                 loss: -0.2227
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 7121/30000 (23.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1001.7511s / 400480.1776 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.3132
env0_second_0:                 episode reward: -6.6500,                 loss: -0.2363
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 7141/30000 (23.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1000.1751s / 401480.3527 s
env0_first_0:                 episode reward: 7.5500,                 loss: -0.3137
env0_second_0:                 episode reward: -7.5500,                 loss: -0.2208
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 7161/30000 (23.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 983.7852s / 402464.1379 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3453
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2501
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 7181/30000 (23.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1003.7635s / 403467.9014 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3435
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2650
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 7201/30000 (24.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.7896s / 404593.6910 s
env0_first_0:                 episode reward: -27.7500,                 loss: -0.2854
env0_second_0:                 episode reward: 27.7500,                 loss: -0.2009
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 7221/30000 (24.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1001.3183s / 405595.0093 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3350
env0_second_0:                 episode reward: 0.7000,                 loss: -0.1557
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 7241/30000 (24.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.9102s / 406720.9195 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3701
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0905
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7261/30000 (24.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1127.9085s / 407848.8280 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3454
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1546
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 7281/30000 (24.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1101.1305s / 408949.9585 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.3303
env0_second_0:                 episode reward: 1.8000,                 loss: -0.2149
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 7301/30000 (24.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1099.0257s / 410048.9843 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.3254
env0_second_0:                 episode reward: 1.5500,                 loss: -0.1950
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 7321/30000 (24.4033%),                 avg. length: 7418.65,                last time consumption/overall running time: 776.3319s / 410825.3161 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1895
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0210
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 7341/30000 (24.4700%),                 avg. length: 9966.95,                last time consumption/overall running time: 991.7583s / 411817.0745 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.3191
env0_second_0:                 episode reward: 1.2500,                 loss: -0.1830
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 7361/30000 (24.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1155.2367s / 412972.3112 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3240
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2089
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 7381/30000 (24.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1103.9258s / 414076.2369 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.3533
env0_second_0:                 episode reward: -3.1000,                 loss: -0.2483
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 7401/30000 (24.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1128.1768s / 415204.4137 s
env0_first_0:                 episode reward: 7.5500,                 loss: -0.3164
env0_second_0:                 episode reward: -7.5500,                 loss: -0.2137
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 7421/30000 (24.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1028.2591s / 416232.6728 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3138
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1798
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7441/30000 (24.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 987.9845s / 417220.6573 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.3705
env0_second_0:                 episode reward: -3.9000,                 loss: -0.2562
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 7461/30000 (24.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1043.5562s / 418264.2135 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3534
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2786
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 7481/30000 (24.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 981.0709s / 419245.2844 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3460
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2297
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 7501/30000 (25.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 985.9100s / 420231.1944 s
env0_first_0:                 episode reward: -14.4000,                 loss: -0.2828
env0_second_0:                 episode reward: 14.4000,                 loss: -0.1891
env1_first_0:                 episode reward: -20.0500,                 loss: nan
env1_second_0:                 episode reward: 20.0500,                 loss: nan
Episode: 7521/30000 (25.0700%),                 avg. length: 9978.65,                last time consumption/overall running time: 1075.3112s / 421306.5055 s
env0_first_0:                 episode reward: -13.1000,                 loss: -0.3035
env0_second_0:                 episode reward: 13.1000,                 loss: -0.2175
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 7541/30000 (25.1367%),                 avg. length: 9417.15,                last time consumption/overall running time: 924.5338s / 422231.0393 s
env0_first_0:                 episode reward: 12.6000,                 loss: -0.2913
env0_second_0:                 episode reward: -12.6000,                 loss: -0.1855
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 7561/30000 (25.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.8141s / 423255.8534 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3570
env0_second_0:                 episode reward: 0.7000,                 loss: -0.2742
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 7581/30000 (25.2700%),                 avg. length: 9992.95,                last time consumption/overall running time: 1003.4167s / 424259.2702 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3419
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2653
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 7601/30000 (25.3367%),                 avg. length: 9996.35,                last time consumption/overall running time: 1021.9497s / 425281.2199 s
env0_first_0:                 episode reward: -23.5500,                 loss: -0.2892
env0_second_0:                 episode reward: 23.5500,                 loss: -0.1869
env1_first_0:                 episode reward: -11.0500,                 loss: nan
env1_second_0:                 episode reward: 11.0500,                 loss: nan
Episode: 7621/30000 (25.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.6098s / 426328.8298 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3533
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2943
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 7641/30000 (25.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.2380s / 427365.0677 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.3394
env0_second_0:                 episode reward: 1.8500,                 loss: -0.2748
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 7661/30000 (25.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 959.1912s / 428324.2590 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.3127
env0_second_0:                 episode reward: 3.1000,                 loss: -0.2331
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 7681/30000 (25.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1010.9263s / 429335.1853 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.2878
env0_second_0:                 episode reward: 1.2500,                 loss: 0.1676
env1_first_0:                 episode reward: -22.3500,                 loss: nan
env1_second_0:                 episode reward: 22.3500,                 loss: nan
Episode: 7701/30000 (25.6700%),                 avg. length: 9936.2,                last time consumption/overall running time: 1111.9055s / 430447.0908 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3538
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2357
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 7721/30000 (25.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.3029s / 431489.3936 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.3186
env0_second_0:                 episode reward: 1.8000,                 loss: -0.2022
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 7741/30000 (25.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1012.9604s / 432502.3541 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.2902
env0_second_0:                 episode reward: 1.5500,                 loss: -0.1811
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7761/30000 (25.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 982.3984s / 433484.7525 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3239
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2240
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7781/30000 (25.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1215.0545s / 434699.8069 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3539
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2675
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 7801/30000 (26.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1085.4421s / 435785.2491 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3775
env0_second_0:                 episode reward: -0.3000,                 loss: -0.3175
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 7821/30000 (26.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1172.6981s / 436957.9472 s
env0_first_0:                 episode reward: -5.7500,                 loss: -0.3494
env0_second_0:                 episode reward: 5.7500,                 loss: -0.2791
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7841/30000 (26.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.7995s / 438100.7466 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.3529
env0_second_0:                 episode reward: 1.1500,                 loss: -0.2657
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 7861/30000 (26.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.1152s / 439242.8618 s
env0_first_0:                 episode reward: -9.4500,                 loss: -0.3080
env0_second_0:                 episode reward: 9.4500,                 loss: -0.1659
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 7881/30000 (26.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.9578s / 440297.8196 s
env0_first_0:                 episode reward: -12.3500,                 loss: -0.3007
env0_second_0:                 episode reward: 12.3500,                 loss: -0.1210
env1_first_0:                 episode reward: -29.4000,                 loss: nan
env1_second_0:                 episode reward: 29.4000,                 loss: nan
Episode: 7901/30000 (26.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1140.4013s / 441438.2209 s
env0_first_0:                 episode reward: -28.3500,                 loss: -0.2846
env0_second_0:                 episode reward: 28.3500,                 loss: -0.1770
env1_first_0:                 episode reward: -30.7000,                 loss: nan
env1_second_0:                 episode reward: 30.7000,                 loss: nan
Episode: 7921/30000 (26.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.0633s / 442495.2842 s
env0_first_0:                 episode reward: -11.5000,                 loss: -0.3226
env0_second_0:                 episode reward: 11.5000,                 loss: -0.2622
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 7941/30000 (26.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1025.3954s / 443520.6796 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3687
env0_second_0:                 episode reward: -1.0500,                 loss: -0.3147
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 7961/30000 (26.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 980.8726s / 444501.5522 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3499
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2862
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 7981/30000 (26.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 996.0858s / 445497.6380 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3817
env0_second_0:                 episode reward: 0.7000,                 loss: -0.3518
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 8001/30000 (26.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 996.7779s / 446494.4158 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3725
env0_second_0:                 episode reward: 0.8500,                 loss: -0.3070
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8021/30000 (26.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 999.7416s / 447494.1575 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.3350
env0_second_0:                 episode reward: 2.9500,                 loss: -0.2364
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 8041/30000 (26.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1184.9416s / 448679.0991 s
env0_first_0:                 episode reward: -12.3500,                 loss: -0.2970
env0_second_0:                 episode reward: 12.3500,                 loss: -0.1482
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 8061/30000 (26.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1087.4041s / 449766.5032 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3225
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2525
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 8081/30000 (26.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1184.8036s / 450951.3068 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3497
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2687
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 8101/30000 (27.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1179.1462s / 452130.4530 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3571
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2947
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 8121/30000 (27.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1154.8379s / 453285.2909 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3619
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2974
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 8141/30000 (27.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1185.1576s / 454470.4485 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3289
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2085
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8161/30000 (27.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1109.9862s / 455580.4347 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3620
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2053
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 8181/30000 (27.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1160.0430s / 456740.4777 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3604
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2615
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 8201/30000 (27.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 984.2438s / 457724.7216 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3441
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2556
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 8221/30000 (27.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1137.2482s / 458861.9698 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.3448
env0_second_0:                 episode reward: 1.5000,                 loss: -0.2294
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 8241/30000 (27.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1137.7605s / 459999.7303 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.2925
env0_second_0:                 episode reward: -6.3000,                 loss: 0.5801
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 8261/30000 (27.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1152.5571s / 461152.2874 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3597
env0_second_0:                 episode reward: -2.1500,                 loss: 0.4832
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 8281/30000 (27.6033%),                 avg. length: 9503.95,                last time consumption/overall running time: 1147.5498s / 462299.8372 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.3370
env0_second_0:                 episode reward: -3.3500,                 loss: 0.1503
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 8301/30000 (27.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1198.0088s / 463497.8459 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.3098
env0_second_0:                 episode reward: 4.4500,                 loss: 0.4426
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 8321/30000 (27.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1128.3285s / 464626.1744 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.3505
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0153
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8341/30000 (27.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1146.7420s / 465772.9164 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.3260
env0_second_0:                 episode reward: 5.3500,                 loss: -0.1550
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8361/30000 (27.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1178.9324s / 466951.8488 s
env0_first_0:                 episode reward: -10.8500,                 loss: -0.3256
env0_second_0:                 episode reward: 10.8500,                 loss: -0.1374
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8381/30000 (27.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1204.7940s / 468156.6428 s
env0_first_0:                 episode reward: -14.1500,                 loss: -0.2870
env0_second_0:                 episode reward: 14.1500,                 loss: -0.1469
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 8401/30000 (28.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1216.8055s / 469373.4484 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.3302
env0_second_0:                 episode reward: 2.0500,                 loss: -0.1973
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 8421/30000 (28.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1183.3989s / 470556.8472 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.3015
env0_second_0:                 episode reward: 2.2000,                 loss: -0.1371
env1_first_0:                 episode reward: -30.4000,                 loss: nan
env1_second_0:                 episode reward: 30.4000,                 loss: nan
Episode: 8441/30000 (28.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1178.0096s / 471734.8569 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.3227
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2140
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 8461/30000 (28.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.1088s / 472826.9657 s
env0_first_0:                 episode reward: -19.4000,                 loss: -0.2803
env0_second_0:                 episode reward: 19.4000,                 loss: -0.1392
env1_first_0:                 episode reward: -10.9000,                 loss: nan
env1_second_0:                 episode reward: 10.9000,                 loss: nan
Episode: 8481/30000 (28.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.4257s / 473958.3915 s
env0_first_0:                 episode reward: -27.8000,                 loss: -0.3021
env0_second_0:                 episode reward: 27.8000,                 loss: -0.1995
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 8501/30000 (28.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1215.3974s / 475173.7889 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.3036
env0_second_0:                 episode reward: 3.2000,                 loss: -0.2135
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 8521/30000 (28.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1117.6231s / 476291.4119 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.3641
env0_second_0:                 episode reward: 1.1500,                 loss: -0.2950
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 8541/30000 (28.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1184.8252s / 477476.2372 s
env0_first_0:                 episode reward: -23.8000,                 loss: -0.2780
env0_second_0:                 episode reward: 23.8000,                 loss: -0.1869
env1_first_0:                 episode reward: -11.7000,                 loss: nan
env1_second_0:                 episode reward: 11.7000,                 loss: nan
Episode: 8561/30000 (28.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1159.4649s / 478635.7020 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.3279
env0_second_0:                 episode reward: 1.4500,                 loss: -0.2278
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 8581/30000 (28.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1178.5782s / 479814.2803 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.3259
env0_second_0:                 episode reward: 2.9500,                 loss: -0.2374
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 8601/30000 (28.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1055.1207s / 480869.4010 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.3109
env0_second_0:                 episode reward: 2.3000,                 loss: -0.1823
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 8621/30000 (28.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1098.7098s / 481968.1108 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.3353
env0_second_0:                 episode reward: 2.4000,                 loss: -0.2114
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 8641/30000 (28.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1149.2244s / 483117.3352 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3286
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2265
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8661/30000 (28.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.8339s / 484240.1691 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3129
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0692
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 8681/30000 (28.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1061.0415s / 485301.2106 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.3181
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0305
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 8701/30000 (29.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1188.9235s / 486490.1341 s
env0_first_0:                 episode reward: -13.9500,                 loss: -0.3065
env0_second_0:                 episode reward: 13.9500,                 loss: -0.1629
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 8721/30000 (29.0700%),                 avg. length: 9598.2,                last time consumption/overall running time: 1178.8976s / 487669.0317 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.2821
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1326
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 8741/30000 (29.1367%),                 avg. length: 9653.6,                last time consumption/overall running time: 1114.7095s / 488783.7411 s
env0_first_0:                 episode reward: -63.2000,                 loss: -0.1863
env0_second_0:                 episode reward: 63.2000,                 loss: 0.0441
env1_first_0:                 episode reward: -19.2500,                 loss: nan
env1_second_0:                 episode reward: 19.2500,                 loss: nan
Episode: 8761/30000 (29.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1202.4526s / 489986.1937 s
env0_first_0:                 episode reward: -33.8000,                 loss: -0.1943
env0_second_0:                 episode reward: 33.8000,                 loss: -0.0745
env1_first_0:                 episode reward: -109.5000,                 loss: nan
env1_second_0:                 episode reward: 109.5000,                 loss: nan
Episode: 8781/30000 (29.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1164.0652s / 491150.2589 s
env0_first_0:                 episode reward: -31.0000,                 loss: -0.3020
env0_second_0:                 episode reward: 31.0000,                 loss: -0.1080
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 8801/30000 (29.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1184.3141s / 492334.5730 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3397
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1777
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 8821/30000 (29.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1145.4021s / 493479.9751 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.3155
env0_second_0:                 episode reward: 4.5000,                 loss: -0.2012
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 8841/30000 (29.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1144.9278s / 494624.9029 s
env0_first_0:                 episode reward: -4.2500,                 loss: -0.2997
env0_second_0:                 episode reward: 4.2500,                 loss: -0.0967
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 8861/30000 (29.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1087.2920s / 495712.1950 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.3131
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0402
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 8881/30000 (29.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1118.6359s / 496830.8308 s
env0_first_0:                 episode reward: -7.6000,                 loss: -0.2850
env0_second_0:                 episode reward: 7.6000,                 loss: -0.0368
env1_first_0:                 episode reward: -9.8000,                 loss: nan
env1_second_0:                 episode reward: 9.8000,                 loss: nan
Episode: 8901/30000 (29.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1217.2548s / 498048.0856 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3189
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0884
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 8921/30000 (29.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1066.3258s / 499114.4115 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.3366
env0_second_0:                 episode reward: 2.7500,                 loss: -0.1109
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 8941/30000 (29.8033%),                 avg. length: 9948.3,                last time consumption/overall running time: 1149.3090s / 500263.7205 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.3219
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0808
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 8961/30000 (29.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1120.7842s / 501384.5047 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.3572
env0_second_0:                 episode reward: -3.3000,                 loss: -0.2363
env1_first_0:                 episode reward: 12.3000,                 loss: nan
env1_second_0:                 episode reward: -12.3000,                 loss: nan
Episode: 8981/30000 (29.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1147.5813s / 502532.0859 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3774
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2977
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 9001/30000 (30.0033%),                 avg. length: 9700.55,                last time consumption/overall running time: 1129.4922s / 503661.5781 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.3001
env0_second_0:                 episode reward: 3.1000,                 loss: -0.2032
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 9021/30000 (30.0700%),                 avg. length: 9891.35,                last time consumption/overall running time: 1127.1312s / 504788.7094 s
env0_first_0:                 episode reward: -10.6500,                 loss: -0.3081
env0_second_0:                 episode reward: 10.6500,                 loss: -0.1440
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 9041/30000 (30.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.1848s / 505907.8942 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.2961
env0_second_0:                 episode reward: 2.3500,                 loss: -0.0602
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 9061/30000 (30.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1154.9965s / 507062.8907 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.2919
env0_second_0:                 episode reward: 7.3500,                 loss: -0.1688
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 9081/30000 (30.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1179.4946s / 508242.3852 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.3111
env0_second_0:                 episode reward: 2.3500,                 loss: -0.2150
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 9101/30000 (30.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1136.1664s / 509378.5516 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.3100
env0_second_0:                 episode reward: 1.9000,                 loss: -0.2122
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 9121/30000 (30.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1220.0936s / 510598.6452 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3006
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1295
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 9141/30000 (30.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.6949s / 511741.3401 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3254
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2000
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 9161/30000 (30.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1210.7652s / 512952.1053 s
env0_first_0:                 episode reward: -7.3500,                 loss: -0.3340
env0_second_0:                 episode reward: 7.3500,                 loss: -0.2590
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 9181/30000 (30.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1218.7003s / 514170.8056 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.3110
env0_second_0:                 episode reward: 1.6000,                 loss: -0.2108
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 9201/30000 (30.6700%),                 avg. length: 9913.45,                last time consumption/overall running time: 1174.4630s / 515345.2686 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.3334
env0_second_0:                 episode reward: 7.5000,                 loss: -0.2365
env1_first_0:                 episode reward: -13.9500,                 loss: nan
env1_second_0:                 episode reward: 13.9500,                 loss: nan
Episode: 9221/30000 (30.7367%),                 avg. length: 9796.1,                last time consumption/overall running time: 1184.1299s / 516529.3985 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.2777
env0_second_0:                 episode reward: 2.5000,                 loss: -0.0983
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9241/30000 (30.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1165.2565s / 517694.6550 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.3067
env0_second_0:                 episode reward: 6.6000,                 loss: -0.1046
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 9261/30000 (30.8700%),                 avg. length: 9815.05,                last time consumption/overall running time: 1190.2829s / 518884.9379 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.2996
env0_second_0:                 episode reward: 0.6500,                 loss: -0.1993
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 9281/30000 (30.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1170.6852s / 520055.6231 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.3089
env0_second_0:                 episode reward: 2.1000,                 loss: -0.2181
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 9301/30000 (31.0033%),                 avg. length: 9664.05,                last time consumption/overall running time: 1103.6757s / 521159.2988 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.2692
env0_second_0:                 episode reward: -6.2500,                 loss: -0.1476
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 9321/30000 (31.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1185.8515s / 522345.1503 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3366
env0_second_0:                 episode reward: -1.2500,                 loss: 0.3063
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 9341/30000 (31.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1071.4270s / 523416.5773 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.3198
env0_second_0:                 episode reward: -9.7000,                 loss: -0.1888
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 9361/30000 (31.2033%),                 avg. length: 9972.6,                last time consumption/overall running time: 1147.6634s / 524564.2407 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3656
env0_second_0:                 episode reward: -2.7500,                 loss: -0.3079
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 9381/30000 (31.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1136.6961s / 525700.9368 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3716
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3058
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 9401/30000 (31.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1144.4238s / 526845.3606 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.3685
env0_second_0:                 episode reward: 0.7500,                 loss: -0.3193
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 9421/30000 (31.4033%),                 avg. length: 9853.9,                last time consumption/overall running time: 1181.0125s / 528026.3730 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.3127
env0_second_0:                 episode reward: 2.4500,                 loss: -0.2096
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 9441/30000 (31.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1112.5112s / 529138.8842 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3392
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2436
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 9461/30000 (31.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1225.8319s / 530364.7161 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3262
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2349
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 9481/30000 (31.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.8830s / 531389.5991 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3342
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1310
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 9501/30000 (31.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1178.0202s / 532567.6193 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3480
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0968
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 9521/30000 (31.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1090.3115s / 533657.9308 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.3207
env0_second_0:                 episode reward: 3.7500,                 loss: -0.1320
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 9541/30000 (31.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1166.6815s / 534824.6123 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.3053
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1910
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 9561/30000 (31.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1160.1714s / 535984.7837 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.3075
env0_second_0:                 episode reward: 0.4500,                 loss: -0.2135
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 9581/30000 (31.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1170.1986s / 537154.9824 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.3198
env0_second_0:                 episode reward: 2.1500,                 loss: -0.1058
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 9601/30000 (32.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1188.5672s / 538343.5496 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2995
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0701
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 9621/30000 (32.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1224.7123s / 539568.2618 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.3288
env0_second_0:                 episode reward: 1.9500,                 loss: -0.2423
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9641/30000 (32.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1228.9428s / 540797.2046 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.3523
env0_second_0:                 episode reward: 3.0000,                 loss: -0.1914
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 9661/30000 (32.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1178.3397s / 541975.5443 s
env0_first_0:                 episode reward: -10.6000,                 loss: -0.3271
env0_second_0:                 episode reward: 10.6000,                 loss: 0.0227
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 9681/30000 (32.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1173.2654s / 543148.8097 s
env0_first_0:                 episode reward: -6.6500,                 loss: -0.3204
env0_second_0:                 episode reward: 6.6500,                 loss: -0.2138
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 9701/30000 (32.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.4188s / 544280.2285 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.3422
env0_second_0:                 episode reward: -0.7000,                 loss: -0.2470
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 9721/30000 (32.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1222.4757s / 545502.7041 s
env0_first_0:                 episode reward: -10.6000,                 loss: -0.3036
env0_second_0:                 episode reward: 10.6000,                 loss: -0.2102
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 9741/30000 (32.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1178.4433s / 546681.1474 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3360
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2495
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 9761/30000 (32.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1198.9904s / 547880.1378 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3300
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2451
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 9781/30000 (32.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1094.8215s / 548974.9593 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3323
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2253
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 9801/30000 (32.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1220.1077s / 550195.0670 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.3163
env0_second_0:                 episode reward: 3.3000,                 loss: -0.2104
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 9821/30000 (32.7367%),                 avg. length: 9296.1,                last time consumption/overall running time: 1106.4190s / 551301.4860 s
env0_first_0:                 episode reward: 7.0500,                 loss: -0.2721
env0_second_0:                 episode reward: -7.0500,                 loss: -0.0453
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 9841/30000 (32.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.0127s / 552406.4987 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3280
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2377
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 9861/30000 (32.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1180.3696s / 553586.8683 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3322
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2492
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 9881/30000 (32.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1148.6365s / 554735.5049 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3384
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2352
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 9901/30000 (33.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1164.9556s / 555900.4605 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3138
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1833
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9921/30000 (33.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1090.5032s / 556990.9637 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.3023
env0_second_0:                 episode reward: 1.0500,                 loss: -0.1964
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9941/30000 (33.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.2267s / 558112.1904 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3087
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2022
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 9961/30000 (33.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1172.7740s / 559284.9644 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.2708
env0_second_0:                 episode reward: 2.9000,                 loss: -0.1474
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 9981/30000 (33.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1113.3148s / 560398.2792 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3241
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2041
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 10001/30000 (33.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.4515s / 561524.7307 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.3440
env0_second_0:                 episode reward: -4.1000,                 loss: -0.2499
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 10021/30000 (33.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1127.9762s / 562652.7069 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.3617
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2014
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 10041/30000 (33.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1142.4131s / 563795.1201 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3497
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2540
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 10061/30000 (33.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1114.8626s / 564909.9827 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3396
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2467
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 10081/30000 (33.6033%),                 avg. length: 9982.2,                last time consumption/overall running time: 1125.1296s / 566035.1122 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3027
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2242
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 10101/30000 (33.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.5895s / 567077.7017 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3562
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2764
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 10121/30000 (33.7367%),                 avg. length: 8064.65,                last time consumption/overall running time: 875.5989s / 567953.3006 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.1949
env0_second_0:                 episode reward: -6.4000,                 loss: -0.0059
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 10141/30000 (33.8033%),                 avg. length: 9635.2,                last time consumption/overall running time: 972.5761s / 568925.8767 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.3315
env0_second_0:                 episode reward: -4.8000,                 loss: -0.1659
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 10161/30000 (33.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1044.6559s / 569970.5326 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3200
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2018
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 10181/30000 (33.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1223.9490s / 571194.4816 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.3862
env0_second_0:                 episode reward: -3.7000,                 loss: -0.2888
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 10201/30000 (34.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1138.1189s / 572332.6005 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3548
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2715
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 10221/30000 (34.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1176.9083s / 573509.5087 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.3498
env0_second_0:                 episode reward: -2.8000,                 loss: -0.2559
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 10241/30000 (34.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1102.1858s / 574611.6945 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3517
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2945
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 10261/30000 (34.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.3256s / 575743.0201 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3706
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3030
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 10281/30000 (34.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.9814s / 576869.0015 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3603
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2940
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 10301/30000 (34.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1232.7547s / 578101.7563 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3765
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2993
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 10321/30000 (34.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1082.8534s / 579184.6097 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3658
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2950
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 10341/30000 (34.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1031.5903s / 580216.2000 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3765
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2983
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 10361/30000 (34.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.7700s / 581273.9700 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3629
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2845
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 10381/30000 (34.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.6143s / 582325.5843 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.3678
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2972
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 10401/30000 (34.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1161.9685s / 583487.5528 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3473
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2663
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 10421/30000 (34.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1217.4257s / 584704.9786 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3576
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1680
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 10441/30000 (34.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1207.2618s / 585912.2404 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.3463
env0_second_0:                 episode reward: 1.7000,                 loss: -0.2512
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 10461/30000 (34.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1226.8372s / 587139.0776 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3402
env0_second_0:                 episode reward: -0.8500,                 loss: -0.2490
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 10481/30000 (34.9367%),                 avg. length: 9891.1,                last time consumption/overall running time: 1152.7580s / 588291.8356 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3262
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2238
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 10501/30000 (35.0033%),                 avg. length: 9911.0,                last time consumption/overall running time: 1210.4431s / 589502.2787 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3176
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2009
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 10521/30000 (35.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1149.7224s / 590652.0011 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3530
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2593
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 10541/30000 (35.1367%),                 avg. length: 9811.15,                last time consumption/overall running time: 1154.8110s / 591806.8121 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3523
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2730
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 10561/30000 (35.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1133.8396s / 592940.6517 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3618
env0_second_0:                 episode reward: -1.1500,                 loss: -0.1773
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 10581/30000 (35.2700%),                 avg. length: 9785.0,                last time consumption/overall running time: 1135.3937s / 594076.0454 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.2739
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0327
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 10601/30000 (35.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1162.7222s / 595238.7676 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.3622
env0_second_0:                 episode reward: -2.9000,                 loss: -0.2652
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 10621/30000 (35.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1185.1733s / 596423.9409 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3618
env0_second_0:                 episode reward: -0.8500,                 loss: -0.2690
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 10641/30000 (35.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1226.6982s / 597650.6391 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3619
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2768
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 10661/30000 (35.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1165.9586s / 598816.5977 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3363
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2592
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 10681/30000 (35.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1144.8050s / 599961.4027 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.3259
env0_second_0:                 episode reward: 1.9000,                 loss: -0.2603
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 10701/30000 (35.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1226.9762s / 601188.3789 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.3198
env0_second_0:                 episode reward: 1.5500,                 loss: -0.2201
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 10721/30000 (35.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.7474s / 602311.1263 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3270
env0_second_0:                 episode reward: 0.3000,                 loss: -0.2011
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 10741/30000 (35.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1161.9647s / 603473.0910 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3426
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2569
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 10761/30000 (35.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1171.3920s / 604644.4829 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3408
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2654
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 10781/30000 (35.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1138.2464s / 605782.7293 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.3623
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2515
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 10801/30000 (36.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1143.7438s / 606926.4730 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.3297
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0310
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 10821/30000 (36.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1104.5460s / 608031.0190 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.3106
env0_second_0:                 episode reward: 2.2500,                 loss: -0.1832
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 10841/30000 (36.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1174.5932s / 609205.6122 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3388
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2358
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10861/30000 (36.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1172.5980s / 610378.2102 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3647
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2870
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 10881/30000 (36.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1114.4151s / 611492.6253 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3450
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2759
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 10901/30000 (36.3367%),                 avg. length: 9913.75,                last time consumption/overall running time: 1062.5133s / 612555.1386 s
env0_first_0:                 episode reward: 8.1500,                 loss: -0.3628
env0_second_0:                 episode reward: -8.1500,                 loss: -0.2895
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 10921/30000 (36.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1181.4753s / 613736.6139 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3632
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2873
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 10941/30000 (36.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1110.9376s / 614847.5514 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3463
env0_second_0:                 episode reward: -2.2500,                 loss: -0.1591
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 10961/30000 (36.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1227.0699s / 616074.6213 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3005
env0_second_0:                 episode reward: -1.2500,                 loss: 0.2706
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 10981/30000 (36.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1213.8466s / 617288.4679 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.2885
env0_second_0:                 episode reward: -1.4000,                 loss: -0.1250
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 11001/30000 (36.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1180.0793s / 618468.5472 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3493
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2664
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 11021/30000 (36.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1226.0066s / 619694.5538 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3311
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2592
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 11041/30000 (36.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1227.6916s / 620922.2453 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3542
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2818
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 11061/30000 (36.8700%),                 avg. length: 9808.4,                last time consumption/overall running time: 1206.5229s / 622128.7682 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.3496
env0_second_0:                 episode reward: -8.6000,                 loss: -0.2376
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 11081/30000 (36.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1227.8581s / 623356.6263 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3747
env0_second_0:                 episode reward: -1.4000,                 loss: -0.3058
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 11101/30000 (37.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1225.3353s / 624581.9616 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3574
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2972
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 11121/30000 (37.0700%),                 avg. length: 8626.6,                last time consumption/overall running time: 1057.2177s / 625639.1793 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.2637
env0_second_0:                 episode reward: 3.9000,                 loss: -0.1613
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 11141/30000 (37.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1225.8219s / 626865.0012 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3394
env0_second_0:                 episode reward: -0.8500,                 loss: -0.2698
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11161/30000 (37.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1229.6397s / 628094.6409 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.3405
env0_second_0:                 episode reward: 0.7500,                 loss: -0.2563
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 11181/30000 (37.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1223.8652s / 629318.5061 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3376
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2512
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 11201/30000 (37.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1224.3495s / 630542.8556 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3356
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2271
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 11221/30000 (37.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1222.6868s / 631765.5424 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3331
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1826
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 11241/30000 (37.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1222.6846s / 632988.2269 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3338
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2348
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 11261/30000 (37.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1223.3725s / 634211.5995 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.3449
env0_second_0:                 episode reward: 1.2000,                 loss: -0.2642
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 11281/30000 (37.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1220.4278s / 635432.0273 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3423
env0_second_0:                 episode reward: -1.9500,                 loss: -0.2619
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 11301/30000 (37.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1224.8271s / 636656.8544 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3414
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2598
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 11321/30000 (37.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1222.0350s / 637878.8893 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3500
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2408
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 11341/30000 (37.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1225.2580s / 639104.1473 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.3078
env0_second_0:                 episode reward: 1.0500,                 loss: -0.2301
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 11361/30000 (37.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1228.2220s / 640332.3693 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3297
env0_second_0:                 episode reward: -0.5500,                 loss: -0.2372
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 11381/30000 (37.9367%),                 avg. length: 8713.6,                last time consumption/overall running time: 1063.5627s / 641395.9320 s
env0_first_0:                 episode reward: 175.5000,                 loss: 0.8196
env0_second_0:                 episode reward: -175.5000,                 loss: 0.3692
env1_first_0:                 episode reward: 175.6000,                 loss: nan
env1_second_0:                 episode reward: -175.6000,                 loss: nan
Episode: 11401/30000 (38.0033%),                 avg. length: 5260.3,                last time consumption/overall running time: 644.4755s / 642040.4075 s
env0_first_0:                 episode reward: 110.9000,                 loss: -0.0144
env0_second_0:                 episode reward: -110.9000,                 loss: 0.2292
env1_first_0:                 episode reward: 111.8000,                 loss: nan
env1_second_0:                 episode reward: -111.8000,                 loss: nan
Episode: 11421/30000 (38.0700%),                 avg. length: 6980.8,                last time consumption/overall running time: 854.3052s / 642894.7128 s
env0_first_0:                 episode reward: 127.7000,                 loss: 0.0392
env0_second_0:                 episode reward: -127.7000,                 loss: 0.4006
env1_first_0:                 episode reward: 126.9000,                 loss: nan
env1_second_0:                 episode reward: -126.9000,                 loss: nan
Episode: 11441/30000 (38.1367%),                 avg. length: 4834.4,                last time consumption/overall running time: 593.4318s / 643488.1446 s
env0_first_0:                 episode reward: 78.2500,                 loss: 0.0229
env0_second_0:                 episode reward: -78.2500,                 loss: 0.3453
env1_first_0:                 episode reward: 101.1500,                 loss: nan
env1_second_0:                 episode reward: -101.1500,                 loss: nan
Episode: 11461/30000 (38.2033%),                 avg. length: 4823.95,                last time consumption/overall running time: 591.2014s / 644079.3460 s
env0_first_0:                 episode reward: 99.6000,                 loss: -0.0312
env0_second_0:                 episode reward: -99.6000,                 loss: 0.1559
env1_first_0:                 episode reward: 100.4000,                 loss: nan
env1_second_0:                 episode reward: -100.4000,                 loss: nan
Episode: 11481/30000 (38.2700%),                 avg. length: 4467.1,                last time consumption/overall running time: 549.3831s / 644628.7291 s
env0_first_0:                 episode reward: 89.9500,                 loss: -0.0379
env0_second_0:                 episode reward: -89.9500,                 loss: 0.0818
env1_first_0:                 episode reward: 88.9000,                 loss: nan
env1_second_0:                 episode reward: -88.9000,                 loss: nan
Episode: 11501/30000 (38.3367%),                 avg. length: 4049.7,                last time consumption/overall running time: 497.7154s / 645126.4445 s
env0_first_0:                 episode reward: 79.1500,                 loss: -0.0793
env0_second_0:                 episode reward: -79.1500,                 loss: 0.0829
env1_first_0:                 episode reward: 80.8500,                 loss: nan
env1_second_0:                 episode reward: -80.8500,                 loss: nan
Episode: 11521/30000 (38.4033%),                 avg. length: 1373.6,                last time consumption/overall running time: 173.3144s / 645299.7588 s
env0_first_0:                 episode reward: 23.8000,                 loss: -0.2261
env0_second_0:                 episode reward: -23.8000,                 loss: -0.0462
env1_first_0:                 episode reward: 22.6000,                 loss: nan
env1_second_0:                 episode reward: -22.6000,                 loss: nan
Episode: 11541/30000 (38.4700%),                 avg. length: 1803.7,                last time consumption/overall running time: 226.0005s / 645525.7593 s
env0_first_0:                 episode reward: 31.4500,                 loss: -0.2310
env0_second_0:                 episode reward: -31.4500,                 loss: -0.0963
env1_first_0:                 episode reward: 33.6500,                 loss: nan
env1_second_0:                 episode reward: -33.6500,                 loss: nan
Episode: 11561/30000 (38.5367%),                 avg. length: 1363.6,                last time consumption/overall running time: 172.6596s / 645698.4188 s
env0_first_0:                 episode reward: 23.6500,                 loss: -0.2798
env0_second_0:                 episode reward: -23.6500,                 loss: -0.1104
env1_first_0:                 episode reward: 23.3500,                 loss: nan
env1_second_0:                 episode reward: -23.3500,                 loss: nan
Episode: 11581/30000 (38.6033%),                 avg. length: 1762.85,                last time consumption/overall running time: 221.4547s / 645919.8735 s
env0_first_0:                 episode reward: 21.7500,                 loss: -0.2415
env0_second_0:                 episode reward: -21.7500,                 loss: -0.0203
env1_first_0:                 episode reward: 21.3500,                 loss: nan
env1_second_0:                 episode reward: -21.3500,                 loss: nan
Episode: 11601/30000 (38.6700%),                 avg. length: 6393.35,                last time consumption/overall running time: 785.2402s / 646705.1137 s
env0_first_0:                 episode reward: 15.7000,                 loss: -0.1707
env0_second_0:                 episode reward: -15.7000,                 loss: 0.0626
env1_first_0:                 episode reward: 25.6500,                 loss: nan
env1_second_0:                 episode reward: -25.6500,                 loss: nan
Episode: 11621/30000 (38.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1218.5791s / 647923.6928 s
env0_first_0:                 episode reward: 16.8000,                 loss: -0.2876
env0_second_0:                 episode reward: -16.8000,                 loss: -0.1723
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 11641/30000 (38.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1224.1074s / 649147.8002 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3358
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2357
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 11661/30000 (38.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1217.4582s / 650365.2585 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3452
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2399
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11681/30000 (38.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1216.2865s / 651581.5449 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3529
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2590
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 11701/30000 (39.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1220.2041s / 652801.7491 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3580
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2770
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 11721/30000 (39.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1221.3284s / 654023.0774 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.3519
env0_second_0:                 episode reward: 0.9000,                 loss: -0.2443
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 11741/30000 (39.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1221.0041s / 655244.0816 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3585
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2611
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 11761/30000 (39.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1221.7317s / 656465.8132 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3756
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2769
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 11781/30000 (39.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1221.5578s / 657687.3711 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.3507
env0_second_0:                 episode reward: 1.7000,                 loss: -0.2358
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 11801/30000 (39.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1220.0000s / 658907.3711 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3560
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2636
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 11821/30000 (39.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1221.7252s / 660129.0962 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.3269
env0_second_0:                 episode reward: 1.0500,                 loss: -0.1509
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 11841/30000 (39.4700%),                 avg. length: 8035.45,                last time consumption/overall running time: 981.9772s / 661111.0734 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.2292
env0_second_0:                 episode reward: -4.0000,                 loss: 0.0046
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 11861/30000 (39.5367%),                 avg. length: 9891.5,                last time consumption/overall running time: 1205.5801s / 662316.6536 s
env0_first_0:                 episode reward: 12.7500,                 loss: -0.3513
env0_second_0:                 episode reward: -12.7500,                 loss: -0.2506
env1_first_0:                 episode reward: 10.9500,                 loss: nan
env1_second_0:                 episode reward: -10.9500,                 loss: nan
Episode: 11881/30000 (39.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1202.0073s / 663518.6608 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2901
env0_second_0:                 episode reward: 0.4500,                 loss: 0.1716
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 11901/30000 (39.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1173.3119s / 664691.9728 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.3136
env0_second_0:                 episode reward: 3.8500,                 loss: -0.1345
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 11921/30000 (39.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1189.4491s / 665881.4218 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.2864
env0_second_0:                 episode reward: 2.1500,                 loss: -0.1953
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 11941/30000 (39.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1178.2576s / 667059.6794 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.2727
env0_second_0:                 episode reward: 1.3500,                 loss: -0.1650
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 11961/30000 (39.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1162.6673s / 668222.3467 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3201
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2203
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 11981/30000 (39.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1209.7628s / 669432.1095 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3197
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2227
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 12001/30000 (40.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1218.1358s / 670650.2453 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3383
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2581
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 12021/30000 (40.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1218.5516s / 671868.7968 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3307
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2303
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 12041/30000 (40.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1222.1665s / 673090.9633 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3517
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2585
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 12061/30000 (40.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1225.9257s / 674316.8890 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3492
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2207
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 12081/30000 (40.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1221.5584s / 675538.4475 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3658
env0_second_0:                 episode reward: -2.3500,                 loss: -0.2762
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 12101/30000 (40.3367%),                 avg. length: 8693.35,                last time consumption/overall running time: 1029.9102s / 676568.3576 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.2538
env0_second_0:                 episode reward: -4.6500,                 loss: -0.0779
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 12121/30000 (40.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1156.1132s / 677724.4708 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.3318
env0_second_0:                 episode reward: 4.4500,                 loss: -0.1920
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 12141/30000 (40.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1065.1201s / 678789.5909 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.3052
env0_second_0:                 episode reward: 1.6000,                 loss: -0.1752
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 12161/30000 (40.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1070.5701s / 679860.1610 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.3082
env0_second_0:                 episode reward: 2.3000,                 loss: -0.0606
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 12181/30000 (40.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1085.0922s / 680945.2533 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.3156
env0_second_0:                 episode reward: 2.2000,                 loss: -0.2017
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 12201/30000 (40.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1075.0007s / 682020.2540 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.3299
env0_second_0:                 episode reward: 2.2000,                 loss: -0.2299
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 12221/30000 (40.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1068.7244s / 683088.9784 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.3189
env0_second_0:                 episode reward: 2.4500,                 loss: -0.2207
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 12241/30000 (40.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1148.8058s / 684237.7843 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.3113
env0_second_0:                 episode reward: 2.3000,                 loss: -0.2138
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 12261/30000 (40.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1097.5897s / 685335.3740 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.3329
env0_second_0:                 episode reward: 1.0000,                 loss: -0.2644
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 12281/30000 (40.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1058.4103s / 686393.7843 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3069
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2243
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 12301/30000 (41.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1067.1504s / 687460.9347 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3323
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2534
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 12321/30000 (41.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1061.2874s / 688522.2221 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3287
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2405
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 12341/30000 (41.1367%),                 avg. length: 9566.55,                last time consumption/overall running time: 1033.6090s / 689555.8311 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.3364
env0_second_0:                 episode reward: -5.6000,                 loss: -0.2204
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 12361/30000 (41.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.3341s / 690613.1652 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3333
env0_second_0:                 episode reward: -1.2500,                 loss: -0.2588
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 12381/30000 (41.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1059.3005s / 691672.4657 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.3042
env0_second_0:                 episode reward: 2.4000,                 loss: -0.1874
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 12401/30000 (41.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1031.7216s / 692704.1873 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3287
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1484
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 12421/30000 (41.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1160.5439s / 693864.7312 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.3475
env0_second_0:                 episode reward: -3.2500,                 loss: -0.2611
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 12441/30000 (41.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.2506s / 694986.9817 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3340
env0_second_0:                 episode reward: -0.5500,                 loss: -0.2423
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 12461/30000 (41.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1008.3968s / 695995.3785 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3257
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2480
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 12481/30000 (41.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1034.9994s / 697030.3779 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3253
env0_second_0:                 episode reward: -1.3000,                 loss: 0.1074
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 12501/30000 (41.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.9590s / 698084.3369 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.3141
env0_second_0:                 episode reward: 2.8000,                 loss: -0.0194
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 12521/30000 (41.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1062.1758s / 699146.5127 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.3389
env0_second_0:                 episode reward: 2.0000,                 loss: -0.2353
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 12541/30000 (41.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1049.1791s / 700195.6918 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.3204
env0_second_0:                 episode reward: 2.4000,                 loss: -0.2235
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 12561/30000 (41.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1055.3419s / 701251.0338 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2424
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1811
env1_first_0:                 episode reward: -11.3000,                 loss: nan
env1_second_0:                 episode reward: 11.3000,                 loss: nan
Episode: 12581/30000 (41.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.9001s / 702305.9339 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.3401
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2154
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 12601/30000 (42.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1041.7861s / 703347.7200 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3326
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2332
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 12621/30000 (42.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1041.5476s / 704389.2676 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3447
env0_second_0:                 episode reward: -0.9000,                 loss: -0.1874
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 12641/30000 (42.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1074.7582s / 705464.0258 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3082
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2276
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12661/30000 (42.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.6715s / 706468.6973 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3448
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0097
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12681/30000 (42.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1002.0337s / 707470.7310 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3252
env0_second_0:                 episode reward: -2.1500,                 loss: -0.1276
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 12701/30000 (42.3367%),                 avg. length: 9881.35,                last time consumption/overall running time: 1029.9857s / 708500.7168 s
env0_first_0:                 episode reward: 8.7500,                 loss: -0.3276
env0_second_0:                 episode reward: -8.7500,                 loss: -0.1230
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 12721/30000 (42.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1122.9562s / 709623.6730 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3444
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2441
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 12741/30000 (42.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1221.3658s / 710845.0388 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3404
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2314
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12761/30000 (42.5367%),                 avg. length: 9950.95,                last time consumption/overall running time: 1215.1780s / 712060.2168 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3149
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2392
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12781/30000 (42.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1224.0595s / 713284.2763 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3299
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0814
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12801/30000 (42.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1225.8704s / 714510.1467 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.3417
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1089
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 12821/30000 (42.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1225.5684s / 715735.7152 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.3434
env0_second_0:                 episode reward: 2.1000,                 loss: -0.2647
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 12841/30000 (42.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1226.2838s / 716961.9989 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.3372
env0_second_0:                 episode reward: -0.7000,                 loss: -0.2708
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 12861/30000 (42.8700%),                 avg. length: 9125.3,                last time consumption/overall running time: 1119.7872s / 718081.7861 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2655
env0_second_0:                 episode reward: -2.1000,                 loss: -0.1348
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 12881/30000 (42.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1228.9443s / 719310.7304 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.3755
env0_second_0:                 episode reward: -3.9000,                 loss: -0.2924
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 12901/30000 (43.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1231.2432s / 720541.9736 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.3851
env0_second_0:                 episode reward: -4.1000,                 loss: -0.3081
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 12921/30000 (43.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1227.2674s / 721769.2409 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3768
env0_second_0:                 episode reward: 0.6000,                 loss: -0.3128
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 12941/30000 (43.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1230.0467s / 722999.2877 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.3627
env0_second_0:                 episode reward: 2.2500,                 loss: -0.2848
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 12961/30000 (43.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1229.5459s / 724228.8335 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3430
env0_second_0:                 episode reward: 0.7000,                 loss: -0.2716
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12981/30000 (43.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1226.7032s / 725455.5367 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.3555
env0_second_0:                 episode reward: 1.0000,                 loss: -0.2554
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 13001/30000 (43.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1230.7131s / 726686.2498 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3324
env0_second_0:                 episode reward: 0.6000,                 loss: 0.1928
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 13021/30000 (43.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1234.3501s / 727920.5999 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3474
env0_second_0:                 episode reward: -2.3500,                 loss: -0.2394
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 13041/30000 (43.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1232.3587s / 729152.9587 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3536
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2373
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 13061/30000 (43.5367%),                 avg. length: 9840.2,                last time consumption/overall running time: 1209.6725s / 730362.6312 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.3430
env0_second_0:                 episode reward: 2.5500,                 loss: -0.0971
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 13081/30000 (43.6033%),                 avg. length: 9896.85,                last time consumption/overall running time: 1212.8420s / 731575.4732 s
env0_first_0:                 episode reward: 7.9000,                 loss: -0.3794
env0_second_0:                 episode reward: -7.9000,                 loss: -0.2501
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 13101/30000 (43.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1222.6349s / 732798.1080 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3699
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2456
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 13121/30000 (43.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1223.9311s / 734022.0391 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.3716
env0_second_0:                 episode reward: 1.2500,                 loss: -0.3058
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 13141/30000 (43.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1225.3045s / 735247.3436 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.3621
env0_second_0:                 episode reward: 0.4500,                 loss: -0.3041
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 13161/30000 (43.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1223.0279s / 736470.3715 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3644
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2952
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 13181/30000 (43.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1220.2596s / 737690.6311 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3515
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2761
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 13201/30000 (44.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1226.2233s / 738916.8544 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3564
env0_second_0:                 episode reward: 0.1500,                 loss: -0.3110
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 13221/30000 (44.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1195.3203s / 740112.1747 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3683
env0_second_0:                 episode reward: 0.5000,                 loss: -0.3176
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 13241/30000 (44.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1161.4034s / 741273.5781 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3581
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2996
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 13261/30000 (44.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1089.2378s / 742362.8159 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3497
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2485
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 13281/30000 (44.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1155.7358s / 743518.5517 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3591
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2791
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 13301/30000 (44.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1157.8844s / 744676.4362 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3575
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2799
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 13321/30000 (44.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1150.6511s / 745827.0873 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3532
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2735
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 13341/30000 (44.4700%),                 avg. length: 9893.95,                last time consumption/overall running time: 1141.4284s / 746968.5157 s
env0_first_0:                 episode reward: 8.3500,                 loss: -0.3366
env0_second_0:                 episode reward: -8.3500,                 loss: -0.2374
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 13361/30000 (44.5367%),                 avg. length: 7030.25,                last time consumption/overall running time: 832.3649s / 747800.8806 s
env0_first_0:                 episode reward: 20.3000,                 loss: -0.2248
env0_second_0:                 episode reward: -20.3000,                 loss: 0.1883
env1_first_0:                 episode reward: 19.6500,                 loss: nan
env1_second_0:                 episode reward: -19.6500,                 loss: nan
Episode: 13381/30000 (44.6033%),                 avg. length: 7937.3,                last time consumption/overall running time: 912.0017s / 748712.8823 s
env0_first_0:                 episode reward: 15.7000,                 loss: -0.2690
env0_second_0:                 episode reward: -15.7000,                 loss: -0.0663
env1_first_0:                 episode reward: 13.2000,                 loss: nan
env1_second_0:                 episode reward: -13.2000,                 loss: nan
Episode: 13401/30000 (44.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1144.0761s / 749856.9584 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.3137
env0_second_0:                 episode reward: -2.9000,                 loss: -0.2013
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 13421/30000 (44.7367%),                 avg. length: 9948.65,                last time consumption/overall running time: 1136.5655s / 750993.5239 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2979
env0_second_0:                 episode reward: -2.0000,                 loss: -0.1772
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 13441/30000 (44.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1144.0452s / 752137.5691 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.3317
env0_second_0:                 episode reward: -3.2000,                 loss: -0.1913
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 13461/30000 (44.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1151.1400s / 753288.7091 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.3166
env0_second_0:                 episode reward: 2.1000,                 loss: -0.1864
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 13481/30000 (44.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1148.2898s / 754436.9988 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3218
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2084
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 13501/30000 (45.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1167.2218s / 755604.2207 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.3053
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2026
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 13521/30000 (45.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1165.4976s / 756769.7183 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.3375
env0_second_0:                 episode reward: 1.9000,                 loss: -0.2562
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 13541/30000 (45.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1158.1920s / 757927.9103 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3376
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2806
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 13561/30000 (45.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1153.2099s / 759081.1202 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3317
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2483
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 13581/30000 (45.2700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1162.2781s / 760243.3983 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3725
env0_second_0:                 episode reward: -2.2500,                 loss: -0.3078
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 13601/30000 (45.3367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1158.4315s / 761401.8299 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3507
env0_second_0:                 episode reward: 1.1000,                 loss: -0.2672
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 13621/30000 (45.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1155.6034s / 762557.4333 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3437
env0_second_0:                 episode reward: 0.7000,                 loss: -0.2577
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 13641/30000 (45.4700%),                 avg. length: 9833.65,                last time consumption/overall running time: 1138.3458s / 763695.7790 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.3047
env0_second_0:                 episode reward: 1.7500,                 loss: -0.1856
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 13661/30000 (45.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1085.3754s / 764781.1544 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.3579
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2738
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 13681/30000 (45.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 987.7726s / 765768.9271 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3498
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2497
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 13701/30000 (45.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 984.5865s / 766753.5136 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3407
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2510
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 13721/30000 (45.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1020.4267s / 767773.9402 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.3229
env0_second_0:                 episode reward: -4.4500,                 loss: -0.2079
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 13741/30000 (45.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1169.4883s / 768943.4285 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3459
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2472
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 13761/30000 (45.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1235.0023s / 770178.4308 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.3421
env0_second_0:                 episode reward: 2.7000,                 loss: -0.2516
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 13781/30000 (45.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1190.8123s / 771369.2431 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3385
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1881
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 13801/30000 (46.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1147.7160s / 772516.9591 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3333
env0_second_0:                 episode reward: 0.8500,                 loss: -0.1646
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 13821/30000 (46.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1164.4060s / 773681.3651 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3387
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2377
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 13841/30000 (46.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1141.1790s / 774822.5441 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3227
env0_second_0:                 episode reward: 0.7000,                 loss: -0.2068
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 13861/30000 (46.2033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1141.4441s / 775963.9882 s