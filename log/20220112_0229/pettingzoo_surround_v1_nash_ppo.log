pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
random seed: [44, 57]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220112_0229/pettingzoo_surround_v1_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220112_0229/pettingzoo_surround_v1_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 1255.0,                last time consumption/overall running time: 8.6033s / 8.6033 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.0322
env0_second_0:                 episode reward: 5.0000,                 loss: -0.0438
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1387.3,                last time consumption/overall running time: 196.9343s / 205.5377 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0410
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0360
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1563.4,                last time consumption/overall running time: 220.7714s / 426.3091 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0409
env0_second_0:                 episode reward: -2.0000,                 loss: -0.0392
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1505.1,                last time consumption/overall running time: 211.9856s / 638.2947 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0412
env0_second_0:                 episode reward: -4.8500,                 loss: -0.0414
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1855.05,                last time consumption/overall running time: 258.9883s / 897.2830 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.0703
env0_second_0:                 episode reward: -4.4500,                 loss: -0.0698
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 2159.0,                last time consumption/overall running time: 300.4141s / 1197.6971 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.0712
env0_second_0:                 episode reward: 1.3500,                 loss: -0.0690
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 2339.45,                last time consumption/overall running time: 322.2306s / 1519.9277 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.0883
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0863
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 2333.1,                last time consumption/overall running time: 321.9575s / 1841.8852 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1011
env0_second_0:                 episode reward: 2.1500,                 loss: -0.0994
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2295.2,                last time consumption/overall running time: 317.6884s / 2159.5736 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0958
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0939
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2415.45,                last time consumption/overall running time: 332.5584s / 2492.1319 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1039
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0995
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 2530.3,                last time consumption/overall running time: 348.0184s / 2840.1503 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1048
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0988
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 2574.4,                last time consumption/overall running time: 352.1389s / 3192.2891 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1047
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1014
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 2438.7,                last time consumption/overall running time: 336.7981s / 3529.0872 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.1076
env0_second_0:                 episode reward: 2.2000,                 loss: -0.1047
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2514.05,                last time consumption/overall running time: 346.7076s / 3875.7949 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1043
env0_second_0:                 episode reward: 2.9000,                 loss: -0.1035
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2722.95,                last time consumption/overall running time: 372.4308s / 4248.2256 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.0884
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0833
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2544.2,                last time consumption/overall running time: 348.0271s / 4596.2527 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1016
env0_second_0:                 episode reward: 1.2500,                 loss: -0.0924
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 2716.55,                last time consumption/overall running time: 366.3210s / 4962.5737 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.0946
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0876
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2709.8,                last time consumption/overall running time: 365.9712s / 5328.5449 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1057
env0_second_0:                 episode reward: -0.7000,                 loss: -0.0987
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2678.85,                last time consumption/overall running time: 364.4052s / 5692.9501 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1044
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1033
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 2514.55,                last time consumption/overall running time: 341.1841s / 6034.1342 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1118
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1069
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 2589.55,                last time consumption/overall running time: 349.5052s / 6383.6394 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1064
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1036
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 2594.2,                last time consumption/overall running time: 354.2928s / 6737.9321 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1066
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0995
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2741.05,                last time consumption/overall running time: 373.7552s / 7111.6873 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1054
env0_second_0:                 episode reward: -1.3000,                 loss: -0.1025
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2737.4,                last time consumption/overall running time: 372.7497s / 7484.4370 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1016
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0966
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2736.65,                last time consumption/overall running time: 373.2525s / 7857.6895 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.1161
env0_second_0:                 episode reward: 2.5500,                 loss: -0.1037
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2923.95,                last time consumption/overall running time: 397.7093s / 8255.3988 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.1129
env0_second_0:                 episode reward: 0.9500,                 loss: -0.1059
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2714.0,                last time consumption/overall running time: 370.3029s / 8625.7017 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1114
env0_second_0:                 episode reward: 1.0000,                 loss: -0.1055
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2868.35,                last time consumption/overall running time: 393.7010s / 9019.4027 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1123
env0_second_0:                 episode reward: 1.8000,                 loss: -0.1065
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2861.45,                last time consumption/overall running time: 389.0107s / 9408.4133 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1113
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0974
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2820.05,                last time consumption/overall running time: 382.3330s / 9790.7463 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.1216
env0_second_0:                 episode reward: 0.7500,                 loss: -0.1125
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2606.85,                last time consumption/overall running time: 357.7732s / 10148.5196 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1172
env0_second_0:                 episode reward: -2.8000,                 loss: -0.1063
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2609.7,                last time consumption/overall running time: 355.3163s / 10503.8358 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1174
env0_second_0:                 episode reward: -3.1500,                 loss: -0.1069
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2691.4,                last time consumption/overall running time: 366.7138s / 10870.5497 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1092
env0_second_0:                 episode reward: -2.2000,                 loss: -0.0998
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2780.15,                last time consumption/overall running time: 373.4095s / 11243.9592 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1217
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1131
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2918.1,                last time consumption/overall running time: 392.7762s / 11636.7354 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1185
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1138
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2761.05,                last time consumption/overall running time: 370.7851s / 12007.5206 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1183
env0_second_0:                 episode reward: 1.5500,                 loss: -0.1042
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2912.25,                last time consumption/overall running time: 390.5615s / 12398.0820 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1167
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1077
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2661.1,                last time consumption/overall running time: 359.4702s / 12757.5522 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1209
env0_second_0:                 episode reward: 3.8500,                 loss: -0.1041
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2759.5,                last time consumption/overall running time: 370.9509s / 13128.5031 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1179
env0_second_0:                 episode reward: 1.5500,                 loss: -0.1130
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2931.3,                last time consumption/overall running time: 391.0758s / 13519.5789 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1261
env0_second_0:                 episode reward: 2.1000,                 loss: -0.1105
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2908.65,                last time consumption/overall running time: 393.7955s / 13913.3743 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1212
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1067
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2949.2,                last time consumption/overall running time: 395.7661s / 14309.1404 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.1209
env0_second_0:                 episode reward: 1.5000,                 loss: -0.1041
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2940.95,                last time consumption/overall running time: 394.2505s / 14703.3908 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1221
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1094
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2882.6,                last time consumption/overall running time: 386.4367s / 15089.8275 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.1222
env0_second_0:                 episode reward: 1.6500,                 loss: -0.1102
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2917.2,                last time consumption/overall running time: 392.2027s / 15482.0302 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1275
env0_second_0:                 episode reward: 1.4500,                 loss: -0.1147
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2851.55,                last time consumption/overall running time: 386.2326s / 15868.2628 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1317
env0_second_0:                 episode reward: 1.9000,                 loss: -0.1159
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2707.7,                last time consumption/overall running time: 366.5850s / 16234.8478 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.1305
env0_second_0:                 episode reward: 1.7000,                 loss: -0.1096
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2905.05,                last time consumption/overall running time: 390.1394s / 16624.9872 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1290
env0_second_0:                 episode reward: 2.3500,                 loss: -0.1112
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2725.65,                last time consumption/overall running time: 365.7645s / 16990.7517 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1227
env0_second_0:                 episode reward: 2.4500,                 loss: -0.1070
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 3124.9,                last time consumption/overall running time: 419.9690s / 17410.7207 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1205
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1072
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2967.05,                last time consumption/overall running time: 395.8239s / 17806.5446 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1330
env0_second_0:                 episode reward: 1.0000,                 loss: -0.1129
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2924.6,                last time consumption/overall running time: 391.9907s / 18198.5353 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1336
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1188
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2852.2,                last time consumption/overall running time: 386.4542s / 18584.9894 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.1349
env0_second_0:                 episode reward: 3.0000,                 loss: -0.1173
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2944.9,                last time consumption/overall running time: 395.6553s / 18980.6447 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1332
env0_second_0:                 episode reward: 1.1500,                 loss: -0.1148
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2973.75,                last time consumption/overall running time: 405.1329s / 19385.7776 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1393
env0_second_0:                 episode reward: 2.0500,                 loss: -0.1224
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2870.1,                last time consumption/overall running time: 388.1357s / 19773.9133 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1320
env0_second_0:                 episode reward: 1.3000,                 loss: -0.1113
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2938.1,                last time consumption/overall running time: 391.3112s / 20165.2245 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1391
env0_second_0:                 episode reward: 1.3000,                 loss: -0.1163
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2987.0,                last time consumption/overall running time: 400.6605s / 20565.8850 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1295
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1124
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2857.1,                last time consumption/overall running time: 381.9728s / 20947.8578 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1223
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0960
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2887.2,                last time consumption/overall running time: 389.0809s / 21336.9387 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.1270
env0_second_0:                 episode reward: 1.7000,                 loss: -0.1055
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2962.55,                last time consumption/overall running time: 395.6511s / 21732.5898 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1369
env0_second_0:                 episode reward: 1.8500,                 loss: -0.1167
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2849.1,                last time consumption/overall running time: 381.5250s / 22114.1148 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1356
env0_second_0:                 episode reward: 3.6500,                 loss: -0.1113
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2921.95,                last time consumption/overall running time: 390.5204s / 22504.6352 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1385
env0_second_0:                 episode reward: 3.1000,                 loss: -0.1148
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 3105.85,                last time consumption/overall running time: 414.3510s / 22918.9862 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.1317
env0_second_0:                 episode reward: 1.5000,                 loss: -0.0982
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 3004.3,                last time consumption/overall running time: 402.6823s / 23321.6685 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.1292
env0_second_0:                 episode reward: 1.5000,                 loss: -0.1001
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 3180.6,                last time consumption/overall running time: 426.1871s / 23747.8557 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1229
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0954
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 3063.55,                last time consumption/overall running time: 410.3723s / 24158.2280 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1288
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1054
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 3107.15,                last time consumption/overall running time: 422.4385s / 24580.6665 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1253
env0_second_0:                 episode reward: -1.9500,                 loss: -0.0972
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 3010.45,                last time consumption/overall running time: 412.9434s / 24993.6099 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1200
env0_second_0:                 episode reward: -3.6500,                 loss: -0.0835
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 3016.5,                last time consumption/overall running time: 412.2856s / 25405.8955 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1236
env0_second_0:                 episode reward: -4.5500,                 loss: -0.0951
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2925.1,                last time consumption/overall running time: 400.4545s / 25806.3500 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1403
env0_second_0:                 episode reward: -3.6000,                 loss: -0.1078
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2892.55,                last time consumption/overall running time: 397.4277s / 26203.7777 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1413
env0_second_0:                 episode reward: -3.1000,                 loss: -0.1093
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2910.65,                last time consumption/overall running time: 394.6721s / 26598.4498 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1356
env0_second_0:                 episode reward: -3.0500,                 loss: -0.1021
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2934.4,                last time consumption/overall running time: 394.3617s / 26992.8115 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1363
env0_second_0:                 episode reward: -3.8500,                 loss: -0.1120
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 3082.5,                last time consumption/overall running time: 413.8033s / 27406.6148 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1338
env0_second_0:                 episode reward: -2.8500,                 loss: -0.1057
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 3015.95,                last time consumption/overall running time: 407.0295s / 27813.6443 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.1450
env0_second_0:                 episode reward: -4.1000,                 loss: -0.1122
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2930.45,                last time consumption/overall running time: 399.5664s / 28213.2107 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1448
env0_second_0:                 episode reward: -5.0500,                 loss: -0.1129
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2996.1,                last time consumption/overall running time: 401.3205s / 28614.5312 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.1345
env0_second_0:                 episode reward: -4.3000,                 loss: -0.1076
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 2908.45,                last time consumption/overall running time: 392.4953s / 29007.0265 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.1339
env0_second_0:                 episode reward: -5.2000,                 loss: -0.0983
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 3153.6,                last time consumption/overall running time: 423.0931s / 29430.1196 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1316
env0_second_0:                 episode reward: -5.1000,                 loss: -0.1030
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 3329.05,                last time consumption/overall running time: 444.2138s / 29874.3334 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1320
env0_second_0:                 episode reward: -3.7000,                 loss: -0.1005
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 3173.7,                last time consumption/overall running time: 422.7717s / 30297.1051 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1340
env0_second_0:                 episode reward: -4.8000,                 loss: -0.1021
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 3079.15,                last time consumption/overall running time: 411.9541s / 30709.0592 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.1378
env0_second_0:                 episode reward: -4.8500,                 loss: -0.1014
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2920.85,                last time consumption/overall running time: 392.3397s / 31101.3989 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1370
env0_second_0:                 episode reward: -5.1500,                 loss: -0.0968
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 3001.1,                last time consumption/overall running time: 402.6952s / 31504.0941 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1364
env0_second_0:                 episode reward: -4.1500,                 loss: -0.0949
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 3246.95,                last time consumption/overall running time: 433.5535s / 31937.6476 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1393
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0943
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2870.9,                last time consumption/overall running time: 390.1674s / 32327.8150 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1402
env0_second_0:                 episode reward: -4.9500,                 loss: -0.1024
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 3040.15,                last time consumption/overall running time: 409.7182s / 32737.5332 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1266
env0_second_0:                 episode reward: -4.4000,                 loss: -0.0851
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 3132.25,                last time consumption/overall running time: 420.4404s / 33157.9736 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1269
env0_second_0:                 episode reward: -3.4000,                 loss: -0.0885
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 3156.45,                last time consumption/overall running time: 421.7677s / 33579.7413 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1261
env0_second_0:                 episode reward: -5.2500,                 loss: -0.0773
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 3010.35,                last time consumption/overall running time: 400.8410s / 33980.5823 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1219
env0_second_0:                 episode reward: -4.6000,                 loss: -0.0816
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 3291.7,                last time consumption/overall running time: 438.6360s / 34419.2183 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1193
env0_second_0:                 episode reward: -3.5500,                 loss: -0.0864
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 2984.95,                last time consumption/overall running time: 398.3827s / 34817.6010 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.1247
env0_second_0:                 episode reward: -5.5500,                 loss: -0.0915
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 3198.7,                last time consumption/overall running time: 424.1300s / 35241.7310 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1254
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0842
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 3181.95,                last time consumption/overall running time: 421.6617s / 35663.3928 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1272
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0849
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 3342.2,                last time consumption/overall running time: 446.1259s / 36109.5187 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1208
env0_second_0:                 episode reward: -3.4000,                 loss: -0.0866
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 3685.25,                last time consumption/overall running time: 491.8006s / 36601.3193 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1213
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0901
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 3376.2,                last time consumption/overall running time: 450.8768s / 37052.1961 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1254
env0_second_0:                 episode reward: -3.7000,                 loss: -0.0917
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 3709.35,                last time consumption/overall running time: 487.3749s / 37539.5710 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1315
env0_second_0:                 episode reward: -2.0500,                 loss: -0.0927
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 3556.15,                last time consumption/overall running time: 479.2693s / 38018.8403 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1340
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0959
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 3556.7,                last time consumption/overall running time: 480.8499s / 38499.6902 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1416
env0_second_0:                 episode reward: -3.3500,                 loss: -0.1050
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 3740.25,                last time consumption/overall running time: 500.5047s / 39000.1949 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1393
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0997
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 4098.45,                last time consumption/overall running time: 543.7965s / 39543.9913 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1326
env0_second_0:                 episode reward: 2.3500,                 loss: -0.0937
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 3625.65,                last time consumption/overall running time: 485.3961s / 40029.3875 s
env0_first_0:                 episode reward: -4.0500,                 loss: -0.1336
env0_second_0:                 episode reward: 4.0500,                 loss: -0.0920
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 3427.3,                last time consumption/overall running time: 459.8900s / 40489.2775 s
env0_first_0:                 episode reward: -4.7500,                 loss: -0.1428
env0_second_0:                 episode reward: 4.7500,                 loss: -0.0908
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 3573.15,                last time consumption/overall running time: 478.5422s / 40967.8197 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.1397
env0_second_0:                 episode reward: 2.2000,                 loss: -0.0863
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 3430.35,                last time consumption/overall running time: 458.9008s / 41426.7204 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1351
env0_second_0:                 episode reward: 3.5000,                 loss: -0.0837
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 3801.05,                last time consumption/overall running time: 513.8219s / 41940.5423 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1316
env0_second_0:                 episode reward: 1.1500,                 loss: -0.0846
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 3778.9,                last time consumption/overall running time: 510.1395s / 42450.6818 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.1333
env0_second_0:                 episode reward: 4.1000,                 loss: -0.0857
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 3763.75,                last time consumption/overall running time: 511.5875s / 42962.2693 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.1392
env0_second_0:                 episode reward: 3.5500,                 loss: -0.0888
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 3754.65,                last time consumption/overall running time: 510.8270s / 43473.0963 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1367
env0_second_0:                 episode reward: 3.8500,                 loss: -0.0948
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 3515.9,                last time consumption/overall running time: 477.1950s / 43950.2914 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.1458
env0_second_0:                 episode reward: 4.7000,                 loss: -0.0921
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 3323.9,                last time consumption/overall running time: 446.2622s / 44396.5535 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1480
env0_second_0:                 episode reward: 3.8000,                 loss: -0.1075
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 3283.55,                last time consumption/overall running time: 436.5373s / 44833.0908 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.1461
env0_second_0:                 episode reward: 5.3500,                 loss: -0.0905
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 3801.0,                last time consumption/overall running time: 508.6114s / 45341.7022 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.1335
env0_second_0:                 episode reward: 3.3000,                 loss: -0.0770
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 3654.95,                last time consumption/overall running time: 499.2008s / 45840.9030 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1284
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0682
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 3836.45,                last time consumption/overall running time: 513.3804s / 46354.2834 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1361
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0682
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 3718.85,                last time consumption/overall running time: 496.7222s / 46851.0056 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1468
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0973
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 3585.3,                last time consumption/overall running time: 480.4461s / 47331.4516 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.1521
env0_second_0:                 episode reward: 5.1000,                 loss: -0.1039
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 3316.4,                last time consumption/overall running time: 449.1444s / 47780.5960 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.1556
env0_second_0:                 episode reward: 5.9500,                 loss: -0.1054
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 3489.9,                last time consumption/overall running time: 471.6674s / 48252.2634 s
env0_first_0:                 episode reward: -4.5500,                 loss: -0.1524
env0_second_0:                 episode reward: 4.5500,                 loss: -0.0990
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 3548.8,                last time consumption/overall running time: 477.4291s / 48729.6925 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.1467
env0_second_0:                 episode reward: 2.7000,                 loss: -0.0935
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 3918.65,                last time consumption/overall running time: 527.4295s / 49257.1220 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1509
env0_second_0:                 episode reward: 2.4500,                 loss: -0.1055
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 3611.15,                last time consumption/overall running time: 485.6686s / 49742.7906 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1462
env0_second_0:                 episode reward: 3.1000,                 loss: -0.1039
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 3796.3,                last time consumption/overall running time: 508.1517s / 50250.9423 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1440
env0_second_0:                 episode reward: 2.9000,                 loss: -0.0890
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 3526.65,                last time consumption/overall running time: 473.0692s / 50724.0115 s
env0_first_0:                 episode reward: -4.7500,                 loss: -0.1434
env0_second_0:                 episode reward: 4.7500,                 loss: -0.0904
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 3820.25,                last time consumption/overall running time: 513.4604s / 51237.4719 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.1522
env0_second_0:                 episode reward: 4.0000,                 loss: -0.1100
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 3627.6,                last time consumption/overall running time: 484.0381s / 51721.5099 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1534
env0_second_0:                 episode reward: 1.9000,                 loss: -0.1068
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 3774.5,                last time consumption/overall running time: 504.0682s / 52225.5782 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.1503
env0_second_0:                 episode reward: 3.4500,                 loss: -0.1047
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 4004.55,                last time consumption/overall running time: 538.1373s / 52763.7155 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.1403
env0_second_0:                 episode reward: 1.9500,                 loss: -0.0940
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 3883.9,                last time consumption/overall running time: 519.6057s / 53283.3212 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.1303
env0_second_0:                 episode reward: 2.2500,                 loss: -0.0823
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 3905.1,                last time consumption/overall running time: 524.4480s / 53807.7692 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1409
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0982
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 3668.8,                last time consumption/overall running time: 492.3624s / 54300.1316 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.1301
env0_second_0:                 episode reward: 3.9500,                 loss: -0.0862
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 3943.25,                last time consumption/overall running time: 533.1444s / 54833.2761 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.1299
env0_second_0:                 episode reward: 3.2000,                 loss: -0.0930
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 3830.05,                last time consumption/overall running time: 520.4923s / 55353.7684 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.1249
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0845
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 4056.05,                last time consumption/overall running time: 542.2089s / 55895.9773 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1255
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0821
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 4521.35,                last time consumption/overall running time: 604.8652s / 56500.8425 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1270
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0842
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 4055.75,                last time consumption/overall running time: 544.7453s / 57045.5879 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1306
env0_second_0:                 episode reward: 1.8000,                 loss: -0.0942
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 3785.05,                last time consumption/overall running time: 505.4584s / 57551.0462 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.1304
env0_second_0:                 episode reward: 2.3000,                 loss: -0.0787
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 4331.55,                last time consumption/overall running time: 572.9565s / 58124.0027 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1291
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0856
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 4303.95,                last time consumption/overall running time: 566.8452s / 58690.8479 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1271
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0890
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 4379.3,                last time consumption/overall running time: 576.7546s / 59267.6024 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1345
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0943
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 4523.7,                last time consumption/overall running time: 596.6381s / 59864.2405 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1325
env0_second_0:                 episode reward: -0.6000,                 loss: -0.0938
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 3993.9,                last time consumption/overall running time: 527.9331s / 60392.1736 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1397
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0993
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 4148.9,                last time consumption/overall running time: 549.1499s / 60941.3235 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1414
env0_second_0:                 episode reward: -0.5500,                 loss: -0.0973
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 3703.75,                last time consumption/overall running time: 490.4924s / 61431.8159 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.1292
env0_second_0:                 episode reward: 1.4000,                 loss: -0.0843
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 3649.2,                last time consumption/overall running time: 481.9353s / 61913.7512 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1326
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0905
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 4092.3,                last time consumption/overall running time: 544.8004s / 62458.5516 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1356
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0976
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 4081.8,                last time consumption/overall running time: 534.9218s / 62993.4733 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1376
env0_second_0:                 episode reward: -1.3000,                 loss: -0.1011
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 4301.9,                last time consumption/overall running time: 592.0937s / 63585.5671 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1279
env0_second_0:                 episode reward: -2.3000,                 loss: -0.0913
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 4114.75,                last time consumption/overall running time: 566.7763s / 64152.3434 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1321
env0_second_0:                 episode reward: -3.1500,                 loss: -0.0901
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 3866.1,                last time consumption/overall running time: 530.9120s / 64683.2554 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.1388
env0_second_0:                 episode reward: -4.0500,                 loss: -0.0955
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 3594.1,                last time consumption/overall running time: 482.4841s / 65165.7394 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.1475
env0_second_0:                 episode reward: -4.7500,                 loss: -0.1047
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 3789.15,                last time consumption/overall running time: 485.2170s / 65650.9564 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.1407
env0_second_0:                 episode reward: -4.7500,                 loss: -0.1060
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 4304.6,                last time consumption/overall running time: 550.7662s / 66201.7226 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1303
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0862
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 4150.55,                last time consumption/overall running time: 526.0815s / 66727.8042 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1348
env0_second_0:                 episode reward: -2.9500,                 loss: -0.0902
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 4188.05,                last time consumption/overall running time: 530.4691s / 67258.2733 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1385
env0_second_0:                 episode reward: -5.2500,                 loss: -0.0988
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 4075.5,                last time consumption/overall running time: 518.1055s / 67776.3788 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1394
env0_second_0:                 episode reward: -1.8000,                 loss: -0.0939
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 4570.1,                last time consumption/overall running time: 567.5879s / 68343.9667 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1355
env0_second_0:                 episode reward: -1.3500,                 loss: -0.0898
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 4234.0,                last time consumption/overall running time: 531.8109s / 68875.7776 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1440
env0_second_0:                 episode reward: -0.6000,                 loss: -0.1054
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 4261.3,                last time consumption/overall running time: 536.7365s / 69412.5141 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1360
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0940
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 4129.45,                last time consumption/overall running time: 516.2432s / 69928.7574 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1419
env0_second_0:                 episode reward: -1.3500,                 loss: -0.0973
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 4038.35,                last time consumption/overall running time: 506.5465s / 70435.3039 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1382
env0_second_0:                 episode reward: -2.9500,                 loss: -0.0823
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 3687.8,                last time consumption/overall running time: 463.9667s / 70899.2706 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1233
env0_second_0:                 episode reward: -2.9000,                 loss: -0.0721
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 3615.1,                last time consumption/overall running time: 432.1807s / 71331.4513 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1385
env0_second_0:                 episode reward: -2.8000,                 loss: -0.0730
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 3201.85,                last time consumption/overall running time: 381.1020s / 71712.5533 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1301
env0_second_0:                 episode reward: -2.2500,                 loss: -0.0740
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 4239.9,                last time consumption/overall running time: 501.8051s / 72214.3584 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1334
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0918
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 4071.8,                last time consumption/overall running time: 486.3524s / 72700.7108 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1338
env0_second_0:                 episode reward: 0.8000,                 loss: -0.0899
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 3512.35,                last time consumption/overall running time: 416.4934s / 73117.2041 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1256
env0_second_0:                 episode reward: 1.8000,                 loss: -0.0777
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 4208.85,                last time consumption/overall running time: 495.3456s / 73612.5498 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.1381
env0_second_0:                 episode reward: -4.3000,                 loss: -0.1047
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 3671.15,                last time consumption/overall running time: 431.7844s / 74044.3341 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1459
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1075
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 4336.3,                last time consumption/overall running time: 509.6401s / 74553.9742 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1462
env0_second_0:                 episode reward: -3.1000,                 loss: -0.0940
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 4127.25,                last time consumption/overall running time: 492.1845s / 75046.1587 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1386
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0922
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 3901.65,                last time consumption/overall running time: 456.5471s / 75502.7058 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1414
env0_second_0:                 episode reward: -0.8000,                 loss: -0.0926
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 4069.6,                last time consumption/overall running time: 470.4548s / 75973.1606 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.1488
env0_second_0:                 episode reward: -3.9500,                 loss: -0.0966
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 4311.5,                last time consumption/overall running time: 499.9540s / 76473.1146 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1514
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1055
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 4511.2,                last time consumption/overall running time: 526.9886s / 77000.1033 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1417
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1000
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 3847.65,                last time consumption/overall running time: 452.9998s / 77453.1031 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1307
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0781
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 3230.7,                last time consumption/overall running time: 381.6282s / 77834.7313 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1362
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0842
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 3323.35,                last time consumption/overall running time: 387.0995s / 78221.8308 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1564
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0875
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 3664.65,                last time consumption/overall running time: 432.0221s / 78653.8529 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1518
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0976
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 4056.05,                last time consumption/overall running time: 474.8333s / 79128.6861 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1536
env0_second_0:                 episode reward: -3.3500,                 loss: -0.0964
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 4089.0,                last time consumption/overall running time: 473.9336s / 79602.6198 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1522
env0_second_0:                 episode reward: -3.6000,                 loss: -0.1015
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 4127.4,                last time consumption/overall running time: 481.9896s / 80084.6094 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1435
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0801
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 4204.1,                last time consumption/overall running time: 487.9812s / 80572.5905 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1447
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0878
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 4101.55,                last time consumption/overall running time: 476.6888s / 81049.2794 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1399
env0_second_0:                 episode reward: -2.2000,                 loss: -0.0846
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 3824.05,                last time consumption/overall running time: 442.9548s / 81492.2341 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.1424
env0_second_0:                 episode reward: -5.5000,                 loss: -0.0802
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 4044.6,                last time consumption/overall running time: 460.6347s / 81952.8688 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1477
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0992
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 4417.8,                last time consumption/overall running time: 501.0442s / 82453.9130 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1504
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0928
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 4292.75,                last time consumption/overall running time: 480.4508s / 82934.3638 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1450
env0_second_0:                 episode reward: -2.7000,                 loss: -0.0908
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 4250.35,                last time consumption/overall running time: 470.8956s / 83405.2594 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1536
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0929
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 3999.8,                last time consumption/overall running time: 448.4954s / 83853.7549 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1430
env0_second_0:                 episode reward: -1.9500,                 loss: -0.0781
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 3697.45,                last time consumption/overall running time: 410.8480s / 84264.6028 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1369
env0_second_0:                 episode reward: 0.9000,                 loss: -0.0739
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 3702.85,                last time consumption/overall running time: 414.2954s / 84678.8983 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1480
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0794
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 3751.65,                last time consumption/overall running time: 416.4961s / 85095.3944 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1430
env0_second_0:                 episode reward: -3.4500,                 loss: -0.0580
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 3655.5,                last time consumption/overall running time: 404.5632s / 85499.9576 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1506
env0_second_0:                 episode reward: -2.8000,                 loss: -0.0814
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 4011.7,                last time consumption/overall running time: 439.7416s / 85939.6992 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1440
env0_second_0:                 episode reward: -3.9000,                 loss: -0.0589
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 4088.1,                last time consumption/overall running time: 443.4067s / 86383.1060 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.1544
env0_second_0:                 episode reward: -4.4500,                 loss: -0.0738
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 4005.05,                last time consumption/overall running time: 436.6029s / 86819.7088 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1475
env0_second_0:                 episode reward: -4.8000,                 loss: -0.0755
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 4329.55,                last time consumption/overall running time: 464.9138s / 87284.6227 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1486
env0_second_0:                 episode reward: -3.8500,                 loss: -0.0720
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 3956.05,                last time consumption/overall running time: 431.6144s / 87716.2371 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1499
env0_second_0:                 episode reward: -2.9000,                 loss: -0.0791
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 3898.55,                last time consumption/overall running time: 417.3750s / 88133.6121 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1528
env0_second_0:                 episode reward: -3.8500,                 loss: -0.0751
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 4316.6,                last time consumption/overall running time: 458.0101s / 88591.6221 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1480
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0739
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 4272.55,                last time consumption/overall running time: 458.2940s / 89049.9162 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1464
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0762
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 4114.7,                last time consumption/overall running time: 438.1938s / 89488.1100 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.1474
env0_second_0:                 episode reward: 2.7000,                 loss: -0.0696
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 3223.6,                last time consumption/overall running time: 338.2433s / 89826.3533 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.1507
env0_second_0:                 episode reward: 6.6000,                 loss: -0.0845
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 3914.35,                last time consumption/overall running time: 407.5982s / 90233.9515 s
env0_first_0:                 episode reward: -4.6500,                 loss: -0.1538
env0_second_0:                 episode reward: 4.6500,                 loss: -0.0789
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 4081.85,                last time consumption/overall running time: 424.6655s / 90658.6170 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1444
env0_second_0:                 episode reward: 4.4000,                 loss: -0.0770
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 3994.2,                last time consumption/overall running time: 407.9570s / 91066.5740 s
env0_first_0:                 episode reward: -5.2000,                 loss: -0.1481
env0_second_0:                 episode reward: 5.2000,                 loss: -0.0720
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 4331.1,                last time consumption/overall running time: 459.8295s / 91526.4034 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.1506
env0_second_0:                 episode reward: 2.8000,                 loss: -0.0635
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 4276.05,                last time consumption/overall running time: 445.7561s / 91972.1596 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1564
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0593
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 4288.3,                last time consumption/overall running time: 446.8176s / 92418.9771 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1492
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0606
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 3894.9,                last time consumption/overall running time: 409.7947s / 92828.7718 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1610
env0_second_0:                 episode reward: 1.3000,                 loss: -0.0710
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 4041.6,                last time consumption/overall running time: 418.4882s / 93247.2600 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.1527
env0_second_0:                 episode reward: 1.5000,                 loss: -0.0621
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 4374.7,                last time consumption/overall running time: 454.6936s / 93701.9537 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1493
env0_second_0:                 episode reward: 2.4000,                 loss: -0.0658
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 4368.15,                last time consumption/overall running time: 453.1272s / 94155.0808 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.1506
env0_second_0:                 episode reward: 2.2500,                 loss: -0.0627
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 4288.3,                last time consumption/overall running time: 444.9647s / 94600.0456 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.1620
env0_second_0:                 episode reward: 2.8000,                 loss: -0.0601
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 4540.15,                last time consumption/overall running time: 473.3503s / 95073.3959 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1533
env0_second_0:                 episode reward: 3.5000,                 loss: -0.0553
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 3911.85,                last time consumption/overall running time: 402.0718s / 95475.4677 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.1568
env0_second_0:                 episode reward: 2.3000,                 loss: -0.0709
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 3967.45,                last time consumption/overall running time: 415.2306s / 95890.6982 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1508
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0796
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 3618.35,                last time consumption/overall running time: 371.2600s / 96261.9582 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1596
env0_second_0:                 episode reward: 4.2000,                 loss: -0.0769
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 4160.85,                last time consumption/overall running time: 427.7109s / 96689.6692 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.1557
env0_second_0:                 episode reward: 2.9500,                 loss: -0.0807
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 2756.65,                last time consumption/overall running time: 283.6738s / 96973.3429 s
env0_first_0:                 episode reward: -8.1000,                 loss: -0.1330
env0_second_0:                 episode reward: 8.1000,                 loss: -0.0743
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 2276.15,                last time consumption/overall running time: 236.5985s / 97209.9414 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.1374
env0_second_0:                 episode reward: 9.3000,                 loss: -0.0680
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 2895.7,                last time consumption/overall running time: 303.2657s / 97513.2071 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.1377
env0_second_0:                 episode reward: 8.3500,                 loss: -0.0844
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 3909.3,                last time consumption/overall running time: 407.8166s / 97921.0237 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.1315
env0_second_0:                 episode reward: 5.0500,                 loss: -0.0552
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 4297.85,                last time consumption/overall running time: 439.9945s / 98361.0181 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1345
env0_second_0:                 episode reward: 2.3500,                 loss: -0.0503
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 3868.85,                last time consumption/overall running time: 402.3091s / 98763.3272 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.1432
env0_second_0:                 episode reward: 1.9500,                 loss: -0.0634
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 2874.35,                last time consumption/overall running time: 293.4571s / 99056.7843 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.1257
env0_second_0:                 episode reward: 5.3000,                 loss: -0.0555
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 2121.75,                last time consumption/overall running time: 222.1450s / 99278.9294 s
env0_first_0:                 episode reward: -7.9000,                 loss: -0.1114
env0_second_0:                 episode reward: 7.9000,                 loss: -0.0396
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 3841.25,                last time consumption/overall running time: 399.1277s / 99678.0571 s
env0_first_0:                 episode reward: -4.5500,                 loss: -0.1418
env0_second_0:                 episode reward: 4.5500,                 loss: -0.0717
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 4292.05,                last time consumption/overall running time: 439.0817s / 100117.1388 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1370
env0_second_0:                 episode reward: 3.8000,                 loss: -0.0691
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 4019.8,                last time consumption/overall running time: 410.0555s / 100527.1942 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1393
env0_second_0:                 episode reward: 3.5000,                 loss: -0.0653
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 4238.15,                last time consumption/overall running time: 438.5804s / 100965.7747 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.1379
env0_second_0:                 episode reward: 3.2000,                 loss: -0.0714
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 3746.95,                last time consumption/overall running time: 395.5551s / 101361.3298 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.1378
env0_second_0:                 episode reward: 4.6000,                 loss: -0.0628
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 3586.5,                last time consumption/overall running time: 370.1291s / 101731.4589 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.1449
env0_second_0:                 episode reward: 6.5000,                 loss: -0.0356
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 3980.85,                last time consumption/overall running time: 403.4815s / 102134.9404 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1360
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0442
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 4072.8,                last time consumption/overall running time: 418.6989s / 102553.6392 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1394
env0_second_0:                 episode reward: 1.5500,                 loss: -0.0552
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 4086.75,                last time consumption/overall running time: 420.9941s / 102974.6333 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1329
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0550
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 3312.65,                last time consumption/overall running time: 337.3851s / 103312.0184 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.1267
env0_second_0:                 episode reward: 6.2500,                 loss: -0.0312
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 3248.65,                last time consumption/overall running time: 332.5997s / 103644.6181 s
env0_first_0:                 episode reward: -5.2000,                 loss: -0.1379
env0_second_0:                 episode reward: 5.2000,                 loss: -0.0533
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 4088.3,                last time consumption/overall running time: 414.4562s / 104059.0742 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.1464
env0_second_0:                 episode reward: 1.3500,                 loss: -0.0409
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 4300.9,                last time consumption/overall running time: 435.4116s / 104494.4859 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1496
env0_second_0:                 episode reward: 1.9000,                 loss: -0.0580
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 4460.4,                last time consumption/overall running time: 458.9118s / 104953.3976 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1588
env0_second_0:                 episode reward: 1.5500,                 loss: -0.0775
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 4163.4,                last time consumption/overall running time: 423.5608s / 105376.9584 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.1546
env0_second_0:                 episode reward: 3.4000,                 loss: -0.0720
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 4603.95,                last time consumption/overall running time: 473.9927s / 105850.9511 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1393
env0_second_0:                 episode reward: -0.2000,                 loss: -0.0564
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 4228.4,                last time consumption/overall running time: 437.7205s / 106288.6716 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1354
env0_second_0:                 episode reward: 2.6000,                 loss: -0.0300
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 4498.55,                last time consumption/overall running time: 462.4883s / 106751.1599 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1382
env0_second_0:                 episode reward: 1.2000,                 loss: -0.0588
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 4308.1,                last time consumption/overall running time: 435.4446s / 107186.6044 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1530
env0_second_0:                 episode reward: 2.1500,                 loss: -0.0739
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 4618.2,                last time consumption/overall running time: 465.9209s / 107652.5253 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1419
env0_second_0:                 episode reward: 1.0500,                 loss: -0.0690
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 4186.9,                last time consumption/overall running time: 422.5995s / 108075.1248 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.1420
env0_second_0:                 episode reward: 3.2500,                 loss: -0.0789
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 4298.3,                last time consumption/overall running time: 434.5386s / 108509.6634 s
env0_first_0:                 episode reward: -3.7500,                 loss: -0.1436
env0_second_0:                 episode reward: 3.7500,                 loss: -0.0752
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 4653.15,                last time consumption/overall running time: 476.4376s / 108986.1011 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1355
env0_second_0:                 episode reward: 3.6500,                 loss: -0.0674
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 4632.6,                last time consumption/overall running time: 465.8906s / 109451.9917 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1340
env0_second_0:                 episode reward: 0.3000,                 loss: -0.0573
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 3939.2,                last time consumption/overall running time: 400.1762s / 109852.1679 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.1402
env0_second_0:                 episode reward: 2.1000,                 loss: -0.0558
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 4442.6,                last time consumption/overall running time: 448.4087s / 110300.5766 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1390
env0_second_0:                 episode reward: 2.0500,                 loss: -0.0686
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 3643.4,                last time consumption/overall running time: 362.5442s / 110663.1208 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1404
env0_second_0:                 episode reward: 3.8500,                 loss: -0.0717
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 4049.3,                last time consumption/overall running time: 415.8207s / 111078.9415 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.1451
env0_second_0:                 episode reward: 1.7500,                 loss: -0.0656
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 4148.7,                last time consumption/overall running time: 424.2720s / 111503.2136 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.1474
env0_second_0:                 episode reward: 3.3500,                 loss: -0.0684
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 4271.4,                last time consumption/overall running time: 432.4022s / 111935.6157 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.1458
env0_second_0:                 episode reward: 1.9500,                 loss: -0.0653
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 4307.85,                last time consumption/overall running time: 429.5758s / 112365.1915 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1385
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0629
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 4474.25,                last time consumption/overall running time: 447.8609s / 112813.0524 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1457
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0417
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 4527.9,                last time consumption/overall running time: 455.7563s / 113268.8087 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1451
env0_second_0:                 episode reward: 0.2000,                 loss: -0.0447
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 4701.8,                last time consumption/overall running time: 474.9041s / 113743.7129 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1469
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0508
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 3946.55,                last time consumption/overall running time: 393.0367s / 114136.7495 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1295
env0_second_0:                 episode reward: 1.5500,                 loss: -0.0417
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 4259.5,                last time consumption/overall running time: 424.3051s / 114561.0547 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.1476
env0_second_0:                 episode reward: 2.6500,                 loss: -0.0480
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 3749.05,                last time consumption/overall running time: 371.5368s / 114932.5915 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1452
env0_second_0:                 episode reward: -0.8500,                 loss: -0.0589
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 4289.9,                last time consumption/overall running time: 427.5171s / 115360.1085 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1363
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0388
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 3927.55,                last time consumption/overall running time: 394.5945s / 115754.7030 s
env0_first_0:                 episode reward: -3.1500,                 loss: -0.1257
env0_second_0:                 episode reward: 3.1500,                 loss: -0.0351
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 4151.65,                last time consumption/overall running time: 419.9642s / 116174.6672 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.1302
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0294
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 4514.55,                last time consumption/overall running time: 460.4952s / 116635.1624 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1365
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0238
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 4474.75,                last time consumption/overall running time: 453.3025s / 117088.4649 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1380
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0342
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 4833.2,                last time consumption/overall running time: 487.1688s / 117575.6337 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1393
env0_second_0:                 episode reward: -3.2500,                 loss: -0.0347
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 4578.25,                last time consumption/overall running time: 465.8310s / 118041.4647 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1362
env0_second_0:                 episode reward: -1.9500,                 loss: -0.0486
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 4847.9,                last time consumption/overall running time: 486.9905s / 118528.4552 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1390
env0_second_0:                 episode reward: -3.4500,                 loss: -0.0292
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 4803.05,                last time consumption/overall running time: 486.3709s / 119014.8261 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1475
env0_second_0:                 episode reward: -2.4000,                 loss: -0.0478
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 4725.2,                last time consumption/overall running time: 475.7844s / 119490.6105 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1521
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0549
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 4959.65,                last time consumption/overall running time: 494.6672s / 119985.2778 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1449
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0332
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 4854.95,                last time consumption/overall running time: 489.6044s / 120474.8822 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1495
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0522
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 5247.1,                last time consumption/overall running time: 530.6766s / 121005.5587 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1473
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0477
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 5244.7,                last time consumption/overall running time: 524.6314s / 121530.1901 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.1414
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0355
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 5092.55,                last time consumption/overall running time: 509.1036s / 122039.2938 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1375
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0198
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 5019.4,                last time consumption/overall running time: 500.2513s / 122539.5451 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1419
env0_second_0:                 episode reward: 0.3500,                 loss: -0.0328
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 5058.75,                last time consumption/overall running time: 505.7083s / 123045.2534 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1435
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0135
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 4325.5,                last time consumption/overall running time: 439.1425s / 123484.3959 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1371
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0518
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 4187.4,                last time consumption/overall running time: 424.8131s / 123909.2091 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1492
env0_second_0:                 episode reward: -2.1000,                 loss: -0.0456
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 5004.15,                last time consumption/overall running time: 511.7855s / 124420.9946 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1421
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0344
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 5194.7,                last time consumption/overall running time: 529.4853s / 124950.4799 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1459
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0378
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 5096.1,                last time consumption/overall running time: 513.8930s / 125464.3729 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1510
env0_second_0:                 episode reward: -2.9000,                 loss: -0.0455
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 4766.65,                last time consumption/overall running time: 483.5708s / 125947.9437 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1604
env0_second_0:                 episode reward: -2.8000,                 loss: -0.0320
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 4809.05,                last time consumption/overall running time: 485.9790s / 126433.9227 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1528
env0_second_0:                 episode reward: -3.2500,                 loss: -0.0589
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 5242.8,                last time consumption/overall running time: 540.5041s / 126974.4267 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1542
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0536
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 4577.4,                last time consumption/overall running time: 463.6375s / 127438.0642 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1681
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0550
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 5813.45,                last time consumption/overall running time: 582.0583s / 128020.1225 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1562
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0666
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 5347.0,                last time consumption/overall running time: 533.6659s / 128553.7883 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1606
env0_second_0:                 episode reward: -0.9500,                 loss: -0.0771
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 5714.25,                last time consumption/overall running time: 571.1764s / 129124.9647 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1566
env0_second_0:                 episode reward: 0.6500,                 loss: -0.0654
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 5699.45,                last time consumption/overall running time: 572.7866s / 129697.7513 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1527
env0_second_0:                 episode reward: -3.1500,                 loss: -0.0497
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 5627.5,                last time consumption/overall running time: 564.4213s / 130262.1726 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1550
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0527
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 5779.45,                last time consumption/overall running time: 588.8772s / 130851.0498 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1427
env0_second_0:                 episode reward: -1.5000,                 loss: -0.0359
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 5342.6,                last time consumption/overall running time: 538.0864s / 131389.1361 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.1487
env0_second_0:                 episode reward: -2.6000,                 loss: -0.0366
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 5056.85,                last time consumption/overall running time: 515.9812s / 131905.1174 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1560
env0_second_0:                 episode reward: -2.0500,                 loss: -0.0484
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 5787.2,                last time consumption/overall running time: 579.2481s / 132484.3655 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1504
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0333
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 6316.1,                last time consumption/overall running time: 639.4926s / 133123.8582 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1453
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0209
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 6079.1,                last time consumption/overall running time: 609.0676s / 133732.9258 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1486
env0_second_0:                 episode reward: -1.7000,                 loss: -0.0233
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 5722.25,                last time consumption/overall running time: 579.4148s / 134312.3406 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1532
env0_second_0:                 episode reward: -3.0500,                 loss: -0.0435
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 5634.95,                last time consumption/overall running time: 561.1569s / 134873.4975 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1445
env0_second_0:                 episode reward: -2.0000,                 loss: -0.0532
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 5531.85,                last time consumption/overall running time: 546.8658s / 135420.3633 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1549
env0_second_0:                 episode reward: -3.9000,                 loss: -0.0587
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 5137.05,                last time consumption/overall running time: 510.4440s / 135930.8073 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1452
env0_second_0:                 episode reward: 1.9000,                 loss: -0.0529
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 6203.4,                last time consumption/overall running time: 621.5690s / 136552.3763 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1407
env0_second_0:                 episode reward: -0.8000,                 loss: -0.0507
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 6199.75,                last time consumption/overall running time: 623.9616s / 137176.3379 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1364
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0457
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 5543.55,                last time consumption/overall running time: 557.0742s / 137733.4121 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.1492
env0_second_0:                 episode reward: -5.2000,                 loss: -0.0576
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 5806.5,                last time consumption/overall running time: 583.2135s / 138316.6256 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1474
env0_second_0:                 episode reward: -5.1000,                 loss: -0.0514
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 5169.15,                last time consumption/overall running time: 520.4910s / 138837.1166 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.1593
env0_second_0:                 episode reward: -6.0500,                 loss: -0.0587
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 5860.2,                last time consumption/overall running time: 603.3805s / 139440.4971 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1472
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0400
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 5603.7,                last time consumption/overall running time: 567.0202s / 140007.5172 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1524
env0_second_0:                 episode reward: -3.8000,                 loss: -0.0532
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 5929.1,                last time consumption/overall running time: 582.9386s / 140590.4558 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1464
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0471
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 5212.45,                last time consumption/overall running time: 514.3609s / 141104.8167 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.1466
env0_second_0:                 episode reward: -4.3000,                 loss: -0.0345
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 5393.5,                last time consumption/overall running time: 539.7389s / 141644.5556 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1528
env0_second_0:                 episode reward: -7.0000,                 loss: -0.0497
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 5346.35,                last time consumption/overall running time: 537.5626s / 142182.1182 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1450
env0_second_0:                 episode reward: -5.3000,                 loss: -0.0575
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 5140.75,                last time consumption/overall running time: 513.9888s / 142696.1070 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.1410
env0_second_0:                 episode reward: -5.6000,                 loss: -0.0406
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 6230.65,                last time consumption/overall running time: 619.7930s / 143315.9000 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1421
env0_second_0:                 episode reward: -3.7500,                 loss: -0.0197
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 6043.05,                last time consumption/overall running time: 601.4925s / 143917.3925 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.1339
env0_second_0:                 episode reward: -4.0500,                 loss: -0.0209
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 6110.75,                last time consumption/overall running time: 612.7980s / 144530.1905 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.1484
env0_second_0:                 episode reward: -4.2500,                 loss: -0.0260
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 6235.1,                last time consumption/overall running time: 625.6713s / 145155.8618 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1541
env0_second_0:                 episode reward: -4.4000,                 loss: -0.0479
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 6008.45,                last time consumption/overall running time: 601.9525s / 145757.8143 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1604
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0559
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 5575.9,                last time consumption/overall running time: 561.4592s / 146319.2735 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.1622
env0_second_0:                 episode reward: -5.7000,                 loss: -0.0772
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 5684.15,                last time consumption/overall running time: 573.7789s / 146893.0524 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.1550
env0_second_0:                 episode reward: -4.7500,                 loss: -0.0505
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 5814.15,                last time consumption/overall running time: 581.4600s / 147474.5124 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.1552
env0_second_0:                 episode reward: -4.3500,                 loss: -0.0691
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 5603.4,                last time consumption/overall running time: 557.5124s / 148032.0249 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1606
env0_second_0:                 episode reward: -4.4000,                 loss: -0.0717
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 5723.95,                last time consumption/overall running time: 571.4743s / 148603.4991 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1557
env0_second_0:                 episode reward: -3.6500,                 loss: -0.0694
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 5789.0,                last time consumption/overall running time: 580.5129s / 149184.0120 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1665
env0_second_0:                 episode reward: -3.7500,                 loss: -0.0750
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 5519.35,                last time consumption/overall running time: 555.5850s / 149739.5970 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1601
env0_second_0:                 episode reward: -2.3000,                 loss: -0.0851
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 5978.85,                last time consumption/overall running time: 610.5946s / 150350.1916 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.1709
env0_second_0:                 episode reward: -4.2000,                 loss: -0.0913
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 5989.9,                last time consumption/overall running time: 597.9935s / 150948.1850 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1490
env0_second_0:                 episode reward: -3.7000,                 loss: -0.0475
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 5921.25,                last time consumption/overall running time: 592.7963s / 151540.9813 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1545
env0_second_0:                 episode reward: -2.0000,                 loss: -0.0348
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 5907.85,                last time consumption/overall running time: 584.8595s / 152125.8408 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1444
env0_second_0:                 episode reward: -3.7500,                 loss: -0.0540
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 6130.75,                last time consumption/overall running time: 612.2883s / 152738.1291 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1478
env0_second_0:                 episode reward: -3.4500,                 loss: -0.0762
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 5638.3,                last time consumption/overall running time: 565.2639s / 153303.3930 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.1549
env0_second_0:                 episode reward: -4.1000,                 loss: -0.0678
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 5656.65,                last time consumption/overall running time: 566.3308s / 153869.7238 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1438
env0_second_0:                 episode reward: -5.1500,                 loss: 0.0182
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 5751.1,                last time consumption/overall running time: 571.1500s / 154440.8738 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.1499
env0_second_0:                 episode reward: -5.8500,                 loss: -0.0255
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 5460.75,                last time consumption/overall running time: 550.5091s / 154991.3829 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.1608
env0_second_0:                 episode reward: -6.1000,                 loss: -0.0800
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 5581.3,                last time consumption/overall running time: 548.3889s / 155539.7718 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1655
env0_second_0:                 episode reward: -3.9000,                 loss: -0.0724
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 5537.4,                last time consumption/overall running time: 547.9780s / 156087.7499 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1594
env0_second_0:                 episode reward: -5.1000,                 loss: -0.0767
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 5971.55,                last time consumption/overall running time: 606.9896s / 156694.7395 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1564
env0_second_0:                 episode reward: -5.3000,                 loss: -0.0654
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 5869.1,                last time consumption/overall running time: 586.2798s / 157281.0193 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1527
env0_second_0:                 episode reward: -4.9500,                 loss: -0.0725
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 5436.3,                last time consumption/overall running time: 530.5039s / 157811.5231 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1570
env0_second_0:                 episode reward: -3.4000,                 loss: -0.0821
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 6191.3,                last time consumption/overall running time: 615.8098s / 158427.3329 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.1606
env0_second_0:                 episode reward: -4.3500,                 loss: -0.0769
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 4941.9,                last time consumption/overall running time: 463.4361s / 158890.7690 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.1687
env0_second_0:                 episode reward: -6.0500,                 loss: -0.0989
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 5381.55,                last time consumption/overall running time: 493.9835s / 159384.7525 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1700
env0_second_0:                 episode reward: -6.6000,                 loss: -0.1093
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 5905.15,                last time consumption/overall running time: 571.6051s / 159956.3576 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1640
env0_second_0:                 episode reward: -3.8500,                 loss: -0.0930
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 6034.4,                last time consumption/overall running time: 566.6098s / 160522.9674 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.1603
env0_second_0:                 episode reward: -4.0000,                 loss: -0.0776
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 5266.45,                last time consumption/overall running time: 500.8843s / 161023.8517 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1705
env0_second_0:                 episode reward: -5.0500,                 loss: -0.0979
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 5005.05,                last time consumption/overall running time: 465.7512s / 161489.6029 s
env0_first_0:                 episode reward: 8.0000,                 loss: -0.1674
env0_second_0:                 episode reward: -8.0000,                 loss: -0.0730
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 4873.05,                last time consumption/overall running time: 463.8393s / 161953.4422 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1672
env0_second_0:                 episode reward: -7.0000,                 loss: -0.0866
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 5711.85,                last time consumption/overall running time: 521.1218s / 162474.5640 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.1483
env0_second_0:                 episode reward: -5.2000,                 loss: -0.0754
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 5920.25,                last time consumption/overall running time: 563.8089s / 163038.3729 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.1474
env0_second_0:                 episode reward: -4.8500,                 loss: -0.0855
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 5937.3,                last time consumption/overall running time: 587.5418s / 163625.9147 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.1556
env0_second_0:                 episode reward: -7.1500,                 loss: -0.0841
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 5516.25,                last time consumption/overall running time: 542.3387s / 164168.2534 s
env0_first_0:                 episode reward: 6.9000,                 loss: -0.1686
env0_second_0:                 episode reward: -6.9000,                 loss: -0.0854
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 6423.85,                last time consumption/overall running time: 625.7015s / 164793.9549 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1570
env0_second_0:                 episode reward: -2.1000,                 loss: -0.0842
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 6063.7,                last time consumption/overall running time: 537.9102s / 165331.8651 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1519
env0_second_0:                 episode reward: -3.3500,                 loss: -0.0954
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 5347.25,                last time consumption/overall running time: 502.9976s / 165834.8627 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1458
env0_second_0:                 episode reward: -3.1500,                 loss: -0.0659
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 5300.9,                last time consumption/overall running time: 499.5322s / 166334.3949 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.1606
env0_second_0:                 episode reward: -5.4500,                 loss: -0.0754
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 5874.95,                last time consumption/overall running time: 616.6477s / 166951.0426 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.1528
env0_second_0:                 episode reward: -5.0000,                 loss: -0.0735
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 5693.9,                last time consumption/overall running time: 506.2581s / 167457.3008 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.1566
env0_second_0:                 episode reward: -6.5000,                 loss: -0.0869
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 5607.85,                last time consumption/overall running time: 497.8232s / 167955.1240 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1523
env0_second_0:                 episode reward: -5.3000,                 loss: -0.0807
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 5653.05,                last time consumption/overall running time: 496.5777s / 168451.7016 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.1713
env0_second_0:                 episode reward: -5.5000,                 loss: -0.0958
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 6560.65,                last time consumption/overall running time: 585.3410s / 169037.0426 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1655
env0_second_0:                 episode reward: -3.3500,                 loss: -0.0864
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 5728.65,                last time consumption/overall running time: 502.5098s / 169539.5525 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.1635
env0_second_0:                 episode reward: -5.6000,                 loss: -0.0992
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 5803.45,                last time consumption/overall running time: 539.0290s / 170078.5814 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.1575
env0_second_0:                 episode reward: -4.3500,                 loss: -0.0831
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 5646.9,                last time consumption/overall running time: 517.2309s / 170595.8123 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1718
env0_second_0:                 episode reward: -5.2500,                 loss: -0.0831
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 6106.15,                last time consumption/overall running time: 552.5217s / 171148.3340 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.1659
env0_second_0:                 episode reward: -6.5000,                 loss: -0.0986
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 6343.15,                last time consumption/overall running time: 564.9122s / 171713.2462 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1577
env0_second_0:                 episode reward: -4.6000,                 loss: -0.0933
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 7146.2,                last time consumption/overall running time: 649.4080s / 172362.6542 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1525
env0_second_0:                 episode reward: -2.4000,                 loss: -0.0778
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 6128.7,                last time consumption/overall running time: 568.2306s / 172930.8848 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1459
env0_second_0:                 episode reward: -4.1500,                 loss: -0.0636
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 6311.45,                last time consumption/overall running time: 680.2392s / 173611.1240 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.1540
env0_second_0:                 episode reward: -4.0500,                 loss: -0.0840
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 6031.8,                last time consumption/overall running time: 540.0450s / 174151.1689 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.1515
env0_second_0:                 episode reward: -3.3000,                 loss: -0.0816
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 6283.7,                last time consumption/overall running time: 564.5178s / 174715.6867 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.1531
env0_second_0:                 episode reward: -5.5000,                 loss: -0.0665
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 5115.95,                last time consumption/overall running time: 460.6942s / 175176.3809 s
env0_first_0:                 episode reward: 7.9000,                 loss: -0.1690
env0_second_0:                 episode reward: -7.9000,                 loss: -0.0701
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 5065.35,                last time consumption/overall running time: 459.9029s / 175636.2838 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1536
env0_second_0:                 episode reward: -5.0500,                 loss: -0.0732
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 5396.1,                last time consumption/overall running time: 515.1069s / 176151.3907 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.1595
env0_second_0:                 episode reward: -6.0500,                 loss: -0.0802
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 5738.55,                last time consumption/overall running time: 588.9796s / 176740.3702 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.1573
env0_second_0:                 episode reward: -4.7500,                 loss: -0.0633
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 5897.4,                last time consumption/overall running time: 516.6804s / 177257.0506 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1616
env0_second_0:                 episode reward: -3.7500,                 loss: -0.0772
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 5386.85,                last time consumption/overall running time: 476.0863s / 177733.1370 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1742
env0_second_0:                 episode reward: -3.3500,                 loss: -0.0868
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 5571.8,                last time consumption/overall running time: 493.0698s / 178226.2068 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1644
env0_second_0:                 episode reward: -4.9500,                 loss: -0.0864
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 6348.45,                last time consumption/overall running time: 676.4664s / 178902.6732 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1667
env0_second_0:                 episode reward: -5.1500,                 loss: -0.0884
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 7149.45,                last time consumption/overall running time: 673.1452s / 179575.8184 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1606
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0924
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 5365.15,                last time consumption/overall running time: 484.7083s / 180060.5266 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.1648
env0_second_0:                 episode reward: -4.8500,                 loss: -0.0644
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 7162.75,                last time consumption/overall running time: 673.8298s / 180734.3564 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1606
env0_second_0:                 episode reward: -2.5500,                 loss: -0.0704