env_args:
    env_name: CarRacing-v1
    env_type: gym
    num_envs: 2
    ram: True
    seed: 1122

agent_args:
    algorithm: PPO
    algorithm_spec:
        episodic_update: True  # as PPO is on-policy, it uses episodic update instead of update per timestep
        gamma: 0.99
        lambda: 0.95
        eps_clip: 0.2
        K_epoch: 4
        GAE: True  # generalized advantage estimation
        max_grad_norm: 0.5
        entropy_coeff: 0.01
        vf_coeff: 0.5

train_args:
    max_episodes: 10000
    max_steps_per_episode: 10000
    train_start_frame: 1000
    optimizer: adam
    learning_rate: 1e-4
    device: gpu
    log_avg_window: 20 # average window length in logging
    log_interval: 20  # log print interval 
    render: False
    net_architecture:   
        feature:
            hidden_dim_list:
            - 64
            hidden_activation: ReLU
            output_activation: false        
        policy:
            hidden_dim_list: [64, 64]  
            hidden_activation: ReLU
            output_activation: Tanh  # action range -1,1
        value:
            hidden_dim_list: [64, 64]  
            hidden_activation: ReLU
            output_activation: False