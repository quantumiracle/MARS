ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2022-08-16 18:06:33.559494: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
/home/zihan/research/MARS/mars/env/mdp/arbitrary_richobs_mdp.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  self.observation_space = Box(low=0.0, high=1.0, shape=(self.observation_dim,),dtype=np.float)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': pil_image.HAMMING,
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': pil_image.BOX,
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': pil_image.LANCZOS,
wandb: Currently logged in as: quantumiracle (use `wandb login --relogin` to force relogin)
pettingzoo_pong_v3
default:  {'env_name': 'pong_v3', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'record_video': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': False, 'wandb_entity': '', 'wandb_project': '', 'wandb_group': '', 'wandb_name': '', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [512], 'channel_list': [32, 64, 64], 'kernel_size_list': [8, 4, 3], 'stride_list': [4, 2, 1], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [512], 'hidden_activation': False, 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [512], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
{'env_name': 'pong_v3', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'record_video': True, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202208161806, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': '', 'wandb_group': '', 'wandb_name': '', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [512], 'channel_list': [32, 64, 64], 'kernel_size_list': [8, 4, 3], 'stride_list': [4, 2, 1], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [512], 'hidden_activation': False, 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [512], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
2022-08-16 18:06:36.562931: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
wandb: wandb version 0.13.1 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.11
wandb: Run data is saved locally in /home/zihan/research/MARS/wandb/run-20220816_180635-34wle93k
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pettingzoo_pong_v3_nash_ppo_202208161806
wandb: ‚≠êÔ∏è View project at https://wandb.ai/quantumiracle/Pettingzoo_MARS
wandb: üöÄ View run at https://wandb.ai/quantumiracle/Pettingzoo_MARS/runs/34wle93k
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
args:  {'env_name': 'pong_v3', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'record_video': True, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202208161806, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': 'Pettingzoo_MARS', 'wandb_group': '202208161806', 'wandb_name': 'pettingzoo_pong_v3_nash_ppo_202208161806', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [512], 'channel_list': [32, 64, 64], 'kernel_size_list': [8, 4, 3], 'stride_list': [4, 2, 1], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [512], 'hidden_activation': False, 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [512], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
pong_v3 pettingzoo
record video:  100000 100
Load pong_v3 environment in type pettingzoo.
Env observation space: <bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f2b0c186150>> action space: <bound method aec_to_parallel_wrapper.action_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f2b0c186150>>
random seed: [963, 950, 815, 554, 788]
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f2b0ea87090>
discrete_policy 6 Discrete(6)
[CNN(
  (features): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (body): Sequential(
    (0): Flatten()
    (1): Linear(in_features=3136, out_features=512, bias=True)
    (2): ReLU()
    (3): Linear(in_features=512, out_features=256, bias=True)
  )
), CNN(
  (features): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (body): Sequential(
    (0): Flatten()
    (1): Linear(in_features=3136, out_features=512, bias=True)
    (2): ReLU()
    (3): Linear(in_features=512, out_features=256, bias=True)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): Linear(in_features=512, out_features=6, bias=True)
    (2): Softmax(dim=-1)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): Linear(in_features=512, out_features=6, bias=True)
    (2): Softmax(dim=-1)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=1, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=1, bias=True)
  )
)] MLP(
  (body): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=1, bias=True)
  )
)
discrete_policy 6 Discrete(6)
[CNN(
  (features): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (body): Sequential(
    (0): Flatten()
    (1): Linear(in_features=3136, out_features=512, bias=True)
    (2): ReLU()
    (3): Linear(in_features=512, out_features=256, bias=True)
  )
), CNN(
  (features): Sequential(
    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
    (1): ReLU()
    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
    (3): ReLU()
    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
    (5): ReLU()
  )
  (body): Sequential(
    (0): Flatten()
    (1): Linear(in_features=3136, out_features=512, bias=True)
    (2): ReLU()
    (3): Linear(in_features=512, out_features=256, bias=True)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): Linear(in_features=512, out_features=6, bias=True)
    (2): Softmax(dim=-1)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): Linear(in_features=512, out_features=6, bias=True)
    (2): Softmax(dim=-1)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=1, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=256, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=1, bias=True)
  )
)] MLP(
  (body): Sequential(
    (0): Linear(in_features=512, out_features=512, bias=True)
    (1): ReLU()
    (2): Linear(in_features=512, out_features=1, bias=True)
  )
)
discrete_policy 6 Discrete(6)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'pong_v3', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'record_video': True, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202208161806, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': 'Pettingzoo_MARS', 'wandb_group': '202208161806', 'wandb_name': 'pettingzoo_pong_v3_nash_ppo_202208161806', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [512], 'channel_list': [32, 64, 64], 'kernel_size_list': [8, 4, 3], 'stride_list': [4, 2, 1], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [512], 'hidden_activation': False, 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [512], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/202208161806/pettingzoo_pong_v3_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/202208161806/pettingzoo_pong_v3_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 15.43s / 15.43 s
first_0:                 episode reward: 8.0000,                 loss: 0.2750
second_0:                 episode reward: -8.0000,                 loss: nanERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)

Episode: 21/10000 (0.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.05s / 301.48 s
first_0:                 episode reward: 1.0500,                 loss: 0.3223
second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 280.67s / 582.15 s
first_0:                 episode reward: 1.0000,                 loss: 0.2081
second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.52s / 868.67 s
first_0:                 episode reward: 3.5000,                 loss: 0.1800
second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.64s / 1153.31 s
first_0:                 episode reward: 2.1500,                 loss: 0.1780
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 285.30s / 1438.61 s
first_0:                 episode reward: 0.0000,                 loss: 0.3011
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 279.77s / 1718.38 s
first_0:                 episode reward: 2.6500,                 loss: 0.2609
second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 291.93s / 2010.31 s
first_0:                 episode reward: 1.8500,                 loss: 0.3530
second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 279.86s / 2290.17 s
first_0:                 episode reward: 2.8500,                 loss: 0.6084
second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.98s / 2577.15 s
first_0:                 episode reward: 1.1500,                 loss: 0.4640
second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.36s / 2860.50 s
first_0:                 episode reward: 2.4500,                 loss: 0.5421
second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 280.09s / 3140.60 s
first_0:                 episode reward: 3.3500,                 loss: 1.0052
second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.00s / 3423.60 s
first_0:                 episode reward: 2.4000,                 loss: 0.7880
second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.22s / 3706.82 s
first_0:                 episode reward: 4.3000,                 loss: 1.0068
second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 287.58s / 3994.40 s
first_0:                 episode reward: 3.8000,                 loss: 0.6514
second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.32s / 4280.72 s
first_0:                 episode reward: 3.7000,                 loss: 0.6451
second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.00s / 4564.73 s
first_0:                 episode reward: 3.2500,                 loss: 0.7057
second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.03s / 4850.76 s
first_0:                 episode reward: 3.7500,                 loss: 0.6460
second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 293.10s / 5143.86 s
first_0:                 episode reward: 0.2000,                 loss: 0.7443
second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 285.23s / 5429.09 s
first_0:                 episode reward: 4.4000,                 loss: 0.6998
second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 280.31s / 5709.39 s
first_0:                 episode reward: 3.0500,                 loss: 0.7221
second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.80s / 5992.19 s
first_0:                 episode reward: 1.1000,                 loss: 0.8096
second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 287.11s / 6279.30 s
first_0:                 episode reward: 3.6000,                 loss: 0.8730
second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.79s / 6563.09 s
first_0:                 episode reward: 3.0000,                 loss: 1.0477
second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.96s / 6850.05 s
first_0:                 episode reward: 2.5000,                 loss: 2.6441
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 285.56s / 7135.60 s
first_0:                 episode reward: 5.4500,                 loss: 2.1971
second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.01s / 7419.61 s
first_0:                 episode reward: 3.0500,                 loss: 1.5845
second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 293.07s / 7712.68 s
first_0:                 episode reward: 2.7000,                 loss: 2.6207
second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.99s / 7999.67 sERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)

first_0:                 episode reward: 3.4000,                 loss: 1.6964
second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 285.83s / 8285.50 s
first_0:                 episode reward: 2.8500,                 loss: 0.9432
second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.14s / 8567.64 s
first_0:                 episode reward: 2.3500,                 loss: 1.0540
second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 287.08s / 8854.71 s
first_0:                 episode reward: 1.9500,                 loss: 0.6884
second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 287.43s / 9142.14 s
first_0:                 episode reward: 3.1500,                 loss: 0.7899
second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 292.53s / 9434.67 s
first_0:                 episode reward: 2.2500,                 loss: 0.7602
second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.87s / 9717.54 s
first_0:                 episode reward: 1.1000,                 loss: 0.6239
second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.76s / 10001.30 s
first_0:                 episode reward: 4.4000,                 loss: 0.7299
second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.92s / 10288.22 s
first_0:                 episode reward: 2.4000,                 loss: 0.8213
second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 288.06s / 10576.29 s
first_0:                 episode reward: 2.6000,                 loss: 0.7787
second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.05s / 10860.34 s
first_0:                 episode reward: 2.8000,                 loss: 0.8380
second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 287.22s / 11147.56 s
first_0:                 episode reward: 3.5500,                 loss: 0.6708
second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 285.70s / 11433.25 s
first_0:                 episode reward: 3.4500,                 loss: 0.6562
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.95s / 11717.20 s
first_0:                 episode reward: 2.7000,                 loss: 0.7028
second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 279.41s / 11996.61 s
first_0:                 episode reward: 2.8000,                 loss: 0.7332
second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.82s / 12281.43 s
first_0:                 episode reward: 3.4500,                 loss: 0.6058
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 294.48s / 12575.91 s
first_0:                 episode reward: 3.8500,                 loss: 0.6345
second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.30s / 12862.20 s
first_0:                 episode reward: 2.6500,                 loss: 0.5935
second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 280.70s / 13142.90 s
first_0:                 episode reward: 2.8500,                 loss: 0.6652
second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 290.13s / 13433.03 s
first_0:                 episode reward: 3.4500,                 loss: 0.7873
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 285.95s / 13718.99 s
first_0:                 episode reward: 1.6000,                 loss: 0.6383
second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.63s / 14001.62 s
first_0:                 episode reward: 2.9000,                 loss: 0.5893
second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.52s / 14288.15 s
first_0:                 episode reward: 3.3500,                 loss: 0.6755
second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.46s / 14574.60 s
first_0:                 episode reward: 3.3500,                 loss: 0.5761
second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.51s / 14857.11 s
first_0:                 episode reward: 1.5000,                 loss: 0.6120
second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.38s / 15141.49 s
first_0:                 episode reward: 3.1500,                 loss: 0.6119
second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 289.02s / 15430.51 s
first_0:                 episode reward: 2.9000,                 loss: 0.5960
second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.33s / 15716.84 s
first_0:                 episode reward: 1.9500,                 loss: 0.7374ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)

second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 285.63s / 16002.48 s
first_0:                 episode reward: 2.1000,                 loss: 0.7071
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 287.59s / 16290.07 s
first_0:                 episode reward: 1.1000,                 loss: 0.6583
second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 279.08s / 16569.15 s
first_0:                 episode reward: 3.5500,                 loss: 0.5480
second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.31s / 16853.46 s
first_0:                 episode reward: 2.3000,                 loss: 0.5463
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.19s / 17137.65 s
first_0:                 episode reward: 2.3500,                 loss: 0.6824
second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 281.00s / 17418.65 s
first_0:                 episode reward: 4.0500,                 loss: 0.6359
second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 288.74s / 17707.39 s
first_0:                 episode reward: 0.3000,                 loss: 0.6885
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.98s / 17991.38 s
first_0:                 episode reward: 3.4500,                 loss: 0.7641
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 280.14s / 18271.52 s
first_0:                 episode reward: 3.2500,                 loss: 0.6534
second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 288.37s / 18559.89 s
first_0:                 episode reward: 3.4500,                 loss: 1.0069
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.74s / 18844.63 s
first_0:                 episode reward: 3.8500,                 loss: 1.3330
second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.71s / 19129.33 s
first_0:                 episode reward: 2.5000,                 loss: 0.7296
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.54s / 19413.87 s
first_0:                 episode reward: 3.8000,                 loss: 0.7452
second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.35s / 19698.22 s
first_0:                 episode reward: 3.4500,                 loss: 0.5756
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.19s / 19980.41 s
first_0:                 episode reward: 0.7500,                 loss: 0.6747
second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.19s / 20262.60 s
first_0:                 episode reward: 2.1000,                 loss: 0.5886
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.43s / 20547.03 s
first_0:                 episode reward: 3.0500,                 loss: 0.6900
second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.44s / 20831.47 s
first_0:                 episode reward: 3.2500,                 loss: 0.7302
second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.09s / 21115.56 s
first_0:                 episode reward: 3.2500,                 loss: 0.7167
second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.64s / 21400.20 s
first_0:                 episode reward: 2.7000,                 loss: 0.5593
second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.18s / 21684.38 s
first_0:                 episode reward: 3.4000,                 loss: 0.7282
second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.51s / 21966.88 s
first_0:                 episode reward: 3.1500,                 loss: 0.9396
second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.77s / 22251.65 s
first_0:                 episode reward: 4.3000,                 loss: 1.0724
second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 279.65s / 22531.30 s
first_0:                 episode reward: 1.7000,                 loss: 0.6898
second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.54s / 22814.84 s
first_0:                 episode reward: 2.4500,                 loss: 0.7512
second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.47s / 23097.31 s
first_0:                 episode reward: 1.6500,                 loss: 0.6720
second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.60s / 23383.91 s
first_0:                 episode reward: 1.1500,                 loss: 0.7527
second_0:                 episode reward: -1.1500,                 loss: nanERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)

Episode: 1661/10000 (16.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 285.04s / 23668.94 s
first_0:                 episode reward: 3.2500,                 loss: 0.6630
second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.30s / 23951.24 s
first_0:                 episode reward: 2.5000,                 loss: 0.5525
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 286.96s / 24238.20 s
first_0:                 episode reward: 2.6500,                 loss: 0.5823
second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 281.29s / 24519.49 s
first_0:                 episode reward: 2.8500,                 loss: 0.6338
second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 289.79s / 24809.28 s
first_0:                 episode reward: 2.7500,                 loss: 0.6556
second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.55s / 25091.83 s
first_0:                 episode reward: 1.4500,                 loss: 0.6455
second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 284.05s / 25375.88 s
first_0:                 episode reward: 4.2000,                 loss: 0.6732
second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 287.97s / 25663.85 s
first_0:                 episode reward: 2.7000,                 loss: 0.5521
second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 281.73s / 25945.58 s
first_0:                 episode reward: 2.3000,                 loss: 0.6206
second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.46s / 26228.04 s
first_0:                 episode reward: 2.9500,                 loss: 0.7829
second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.71s / 26510.75 s
first_0:                 episode reward: -0.2500,                 loss: 0.6999
second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 281.56s / 26792.31 s
first_0:                 episode reward: 2.5500,                 loss: 0.7842
second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.91s / 27075.22 s
first_0:                 episode reward: -0.1000,                 loss: 0.6952
second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 282.62s / 27357.85 s
first_0:                 episode reward: 3.5500,                 loss: 0.7164
second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 283.28s / 27641.13 s
first_0:                 episode reward: 2.7500,                 loss: 0.6387
second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 274.73s / 27915.87 s
first_0:                 episode reward: 4.2000,                 loss: 0.6733
second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 259.12s / 28174.98 s
first_0:                 episode reward: 2.3500,                 loss: 0.5606
second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 251.19s / 28426.17 s
first_0:                 episode reward: 1.5500,                 loss: 0.5922
second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 251.13s / 28677.30 s
first_0:                 episode reward: 2.5500,                 loss: 0.7391
second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 250.33s / 28927.63 s
first_0:                 episode reward: 3.3000,                 loss: 0.7660
second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 250.32s / 29177.94 s
first_0:                 episode reward: 3.7000,                 loss: 0.6802
second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 251.84s / 29429.79 s
first_0:                 episode reward: 2.9500,                 loss: 0.7913
second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 252.01s / 29681.80 s
first_0:                 episode reward: 3.8000,                 loss: 0.6845
second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 254.19s / 29935.98 s
first_0:                 episode reward: 1.6500,                 loss: 0.9244
second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 260.40s / 30196.39 s
first_0:                 episode reward: 3.5000,                 loss: 0.7420
second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 260.26s / 30456.65 s
first_0:                 episode reward: 4.0000,                 loss: 0.7019
second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 254.80s / 30711.44 s
first_0:                 episode reward: 3.1000,                 loss: 0.6605
second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 254.63s / 30966.07 sERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.

first_0:                 episode reward: 1.1500,                 loss: 0.8087
second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 253.74s / 31219.81 s
first_0:                 episode reward: 2.2000,                 loss: 0.7860
second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 260.06s / 31479.87 s
first_0:                 episode reward: 2.9500,                 loss: 0.7651
second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 256.18s / 31736.04 s
first_0:                 episode reward: 3.7500,                 loss: 0.7326
second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 256.45s / 31992.49 s
first_0:                 episode reward: 4.3500,                 loss: 0.7055
second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 255.19s / 32247.68 s
first_0:                 episode reward: 3.1500,                 loss: 0.7567
second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 254.83s / 32502.51 s
first_0:                 episode reward: 3.5000,                 loss: 0.8440
second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 263.52s / 32766.03 s
first_0:                 episode reward: 3.4500,                 loss: 0.5934
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 254.58s / 33020.61 s
first_0:                 episode reward: 3.7500,                 loss: 0.9299
second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 253.76s / 33274.37 s
first_0:                 episode reward: 0.1000,                 loss: 0.7237
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 268.56s / 33542.93 s
first_0:                 episode reward: 3.4500,                 loss: 0.7965
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 256.42s / 33799.34 s
first_0:                 episode reward: 1.5000,                 loss: 0.6635
second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 263.04s / 34062.38 s
first_0:                 episode reward: 2.9500,                 loss: 0.6086
second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 259.47s / 34321.85 s
first_0:                 episode reward: 1.1000,                 loss: 0.7025
second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 271.29s / 34593.14 s
first_0:                 episode reward: 4.2500,                 loss: 0.6783
second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 249.59s / 34842.73 s
first_0:                 episode reward: 3.6500,                 loss: 0.5864
second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.62s / 35088.36 s
first_0:                 episode reward: 2.2500,                 loss: 0.6891
second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.20s / 35329.56 s
first_0:                 episode reward: 2.2500,                 loss: 0.7348
second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 238.57s / 35568.13 s
first_0:                 episode reward: 1.6500,                 loss: 0.6486
second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.71s / 35808.84 s
first_0:                 episode reward: 1.5000,                 loss: 0.6503
second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.91s / 36050.75 s
first_0:                 episode reward: 3.8500,                 loss: 0.7191
second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 247.63s / 36298.38 s
first_0:                 episode reward: 3.1000,                 loss: 0.7749
second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.21s / 36540.59 s
first_0:                 episode reward: 2.8000,                 loss: 0.5405
second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.19s / 36781.78 s
first_0:                 episode reward: 2.9500,                 loss: 1.2499
second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.96s / 37024.75 s
first_0:                 episode reward: 2.8000,                 loss: 0.5956
second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 247.66s / 37272.41 s
first_0:                 episode reward: 1.8000,                 loss: 0.8205
second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 247.72s / 37520.13 s
first_0:                 episode reward: 2.8500,                 loss: 1.0028
second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.24s / 37760.37 s
first_0:                 episode reward: 2.6000,                 loss: 0.6496wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/utils/seeding.py:156: DeprecationWarning: [33mWARN: Function `create_seed(a, max_bytes)` is marked as deprecated and will be removed in the future. [0m
  "Function `create_seed(a, max_bytes)` is marked as deprecated and will be removed in the future. "
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/utils/seeding.py:176: DeprecationWarning: [33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. [0m
  "Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. "
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/vector/vector_env.py:209: DeprecationWarning: [33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead in VectorEnvs.[0m
  "Function `env.seed(seed)` is marked as deprecated and will be removed in the future. "
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.
wandb: Network error (ReadTimeout), entering retry loop.

second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 247.94s / 38008.31 s
first_0:                 episode reward: 2.4000,                 loss: 0.8199
second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 248.66s / 38256.98 s
first_0:                 episode reward: 2.5500,                 loss: 0.6384
second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 243.08s / 38500.05 s
first_0:                 episode reward: 2.3500,                 loss: 0.6824
second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.43s / 38740.49 s
first_0:                 episode reward: 2.8000,                 loss: 0.8138
second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 248.30s / 38988.79 s
first_0:                 episode reward: 2.0000,                 loss: 0.7876
second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 249.29s / 39238.07 s
first_0:                 episode reward: 2.6500,                 loss: 0.7422
second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 255.39s / 39493.47 s
first_0:                 episode reward: 3.2500,                 loss: 0.6527
second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.37s / 39738.83 s
first_0:                 episode reward: 3.2000,                 loss: 0.6741
second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 246.42s / 39985.25 s
first_0:                 episode reward: 1.4500,                 loss: 0.6493
second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.80s / 40228.05 s
first_0:                 episode reward: 2.8500,                 loss: 0.7643
second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.53s / 40468.58 s
first_0:                 episode reward: 5.0000,                 loss: 0.6252
second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 249.20s / 40717.79 s
first_0:                 episode reward: 2.5500,                 loss: 0.7054
second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.79s / 40959.58 s
first_0:                 episode reward: 3.3500,                 loss: 0.7061
second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 248.20s / 41207.78 s
first_0:                 episode reward: 1.2500,                 loss: 0.6647
second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.36s / 41450.14 s
first_0:                 episode reward: 2.8000,                 loss: 1.6910
second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.64s / 41695.78 s
first_0:                 episode reward: 2.9000,                 loss: 0.6137
second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.67s / 41937.45 s
first_0:                 episode reward: 2.0000,                 loss: 1.1700
second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.75s / 42183.20 s
first_0:                 episode reward: 2.7000,                 loss: 0.6788
second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.59s / 42425.80 s
first_0:                 episode reward: 2.1500,                 loss: 0.7615
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.10s / 42666.89 s
first_0:                 episode reward: 4.8000,                 loss: 2.3881
second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.13s / 42907.02 s
first_0:                 episode reward: 3.9500,                 loss: 4.0005
second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 247.34s / 43154.37 s
first_0:                 episode reward: 3.4500,                 loss: 1.2554
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.68s / 43400.04 s
first_0:                 episode reward: 3.3000,                 loss: 0.6512
second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.65s / 43641.69 s
first_0:                 episode reward: 3.9000,                 loss: 1.5001
second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.95s / 43887.64 s
first_0:                 episode reward: 1.8000,                 loss: 1.0424
second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.21s / 44131.86 s
first_0:                 episode reward: 4.0500,                 loss: 0.7338
second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 255.86s / 44387.72 s
first_0:                 episode reward: 4.6000,                 loss: 1.6311
second_0:                 episode reward: -4.6000,                 loss: nanERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)

Episode: 3301/10000 (33.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.82s / 44628.54 s
first_0:                 episode reward: 8.0000,                 loss: 0.5814
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.04s / 44868.58 s
first_0:                 episode reward: 8.0000,                 loss: 0.9026
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 243.16s / 45111.74 s
first_0:                 episode reward: 8.0000,                 loss: 0.6353
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.53s / 45353.27 s
first_0:                 episode reward: 8.0000,                 loss: 0.6067
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.64s / 45597.91 s
first_0:                 episode reward: 8.0000,                 loss: 0.7275
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 251.01s / 45848.92 s
first_0:                 episode reward: 8.0000,                 loss: 0.6000
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.27s / 46093.19 s
first_0:                 episode reward: 8.0000,                 loss: 0.9044
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 243.93s / 46337.11 s
first_0:                 episode reward: 5.8000,                 loss: 2.0711
second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.08s / 46579.19 s
first_0:                 episode reward: 4.4000,                 loss: 1.5850
second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.97s / 46821.16 s
first_0:                 episode reward: 7.8000,                 loss: 0.8183
second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 248.64s / 47069.80 s
first_0:                 episode reward: 7.7000,                 loss: 1.1191
second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.64s / 47310.44 s
first_0:                 episode reward: 7.4500,                 loss: 1.1014
second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 246.74s / 47557.18 s
first_0:                 episode reward: 6.6000,                 loss: 0.9699
second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.77s / 47801.96 s
first_0:                 episode reward: 5.0500,                 loss: 1.6802
second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.69s / 48047.64 s
first_0:                 episode reward: 5.0000,                 loss: 1.1670
second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.93s / 48290.58 s
first_0:                 episode reward: 5.6500,                 loss: 4.6405
second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 239.95s / 48530.53 s
first_0:                 episode reward: 7.1500,                 loss: 3.4479
second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.35s / 48775.88 s
first_0:                 episode reward: 7.2000,                 loss: 0.8285
second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.67s / 49018.55 s
first_0:                 episode reward: 7.3500,                 loss: 2.8852
second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 248.71s / 49267.26 s
first_0:                 episode reward: 5.5500,                 loss: 1.2819
second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.12s / 49509.38 s
first_0:                 episode reward: 4.4500,                 loss: 3.8219
second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 246.76s / 49756.14 s
first_0:                 episode reward: 7.5500,                 loss: 1.3672
second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 249.71s / 50005.85 s
first_0:                 episode reward: 6.8000,                 loss: 2.0317
second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 249.26s / 50255.12 s
first_0:                 episode reward: 7.0000,                 loss: 1.4172
second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 251.68s / 50506.80 s
first_0:                 episode reward: 6.9500,                 loss: 6.0463
second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.20s / 50747.99 s
first_0:                 episode reward: 8.0000,                 loss: 1.6704
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.75s / 50988.74 s
first_0:                 episode reward: 8.0000,                 loss: 0.8032
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.57s / 51234.32 sERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ERROR: ld.so: object '/usr/lib/nvidia-384/libGL.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libtinfo.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)
ffmpeg: /home/zihan/anaconda3/envs/x/lib/libncursesw.so.6: no version information available (required by /lib/x86_64-linux-gnu/libcaca.so.0)

first_0:                 episode reward: 8.0000,                 loss: 0.8224
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.94s / 51480.26 s
first_0:                 episode reward: 8.0000,                 loss: 0.6205
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.88s / 51725.13 s
first_0:                 episode reward: 8.0000,                 loss: 0.6122
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.02s / 51966.16 s
first_0:                 episode reward: 8.0000,                 loss: 0.7062
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.55s / 52207.71 s
first_0:                 episode reward: 8.0000,                 loss: 0.5893
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 243.24s / 52450.95 s
first_0:                 episode reward: 8.0000,                 loss: 0.6056
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.60s / 52692.55 s
first_0:                 episode reward: 8.0000,                 loss: 0.6135
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.80s / 52937.35 s
first_0:                 episode reward: 8.0000,                 loss: 0.6069
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.94s / 53182.29 s
first_0:                 episode reward: 5.9500,                 loss: 1.9746
second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.49s / 53423.78 s
first_0:                 episode reward: 8.0000,                 loss: 1.4006
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.75s / 53664.54 s
first_0:                 episode reward: 7.0000,                 loss: 1.2425
second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.97s / 53905.51 s
first_0:                 episode reward: 3.9500,                 loss: 3.2200
second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.64s / 54151.15 s
first_0:                 episode reward: 8.0000,                 loss: 0.8852
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.27s / 54392.42 s
first_0:                 episode reward: 8.0000,                 loss: 1.0954
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.36s / 54633.78 s
first_0:                 episode reward: 7.6500,                 loss: 0.8968
second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.62s / 54875.40 s
first_0:                 episode reward: 6.5500,                 loss: 3.6541
second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 243.34s / 55118.74 s
first_0:                 episode reward: 7.1500,                 loss: 3.9414
second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 241.34s / 55360.08 s
first_0:                 episode reward: 6.4000,                 loss: 2.4333
second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.97s / 55601.05 s
first_0:                 episode reward: 6.7500,                 loss: 4.1162
second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.35s / 55843.40 s
first_0:                 episode reward: 6.3000,                 loss: 2.0837
second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 247.92s / 56091.32 s
first_0:                 episode reward: 4.5000,                 loss: 2.5834
second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.99s / 56334.31 s
first_0:                 episode reward: 7.6500,                 loss: 1.3561
second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.22s / 56578.52 s
first_0:                 episode reward: 8.0000,                 loss: 1.2533
second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 299.0,                last time consumption/overall running time: 244.00s / 56822.52 s
first_0:                 episode reward: 7.5000,                 loss: 1.3531
second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 299.0,                last time consumption/overall running time: 242.78s / 57065.30 s
first_0:                 episode reward: 5.6000,                 loss: 0.8833
second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 299.0,                last time consumption/overall running time: 240.85s / 57306.15 s
first_0:                 episode reward: 7.8500,                 loss: 1.4711
second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.46s / 57551.61 s
first_0:                 episode reward: 4.0000,                 loss: 1.6807
second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 299.0,                last time consumption/overall running time: 245.02s / 57796.63 s
first_0:                 episode reward: 5.7000,                 loss: 2.7801/home/zihan/anaconda3/envs/x/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
