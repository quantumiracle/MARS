agent_args:
  algorithm: PPO
  algorithm_spec:
    episodic_update: False  # need to specify this to overwrite
    batch_update: 1280
    gamma: 0.995
    lambda: 0.95
    eps_clip: 0.2
    K_epoch: 10
    GAE: True
    max_grad_norm: 0.5
    entropy_coeff: 0.01
    vf_coeff: 0.5 # normalize value loss to match policy loss
train_args:
  # batch_size: 1280
  max_episodes: 300000
  max_steps_per_episode: 10000
  train_start_frame: 0
  optimizer: adam
  learning_rate: 3e-4
  device: gpu
  update_itr: 1
  log_avg_window: 20
  log_interval: 20
  net_architecture:
    feature:
      hidden_dim_list:
      - 128
      - 128
      hidden_activation: Tanh
      output_activation: false
    policy:
      hidden_dim_list:
      - 128
      hidden_activation: Tanh
      output_activation: false
    value:
      hidden_dim_list:
      - 128
      hidden_activation: Tanh
      output_activation: false