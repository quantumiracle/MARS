pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 16, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0521/pettingzoo_surround_v1_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0521/pettingzoo_surround_v1_nxdo2.
surround_v1 pettingzoo
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
random seed: 11
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
Episode: 1/10000 (0.0100%),                 avg. length: 1047.0,                last time consumption/overall running time: 10.0712s / 10.0712 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0039
env0_second_0:                 episode reward: 8.0000,                 loss: nan
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1333.55,                last time consumption/overall running time: 269.8980s / 279.9692 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0038
env0_second_0:                 episode reward: 3.5500,                 loss: nan
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1457.85,                last time consumption/overall running time: 613.4233s / 893.3925 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0053
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1156.8,                last time consumption/overall running time: 506.9048s / 1400.2973 s
env0_first_0:                 episode reward: 7.3500,                 loss: 0.0037
env0_second_0:                 episode reward: -7.3500,                 loss: nan
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/61_0.
Episode: 81/10000 (0.8100%),                 avg. length: 1867.6,                last time consumption/overall running time: 810.3708s / 2210.6681 s
env0_first_0:                 episode reward: 2.4000,                 loss: nan
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0023
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1851.0,                last time consumption/overall running time: 819.9041s / 3030.5722 s
env0_first_0:                 episode reward: 1.1500,                 loss: nan
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0024
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1778.75,                last time consumption/overall running time: 789.0013s / 3819.5735 s
env0_first_0:                 episode reward: -2.4000,                 loss: nan
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0019
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1588.7,                last time consumption/overall running time: 731.8568s / 4551.4302 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0028
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0016
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Score delta: 16.8, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/134_1.
Episode: 161/10000 (1.6100%),                 avg. length: 1745.45,                last time consumption/overall running time: 775.6149s / 5327.0451 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0018
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1678.35,                last time consumption/overall running time: 742.1148s / 6069.1599 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0013
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1529.5,                last time consumption/overall running time: 674.9195s / 6744.0795 s
env0_first_0:                 episode reward: 7.1500,                 loss: 0.0012
env0_second_0:                 episode reward: -7.1500,                 loss: nan
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1459.65,                last time consumption/overall running time: 670.0241s / 7414.1036 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0011
env0_second_0:                 episode reward: -7.0500,                 loss: 0.0016
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Score delta: 16.6, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/217_0.
Episode: 241/10000 (2.4100%),                 avg. length: 2092.9,                last time consumption/overall running time: 925.1151s / 8339.2187 s
env0_first_0:                 episode reward: 3.9500,                 loss: nan
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0013
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2066.8,                last time consumption/overall running time: 912.5558s / 9251.7745 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0013
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2067.7,                last time consumption/overall running time: 912.3824s / 10164.1569 s
env0_first_0:                 episode reward: -1.3000,                 loss: nan
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0013
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1891.55,                last time consumption/overall running time: 839.3446s / 11003.5015 s
env0_first_0:                 episode reward: -2.4500,                 loss: nan
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0011
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1489.6,                last time consumption/overall running time: 656.6250s / 11660.1265 s
env0_first_0:                 episode reward: -3.9500,                 loss: nan
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0010
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1446.75,                last time consumption/overall running time: 637.3365s / 12297.4629 s
env0_first_0:                 episode reward: -5.6000,                 loss: nan
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0010
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1481.8,                last time consumption/overall running time: 726.1808s / 13023.6438 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0014
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0010
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/351_1.
Episode: 381/10000 (3.8100%),                 avg. length: 1876.25,                last time consumption/overall running time: 824.3584s / 13848.0022 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -2.1500,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1739.4,                last time consumption/overall running time: 768.4721s / 14616.4743 s
env0_first_0:                 episode reward: 2.7500,                 loss: 0.0017
env0_second_0:                 episode reward: -2.7500,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1851.9,                last time consumption/overall running time: 814.8663s / 15431.3405 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.2000,                 loss: nan
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1414.45,                last time consumption/overall running time: 622.0555s / 16053.3960 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0013
env0_second_0:                 episode reward: -5.5500,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1404.85,                last time consumption/overall running time: 625.0455s / 16678.4415 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0012
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1383.1,                last time consumption/overall running time: 602.0487s / 17280.4902 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0011
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1374.55,                last time consumption/overall running time: 595.9511s / 17876.4412 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0012
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1347.3,                last time consumption/overall running time: 662.6403s / 18539.0815 s
env0_first_0:                 episode reward: 6.1000,                 loss: 0.0012
env0_second_0:                 episode reward: -6.1000,                 loss: 0.0011
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Score delta: 16.8, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/514_0.
Episode: 541/10000 (5.4100%),                 avg. length: 1875.3,                last time consumption/overall running time: 822.7962s / 19361.8777 s
env0_first_0:                 episode reward: 0.3000,                 loss: nan
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0014
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1966.85,                last time consumption/overall running time: 866.9055s / 20228.7832 s
env0_first_0:                 episode reward: 0.4500,                 loss: nan
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0015
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1966.25,                last time consumption/overall running time: 855.8090s / 21084.5922 s
env0_first_0:                 episode reward: -2.1000,                 loss: nan
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0016
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1855.3,                last time consumption/overall running time: 812.0895s / 21896.6817 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0016
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1864.95,                last time consumption/overall running time: 819.0624s / 22715.7441 s
env0_first_0:                 episode reward: -4.4500,                 loss: nan
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0017
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1714.15,                last time consumption/overall running time: 748.3240s / 23464.0682 s
env0_first_0:                 episode reward: -5.6500,                 loss: nan
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0014
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1653.85,                last time consumption/overall running time: 725.4698s / 24189.5380 s
env0_first_0:                 episode reward: -6.0000,                 loss: nan
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0012
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1525.35,                last time consumption/overall running time: 675.3608s / 24864.8988 s
env0_first_0:                 episode reward: -5.7500,                 loss: nan
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0013
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1456.2,                last time consumption/overall running time: 732.7066s / 25597.6054 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.0027
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0012
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/698_1.
Episode: 721/10000 (7.2100%),                 avg. length: 1935.8,                last time consumption/overall running time: 850.6386s / 26448.2440 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0018
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1534.8,                last time consumption/overall running time: 800.5936s / 27248.8376 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0013
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Score delta: 16.8, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/731_0.
Episode: 761/10000 (7.6100%),                 avg. length: 1759.15,                last time consumption/overall running time: 775.3106s / 28024.1482 s
env0_first_0:                 episode reward: -3.8500,                 loss: nan
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0015
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1778.0,                last time consumption/overall running time: 776.9390s / 28801.0872 s
env0_first_0:                 episode reward: -5.0500,                 loss: nan
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0015
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1643.5,                last time consumption/overall running time: 868.5211s / 29669.6083 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0026
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0014
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/788_1.
Episode: 821/10000 (8.2100%),                 avg. length: 1446.8,                last time consumption/overall running time: 634.6949s / 30304.3032 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1537.0,                last time consumption/overall running time: 675.7721s / 30980.0753 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1614.45,                last time consumption/overall running time: 705.7573s / 31685.8327 s
env0_first_0:                 episode reward: 5.3000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.3000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1519.05,                last time consumption/overall running time: 660.7853s / 32346.6180 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1415.35,                last time consumption/overall running time: 622.9944s / 32969.6124 s
env0_first_0:                 episode reward: 6.2500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.2500,                 loss: nan
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1718.8,                last time consumption/overall running time: 741.6652s / 33711.2776 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0014
env0_second_0:                 episode reward: -2.3500,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1449.5,                last time consumption/overall running time: 641.3586s / 34352.6362 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1365.25,                last time consumption/overall running time: 608.2788s / 34960.9150 s
env0_first_0:                 episode reward: 6.6000,                 loss: 0.0014
env0_second_0:                 episode reward: -6.6000,                 loss: nan
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1396.3,                last time consumption/overall running time: 760.6400s / 35721.5550 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.5500,                 loss: 0.0028
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/972_0.
Episode: 1001/10000 (10.0100%),                 avg. length: 1938.95,                last time consumption/overall running time: 852.9925s / 36574.5474 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0019
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1979.55,                last time consumption/overall running time: 877.0379s / 37451.5854 s
env0_first_0:                 episode reward: -2.8000,                 loss: nan
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0017
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1887.5,                last time consumption/overall running time: 964.1753s / 38415.7606 s
env0_first_0:                 episode reward: -3.3500,                 loss: nan
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0015
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1801.45,                last time consumption/overall running time: 944.2704s / 39360.0310 s
env0_first_0:                 episode reward: -3.9000,                 loss: nan
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0016
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1647.55,                last time consumption/overall running time: 866.6531s / 40226.6841 s
env0_first_0:                 episode reward: -6.3000,                 loss: nan
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0015
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1689.0,                last time consumption/overall running time: 887.3470s / 41114.0311 s
env0_first_0:                 episode reward: -6.2500,                 loss: nan
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0014
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1512.3,                last time consumption/overall running time: 792.6333s / 41906.6643 s
env0_first_0:                 episode reward: -7.3500,                 loss: nan
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0013
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1726.6,                last time consumption/overall running time: 907.1945s / 42813.8588 s
env0_first_0:                 episode reward: -5.3000,                 loss: nan
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0013
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1732.35,                last time consumption/overall running time: 908.5016s / 43722.3604 s
env0_first_0:                 episode reward: -3.9500,                 loss: nan
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0014
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1656.55,                last time consumption/overall running time: 872.2380s / 44594.5984 s
env0_first_0:                 episode reward: -4.7000,                 loss: nan
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0014
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1606.95,                last time consumption/overall running time: 845.8986s / 45440.4970 s
env0_first_0:                 episode reward: -5.7500,                 loss: nan
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0014
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1724.25,                last time consumption/overall running time: 907.7926s / 46348.2896 s
env0_first_0:                 episode reward: -5.1000,                 loss: nan
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0014
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1603.9,                last time consumption/overall running time: 840.2441s / 47188.5337 s
env0_first_0:                 episode reward: -6.5000,                 loss: nan
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0013
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1560.6,                last time consumption/overall running time: 1033.8936s / 48222.4273 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0041
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0013
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/1259_1.
Episode: 1281/10000 (12.8100%),                 avg. length: 1885.3,                last time consumption/overall running time: 987.7979s / 49210.2252 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0019
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1887.55,                last time consumption/overall running time: 996.4521s / 50206.6773 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0018
env0_second_0:                 episode reward: -1.3000,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1855.25,                last time consumption/overall running time: 974.3085s / 51180.9858 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0018
env0_second_0:                 episode reward: -1.9000,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1742.9,                last time consumption/overall running time: 911.2677s / 52092.2535 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1660.45,                last time consumption/overall running time: 873.0378s / 52965.2913 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1583.05,                last time consumption/overall running time: 829.8932s / 53795.1846 s
env0_first_0:                 episode reward: 5.7500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.7500,                 loss: nan
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1633.0,                last time consumption/overall running time: 851.2702s / 54646.4548 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1732.35,                last time consumption/overall running time: 908.7141s / 55555.1689 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0013
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1516.05,                last time consumption/overall running time: 792.0415s / 56347.2103 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0014
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1592.0,                last time consumption/overall running time: 826.5612s / 57173.7716 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1599.15,                last time consumption/overall running time: 833.6395s / 58007.4110 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1686.9,                last time consumption/overall running time: 876.7311s / 58884.1421 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1687.55,                last time consumption/overall running time: 872.6230s / 59756.7650 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1924.3,                last time consumption/overall running time: 984.9218s / 60741.6869 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0028
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1845.9,                last time consumption/overall running time: 967.5316s / 61709.2185 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0047
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1797.5,                last time consumption/overall running time: 933.2730s / 62642.4915 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0021
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1768.35,                last time consumption/overall running time: 927.0242s / 63569.5157 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0018
env0_second_0:                 episode reward: -4.8000,                 loss: nan
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1668.3,                last time consumption/overall running time: 882.0162s / 64451.5318 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1949.65,                last time consumption/overall running time: 1015.6701s / 65467.2019 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0018
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1908.65,                last time consumption/overall running time: 984.7665s / 66451.9684 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0020
env0_second_0:                 episode reward: -1.7500,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1616.2,                last time consumption/overall running time: 833.7921s / 67285.7605 s
env0_first_0:                 episode reward: 6.7000,                 loss: 0.0019
env0_second_0:                 episode reward: -6.7000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1692.2,                last time consumption/overall running time: 888.7065s / 68174.4670 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0019
env0_second_0:                 episode reward: -3.2000,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1879.2,                last time consumption/overall running time: 993.5359s / 69168.0029 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0025
env0_second_0:                 episode reward: 3.0000,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1730.2,                last time consumption/overall running time: 905.7158s / 70073.7187 s
env0_first_0:                 episode reward: 3.4500,                 loss: 0.0025
env0_second_0:                 episode reward: -3.4500,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1713.2,                last time consumption/overall running time: 904.1826s / 70977.9013 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0022
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1564.5,                last time consumption/overall running time: 814.2548s / 71792.1561 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -6.1500,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1789.65,                last time consumption/overall running time: 915.2078s / 72707.3639 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0018
env0_second_0:                 episode reward: -3.2000,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1625.65,                last time consumption/overall running time: 841.1929s / 73548.5569 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0018
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1667.55,                last time consumption/overall running time: 859.2389s / 74407.7958 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1848.4,                last time consumption/overall running time: 973.2533s / 75381.0491 s
env0_first_0:                 episode reward: 3.1000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.1000,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1616.0,                last time consumption/overall running time: 853.2204s / 76234.2694 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0018
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1809.75,                last time consumption/overall running time: 948.7445s / 77183.0139 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0018
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1542.7,                last time consumption/overall running time: 810.4821s / 77993.4960 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0017
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1613.05,                last time consumption/overall running time: 840.2998s / 78833.7958 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1676.6,                last time consumption/overall running time: 876.5300s / 79710.3258 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0016
env0_second_0:                 episode reward: -2.6000,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1595.35,                last time consumption/overall running time: 835.7688s / 80546.0946 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1589.6,                last time consumption/overall running time: 833.9354s / 81380.0300 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1434.5,                last time consumption/overall running time: 746.2993s / 82126.3293 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1603.15,                last time consumption/overall running time: 836.2794s / 82962.6088 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1515.5,                last time consumption/overall running time: 794.2473s / 83756.8560 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -6.5500,                 loss: nan
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1443.8,                last time consumption/overall running time: 746.8593s / 84503.7153 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1379.85,                last time consumption/overall running time: 704.1981s / 85207.9134 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1405.6,                last time consumption/overall running time: 986.6599s / 86194.5733 s
env0_first_0:                 episode reward: 6.4000,                 loss: 0.0013
env0_second_0:                 episode reward: -6.4000,                 loss: 0.0013
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/2119_0.
Episode: 2141/10000 (21.4100%),                 avg. length: 1608.7,                last time consumption/overall running time: 848.9362s / 87043.5094 s
env0_first_0:                 episode reward: -5.1000,                 loss: nan
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0012
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1642.45,                last time consumption/overall running time: 862.6647s / 87906.1742 s
env0_first_0:                 episode reward: -6.4000,                 loss: nan
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0012
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1814.75,                last time consumption/overall running time: 954.5710s / 88860.7451 s
env0_first_0:                 episode reward: -3.3000,                 loss: nan
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0014
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1746.4,                last time consumption/overall running time: 915.0982s / 89775.8433 s
env0_first_0:                 episode reward: -4.9000,                 loss: nan
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0015
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1555.0,                last time consumption/overall running time: 812.4366s / 90588.2799 s
env0_first_0:                 episode reward: -6.4000,                 loss: nan
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0013
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1654.0,                last time consumption/overall running time: 853.6028s / 91441.8826 s
env0_first_0:                 episode reward: -5.4000,                 loss: nan
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0014
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1616.45,                last time consumption/overall running time: 843.8059s / 92285.6885 s
env0_first_0:                 episode reward: -6.5000,                 loss: nan
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0014
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1632.7,                last time consumption/overall running time: 1122.9629s / 93408.6515 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0022
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0014
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/2268_1.
Episode: 2301/10000 (23.0100%),                 avg. length: 1793.8,                last time consumption/overall running time: 947.8619s / 94356.5134 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0018
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1841.5,                last time consumption/overall running time: 972.5286s / 95329.0420 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1790.2,                last time consumption/overall running time: 934.8863s / 96263.9282 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -2.8500,                 loss: nan
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1801.8,                last time consumption/overall running time: 940.9916s / 97204.9198 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1719.6,                last time consumption/overall running time: 881.4046s / 98086.3244 s
env0_first_0:                 episode reward: 3.6000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.6000,                 loss: nan
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1569.25,                last time consumption/overall running time: 797.1354s / 98883.4598 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1717.25,                last time consumption/overall running time: 885.7110s / 99769.1708 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0014
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1730.05,                last time consumption/overall running time: 900.0992s / 100669.2700 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0014
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1605.1,                last time consumption/overall running time: 826.0005s / 101495.2705 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.8000,                 loss: nan
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1797.25,                last time consumption/overall running time: 936.3287s / 102431.5991 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1737.0,                last time consumption/overall running time: 915.8032s / 103347.4024 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -2.8500,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1507.9,                last time consumption/overall running time: 789.8143s / 104137.2167 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1808.4,                last time consumption/overall running time: 925.5782s / 105062.7949 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0013
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1542.45,                last time consumption/overall running time: 808.8122s / 105871.6071 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1584.9,                last time consumption/overall running time: 822.0642s / 106693.6713 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0014
env0_second_0:                 episode reward: -3.1500,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1778.75,                last time consumption/overall running time: 924.4615s / 107618.1328 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1422.05,                last time consumption/overall running time: 741.2216s / 108359.3544 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0013
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1519.15,                last time consumption/overall running time: 799.1627s / 109158.5171 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0013
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1684.2,                last time consumption/overall running time: 882.1725s / 110040.6896 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0014
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1491.95,                last time consumption/overall running time: 778.9714s / 110819.6610 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0014
env0_second_0:                 episode reward: -3.5500,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1639.35,                last time consumption/overall running time: 848.4726s / 111668.1336 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -2.8500,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1656.7,                last time consumption/overall running time: 856.6062s / 112524.7399 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1792.25,                last time consumption/overall running time: 934.5159s / 113459.2557 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0017
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1352.65,                last time consumption/overall running time: 708.6904s / 114167.9461 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -3.5500,                 loss: nan
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1648.25,                last time consumption/overall running time: 865.8263s / 115033.7724 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1569.5,                last time consumption/overall running time: 830.7470s / 115864.5194 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.0500,                 loss: nan
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1636.25,                last time consumption/overall running time: 858.3083s / 116722.8278 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -0.5500,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1599.95,                last time consumption/overall running time: 835.2189s / 117558.0467 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -2.2000,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1432.9,                last time consumption/overall running time: 726.5509s / 118284.5976 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1463.75,                last time consumption/overall running time: 747.3567s / 119031.9543 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -2.5500,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1506.3,                last time consumption/overall running time: 791.4092s / 119823.3635 s
env0_first_0:                 episode reward: 2.7500,                 loss: 0.0015
env0_second_0:                 episode reward: -2.7500,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1553.2,                last time consumption/overall running time: 817.8062s / 120641.1697 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1585.1,                last time consumption/overall running time: 830.4486s / 121471.6183 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -1.8500,                 loss: nan
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1492.45,                last time consumption/overall running time: 775.0833s / 122246.7016 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1471.7,                last time consumption/overall running time: 769.8846s / 123016.5862 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -2.2000,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1338.7,                last time consumption/overall running time: 705.1535s / 123721.7397 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0019
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1569.25,                last time consumption/overall running time: 823.9412s / 124545.6809 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0022
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1853.6,                last time consumption/overall running time: 939.9815s / 125485.6625 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0034
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1544.65,                last time consumption/overall running time: 779.8785s / 126265.5409 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0030
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1592.3,                last time consumption/overall running time: 830.5554s / 127096.0963 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0021
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1424.2,                last time consumption/overall running time: 738.6149s / 127834.7113 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0018
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1466.4,                last time consumption/overall running time: 757.2708s / 128591.9820 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0018
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1363.1,                last time consumption/overall running time: 703.2993s / 129295.2813 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1283.05,                last time consumption/overall running time: 666.5778s / 129961.8591 s
env0_first_0:                 episode reward: 6.5000,                 loss: 0.0016
env0_second_0:                 episode reward: -6.5000,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1426.75,                last time consumption/overall running time: 738.0396s / 130699.8987 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1401.95,                last time consumption/overall running time: 730.4097s / 131430.3084 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1362.15,                last time consumption/overall running time: 693.8299s / 132124.1383 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1304.55,                last time consumption/overall running time: 660.2956s / 132784.4339 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1438.3,                last time consumption/overall running time: 731.0243s / 133515.4582 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.5500,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1439.15,                last time consumption/overall running time: 744.1238s / 134259.5820 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0016
env0_second_0:                 episode reward: -1.3000,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1349.3,                last time consumption/overall running time: 705.4994s / 134965.0814 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1296.55,                last time consumption/overall running time: 675.8097s / 135640.8911 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1441.55,                last time consumption/overall running time: 754.2051s / 136395.0961 s
env0_first_0:                 episode reward: 5.7500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.7500,                 loss: nan
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1465.05,                last time consumption/overall running time: 764.3632s / 137159.4593 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1453.25,                last time consumption/overall running time: 763.8711s / 137923.3304 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1516.6,                last time consumption/overall running time: 788.1686s / 138711.4990 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1436.5,                last time consumption/overall running time: 737.5946s / 139449.0936 s
env0_first_0:                 episode reward: 6.4000,                 loss: 0.0015
env0_second_0:                 episode reward: -6.4000,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1457.7,                last time consumption/overall running time: 744.8798s / 140193.9733 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1538.2,                last time consumption/overall running time: 797.7943s / 140991.7676 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1340.65,                last time consumption/overall running time: 692.6217s / 141684.3894 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1232.35,                last time consumption/overall running time: 636.0636s / 142320.4530 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0014
env0_second_0:                 episode reward: -6.0500,                 loss: nan
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1358.4,                last time consumption/overall running time: 697.9144s / 143018.3674 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0013
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1317.05,                last time consumption/overall running time: 683.2379s / 143701.6053 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1227.65,                last time consumption/overall running time: 641.0351s / 144342.6404 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0013
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1282.3,                last time consumption/overall running time: 672.0955s / 145014.7359 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0012
env0_second_0:                 episode reward: -6.1500,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1260.5,                last time consumption/overall running time: 655.1589s / 145669.8948 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0011
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1202.35,                last time consumption/overall running time: 622.4629s / 146292.3576 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0011
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1422.9,                last time consumption/overall running time: 729.9524s / 147022.3100 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0012
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1200.1,                last time consumption/overall running time: 617.0378s / 147639.3478 s
env0_first_0:                 episode reward: 6.6500,                 loss: 0.0013
env0_second_0:                 episode reward: -6.6500,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1295.75,                last time consumption/overall running time: 667.2570s / 148306.6048 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0012
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1254.35,                last time consumption/overall running time: 653.3105s / 148959.9153 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0012
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1320.05,                last time consumption/overall running time: 687.2600s / 149647.1753 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0012
env0_second_0:                 episode reward: -6.1500,                 loss: nan
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1335.9,                last time consumption/overall running time: 689.8641s / 150337.0393 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0011
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1251.35,                last time consumption/overall running time: 652.1692s / 150989.2085 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0012
env0_second_0:                 episode reward: -6.5500,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1214.4,                last time consumption/overall running time: 627.6738s / 151616.8823 s
env0_first_0:                 episode reward: 6.8500,                 loss: 0.0011
env0_second_0:                 episode reward: -6.8500,                 loss: nan
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1276.0,                last time consumption/overall running time: 915.4183s / 152532.3006 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0010
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0013
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Score delta: 16.2, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/3796_0.
Episode: 3821/10000 (38.2100%),                 avg. length: 1499.3,                last time consumption/overall running time: 783.8880s / 153316.1886 s
env0_first_0:                 episode reward: -6.6000,                 loss: nan
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0011
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1490.0,                last time consumption/overall running time: 770.2697s / 154086.4583 s
env0_first_0:                 episode reward: -7.0500,                 loss: nan
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0012
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1608.85,                last time consumption/overall running time: 822.0682s / 154908.5265 s
env0_first_0:                 episode reward: -5.0500,                 loss: nan
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0012
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1696.9,                last time consumption/overall running time: 865.4341s / 155773.9606 s
env0_first_0:                 episode reward: -5.3000,                 loss: nan
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0013
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1702.6,                last time consumption/overall running time: 882.6208s / 156656.5814 s
env0_first_0:                 episode reward: -6.0000,                 loss: nan
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0014
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1544.0,                last time consumption/overall running time: 1103.4508s / 157760.0322 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0024
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0013
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/3915_1.
Episode: 3941/10000 (39.4100%),                 avg. length: 1708.3,                last time consumption/overall running time: 891.8223s / 158651.8545 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1668.8,                last time consumption/overall running time: 860.4170s / 159512.2715 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1583.9,                last time consumption/overall running time: 826.5826s / 160338.8541 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1547.65,                last time consumption/overall running time: 809.4608s / 161148.3149 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0014
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1564.65,                last time consumption/overall running time: 798.7316s / 161947.0466 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0013
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1538.85,                last time consumption/overall running time: 767.9766s / 162715.0231 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1621.25,                last time consumption/overall running time: 822.7300s / 163537.7531 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.0500,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1519.7,                last time consumption/overall running time: 790.5380s / 164328.2912 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0013
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1574.95,                last time consumption/overall running time: 823.6605s / 165151.9516 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1646.4,                last time consumption/overall running time: 862.4083s / 166014.3600 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0015
env0_second_0:                 episode reward: -3.5500,                 loss: nan
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1562.85,                last time consumption/overall running time: 823.8518s / 166838.2117 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1519.05,                last time consumption/overall running time: 796.7268s / 167634.9385 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0013
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1548.65,                last time consumption/overall running time: 805.2175s / 168440.1560 s
env0_first_0:                 episode reward: 3.6000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.6000,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1520.45,                last time consumption/overall running time: 787.0190s / 169227.1750 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1627.7,                last time consumption/overall running time: 838.2352s / 170065.4101 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1597.8,                last time consumption/overall running time: 821.2620s / 170886.6721 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1663.2,                last time consumption/overall running time: 853.0881s / 171739.7602 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0016
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1654.1,                last time consumption/overall running time: 853.9279s / 172593.6881 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1731.6,                last time consumption/overall running time: 898.5666s / 173492.2547 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.2000,                 loss: nan
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1663.75,                last time consumption/overall running time: 872.5775s / 174364.8322 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0017
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1645.3,                last time consumption/overall running time: 862.7817s / 175227.6139 s
env0_first_0:                 episode reward: 3.0500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.0500,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1701.1,                last time consumption/overall running time: 891.8678s / 176119.4817 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0018
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1680.7,                last time consumption/overall running time: 877.4463s / 176996.9280 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0017
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1693.95,                last time consumption/overall running time: 870.3415s / 177867.2695 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1665.45,                last time consumption/overall running time: 845.3554s / 178712.6249 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0019
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1610.8,                last time consumption/overall running time: 822.3915s / 179535.0164 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1644.25,                last time consumption/overall running time: 850.2176s / 180385.2340 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1615.1,                last time consumption/overall running time: 835.4794s / 181220.7134 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1562.8,                last time consumption/overall running time: 806.9338s / 182027.6472 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.3000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1653.25,                last time consumption/overall running time: 859.5532s / 182887.2005 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0018
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1511.65,                last time consumption/overall running time: 792.1299s / 183679.3304 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1481.95,                last time consumption/overall running time: 776.0513s / 184455.3817 s
env0_first_0:                 episode reward: 5.9500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.9500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1542.3,                last time consumption/overall running time: 797.7968s / 185253.1785 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1640.85,                last time consumption/overall running time: 836.2663s / 186089.4448 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0018
env0_second_0:                 episode reward: -3.4000,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1629.6,                last time consumption/overall running time: 794.0087s / 186883.4535 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0018
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1946.5,                last time consumption/overall running time: 959.4809s / 187842.9344 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0019
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1736.75,                last time consumption/overall running time: 865.8169s / 188708.7513 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0021
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1653.45,                last time consumption/overall running time: 829.0711s / 189537.8224 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0020
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1620.4,                last time consumption/overall running time: 810.5014s / 190348.3238 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0020
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1741.15,                last time consumption/overall running time: 861.8140s / 191210.1378 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0019
env0_second_0:                 episode reward: -1.8500,                 loss: nan
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1607.55,                last time consumption/overall running time: 791.1137s / 192001.2515 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0020
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1664.15,                last time consumption/overall running time: 806.6877s / 192807.9392 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0022
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1566.35,                last time consumption/overall running time: 754.1434s / 193562.0826 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0022
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1465.75,                last time consumption/overall running time: 708.4546s / 194270.5372 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0020
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1503.3,                last time consumption/overall running time: 729.7392s / 195000.2764 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1552.9,                last time consumption/overall running time: 761.3427s / 195761.6191 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.0500,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1684.65,                last time consumption/overall running time: 826.2467s / 196587.8658 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0018
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1511.6,                last time consumption/overall running time: 744.9627s / 197332.8285 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0018
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1521.9,                last time consumption/overall running time: 741.8131s / 198074.6416 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0019
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1445.65,                last time consumption/overall running time: 715.2483s / 198789.8899 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1676.25,                last time consumption/overall running time: 820.9516s / 199610.8415 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.1000,                 loss: nan
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1460.2,                last time consumption/overall running time: 709.7842s / 200320.6257 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1403.55,                last time consumption/overall running time: 677.6336s / 200998.2592 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1365.9,                last time consumption/overall running time: 657.0168s / 201655.2760 s
env0_first_0:                 episode reward: 6.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -6.8000,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1517.8,                last time consumption/overall running time: 732.7420s / 202388.0179 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0017
env0_second_0:                 episode reward: -2.1000,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1513.65,                last time consumption/overall running time: 728.3727s / 203116.3907 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0021
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1401.15,                last time consumption/overall running time: 689.3272s / 203805.7179 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1619.3,                last time consumption/overall running time: 792.4057s / 204598.1236 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1372.05,                last time consumption/overall running time: 663.5826s / 205261.7062 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0015
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1484.05,                last time consumption/overall running time: 722.3935s / 205984.0997 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0015
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1408.55,                last time consumption/overall running time: 695.7390s / 206679.8387 s
env0_first_0:                 episode reward: 6.6000,                 loss: 0.0014
env0_second_0:                 episode reward: -6.6000,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1398.35,                last time consumption/overall running time: 693.6659s / 207373.5046 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0014
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1389.9,                last time consumption/overall running time: 684.9951s / 208058.4997 s
env0_first_0:                 episode reward: 6.4000,                 loss: 0.0015
env0_second_0:                 episode reward: -6.4000,                 loss: nan
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1500.0,                last time consumption/overall running time: 734.2429s / 208792.7426 s
env0_first_0:                 episode reward: 6.6500,                 loss: 0.0014
env0_second_0:                 episode reward: -6.6500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1425.55,                last time consumption/overall running time: 694.6474s / 209487.3900 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1344.85,                last time consumption/overall running time: 656.0558s / 210143.4458 s
env0_first_0:                 episode reward: 8.0000,                 loss: 0.0014
env0_second_0:                 episode reward: -8.0000,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1725.1,                last time consumption/overall running time: 1135.2845s / 211278.7303 s
env0_first_0:                 episode reward: 3.1000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0022
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/5242_0.
Episode: 5281/10000 (52.8100%),                 avg. length: 1991.45,                last time consumption/overall running time: 977.8225s / 212256.5528 s
env0_first_0:                 episode reward: -0.5000,                 loss: nan
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0019
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 2072.75,                last time consumption/overall running time: 1019.4234s / 213275.9762 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0018
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 2023.45,                last time consumption/overall running time: 1007.7102s / 214283.6864 s
env0_first_0:                 episode reward: -1.0000,                 loss: nan
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0017
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1995.85,                last time consumption/overall running time: 988.9433s / 215272.6297 s
env0_first_0:                 episode reward: -2.1500,                 loss: nan
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0017
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1941.75,                last time consumption/overall running time: 957.6955s / 216230.3253 s
env0_first_0:                 episode reward: -1.1000,                 loss: nan
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0017
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1833.3,                last time consumption/overall running time: 886.3403s / 217116.6655 s
env0_first_0:                 episode reward: -3.4000,                 loss: nan
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0016
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1836.35,                last time consumption/overall running time: 847.9259s / 217964.5915 s
env0_first_0:                 episode reward: -2.8000,                 loss: nan
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0015
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1743.05,                last time consumption/overall running time: 807.1032s / 218771.6946 s
env0_first_0:                 episode reward: -3.6000,                 loss: nan
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0015
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1743.75,                last time consumption/overall running time: 814.4350s / 219586.1296 s
env0_first_0:                 episode reward: -3.5000,                 loss: nan
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0013
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1808.8,                last time consumption/overall running time: 850.0661s / 220436.1957 s
env0_first_0:                 episode reward: -4.9000,                 loss: nan
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0013
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1833.1,                last time consumption/overall running time: 856.8777s / 221293.0734 s
env0_first_0:                 episode reward: -2.7000,                 loss: nan
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0014
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1645.2,                last time consumption/overall running time: 772.4974s / 222065.5708 s
env0_first_0:                 episode reward: -5.6500,                 loss: nan
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0014
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1804.0,                last time consumption/overall running time: 843.4081s / 222908.9789 s
env0_first_0:                 episode reward: -4.0500,                 loss: nan
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0014
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1710.7,                last time consumption/overall running time: 795.0311s / 223704.0100 s
env0_first_0:                 episode reward: -6.0000,                 loss: nan
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0014
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1733.25,                last time consumption/overall running time: 798.3363s / 224502.3463 s
env0_first_0:                 episode reward: -4.8000,                 loss: nan
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0013
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1730.7,                last time consumption/overall running time: 793.2163s / 225295.5626 s
env0_first_0:                 episode reward: -3.5500,                 loss: nan
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0013
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1653.05,                last time consumption/overall running time: 767.6786s / 226063.2411 s
env0_first_0:                 episode reward: -3.5500,                 loss: nan
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0013
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1679.95,                last time consumption/overall running time: 780.3075s / 226843.5486 s
env0_first_0:                 episode reward: -4.6000,                 loss: nan
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0012
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1593.85,                last time consumption/overall running time: 747.1302s / 227590.6788 s
env0_first_0:                 episode reward: -4.9500,                 loss: nan
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0012
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1611.45,                last time consumption/overall running time: 757.1406s / 228347.8195 s
env0_first_0:                 episode reward: -3.7500,                 loss: nan
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0014
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1600.25,                last time consumption/overall running time: 750.5082s / 229098.3276 s
env0_first_0:                 episode reward: -5.6500,                 loss: nan
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0012
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1866.85,                last time consumption/overall running time: 871.1101s / 229969.4378 s
env0_first_0:                 episode reward: -3.2500,                 loss: nan
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0013
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1691.1,                last time consumption/overall running time: 776.0531s / 230745.4908 s
env0_first_0:                 episode reward: -4.5000,                 loss: nan
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0014
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1751.4,                last time consumption/overall running time: 810.4913s / 231555.9821 s
env0_first_0:                 episode reward: -5.0500,                 loss: nan
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0014
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1532.4,                last time consumption/overall running time: 695.9842s / 232251.9663 s
env0_first_0:                 episode reward: -5.6000,                 loss: nan
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0013
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1620.3,                last time consumption/overall running time: 726.4163s / 232978.3826 s
env0_first_0:                 episode reward: -4.8500,                 loss: nan
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0012
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1772.25,                last time consumption/overall running time: 801.1264s / 233779.5090 s
env0_first_0:                 episode reward: -4.4000,                 loss: nan
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0015
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1729.15,                last time consumption/overall running time: 788.0292s / 234567.5382 s
env0_first_0:                 episode reward: -2.5000,                 loss: nan
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0017
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1798.2,                last time consumption/overall running time: 836.7356s / 235404.2739 s
env0_first_0:                 episode reward: -3.2000,                 loss: nan
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0017
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1744.95,                last time consumption/overall running time: 823.9362s / 236228.2101 s
env0_first_0:                 episode reward: -5.2500,                 loss: nan
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0016
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1755.8,                last time consumption/overall running time: 819.7034s / 237047.9135 s
env0_first_0:                 episode reward: -4.3000,                 loss: nan
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0016
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1635.8,                last time consumption/overall running time: 744.7526s / 237792.6661 s
env0_first_0:                 episode reward: -6.0500,                 loss: nan
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0014
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1626.1,                last time consumption/overall running time: 744.1174s / 238536.7835 s
env0_first_0:                 episode reward: -6.4500,                 loss: nan
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0014
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1772.6,                last time consumption/overall running time: 810.5998s / 239347.3833 s
env0_first_0:                 episode reward: -3.6000,                 loss: nan
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0015
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1775.25,                last time consumption/overall running time: 822.0177s / 240169.4009 s
env0_first_0:                 episode reward: -5.2500,                 loss: nan
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0017
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1671.0,                last time consumption/overall running time: 775.9782s / 240945.3792 s
env0_first_0:                 episode reward: -4.2500,                 loss: nan
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0015
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 1566.05,                last time consumption/overall running time: 727.4560s / 241672.8352 s
env0_first_0:                 episode reward: -6.3000,                 loss: nan
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0014
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1800.15,                last time consumption/overall running time: 835.2734s / 242508.1086 s
env0_first_0:                 episode reward: -3.3500,                 loss: nan
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0015
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1551.25,                last time consumption/overall running time: 727.2246s / 243235.3332 s
env0_first_0:                 episode reward: -6.8000,                 loss: nan
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0014
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 1609.65,                last time consumption/overall running time: 1043.1823s / 244278.5155 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0028
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0011
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Score delta: 16.4, save the model to .//data/model/20220119_0521/pettingzoo_surround_v1_nxdo2/6052_1.
Episode: 6081/10000 (60.8100%),                 avg. length: 1707.0,                last time consumption/overall running time: 793.0820s / 245071.5975 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0022
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1631.15,                last time consumption/overall running time: 746.9810s / 245818.5784 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0022
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1644.15,                last time consumption/overall running time: 758.4685s / 246577.0469 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1534.65,                last time consumption/overall running time: 704.7239s / 247281.7709 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1665.2,                last time consumption/overall running time: 768.2035s / 248049.9743 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0018
env0_second_0:                 episode reward: -3.2000,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1758.75,                last time consumption/overall running time: 816.3904s / 248866.3647 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1675.55,                last time consumption/overall running time: 778.0860s / 249644.4507 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1678.6,                last time consumption/overall running time: 779.0853s / 250423.5360 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1524.45,                last time consumption/overall running time: 709.5604s / 251133.0964 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -6.1500,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1570.15,                last time consumption/overall running time: 741.2244s / 251874.3208 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1362.55,                last time consumption/overall running time: 638.2964s / 252512.6172 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0014
env0_second_0:                 episode reward: -6.5500,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1473.2,                last time consumption/overall running time: 684.0085s / 253196.6257 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1604.7,                last time consumption/overall running time: 741.7312s / 253938.3570 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 1719.95,                last time consumption/overall running time: 791.9605s / 254730.3175 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.4000,                 loss: nan
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1768.7,                last time consumption/overall running time: 814.1551s / 255544.4726 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0018
env0_second_0:                 episode reward: -3.8000,                 loss: nan
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1610.5,                last time consumption/overall running time: 744.3093s / 256288.7819 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0019
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 1782.55,                last time consumption/overall running time: 815.4611s / 257104.2430 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0019
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1738.6,                last time consumption/overall running time: 752.6624s / 257856.9054 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1660.0,                last time consumption/overall running time: 696.7866s / 258553.6920 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.0500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1424.4,                last time consumption/overall running time: 581.6195s / 259135.3115 s
env0_first_0:                 episode reward: 7.0000,                 loss: 0.0015
env0_second_0:                 episode reward: -7.0000,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1437.05,                last time consumption/overall running time: 589.6159s / 259724.9274 s
env0_first_0:                 episode reward: 6.2500,                 loss: 0.0014
env0_second_0:                 episode reward: -6.2500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1575.6,                last time consumption/overall running time: 640.7885s / 260365.7160 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0015
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1644.1,                last time consumption/overall running time: 659.6808s / 261025.3967 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0015
env0_second_0:                 episode reward: -2.2500,                 loss: nan
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1758.95,                last time consumption/overall running time: 709.8625s / 261735.2592 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.6000,                 loss: nan
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1639.45,                last time consumption/overall running time: 663.4682s / 262398.7274 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0019
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1538.0,                last time consumption/overall running time: 621.1498s / 263019.8772 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0019
env0_second_0:                 episode reward: -2.4000,                 loss: nan
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1633.9,                last time consumption/overall running time: 652.7699s / 263672.6471 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0019
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1568.5,                last time consumption/overall running time: 629.2927s / 264301.9398 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0018
env0_second_0:                 episode reward: -2.2500,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1643.25,                last time consumption/overall running time: 657.9758s / 264959.9156 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0018
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1597.95,                last time consumption/overall running time: 642.4135s / 265602.3291 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0019
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 1780.7,                last time consumption/overall running time: 714.6114s / 266316.9405 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0018
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1410.55,                last time consumption/overall running time: 564.1956s / 266881.1361 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0018
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1444.8,                last time consumption/overall running time: 578.3695s / 267459.5057 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.8000,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1594.4,                last time consumption/overall running time: 640.1631s / 268099.6688 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1678.0,                last time consumption/overall running time: 682.8883s / 268782.5571 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0018
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1500.85,                last time consumption/overall running time: 612.9568s / 269395.5139 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1625.35,                last time consumption/overall running time: 670.7788s / 270066.2928 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1721.6,                last time consumption/overall running time: 706.8759s / 270773.1687 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0017
env0_second_0:                 episode reward: -1.8500,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1560.55,                last time consumption/overall running time: 630.7769s / 271403.9455 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1453.3,                last time consumption/overall running time: 587.9675s / 271991.9130 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1446.8,                last time consumption/overall running time: 584.4782s / 272576.3913 s
env0_first_0:                 episode reward: 5.5000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.5000,                 loss: nan
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1508.85,                last time consumption/overall running time: 603.4178s / 273179.8091 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 1479.55,                last time consumption/overall running time: 589.8609s / 273769.6701 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1623.1,                last time consumption/overall running time: 654.7243s / 274424.3943 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1533.2,                last time consumption/overall running time: 622.7439s / 275047.1382 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1372.0,                last time consumption/overall running time: 561.1611s / 275608.2993 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1439.95,                last time consumption/overall running time: 585.7850s / 276194.0843 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 1650.7,                last time consumption/overall running time: 669.4736s / 276863.5579 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0014
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1387.15,                last time consumption/overall running time: 567.3690s / 277430.9269 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1503.4,                last time consumption/overall running time: 612.6392s / 278043.5661 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1588.1,                last time consumption/overall running time: 634.9856s / 278678.5517 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.8000,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1426.35,                last time consumption/overall running time: 572.0865s / 279250.6382 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 1553.25,                last time consumption/overall running time: 627.4102s / 279878.0483 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 1565.65,                last time consumption/overall running time: 631.0461s / 280509.0945 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1456.35,                last time consumption/overall running time: 587.1414s / 281096.2358 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 1501.3,                last time consumption/overall running time: 603.4224s / 281699.6582 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1394.6,                last time consumption/overall running time: 563.5122s / 282263.1704 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1502.8,                last time consumption/overall running time: 603.0999s / 282866.2703 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0014
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1571.15,                last time consumption/overall running time: 627.8770s / 283494.1473 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1432.65,                last time consumption/overall running time: 574.2502s / 284068.3975 s
env0_first_0:                 episode reward: 5.9500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.9500,                 loss: nan
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1418.45,                last time consumption/overall running time: 576.1028s / 284644.5003 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0015
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1414.95,                last time consumption/overall running time: 581.0870s / 285225.5873 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1417.55,                last time consumption/overall running time: 575.2618s / 285800.8491 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1379.85,                last time consumption/overall running time: 557.1258s / 286357.9750 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0013
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1505.85,                last time consumption/overall running time: 602.3123s / 286960.2873 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1489.15,                last time consumption/overall running time: 590.6838s / 287550.9711 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1522.0,                last time consumption/overall running time: 602.0470s / 288153.0182 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1509.2,                last time consumption/overall running time: 595.0359s / 288748.0540 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1525.4,                last time consumption/overall running time: 603.7533s / 289351.8074 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1463.7,                last time consumption/overall running time: 587.7994s / 289939.6067 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1489.2,                last time consumption/overall running time: 608.1381s / 290547.7448 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 1649.55,                last time consumption/overall running time: 671.7417s / 291219.4865 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.8000,                 loss: nan
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1509.95,                last time consumption/overall running time: 611.6052s / 291831.0917 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1520.5,                last time consumption/overall running time: 613.6023s / 292444.6939 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1647.6,                last time consumption/overall running time: 655.6811s / 293100.3750 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0015
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1476.4,                last time consumption/overall running time: 502.7486s / 293603.1236 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1329.5,                last time consumption/overall running time: 447.7496s / 294050.8732 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1487.55,                last time consumption/overall running time: 496.0828s / 294546.9560 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1416.2,                last time consumption/overall running time: 463.1196s / 295010.0756 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0014
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1515.1,                last time consumption/overall running time: 503.5718s / 295513.6474 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 1880.9,                last time consumption/overall running time: 634.7105s / 296148.3579 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1448.6,                last time consumption/overall running time: 488.0579s / 296636.4158 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1409.9,                last time consumption/overall running time: 479.0706s / 297115.4864 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1477.0,                last time consumption/overall running time: 502.5562s / 297618.0426 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1620.05,                last time consumption/overall running time: 553.3391s / 298171.3817 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 1592.75,                last time consumption/overall running time: 548.1382s / 298719.5199 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0018
env0_second_0:                 episode reward: -1.3500,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 1422.95,                last time consumption/overall running time: 487.4592s / 299206.9791 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 1403.4,                last time consumption/overall running time: 473.3134s / 299680.2925 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0018
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 1477.0,                last time consumption/overall running time: 494.9953s / 300175.2878 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 1475.85,                last time consumption/overall running time: 495.8008s / 300671.0886 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 1446.4,                last time consumption/overall running time: 485.9030s / 301156.9915 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 1539.85,                last time consumption/overall running time: 511.0175s / 301668.0090 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 1425.7,                last time consumption/overall running time: 471.5804s / 302139.5895 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1383.7,                last time consumption/overall running time: 465.1802s / 302604.7697 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1356.85,                last time consumption/overall running time: 460.9779s / 303065.7476 s
env0_first_0:                 episode reward: 6.3500,                 loss: 0.0015
env0_second_0:                 episode reward: -6.3500,                 loss: nan
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 1357.45,                last time consumption/overall running time: 467.8256s / 303533.5732 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 1504.55,                last time consumption/overall running time: 512.4933s / 304046.0665 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0014
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 1493.2,                last time consumption/overall running time: 504.5181s / 304550.5846 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 1431.25,                last time consumption/overall running time: 487.2654s / 305037.8500 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 1434.65,                last time consumption/overall running time: 489.8094s / 305527.6594 s
env0_first_0:                 episode reward: 6.1000,                 loss: 0.0014
env0_second_0:                 episode reward: -6.1000,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1361.05,                last time consumption/overall running time: 460.0595s / 305987.7189 s
env0_first_0:                 episode reward: 5.3000,                 loss: 0.0013
env0_second_0:                 episode reward: -5.3000,                 loss: nan
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1470.5,                last time consumption/overall running time: 487.7203s / 306475.4392 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0014
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 1498.3,                last time consumption/overall running time: 500.3774s / 306975.8167 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 1494.7,                last time consumption/overall running time: 489.1543s / 307464.9710 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1460.75,                last time consumption/overall running time: 473.1995s / 307938.1704 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1413.65,                last time consumption/overall running time: 457.7562s / 308395.9267 s
env0_first_0:                 episode reward: 5.7500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.7500,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 1542.4,                last time consumption/overall running time: 507.0988s / 308903.0255 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1452.3,                last time consumption/overall running time: 486.9534s / 309389.9789 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 1479.45,                last time consumption/overall running time: 496.1438s / 309886.1227 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1362.15,                last time consumption/overall running time: 457.4874s / 310343.6100 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1383.25,                last time consumption/overall running time: 469.6096s / 310813.2196 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 1440.7,                last time consumption/overall running time: 493.5427s / 311306.7623 s
env0_first_0:                 episode reward: 5.9500,                 loss: 0.0014
env0_second_0:                 episode reward: -5.9500,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 1419.0,                last time consumption/overall running time: 493.5655s / 311800.3278 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 1581.5,                last time consumption/overall running time: 541.5108s / 312341.8386 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 1416.4,                last time consumption/overall running time: 478.1989s / 312820.0376 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 1346.4,                last time consumption/overall running time: 444.5133s / 313264.5509 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1507.6,                last time consumption/overall running time: 494.7179s / 313759.2688 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.4000,                 loss: nan
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 1498.25,                last time consumption/overall running time: 494.7872s / 314254.0560 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1518.25,                last time consumption/overall running time: 499.0875s / 314753.1436 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1440.35,                last time consumption/overall running time: 480.5324s / 315233.6759 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1432.55,                last time consumption/overall running time: 490.2589s / 315723.9348 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 1385.2,                last time consumption/overall running time: 463.7832s / 316187.7181 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 1525.7,                last time consumption/overall running time: 508.8955s / 316696.6135 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1534.55,                last time consumption/overall running time: 516.6205s / 317213.2340 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 1689.25,                last time consumption/overall running time: 575.4546s / 317788.6887 s
env0_first_0:                 episode reward: 3.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.4500,                 loss: nan
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 1478.25,                last time consumption/overall running time: 496.9683s / 318285.6570 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0018
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 1466.85,                last time consumption/overall running time: 489.9371s / 318775.5941 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0018
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 1395.85,                last time consumption/overall running time: 459.2615s / 319234.8556 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0018
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1519.2,                last time consumption/overall running time: 500.3895s / 319735.2450 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0018
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1476.4,                last time consumption/overall running time: 493.8268s / 320229.0719 s
env0_first_0:                 episode reward: 3.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.4500,                 loss: nan
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 1561.7,                last time consumption/overall running time: 527.1313s / 320756.2031 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1554.55,                last time consumption/overall running time: 523.4616s / 321279.6648 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0016
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1548.35,                last time consumption/overall running time: 523.7745s / 321803.4392 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1484.4,                last time consumption/overall running time: 506.1331s / 322309.5723 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1472.05,                last time consumption/overall running time: 503.4710s / 322813.0434 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1416.7,                last time consumption/overall running time: 462.6007s / 323275.6441 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0015
env0_second_0:                 episode reward: -6.0500,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1467.8,                last time consumption/overall running time: 463.6370s / 323739.2811 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1389.55,                last time consumption/overall running time: 440.6250s / 324179.9062 s
env0_first_0:                 episode reward: 5.5000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.5000,                 loss: nan
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1686.3,                last time consumption/overall running time: 540.5474s / 324720.4535 s
env0_first_0:                 episode reward: 3.3500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.3500,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 1500.0,                last time consumption/overall running time: 474.6109s / 325195.0644 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1591.95,                last time consumption/overall running time: 498.6220s / 325693.6864 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.8000,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 1452.4,                last time consumption/overall running time: 453.1449s / 326146.8314 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.5500,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1490.35,                last time consumption/overall running time: 466.2713s / 326613.1027 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 1404.3,                last time consumption/overall running time: 439.3498s / 327052.4525 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1407.85,                last time consumption/overall running time: 439.2167s / 327491.6692 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1513.9,                last time consumption/overall running time: 465.5761s / 327957.2453 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1422.6,                last time consumption/overall running time: 436.3052s / 328393.5505 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1432.7,                last time consumption/overall running time: 441.5733s / 328835.1239 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1559.9,                last time consumption/overall running time: 480.4477s / 329315.5716 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 1454.75,                last time consumption/overall running time: 448.7387s / 329764.3103 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0018
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 1491.95,                last time consumption/overall running time: 467.0294s / 330231.3397 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0018
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1408.65,                last time consumption/overall running time: 448.7806s / 330680.1202 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 1478.15,                last time consumption/overall running time: 468.9601s / 331149.0803 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0017
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 1439.45,                last time consumption/overall running time: 450.7421s / 331599.8223 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0018
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 1418.7,                last time consumption/overall running time: 437.4408s / 332037.2631 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1374.3,                last time consumption/overall running time: 415.3033s / 332452.5664 s
env0_first_0:                 episode reward: 6.3500,                 loss: 0.0015
env0_second_0:                 episode reward: -6.3500,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1435.95,                last time consumption/overall running time: 436.8621s / 332889.4286 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0015
env0_second_0:                 episode reward: -4.0500,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 1375.85,                last time consumption/overall running time: 422.1658s / 333311.5943 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 1544.6,                last time consumption/overall running time: 474.8722s / 333786.4665 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0015
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1725.0,                last time consumption/overall running time: 523.1653s / 334309.6318 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0018
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1549.3,                last time consumption/overall running time: 477.7385s / 334787.3703 s
env0_first_0:                 episode reward: 3.1000,                 loss: 0.0019
env0_second_0:                 episode reward: -3.1000,                 loss: nan
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 1429.3,                last time consumption/overall running time: 446.3004s / 335233.6707 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 1534.85,                last time consumption/overall running time: 479.7931s / 335713.4638 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 1570.2,                last time consumption/overall running time: 490.5068s / 336203.9706 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0018
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1526.5,                last time consumption/overall running time: 476.1959s / 336680.1664 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1457.75,                last time consumption/overall running time: 459.3832s / 337139.5496 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1535.4,                last time consumption/overall running time: 477.1539s / 337616.7035 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1436.25,                last time consumption/overall running time: 443.0085s / 338059.7120 s
env0_first_0:                 episode reward: 5.5000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.5000,                 loss: nan
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 1537.4,                last time consumption/overall running time: 468.7596s / 338528.4716 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 1495.0,                last time consumption/overall running time: 453.5159s / 338981.9875 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 1445.8,                last time consumption/overall running time: 439.9744s / 339421.9619 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0016
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 1312.5,                last time consumption/overall running time: 396.1553s / 339818.1172 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0015
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1430.95,                last time consumption/overall running time: 432.1965s / 340250.3137 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 1401.5,                last time consumption/overall running time: 432.7175s / 340683.0312 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0014
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 1568.65,                last time consumption/overall running time: 496.2561s / 341179.2873 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0016
env0_second_0:                 episode reward: -3.2000,                 loss: nan
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1441.6,                last time consumption/overall running time: 455.7745s / 341635.0618 s
env0_first_0:                 episode reward: 5.0000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.0000,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 1444.4,                last time consumption/overall running time: 457.1513s / 342092.2131 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1462.95,                last time consumption/overall running time: 460.9080s / 342553.1211 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 1450.75,                last time consumption/overall running time: 453.7729s / 343006.8940 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0016
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 1428.35,                last time consumption/overall running time: 439.9382s / 343446.8322 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0017
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1531.35,                last time consumption/overall running time: 476.2439s / 343923.0761 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.5000,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 1420.95,                last time consumption/overall running time: 436.9700s / 344360.0461 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0015
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1586.85,                last time consumption/overall running time: 487.7400s / 344847.7861 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0017
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 1400.2,                last time consumption/overall running time: 429.9398s / 345277.7259 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.4000,                 loss: nanLoad surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr = np.asanyarray(arr)

env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 1576.2,                last time consumption/overall running time: 482.8624s / 345760.5883 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0017
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1461.2,                last time consumption/overall running time: 454.2396s / 346214.8279 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 1518.6,                last time consumption/overall running time: 473.1607s / 346687.9886 s
env0_first_0:                 episode reward: 5.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.2500,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1486.6,                last time consumption/overall running time: 462.2767s / 347150.2653 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0017
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1412.7,                last time consumption/overall running time: 446.2401s / 347596.5054 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 1605.1,                last time consumption/overall running time: 506.3481s / 348102.8535 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0016
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 1297.6,                last time consumption/overall running time: 409.6764s / 348512.5299 s
env0_first_0:                 episode reward: 7.5500,                 loss: 0.0014
env0_second_0:                 episode reward: -7.5500,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 1325.55,                last time consumption/overall running time: 418.7140s / 348931.2439 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0014
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1529.25,                last time consumption/overall running time: 478.9693s / 349410.2133 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0016
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1359.3,                last time consumption/overall running time: 424.1509s / 349834.3642 s
env0_first_0:                 episode reward: 6.3500,                 loss: 0.0015
env0_second_0:                 episode reward: -6.3500,                 loss: nan
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1342.8,                last time consumption/overall running time: 415.8142s / 350250.1783 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0014
env0_second_0:                 episode reward: -6.0500,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1522.9,                last time consumption/overall running time: 470.5455s / 350720.7239 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0014
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
