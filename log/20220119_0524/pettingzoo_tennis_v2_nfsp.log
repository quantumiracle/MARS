pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0524/pettingzoo_tennis_v2_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0524/pettingzoo_tennis_v2_nfsp.
Episode: 1/10000 (0.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 42.5146s / 42.5146 s
env0_first_0:                 episode reward: -147.0000,                 loss: nan
env0_second_0:                 episode reward: 147.0000,                 loss: nan
env1_first_0:                 episode reward: -136.0000,                 loss: nan
env1_second_0:                 episode reward: 136.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 4173.65,                last time consumption/overall running time: 3023.2635s / 3065.7782 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0075
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0079
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 7785.25,                last time consumption/overall running time: 5911.5627s / 8977.3409 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0050
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 9727.35,                last time consumption/overall running time: 7388.1410s / 16365.4820 s
env0_first_0:                 episode reward: 19.4500,                 loss: 0.0070
env0_second_0:                 episode reward: -19.4500,                 loss: 0.0069
env1_first_0:                 episode reward: 25.5500,                 loss: nan
env1_second_0:                 episode reward: -25.5500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 9645.55,                last time consumption/overall running time: 7333.1488s / 23698.6307 s
env0_first_0:                 episode reward: 30.2500,                 loss: 0.0058
env0_second_0:                 episode reward: -30.2500,                 loss: 0.0058
env1_first_0:                 episode reward: 21.0500,                 loss: nan
env1_second_0:                 episode reward: -21.0500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 9162.5,                last time consumption/overall running time: 6963.9528s / 30662.5835 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0037
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0039
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 9604.1,                last time consumption/overall running time: 7400.8274s / 38063.4110 s
env0_first_0:                 episode reward: 109.2500,                 loss: 0.0073
env0_second_0:                 episode reward: -109.2500,                 loss: 0.0081
env1_first_0:                 episode reward: 120.9000,                 loss: nan
env1_second_0:                 episode reward: -120.9000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 9045.3,                last time consumption/overall running time: 8169.3033s / 46232.7143 s
env0_first_0:                 episode reward: 27.6000,                 loss: 0.0059
env0_second_0:                 episode reward: -27.6000,                 loss: 0.0060
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 9093.9,                last time consumption/overall running time: 8209.0988s / 54441.8131 s
env0_first_0:                 episode reward: 50.1000,                 loss: 0.0071
env0_second_0:                 episode reward: -50.1000,                 loss: 0.0071
env1_first_0:                 episode reward: 25.6500,                 loss: nan
env1_second_0:                 episode reward: -25.6500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 8829.0,                last time consumption/overall running time: 7984.8642s / 62426.6773 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0051
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 8320.0,                last time consumption/overall running time: 7518.2931s / 69944.9704 s
env0_first_0:                 episode reward: 9.4000,                 loss: 0.0049
env0_second_0:                 episode reward: -9.4000,                 loss: 0.0056
env1_first_0:                 episode reward: -11.4500,                 loss: nan
env1_second_0:                 episode reward: 11.4500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 9102.8,                last time consumption/overall running time: 8224.7969s / 78169.7673 s
env0_first_0:                 episode reward: 35.2500,                 loss: 0.0062
env0_second_0:                 episode reward: -35.2500,                 loss: 0.0066
env1_first_0:                 episode reward: 38.0500,                 loss: nan
env1_second_0:                 episode reward: -38.0500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 8724.0,                last time consumption/overall running time: 7883.4209s / 86053.1881 s
env0_first_0:                 episode reward: 14.4500,                 loss: 0.0053
env0_second_0:                 episode reward: -14.4500,                 loss: 0.0057
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 6181.95,                last time consumption/overall running time: 5582.1469s / 91635.3351 s
env0_first_0:                 episode reward: 34.3000,                 loss: 0.0051
env0_second_0:                 episode reward: -34.3000,                 loss: 0.0084
env1_first_0:                 episode reward: 25.5500,                 loss: nan
env1_second_0:                 episode reward: -25.5500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 7997.55,                last time consumption/overall running time: 7202.4018s / 98837.7369 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0054
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0058
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 8347.25,                last time consumption/overall running time: 7508.6457s / 106346.3826 s
env0_first_0:                 episode reward: 15.9000,                 loss: 0.0046
env0_second_0:                 episode reward: -15.9000,                 loss: 0.0050
env1_first_0:                 episode reward: 21.8000,                 loss: nan
env1_second_0:                 episode reward: -21.8000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 8540.7,                last time consumption/overall running time: 7687.9428s / 114034.3254 s
env0_first_0:                 episode reward: 10.9000,                 loss: 0.0043
env0_second_0:                 episode reward: -10.9000,                 loss: 0.0048
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 8818.25,                last time consumption/overall running time: 7940.0577s / 121974.3831 s
env0_first_0:                 episode reward: 16.6500,                 loss: 0.0044
env0_second_0:                 episode reward: -16.6500,                 loss: 0.0046
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 8810.95,                last time consumption/overall running time: 7924.7821s / 129899.1651 s
env0_first_0:                 episode reward: 12.3500,                 loss: 0.0048
env0_second_0:                 episode reward: -12.3500,                 loss: 0.0049
env1_first_0:                 episode reward: 14.8500,                 loss: nan
env1_second_0:                 episode reward: -14.8500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 9043.8,                last time consumption/overall running time: 8143.6879s / 138042.8530 s
env0_first_0:                 episode reward: -14.3000,                 loss: 0.0048
env0_second_0:                 episode reward: 14.3000,                 loss: 0.0053
env1_first_0:                 episode reward: -11.3500,                 loss: nan
env1_second_0:                 episode reward: 11.3500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 9144.65,                last time consumption/overall running time: 8067.0295s / 146109.8825 s
env0_first_0:                 episode reward: 36.4500,                 loss: 0.0068
env0_second_0:                 episode reward: -36.4500,                 loss: 0.0065
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 8547.6,                last time consumption/overall running time: 6663.2159s / 152773.0985 s
env0_first_0:                 episode reward: 32.7500,                 loss: 0.0076
env0_second_0:                 episode reward: -32.7500,                 loss: 0.0069
env1_first_0:                 episode reward: 37.4000,                 loss: nan
env1_second_0:                 episode reward: -37.4000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 8371.6,                last time consumption/overall running time: 6476.0588s / 159249.1573 s
env0_first_0:                 episode reward: -25.0500,                 loss: 0.0069
env0_second_0:                 episode reward: 25.0500,                 loss: 0.0068
env1_first_0:                 episode reward: -30.1000,                 loss: nan
env1_second_0:                 episode reward: 30.1000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 8418.0,                last time consumption/overall running time: 6507.0740s / 165756.2313 s
env0_first_0:                 episode reward: -12.8000,                 loss: 0.0072
env0_second_0:                 episode reward: 12.8000,                 loss: 0.0088
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 9089.5,                last time consumption/overall running time: 7024.7670s / 172780.9982 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0067
env1_first_0:                 episode reward: 17.1500,                 loss: nan
env1_second_0:                 episode reward: -17.1500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 9645.05,                last time consumption/overall running time: 7450.6519s / 180231.6501 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0064
env0_second_0:                 episode reward: -1.2000,                 loss: 0.0059
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 9161.5,                last time consumption/overall running time: 7071.4944s / 187303.1445 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0062
env0_second_0:                 episode reward: -2.9000,                 loss: 0.0066
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 9203.15,                last time consumption/overall running time: 7103.1565s / 194406.3010 s
env0_first_0:                 episode reward: 13.3500,                 loss: 0.0060
env0_second_0:                 episode reward: -13.3500,                 loss: 0.0056
env1_first_0:                 episode reward: 20.6000,                 loss: nan
env1_second_0:                 episode reward: -20.6000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 8240.1,                last time consumption/overall running time: 6330.7217s / 200737.0227 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0050
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0077
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 6625.3,                last time consumption/overall running time: 5070.8956s / 205807.9184 s
env0_first_0:                 episode reward: 11.7000,                 loss: 0.0061
env0_second_0:                 episode reward: -11.7000,                 loss: 0.0085
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 7827.2,                last time consumption/overall running time: 5991.9649s / 211799.8832 s
env0_first_0:                 episode reward: -11.0000,                 loss: 0.0059
env0_second_0:                 episode reward: 11.0000,                 loss: 0.0066
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 8152.95,                last time consumption/overall running time: 6240.2374s / 218040.1206 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0050
env0_second_0:                 episode reward: -6.0500,                 loss: 0.0057
env1_first_0:                 episode reward: 11.1500,                 loss: nan
env1_second_0:                 episode reward: -11.1500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 8428.9,                last time consumption/overall running time: 6445.8079s / 224485.9285 s
env0_first_0:                 episode reward: 49.1000,                 loss: 0.0065
env0_second_0:                 episode reward: -49.1000,                 loss: 0.0075
env1_first_0:                 episode reward: 47.5000,                 loss: nan
env1_second_0:                 episode reward: -47.5000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 9127.5,                last time consumption/overall running time: 6980.2256s / 231466.1541 s
env0_first_0:                 episode reward: 36.8500,                 loss: 0.0051
env0_second_0:                 episode reward: -36.8500,                 loss: 0.0057
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 8990.75,                last time consumption/overall running time: 6858.1340s / 238324.2880 s
env0_first_0:                 episode reward: 34.9500,                 loss: 0.0059
env0_second_0:                 episode reward: -34.9500,                 loss: 0.0065
env1_first_0:                 episode reward: 33.7000,                 loss: nan
env1_second_0:                 episode reward: -33.7000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 9546.3,                last time consumption/overall running time: 7278.3253s / 245602.6133 s
env0_first_0:                 episode reward: 49.8000,                 loss: 0.0062
env0_second_0:                 episode reward: -49.8000,                 loss: 0.0066
env1_first_0:                 episode reward: 35.6000,                 loss: nan
env1_second_0:                 episode reward: -35.6000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 9705.65,                last time consumption/overall running time: 7397.9817s / 253000.5950 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0051
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0054
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 9000.3,                last time consumption/overall running time: 6848.6097s / 259849.2046 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0054
env0_second_0:                 episode reward: -4.5000,                 loss: 0.0064
env1_first_0:                 episode reward: 17.0000,                 loss: nan
env1_second_0:                 episode reward: -17.0000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 9415.7,                last time consumption/overall running time: 7163.4136s / 267012.6182 s
env0_first_0:                 episode reward: -22.0000,                 loss: 0.0049
env0_second_0:                 episode reward: 22.0000,                 loss: 0.0054
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 8193.0,                last time consumption/overall running time: 6239.4950s / 273252.1132 s
env0_first_0:                 episode reward: 45.7000,                 loss: 0.0067
env0_second_0:                 episode reward: -45.7000,                 loss: 0.0086
env1_first_0:                 episode reward: 38.5000,                 loss: nan
env1_second_0:                 episode reward: -38.5000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 8913.6,                last time consumption/overall running time: 6787.1701s / 280039.2833 s
env0_first_0:                 episode reward: 20.9500,                 loss: 0.0055
env0_second_0:                 episode reward: -20.9500,                 loss: 0.0068
env1_first_0:                 episode reward: 16.0000,                 loss: nan
env1_second_0:                 episode reward: -16.0000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 9514.5,                last time consumption/overall running time: 7230.8268s / 287270.1101 s
env0_first_0:                 episode reward: 24.2500,                 loss: 0.0053
env0_second_0:                 episode reward: -24.2500,                 loss: 0.0059
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 9388.25,                last time consumption/overall running time: 7127.8785s / 294397.9887 s
env0_first_0:                 episode reward: 37.5500,                 loss: 0.0057
env0_second_0:                 episode reward: -37.5500,                 loss: 0.0067
env1_first_0:                 episode reward: 30.7000,                 loss: nan
env1_second_0:                 episode reward: -30.7000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 8947.25,                last time consumption/overall running time: 6773.9820s / 301171.9707 s
env0_first_0:                 episode reward: 39.9500,                 loss: 0.0059
env0_second_0:                 episode reward: -39.9500,                 loss: 0.0065
env1_first_0:                 episode reward: 30.6500,                 loss: nan
env1_second_0:                 episode reward: -30.6500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 9761.0,                last time consumption/overall running time: 7371.3372s / 308543.3079 s
env0_first_0:                 episode reward: -35.5500,                 loss: 0.0066
env0_second_0:                 episode reward: 35.5500,                 loss: 0.0068
env1_first_0:                 episode reward: -23.0000,                 loss: nan
env1_second_0:                 episode reward: 23.0000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 9895.95,                last time consumption/overall running time: 7456.4318s / 315999.7397 s
env0_first_0:                 episode reward: -9.2000,                 loss: 0.0061
env0_second_0:                 episode reward: 9.2000,                 loss: 0.0061
env1_first_0:                 episode reward: -9.3000,                 loss: nan
env1_second_0:                 episode reward: 9.3000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 9333.55,                last time consumption/overall running time: 7000.9561s / 323000.6957 s
env0_first_0:                 episode reward: 16.7500,                 loss: 0.0060
env0_second_0:                 episode reward: -16.7500,                 loss: 0.0063
env1_first_0:                 episode reward: 21.0000,                 loss: nan
env1_second_0:                 episode reward: -21.0000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 9908.7,                last time consumption/overall running time: 7416.7822s / 330417.4780 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0063
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0065
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 8745.5,                last time consumption/overall running time: 6536.4550s / 336953.9330 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0059
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0062
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 9097.25,                last time consumption/overall running time: 6785.3250s / 343739.2579 s
env0_first_0:                 episode reward: 24.8000,                 loss: 0.0063
env0_second_0:                 episode reward: -24.8000,                 loss: 0.0070
env1_first_0:                 episode reward: 25.1000,                 loss: nan
env1_second_0:                 episode reward: -25.1000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 9435.75,                last time consumption/overall running time: 7030.3125s / 350769.5705 s
env0_first_0:                 episode reward: 11.0500,                 loss: 0.0053
env0_second_0:                 episode reward: -11.0500,                 loss: 0.0068
env1_first_0:                 episode reward: 20.2000,                 loss: nan
env1_second_0:                 episode reward: -20.2000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 8528.75,                last time consumption/overall running time: 6354.6666s / 357124.2370 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0084
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0082
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 9188.65,                last time consumption/overall running time: 6839.8876s / 363964.1247 s
env0_first_0:                 episode reward: 14.1000,                 loss: 0.0078
env0_second_0:                 episode reward: -14.1000,                 loss: 0.0066
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 9192.4,                last time consumption/overall running time: 6830.4506s / 370794.5753 s
env0_first_0:                 episode reward: -23.2000,                 loss: 0.0072
env0_second_0:                 episode reward: 23.2000,                 loss: 0.0075
env1_first_0:                 episode reward: -28.8000,                 loss: nan
env1_second_0:                 episode reward: 28.8000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 8999.8,                last time consumption/overall running time: 6690.2917s / 377484.8670 s
env0_first_0:                 episode reward: 8.7500,                 loss: 0.0070
env0_second_0:                 episode reward: -8.7500,                 loss: 0.0064
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 9422.45,                last time consumption/overall running time: 6999.2520s / 384484.1190 s
env0_first_0:                 episode reward: 19.5500,                 loss: 0.0069
env0_second_0:                 episode reward: -19.5500,                 loss: 0.0069
env1_first_0:                 episode reward: 31.6000,                 loss: nan
env1_second_0:                 episode reward: -31.6000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 9058.6,                last time consumption/overall running time: 8050.2868s / 392534.4058 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0074
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 9780.3,                last time consumption/overall running time: 8872.6333s / 401407.0391 s
env0_first_0:                 episode reward: 28.3000,                 loss: 0.0061
env0_second_0:                 episode reward: -28.3000,                 loss: 0.0068
env1_first_0:                 episode reward: 12.9500,                 loss: nan
env1_second_0:                 episode reward: -12.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 9806.55,                last time consumption/overall running time: 8883.7559s / 410290.7950 s
env0_first_0:                 episode reward: 15.9500,                 loss: 0.0050
env0_second_0:                 episode reward: -15.9500,                 loss: 0.0058
env1_first_0:                 episode reward: 24.6500,                 loss: nan
env1_second_0:                 episode reward: -24.6500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 9494.2,                last time consumption/overall running time: 8578.1628s / 418868.9578 s
env0_first_0:                 episode reward: 17.0500,                 loss: 0.0046
env0_second_0:                 episode reward: -17.0500,                 loss: 0.0045
env1_first_0:                 episode reward: 14.5000,                 loss: nan
env1_second_0:                 episode reward: -14.5000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 9469.1,                last time consumption/overall running time: 8541.1439s / 427410.1017 s
env0_first_0:                 episode reward: 7.8000,                 loss: 0.0051
env0_second_0:                 episode reward: -7.8000,                 loss: 0.0047
env1_first_0:                 episode reward: 31.7500,                 loss: nan
env1_second_0:                 episode reward: -31.7500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 8492.9,                last time consumption/overall running time: 7664.1419s / 435074.2436 s
env0_first_0:                 episode reward: 14.7000,                 loss: 0.0070
env0_second_0:                 episode reward: -14.7000,                 loss: 0.0091
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 8635.5,                last time consumption/overall running time: 7780.1001s / 442854.3437 s
env0_first_0:                 episode reward: 57.1000,                 loss: 0.0073
env0_second_0:                 episode reward: -57.1000,                 loss: 0.0069
env1_first_0:                 episode reward: 50.9500,                 loss: nan
env1_second_0:                 episode reward: -50.9500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 9147.0,                last time consumption/overall running time: 8205.2001s / 451059.5438 s
env0_first_0:                 episode reward: 12.1000,                 loss: 0.0065
env0_second_0:                 episode reward: -12.1000,                 loss: 0.0054
env1_first_0:                 episode reward: -5.4000,                 loss: nan
env1_second_0:                 episode reward: 5.4000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 9211.55,                last time consumption/overall running time: 8226.8606s / 459286.4044 s
env0_first_0:                 episode reward: 21.9500,                 loss: 0.0067
env0_second_0:                 episode reward: -21.9500,                 loss: 0.0095
env1_first_0:                 episode reward: 20.4500,                 loss: nan
env1_second_0:                 episode reward: -20.4500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 9090.45,                last time consumption/overall running time: 8265.3016s / 467551.7060 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0056
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0070
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 8391.8,                last time consumption/overall running time: 8608.4964s / 476160.2025 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0066
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0076
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 7490.15,                last time consumption/overall running time: 7683.7048s / 483843.9072 s
env0_first_0:                 episode reward: 12.8500,                 loss: 0.0055
env0_second_0:                 episode reward: -12.8500,                 loss: 0.0057
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 8724.15,                last time consumption/overall running time: 8953.1070s / 492797.0142 s
env0_first_0:                 episode reward: -22.8500,                 loss: 0.0091
env0_second_0:                 episode reward: 22.8500,                 loss: 0.0089
env1_first_0:                 episode reward: -42.2000,                 loss: nan
env1_second_0:                 episode reward: 42.2000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 8991.3,                last time consumption/overall running time: 9234.0755s / 502031.0897 s
env0_first_0:                 episode reward: 14.5000,                 loss: 0.0065
env0_second_0:                 episode reward: -14.5000,                 loss: 0.0061
env1_first_0:                 episode reward: 12.7500,                 loss: nan
env1_second_0:                 episode reward: -12.7500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 8615.6,                last time consumption/overall running time: 8903.6570s / 510934.7467 s
env0_first_0:                 episode reward: -15.6000,                 loss: 0.0071
env0_second_0:                 episode reward: 15.6000,                 loss: 0.0074
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 8075.2,                last time consumption/overall running time: 8531.7663s / 519466.5130 s
env0_first_0:                 episode reward: -12.9000,                 loss: 0.0069
env0_second_0:                 episode reward: 12.9000,                 loss: 0.0074
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 8851.45,                last time consumption/overall running time: 9140.7385s / 528607.2514 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0061
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0072
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 9851.05,                last time consumption/overall running time: 9849.9034s / 538457.1548 s
env0_first_0:                 episode reward: -16.3500,                 loss: 0.0062
env0_second_0:                 episode reward: 16.3500,                 loss: 0.0057
env1_first_0:                 episode reward: -24.7500,                 loss: nan
env1_second_0:                 episode reward: 24.7500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 9338.15,                last time consumption/overall running time: 9345.6068s / 547802.7616 s
env0_first_0:                 episode reward: 8.0500,                 loss: 0.0051
env0_second_0:                 episode reward: -8.0500,                 loss: 0.0051
env1_first_0:                 episode reward: 18.5000,                 loss: nan
env1_second_0:                 episode reward: -18.5000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 9468.95,                last time consumption/overall running time: 9472.6993s / 557275.4609 s
env0_first_0:                 episode reward: 7.5000,                 loss: 0.0037
env0_second_0:                 episode reward: -7.5000,                 loss: 0.0040
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 9523.8,                last time consumption/overall running time: 9506.9680s / 566782.4289 s
env0_first_0:                 episode reward: 17.0500,                 loss: 0.0052
env0_second_0:                 episode reward: -17.0500,                 loss: 0.0057
env1_first_0:                 episode reward: 13.0500,                 loss: nan
env1_second_0:                 episode reward: -13.0500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 9509.6,                last time consumption/overall running time: 9496.6296s / 576279.0585 s
env0_first_0:                 episode reward: 7.2500,                 loss: 0.0041
env0_second_0:                 episode reward: -7.2500,                 loss: 0.0044
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 9899.75,                last time consumption/overall running time: 9468.9013s / 585747.9598 s
env0_first_0:                 episode reward: 32.4500,                 loss: 0.0045
env0_second_0:                 episode reward: -32.4500,                 loss: 0.0052
env1_first_0:                 episode reward: 18.4000,                 loss: nan
env1_second_0:                 episode reward: -18.4000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 9279.3,                last time consumption/overall running time: 8207.0038s / 593954.9636 s
env0_first_0:                 episode reward: 17.0500,                 loss: 0.0042
env0_second_0:                 episode reward: -17.0500,                 loss: 0.0044
env1_first_0:                 episode reward: 21.8000,                 loss: nan
env1_second_0:                 episode reward: -21.8000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 9807.65,                last time consumption/overall running time: 8656.8586s / 602611.8222 s
env0_first_0:                 episode reward: 22.0000,                 loss: 0.0057
env0_second_0:                 episode reward: -22.0000,                 loss: 0.0077
env1_first_0:                 episode reward: 43.5000,                 loss: nan
env1_second_0:                 episode reward: -43.5000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 8655.35,                last time consumption/overall running time: 7443.9610s / 610055.7832 s
env0_first_0:                 episode reward: 21.4000,                 loss: 0.0047
env0_second_0:                 episode reward: -21.4000,                 loss: 0.0052
env1_first_0:                 episode reward: 19.5000,                 loss: nan
env1_second_0:                 episode reward: -19.5000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 9703.4,                last time consumption/overall running time: 8037.1781s / 618092.9613 s
env0_first_0:                 episode reward: 8.3000,                 loss: 0.0048
env0_second_0:                 episode reward: -8.3000,                 loss: 0.0050
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 9287.25,                last time consumption/overall running time: 7666.9045s / 625759.8659 s
env0_first_0:                 episode reward: 18.4000,                 loss: 0.0048
env0_second_0:                 episode reward: -18.4000,                 loss: 0.0050
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 9807.3,                last time consumption/overall running time: 7953.8507s / 633713.7165 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0047
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0049
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 9666.9,                last time consumption/overall running time: 7089.9930s / 640803.7095 s
env0_first_0:                 episode reward: 18.9000,                 loss: 0.0038
env0_second_0:                 episode reward: -18.9000,                 loss: 0.0039
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 9653.8,                last time consumption/overall running time: 6711.5414s / 647515.2509 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0039
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0040
env1_first_0:                 episode reward: -11.8500,                 loss: nan
env1_second_0:                 episode reward: 11.8500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 8938.15,                last time consumption/overall running time: 5656.2841s / 653171.5351 s
env0_first_0:                 episode reward: 10.3500,                 loss: 0.0042
env0_second_0:                 episode reward: -10.3500,                 loss: 0.0054
env1_first_0:                 episode reward: 12.0500,                 loss: nan
env1_second_0:                 episode reward: -12.0500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 9945.15,                last time consumption/overall running time: 6162.2841s / 659333.8192 s
env0_first_0:                 episode reward: -11.3000,                 loss: 0.0042
env0_second_0:                 episode reward: 11.3000,                 loss: 0.0047
env1_first_0:                 episode reward: 12.4000,                 loss: nan
env1_second_0:                 episode reward: -12.4000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 9706.35,                last time consumption/overall running time: 5757.9788s / 665091.7980 s
env0_first_0:                 episode reward: 27.5000,                 loss: 0.0039
env0_second_0:                 episode reward: -27.5000,                 loss: 0.0041
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 9508.15,                last time consumption/overall running time: 5528.6373s / 670620.4353 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0050
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0052
env1_first_0:                 episode reward: 12.8500,                 loss: nan
env1_second_0:                 episode reward: -12.8500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 9020.9,                last time consumption/overall running time: 4911.0103s / 675531.4456 s
env0_first_0:                 episode reward: -9.8000,                 loss: 0.0051
env0_second_0:                 episode reward: 9.8000,                 loss: 0.0062
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 9286.9,                last time consumption/overall running time: 4714.7049s / 680246.1505 s
env0_first_0:                 episode reward: -12.8500,                 loss: 0.0060
env0_second_0:                 episode reward: 12.8500,                 loss: 0.0077
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 8819.45,                last time consumption/overall running time: 4484.4303s / 684730.5809 s
env0_first_0:                 episode reward: 26.9500,                 loss: 0.0060
env0_second_0:                 episode reward: -26.9500,                 loss: 0.0080
env1_first_0:                 episode reward: 31.7000,                 loss: nan
env1_second_0:                 episode reward: -31.7000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 9279.9,                last time consumption/overall running time: 4706.0486s / 689436.6295 s
env0_first_0:                 episode reward: 10.9500,                 loss: 0.0054
env0_second_0:                 episode reward: -10.9500,                 loss: 0.0067
env1_first_0:                 episode reward: 15.8000,                 loss: nan
env1_second_0:                 episode reward: -15.8000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 8620.45,                last time consumption/overall running time: 4347.8622s / 693784.4917 s
env0_first_0:                 episode reward: 13.6000,                 loss: 0.0046
env0_second_0:                 episode reward: -13.6000,                 loss: 0.0074
env1_first_0:                 episode reward: 30.2500,                 loss: nan
env1_second_0:                 episode reward: -30.2500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 9392.45,                last time consumption/overall running time: 4722.3455s / 698506.8371 s
env0_first_0:                 episode reward: 29.2000,                 loss: 0.0063
env0_second_0:                 episode reward: -29.2000,                 loss: 0.0072
env1_first_0:                 episode reward: 29.8000,                 loss: nan
env1_second_0:                 episode reward: -29.8000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 9615.1,                last time consumption/overall running time: 4830.1461s / 703336.9833 s
env0_first_0:                 episode reward: -28.2500,                 loss: 0.0067
env0_second_0:                 episode reward: 28.2500,                 loss: 0.0096
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 9575.9,                last time consumption/overall running time: 4804.7043s / 708141.6876 s
env0_first_0:                 episode reward: 31.9500,                 loss: 0.0053
env0_second_0:                 episode reward: -31.9500,                 loss: 0.0059
env1_first_0:                 episode reward: 40.2000,                 loss: nan
env1_second_0:                 episode reward: -40.2000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 9282.95,                last time consumption/overall running time: 4649.7483s / 712791.4359 s
env0_first_0:                 episode reward: -23.9500,                 loss: 0.0037
env0_second_0:                 episode reward: 23.9500,                 loss: 0.0038
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 9854.75,                last time consumption/overall running time: 4946.9886s / 717738.4244 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0031
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0035
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 9633.85,                last time consumption/overall running time: 4830.7820s / 722569.2064 s
env0_first_0:                 episode reward: 7.8500,                 loss: 0.0042
env0_second_0:                 episode reward: -7.8500,                 loss: 0.0051
env1_first_0:                 episode reward: 13.3500,                 loss: nan
env1_second_0:                 episode reward: -13.3500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 9711.9,                last time consumption/overall running time: 4861.9040s / 727431.1104 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0065
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 9871.25,                last time consumption/overall running time: 4933.6495s / 732364.7599 s
env0_first_0:                 episode reward: 15.1500,                 loss: 0.0049
env0_second_0:                 episode reward: -15.1500,                 loss: 0.0057
env1_first_0:                 episode reward: 30.6500,                 loss: nan
env1_second_0:                 episode reward: -30.6500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 9266.15,                last time consumption/overall running time: 4617.5073s / 736982.2672 s
env0_first_0:                 episode reward: -25.6000,                 loss: 0.0065
env0_second_0:                 episode reward: 25.6000,                 loss: 0.0119
env1_first_0:                 episode reward: -12.6000,                 loss: nan
env1_second_0:                 episode reward: 12.6000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 8086.15,                last time consumption/overall running time: 4045.0821s / 741027.3493 s
env0_first_0:                 episode reward: 22.8500,                 loss: 0.0056
env0_second_0:                 episode reward: -22.8500,                 loss: 0.0124
env1_first_0:                 episode reward: 21.1500,                 loss: nan
env1_second_0:                 episode reward: -21.1500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 8910.9,                last time consumption/overall running time: 4445.7975s / 745473.1468 s
env0_first_0:                 episode reward: 81.0500,                 loss: 0.0084
env0_second_0:                 episode reward: -81.0500,                 loss: 0.0177
env1_first_0:                 episode reward: 83.4000,                 loss: nan
env1_second_0:                 episode reward: -83.4000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 8509.35,                last time consumption/overall running time: 4213.8718s / 749687.0186 s
env0_first_0:                 episode reward: 19.7000,                 loss: 0.0050
env0_second_0:                 episode reward: -19.7000,                 loss: 0.0098
env1_first_0:                 episode reward: 14.7500,                 loss: nan
env1_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 7081.0,                last time consumption/overall running time: 3142.6585s / 752829.6771 s
env0_first_0:                 episode reward: 17.5000,                 loss: 0.0057
env0_second_0:                 episode reward: -17.5000,                 loss: 0.0192
env1_first_0:                 episode reward: 24.9000,                 loss: nan
env1_second_0:                 episode reward: -24.9000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 9246.55,                last time consumption/overall running time: 4114.3014s / 756943.9786 s
env0_first_0:                 episode reward: -9.2000,                 loss: 0.0054
env0_second_0:                 episode reward: 9.2000,                 loss: 0.0113
env1_first_0:                 episode reward: 12.4500,                 loss: nan
env1_second_0:                 episode reward: -12.4500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 8491.5,                last time consumption/overall running time: 3785.2854s / 760729.2640 s
env0_first_0:                 episode reward: 17.4000,                 loss: 0.0056
env0_second_0:                 episode reward: -17.4000,                 loss: 0.0118
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 9069.2,                last time consumption/overall running time: 4043.0060s / 764772.2700 s
env0_first_0:                 episode reward: 25.5000,                 loss: 0.0053
env0_second_0:                 episode reward: -25.5000,                 loss: 0.0090
env1_first_0:                 episode reward: 35.2000,                 loss: nan
env1_second_0:                 episode reward: -35.2000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 9529.55,                last time consumption/overall running time: 4257.2397s / 769029.5097 s
env0_first_0:                 episode reward: -22.9000,                 loss: 0.0059
env0_second_0:                 episode reward: 22.9000,                 loss: 0.0076
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 9441.2,                last time consumption/overall running time: 4433.3429s / 773462.8526 s
env0_first_0:                 episode reward: 11.2500,                 loss: 0.0039
env0_second_0:                 episode reward: -11.2500,                 loss: 0.0051
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 9655.65,                last time consumption/overall running time: 4678.9918s / 778141.8445 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0045
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0056
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 9016.5,                last time consumption/overall running time: 4310.9908s / 782452.8353 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0045
env0_second_0:                 episode reward: -5.7000,                 loss: 0.0056
env1_first_0:                 episode reward: 17.7500,                 loss: nan
env1_second_0:                 episode reward: -17.7500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 8962.8,                last time consumption/overall running time: 3964.6750s / 786417.5103 s
env0_first_0:                 episode reward: -8.8000,                 loss: 0.0037
env0_second_0:                 episode reward: 8.8000,                 loss: 0.0055
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 9440.65,                last time consumption/overall running time: 4016.8188s / 790434.3291 s
env0_first_0:                 episode reward: 23.8500,                 loss: 0.0043
env0_second_0:                 episode reward: -23.8500,                 loss: 0.0071
env1_first_0:                 episode reward: 24.9000,                 loss: nan
env1_second_0:                 episode reward: -24.9000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 9652.5,                last time consumption/overall running time: 3884.2954s / 794318.6246 s
env0_first_0:                 episode reward: 11.4000,                 loss: 0.0042
env0_second_0:                 episode reward: -11.4000,                 loss: 0.0045
env1_first_0:                 episode reward: 22.2000,                 loss: nan
env1_second_0:                 episode reward: -22.2000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 8796.15,                last time consumption/overall running time: 3536.7132s / 797855.3377 s
env0_first_0:                 episode reward: 22.1500,                 loss: 0.0049
env0_second_0:                 episode reward: -22.1500,                 loss: 0.0070
env1_first_0:                 episode reward: 10.5500,                 loss: nan
env1_second_0:                 episode reward: -10.5500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 9859.7,                last time consumption/overall running time: 3962.2639s / 801817.6016 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0031
env0_second_0:                 episode reward: -6.5500,                 loss: 0.0033
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 9632.45,                last time consumption/overall running time: 3547.2741s / 805364.8757 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0042
env0_second_0:                 episode reward: 6.3500,                 loss: 0.0047
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 9669.2,                last time consumption/overall running time: 3502.5437s / 808867.4194 s
env0_first_0:                 episode reward: 22.5000,                 loss: 0.0048
env0_second_0:                 episode reward: -22.5000,                 loss: 0.0047
env1_first_0:                 episode reward: 10.9000,                 loss: nan
env1_second_0:                 episode reward: -10.9000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 9405.5,                last time consumption/overall running time: 3414.8619s / 812282.2813 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0036
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0037
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 9266.3,                last time consumption/overall running time: 3365.3191s / 815647.6004 s
env0_first_0:                 episode reward: 11.8000,                 loss: 0.0044
env0_second_0:                 episode reward: -11.8000,                 loss: 0.0041
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 9213.65,                last time consumption/overall running time: 3363.0546s / 819010.6550 s
env0_first_0:                 episode reward: -9.7500,                 loss: 0.0041
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0039
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 9688.15,                last time consumption/overall running time: 3555.4958s / 822566.1509 s
env0_first_0:                 episode reward: -19.2000,                 loss: 0.0041
env0_second_0:                 episode reward: 19.2000,                 loss: 0.0042
env1_first_0:                 episode reward: -15.7000,                 loss: nan
env1_second_0:                 episode reward: 15.7000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 9715.95,                last time consumption/overall running time: 3566.4060s / 826132.5569 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0051
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0052
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 9276.75,                last time consumption/overall running time: 3406.4625s / 829539.0193 s
env0_first_0:                 episode reward: 14.2000,                 loss: 0.0052
env0_second_0:                 episode reward: -14.2000,                 loss: 0.0048
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 9617.4,                last time consumption/overall running time: 3509.7934s / 833048.8128 s
env0_first_0:                 episode reward: -30.9000,                 loss: 0.0056
env0_second_0:                 episode reward: 30.9000,                 loss: 0.0058
env1_first_0:                 episode reward: -11.6000,                 loss: nan
env1_second_0:                 episode reward: 11.6000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 9654.25,                last time consumption/overall running time: 3522.0945s / 836570.9073 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0049
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0051
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 9158.9,                last time consumption/overall running time: 3330.7374s / 839901.6447 s
env0_first_0:                 episode reward: -10.3500,                 loss: 0.0048
env0_second_0:                 episode reward: 10.3500,                 loss: 0.0056
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 9317.5,                last time consumption/overall running time: 3413.5478s / 843315.1925 s
env0_first_0:                 episode reward: -37.8000,                 loss: 0.0057
env0_second_0:                 episode reward: 37.8000,                 loss: 0.0054
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 8961.85,                last time consumption/overall running time: 3261.2355s / 846576.4279 s
env0_first_0:                 episode reward: -19.2500,                 loss: 0.0059
env0_second_0:                 episode reward: 19.2500,                 loss: 0.0045
env1_first_0:                 episode reward: -14.3000,                 loss: nan
env1_second_0:                 episode reward: 14.3000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 8576.45,                last time consumption/overall running time: 3115.6835s / 849692.1114 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0061
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0056
env1_first_0:                 episode reward: -22.6000,                 loss: nan
env1_second_0:                 episode reward: 22.6000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 9455.3,                last time consumption/overall running time: 3434.4726s / 853126.5840 s
env0_first_0:                 episode reward: -28.7500,                 loss: 0.0069
env0_second_0:                 episode reward: 28.7500,                 loss: 0.0057
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 9409.8,                last time consumption/overall running time: 3407.9537s / 856534.5377 s
env0_first_0:                 episode reward: -13.2500,                 loss: 0.0056
env0_second_0:                 episode reward: 13.2500,                 loss: 0.0059
env1_first_0:                 episode reward: -18.3000,                 loss: nan
env1_second_0:                 episode reward: 18.3000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 9219.65,                last time consumption/overall running time: 3357.9190s / 859892.4567 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0056
env0_second_0:                 episode reward: -4.3500,                 loss: 0.0058
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 9251.65,                last time consumption/overall running time: 3362.8603s / 863255.3169 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0059
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0062
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 9127.9,                last time consumption/overall running time: 3308.8821s / 866564.1991 s
env0_first_0:                 episode reward: -26.1500,                 loss: 0.0067
env0_second_0:                 episode reward: 26.1500,                 loss: 0.0078
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 9513.25,                last time consumption/overall running time: 3461.3066s / 870025.5056 s
env0_first_0:                 episode reward: -11.0000,                 loss: 0.0076
env0_second_0:                 episode reward: 11.0000,                 loss: 0.0061
env1_first_0:                 episode reward: -18.6500,                 loss: nan
env1_second_0:                 episode reward: 18.6500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 9862.65,                last time consumption/overall running time: 3576.2788s / 873601.7844 s
env0_first_0:                 episode reward: -9.2500,                 loss: 0.0082
env0_second_0:                 episode reward: 9.2500,                 loss: 0.0065
env1_first_0:                 episode reward: -19.5000,                 loss: nan
env1_second_0:                 episode reward: 19.5000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 9689.95,                last time consumption/overall running time: 3522.7563s / 877124.5407 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0083
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0079
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 8731.1,                last time consumption/overall running time: 3151.3248s / 880275.8656 s
env0_first_0:                 episode reward: -28.0500,                 loss: 0.0080
env0_second_0:                 episode reward: 28.0500,                 loss: 0.0064
env1_first_0:                 episode reward: -16.9000,                 loss: nan
env1_second_0:                 episode reward: 16.9000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 9561.3,                last time consumption/overall running time: 3461.0828s / 883736.9484 s
env0_first_0:                 episode reward: -9.4000,                 loss: 0.0057
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0051
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 8769.6,                last time consumption/overall running time: 3170.9137s / 886907.8621 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0054
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0051
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 9298.95,                last time consumption/overall running time: 3376.4530s / 890284.3151 s
env0_first_0:                 episode reward: -45.1000,                 loss: 0.0088
env0_second_0:                 episode reward: 45.1000,                 loss: 0.0072
env1_first_0:                 episode reward: -60.2000,                 loss: nan
env1_second_0:                 episode reward: 60.2000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 9669.65,                last time consumption/overall running time: 3495.5599s / 893779.8750 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0095
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0057
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 8855.05,                last time consumption/overall running time: 3241.4113s / 897021.2863 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0104
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0067
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 9303.15,                last time consumption/overall running time: 3384.1563s / 900405.4426 s
env0_first_0:                 episode reward: -64.6000,                 loss: 0.0068
env0_second_0:                 episode reward: 64.6000,                 loss: 0.0059
env1_first_0:                 episode reward: -40.3500,                 loss: nan
env1_second_0:                 episode reward: 40.3500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 9150.75,                last time consumption/overall running time: 3356.9551s / 903762.3976 s
env0_first_0:                 episode reward: 16.6500,                 loss: 0.0068
env0_second_0:                 episode reward: -16.6500,                 loss: 0.0060
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 9624.6,                last time consumption/overall running time: 3500.3725s / 907262.7701 s
env0_first_0:                 episode reward: -13.9000,                 loss: 0.0083
env0_second_0:                 episode reward: 13.9000,                 loss: 0.0065
env1_first_0:                 episode reward: -12.0000,                 loss: nan
env1_second_0:                 episode reward: 12.0000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 9177.75,                last time consumption/overall running time: 3334.3574s / 910597.1275 s
env0_first_0:                 episode reward: -19.4000,                 loss: 0.0064
env0_second_0:                 episode reward: 19.4000,                 loss: 0.0056
env1_first_0:                 episode reward: -34.1500,                 loss: nan
env1_second_0:                 episode reward: 34.1500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 9423.25,                last time consumption/overall running time: 3410.9833s / 914008.1108 s
env0_first_0:                 episode reward: -17.9000,                 loss: 0.0050
env0_second_0:                 episode reward: 17.9000,                 loss: 0.0050
env1_first_0:                 episode reward: -17.2500,                 loss: nan
env1_second_0:                 episode reward: 17.2500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 9399.5,                last time consumption/overall running time: 3386.0982s / 917394.2090 s
env0_first_0:                 episode reward: 4.5000,                 loss: 0.0121
env0_second_0:                 episode reward: -4.5000,                 loss: 0.0061
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 9710.4,                last time consumption/overall running time: 3530.9270s / 920925.1360 s
env0_first_0:                 episode reward: -22.2500,                 loss: 0.0087
env0_second_0:                 episode reward: 22.2500,                 loss: 0.0053
env1_first_0:                 episode reward: -13.0500,                 loss: nan
env1_second_0:                 episode reward: 13.0500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 9058.3,                last time consumption/overall running time: 3293.5085s / 924218.6445 s
env0_first_0:                 episode reward: 7.8000,                 loss: 0.0075
env0_second_0:                 episode reward: -7.8000,                 loss: 0.0062
env1_first_0:                 episode reward: -18.6000,                 loss: nan
env1_second_0:                 episode reward: 18.6000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 9664.45,                last time consumption/overall running time: 3512.7639s / 927731.4084 s
env0_first_0:                 episode reward: -15.6000,                 loss: 0.0049
env0_second_0:                 episode reward: 15.6000,                 loss: 0.0050
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 9331.2,                last time consumption/overall running time: 3382.6864s / 931114.0948 s
env0_first_0:                 episode reward: 14.0500,                 loss: 0.0040
env0_second_0:                 episode reward: -14.0500,                 loss: 0.0043
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 9112.85,                last time consumption/overall running time: 3312.6426s / 934426.7374 s
env0_first_0:                 episode reward: -35.1000,                 loss: 0.0060
env0_second_0:                 episode reward: 35.1000,                 loss: 0.0055
env1_first_0:                 episode reward: -21.2000,                 loss: nan
env1_second_0:                 episode reward: 21.2000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 9573.35,                last time consumption/overall running time: 3466.4820s / 937893.2194 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0055
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0044
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 9106.05,                last time consumption/overall running time: 3308.9576s / 941202.1770 s
env0_first_0:                 episode reward: -27.1000,                 loss: 0.0066
env0_second_0:                 episode reward: 27.1000,                 loss: 0.0058
env1_first_0:                 episode reward: -36.5500,                 loss: nan
env1_second_0:                 episode reward: 36.5500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 9126.25,                last time consumption/overall running time: 3315.1485s / 944517.3254 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0065
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 8990.65,                last time consumption/overall running time: 3259.2383s / 947776.5637 s
env0_first_0:                 episode reward: -17.2500,                 loss: 0.0037
env0_second_0:                 episode reward: 17.2500,                 loss: 0.0039
env1_first_0:                 episode reward: -16.5000,                 loss: nan
env1_second_0:                 episode reward: 16.5000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 9756.7,                last time consumption/overall running time: 3540.9084s / 951317.4721 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0045
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0040
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 9424.55,                last time consumption/overall running time: 3141.7342s / 954459.2063 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0041
env0_second_0:                 episode reward: -4.9500,                 loss: 0.0039
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 9092.7,                last time consumption/overall running time: 2946.7452s / 957405.9515 s
env0_first_0:                 episode reward: -8.5500,                 loss: 0.0035
env0_second_0:                 episode reward: 8.5500,                 loss: 0.0035
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 8624.5,                last time consumption/overall running time: 2796.5841s / 960202.5356 s
env0_first_0:                 episode reward: 11.7500,                 loss: 0.0036
env0_second_0:                 episode reward: -11.7500,                 loss: 0.0038
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 9506.7,                last time consumption/overall running time: 3093.3404s / 963295.8760 s
env0_first_0:                 episode reward: -29.5500,                 loss: 0.0041
env0_second_0:                 episode reward: 29.5500,                 loss: 0.0040
env1_first_0:                 episode reward: -17.4500,                 loss: nan
env1_second_0:                 episode reward: 17.4500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 9017.75,                last time consumption/overall running time: 2958.6301s / 966254.5061 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0047
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0050
env1_first_0:                 episode reward: -12.0000,                 loss: nan
env1_second_0:                 episode reward: 12.0000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 9561.1,                last time consumption/overall running time: 3162.3852s / 969416.8913 s
env0_first_0:                 episode reward: 21.0500,                 loss: 0.0055
env0_second_0:                 episode reward: -21.0500,                 loss: 0.0053
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 8715.1,                last time consumption/overall running time: 2845.7900s / 972262.6812 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0044
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0040
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 9338.1,                last time consumption/overall running time: 3067.6373s / 975330.3186 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0041
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0041
env1_first_0:                 episode reward: -15.7500,                 loss: nan
env1_second_0:                 episode reward: 15.7500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 8877.15,                last time consumption/overall running time: 2924.1564s / 978254.4749 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.0047
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0047
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 9493.85,                last time consumption/overall running time: 3063.5640s / 981318.0390 s
env0_first_0:                 episode reward: -23.9500,                 loss: 0.0039
env0_second_0:                 episode reward: 23.9500,                 loss: 0.0040
env1_first_0:                 episode reward: -17.1000,                 loss: nan
env1_second_0:                 episode reward: 17.1000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 9519.7,                last time consumption/overall running time: 3054.8864s / 984372.9254 s
env0_first_0:                 episode reward: -26.8000,                 loss: 0.0038
env0_second_0:                 episode reward: 26.8000,                 loss: 0.0041
env1_first_0:                 episode reward: -28.9500,                 loss: nan
env1_second_0:                 episode reward: 28.9500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 9425.75,                last time consumption/overall running time: 3030.4286s / 987403.3539 s
env0_first_0:                 episode reward: -12.3500,                 loss: 0.0039
env0_second_0:                 episode reward: 12.3500,                 loss: 0.0041
env1_first_0:                 episode reward: -19.4500,                 loss: nan
env1_second_0:                 episode reward: 19.4500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 9544.2,                last time consumption/overall running time: 3058.1897s / 990461.5437 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0036
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0037
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 9005.6,                last time consumption/overall running time: 2890.7671s / 993352.3108 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0035
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0035
env1_first_0:                 episode reward: -10.7000,                 loss: nan
env1_second_0:                 episode reward: 10.7000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 9010.4,                last time consumption/overall running time: 2875.8865s / 996228.1972 s
env0_first_0:                 episode reward: -16.9500,                 loss: 0.0036
env0_second_0:                 episode reward: 16.9500,                 loss: 0.0035
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 9521.35,                last time consumption/overall running time: 2702.3326s / 998930.5298 s
env0_first_0:                 episode reward: -33.9000,                 loss: 0.0039
env0_second_0:                 episode reward: 33.9000,                 loss: 0.0041
env1_first_0:                 episode reward: -16.6500,                 loss: nan
env1_second_0:                 episode reward: 16.6500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 9322.15,                last time consumption/overall running time: 2389.3909s / 1001319.9207 s
env0_first_0:                 episode reward: -12.6000,                 loss: 0.0042
env0_second_0:                 episode reward: 12.6000,                 loss: 0.0044
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 9719.85,                last time consumption/overall running time: 2466.0390s / 1003785.9597 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0035
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0036
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 8997.8,                last time consumption/overall running time: 2269.6117s / 1006055.5713 s
env0_first_0:                 episode reward: -8.9000,                 loss: 0.0033
env0_second_0:                 episode reward: 8.9000,                 loss: 0.0034
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 9770.45,                last time consumption/overall running time: 2519.0907s / 1008574.6621 s
env0_first_0:                 episode reward: 10.7000,                 loss: 0.0036
env0_second_0:                 episode reward: -10.7000,                 loss: 0.0037
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 9520.85,                last time consumption/overall running time: 2428.4224s / 1011003.0845 s
env0_first_0:                 episode reward: -25.3500,                 loss: 0.0040
env0_second_0:                 episode reward: 25.3500,                 loss: 0.0041
env1_first_0:                 episode reward: -11.3000,                 loss: nan
env1_second_0:                 episode reward: 11.3000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 9675.0,                last time consumption/overall running time: 2456.4685s / 1013459.5530 s
env0_first_0:                 episode reward: -21.4000,                 loss: 0.0034
env0_second_0:                 episode reward: 21.4000,                 loss: 0.0036
env1_first_0:                 episode reward: -22.6000,                 loss: nan
env1_second_0:                 episode reward: 22.6000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 9026.9,                last time consumption/overall running time: 2170.2721s / 1015629.8251 s
env0_first_0:                 episode reward: -13.9000,                 loss: 0.0045
env0_second_0:                 episode reward: 13.9000,                 loss: 0.0047
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 9598.95,                last time consumption/overall running time: 2193.9828s / 1017823.8078 s
env0_first_0:                 episode reward: 7.7000,                 loss: 0.0031
env0_second_0:                 episode reward: -7.7000,                 loss: 0.0031
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 9184.45,                last time consumption/overall running time: 2124.6186s / 1019948.4265 s
env0_first_0:                 episode reward: 3.0500,                 loss: 0.0041
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0041
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 8893.2,                last time consumption/overall running time: 2075.2065s / 1022023.6329 s
env0_first_0:                 episode reward: -11.2500,                 loss: 0.0047
env0_second_0:                 episode reward: 11.2500,                 loss: 0.0047
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 9680.35,                last time consumption/overall running time: 2222.6910s / 1024246.3239 s
env0_first_0:                 episode reward: -15.0500,                 loss: 0.0035
env0_second_0:                 episode reward: 15.0500,                 loss: 0.0037
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 9562.35,                last time consumption/overall running time: 2205.1914s / 1026451.5153 s
env0_first_0:                 episode reward: -17.2500,                 loss: 0.0038
env0_second_0:                 episode reward: 17.2500,                 loss: 0.0041
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 8549.0,                last time consumption/overall running time: 1987.9084s / 1028439.4237 s
env0_first_0:                 episode reward: -4.9000,                 loss: 0.0048
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0048
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 9213.25,                last time consumption/overall running time: 2096.7503s / 1030536.1740 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0036
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0035
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 9258.1,                last time consumption/overall running time: 2113.9069s / 1032650.0809 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0044
env0_second_0:                 episode reward: -4.9500,                 loss: 0.0049
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 9603.1,                last time consumption/overall running time: 2202.3836s / 1034852.4645 s
env0_first_0:                 episode reward: -14.9000,                 loss: 0.0040
env0_second_0:                 episode reward: 14.9000,                 loss: 0.0043
env1_first_0:                 episode reward: -17.2500,                 loss: nan
env1_second_0:                 episode reward: 17.2500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 9621.55,                last time consumption/overall running time: 2235.8903s / 1037088.3548 s
env0_first_0:                 episode reward: -40.1500,                 loss: 0.0049
env0_second_0:                 episode reward: 40.1500,                 loss: 0.0059
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 9440.65,                last time consumption/overall running time: 2187.6519s / 1039276.0066 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.0035
env0_second_0:                 episode reward: 12.2000,                 loss: 0.0044
env1_first_0:                 episode reward: -11.0500,                 loss: nan
env1_second_0:                 episode reward: 11.0500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 9785.7,                last time consumption/overall running time: 2255.6254s / 1041531.6321 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0048
env0_second_0:                 episode reward: -6.5500,                 loss: 0.0054
env1_first_0:                 episode reward: 15.0000,                 loss: nan
env1_second_0:                 episode reward: -15.0000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 9441.0,                last time consumption/overall running time: 2175.9555s / 1043707.5875 s
env0_first_0:                 episode reward: -32.6500,                 loss: 0.0035
env0_second_0:                 episode reward: 32.6500,                 loss: 0.0038
env1_first_0:                 episode reward: -24.1500,                 loss: nan
env1_second_0:                 episode reward: 24.1500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 9773.1,                last time consumption/overall running time: 2258.2953s / 1045965.8829 s
env0_first_0:                 episode reward: -19.2000,                 loss: 0.0041
env0_second_0:                 episode reward: 19.2000,                 loss: 0.0043
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 9381.45,                last time consumption/overall running time: 2136.0765s / 1048101.9593 s
env0_first_0:                 episode reward: 30.2000,                 loss: 0.0043
env0_second_0:                 episode reward: -30.2000,                 loss: 0.0048
env1_first_0:                 episode reward: 30.8500,                 loss: nan
env1_second_0:                 episode reward: -30.8500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 9075.85,                last time consumption/overall running time: 2098.7402s / 1050200.6995 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0047
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0050
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 8932.05,                last time consumption/overall running time: 2033.8723s / 1052234.5718 s
env0_first_0:                 episode reward: 34.1500,                 loss: 0.0047
env0_second_0:                 episode reward: -34.1500,                 loss: 0.0047
env1_first_0:                 episode reward: 33.5000,                 loss: nan
env1_second_0:                 episode reward: -33.5000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 9709.15,                last time consumption/overall running time: 2253.2493s / 1054487.8211 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0046
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0044
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 9371.05,                last time consumption/overall running time: 2164.2684s / 1056652.0895 s
env0_first_0:                 episode reward: -16.1000,                 loss: 0.0050
env0_second_0:                 episode reward: 16.1000,                 loss: 0.0049
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 8702.6,                last time consumption/overall running time: 2012.9993s / 1058665.0888 s
env0_first_0:                 episode reward: -14.0500,                 loss: 0.0059
env0_second_0:                 episode reward: 14.0500,                 loss: 0.0061
env1_first_0:                 episode reward: -15.2000,                 loss: nan
env1_second_0:                 episode reward: 15.2000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 9527.7,                last time consumption/overall running time: 2203.7113s / 1060868.8001 s
env0_first_0:                 episode reward: -32.6000,                 loss: 0.0059
env0_second_0:                 episode reward: 32.6000,                 loss: 0.0063
env1_first_0:                 episode reward: -14.8500,                 loss: nan
env1_second_0:                 episode reward: 14.8500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 9010.0,                last time consumption/overall running time: 2122.1802s / 1062990.9803 s
env0_first_0:                 episode reward: -21.3500,                 loss: 0.0042
env0_second_0:                 episode reward: 21.3500,                 loss: 0.0053
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 9269.75,                last time consumption/overall running time: 2164.6262s / 1065155.6065 s
env0_first_0:                 episode reward: -11.6000,                 loss: 0.0054
env0_second_0:                 episode reward: 11.6000,                 loss: 0.0060
env1_first_0:                 episode reward: -19.9000,                 loss: nan
env1_second_0:                 episode reward: 19.9000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 9044.6,                last time consumption/overall running time: 2072.2004s / 1067227.8069 s
env0_first_0:                 episode reward: 27.6000,                 loss: 0.0058
env0_second_0:                 episode reward: -27.6000,                 loss: 0.0069
env1_first_0:                 episode reward: 11.8500,                 loss: nan
env1_second_0:                 episode reward: -11.8500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 9028.1,                last time consumption/overall running time: 2064.0971s / 1069291.9040 s
env0_first_0:                 episode reward: 18.3500,                 loss: 0.0049
env0_second_0:                 episode reward: -18.3500,                 loss: 0.0058
env1_first_0:                 episode reward: 26.4500,                 loss: nan
env1_second_0:                 episode reward: -26.4500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 8570.8,                last time consumption/overall running time: 2018.1063s / 1071310.0103 s
env0_first_0:                 episode reward: 18.4500,                 loss: 0.0043
env0_second_0:                 episode reward: -18.4500,                 loss: 0.0045
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 9416.0,                last time consumption/overall running time: 2157.0971s / 1073467.1074 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0045
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0052
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 8796.15,                last time consumption/overall running time: 2101.6984s / 1075568.8059 s
env0_first_0:                 episode reward: 9.9500,                 loss: 0.0050
env0_second_0:                 episode reward: -9.9500,                 loss: 0.0070
env1_first_0:                 episode reward: 14.7500,                 loss: nan
env1_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 9668.15,                last time consumption/overall running time: 2206.2596s / 1077775.0654 s
env0_first_0:                 episode reward: -12.5500,                 loss: 0.0087
env0_second_0:                 episode reward: 12.5500,                 loss: 0.0061
env1_first_0:                 episode reward: -17.2500,                 loss: nan
env1_second_0:                 episode reward: 17.2500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 8919.8,                last time consumption/overall running time: 2018.7132s / 1079793.7787 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0097
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0057
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 8523.6,                last time consumption/overall running time: 1979.8520s / 1081773.6306 s
env0_first_0:                 episode reward: -45.4000,                 loss: 0.0141
env0_second_0:                 episode reward: 45.4000,                 loss: 0.0072
env1_first_0:                 episode reward: -29.5500,                 loss: nan
env1_second_0:                 episode reward: 29.5500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 8920.85,                last time consumption/overall running time: 2101.4591s / 1083875.0898 s
env0_first_0:                 episode reward: -26.0000,                 loss: 0.0070
env0_second_0:                 episode reward: 26.0000,                 loss: 0.0074
env1_first_0:                 episode reward: -22.6000,                 loss: nan
env1_second_0:                 episode reward: 22.6000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 8247.15,                last time consumption/overall running time: 1930.3070s / 1085805.3968 s
env0_first_0:                 episode reward: -33.9500,                 loss: 0.0093
env0_second_0:                 episode reward: 33.9500,                 loss: 0.0085
env1_first_0:                 episode reward: -29.8000,                 loss: nan
env1_second_0:                 episode reward: 29.8000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 9365.8,                last time consumption/overall running time: 2154.2457s / 1087959.6425 s
env0_first_0:                 episode reward: 9.7500,                 loss: 0.0081
env0_second_0:                 episode reward: -9.7500,                 loss: 0.0085
env1_first_0:                 episode reward: 30.4500,                 loss: nan
env1_second_0:                 episode reward: -30.4500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 7343.25,                last time consumption/overall running time: 1701.7394s / 1089661.3818 s
env0_first_0:                 episode reward: -42.1000,                 loss: 0.0064
env0_second_0:                 episode reward: 42.1000,                 loss: 0.0078
env1_first_0:                 episode reward: -23.3000,                 loss: nan
env1_second_0:                 episode reward: 23.3000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 9364.45,                last time consumption/overall running time: 2170.0913s / 1091831.4731 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0072
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0074
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 8733.8,                last time consumption/overall running time: 1996.3118s / 1093827.7849 s
env0_first_0:                 episode reward: 20.0000,                 loss: 0.0089
env0_second_0:                 episode reward: -20.0000,                 loss: 0.0099
env1_first_0:                 episode reward: 13.5500,                 loss: nan
env1_second_0:                 episode reward: -13.5500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 8605.15,                last time consumption/overall running time: 2026.4547s / 1095854.2396 s
env0_first_0:                 episode reward: -19.1500,                 loss: 0.0188
env0_second_0:                 episode reward: 19.1500,                 loss: 0.0089
env1_first_0:                 episode reward: -33.3000,                 loss: nan
env1_second_0:                 episode reward: 33.3000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 9208.75,                last time consumption/overall running time: 2158.7975s / 1098013.0371 s
env0_first_0:                 episode reward: -53.8500,                 loss: 0.0545
env0_second_0:                 episode reward: 53.8500,                 loss: 0.0106
env1_first_0:                 episode reward: -65.9500,                 loss: nan
env1_second_0:                 episode reward: 65.9500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 8095.0,                last time consumption/overall running time: 1854.4301s / 1099867.4672 s
env0_first_0:                 episode reward: -44.2500,                 loss: 0.0286
env0_second_0:                 episode reward: 44.2500,                 loss: 0.0083
env1_first_0:                 episode reward: -23.7500,                 loss: nan
env1_second_0:                 episode reward: 23.7500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 8478.6,                last time consumption/overall running time: 1901.8247s / 1101769.2919 s
env0_first_0:                 episode reward: -25.2000,                 loss: 0.0084
env0_second_0:                 episode reward: 25.2000,                 loss: 0.0066
env1_first_0:                 episode reward: -30.1500,                 loss: nan
env1_second_0:                 episode reward: 30.1500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 9046.15,                last time consumption/overall running time: 2056.3584s / 1103825.6502 s
env0_first_0:                 episode reward: -21.4500,                 loss: 0.0081
env0_second_0:                 episode reward: 21.4500,                 loss: 0.0071
env1_first_0:                 episode reward: -23.8000,                 loss: nan
env1_second_0:                 episode reward: 23.8000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 9144.05,                last time consumption/overall running time: 2103.8885s / 1105929.5388 s
env0_first_0:                 episode reward: -21.5000,                 loss: 0.0084
env0_second_0:                 episode reward: 21.5000,                 loss: 0.0083
env1_first_0:                 episode reward: -23.8500,                 loss: nan
env1_second_0:                 episode reward: 23.8500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 9304.55,                last time consumption/overall running time: 2140.1759s / 1108069.7147 s
env0_first_0:                 episode reward: -39.3500,                 loss: 0.0109
env0_second_0:                 episode reward: 39.3500,                 loss: 0.0070
env1_first_0:                 episode reward: -31.3000,                 loss: nan
env1_second_0:                 episode reward: 31.3000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 8607.0,                last time consumption/overall running time: 1990.9959s / 1110060.7105 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0059
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0065
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 8742.15,                last time consumption/overall running time: 2037.2709s / 1112097.9814 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0055
env0_second_0:                 episode reward: -5.3500,                 loss: 0.0060
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 8940.6,                last time consumption/overall running time: 2008.5459s / 1114106.5273 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0074
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0073
env1_first_0:                 episode reward: -10.1000,                 loss: nan
env1_second_0:                 episode reward: 10.1000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 7767.85,                last time consumption/overall running time: 1777.2101s / 1115883.7374 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0068
env0_second_0:                 episode reward: -5.2000,                 loss: 0.0063
env1_first_0:                 episode reward: -10.1000,                 loss: nan
env1_second_0:                 episode reward: 10.1000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 8507.35,                last time consumption/overall running time: 1946.8685s / 1117830.6059 s
env0_first_0:                 episode reward: -28.3000,                 loss: 0.0057
env0_second_0:                 episode reward: 28.3000,                 loss: 0.0062
env1_first_0:                 episode reward: -17.3000,                 loss: nan
env1_second_0:                 episode reward: 17.3000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 9533.1,                last time consumption/overall running time: 2166.2892s / 1119996.8951 s
env0_first_0:                 episode reward: -25.0000,                 loss: 0.0058
env0_second_0:                 episode reward: 25.0000,                 loss: 0.0061
env1_first_0:                 episode reward: -16.7000,                 loss: nan
env1_second_0:                 episode reward: 16.7000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 8575.75,                last time consumption/overall running time: 1945.0455s / 1121941.9406 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0039
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0043
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 8881.95,                last time consumption/overall running time: 2025.4809s / 1123967.4215 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0048
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0051
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 8693.05,                last time consumption/overall running time: 1967.7877s / 1125935.2093 s
env0_first_0:                 episode reward: -17.3000,                 loss: 0.0045
env0_second_0:                 episode reward: 17.3000,                 loss: 0.0045
env1_first_0:                 episode reward: -15.1500,                 loss: nan
env1_second_0:                 episode reward: 15.1500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 8121.95,                last time consumption/overall running time: 1847.0403s / 1127782.2496 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0046
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0045
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 8678.0,                last time consumption/overall running time: 1960.0310s / 1129742.2806 s
env0_first_0:                 episode reward: 7.2500,                 loss: 0.0063
env0_second_0:                 episode reward: -7.2500,                 loss: 0.0054
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 8421.75,                last time consumption/overall running time: 1909.3996s / 1131651.6802 s
env0_first_0:                 episode reward: 7.4000,                 loss: 0.0063
env0_second_0:                 episode reward: -7.4000,                 loss: 0.0048
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 9011.9,                last time consumption/overall running time: 2075.3566s / 1133727.0369 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0058
env0_second_0:                 episode reward: -5.6000,                 loss: 0.0057
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 8869.7,                last time consumption/overall running time: 2065.8321s / 1135792.8689 s
env0_first_0:                 episode reward: -32.0500,                 loss: 0.0047
env0_second_0:                 episode reward: 32.0500,                 loss: 0.0051
env1_first_0:                 episode reward: -32.3000,                 loss: nan
env1_second_0:                 episode reward: 32.3000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 8515.35,                last time consumption/overall running time: 2003.9940s / 1137796.8629 s
env0_first_0:                 episode reward: -10.3500,                 loss: 0.0039
env0_second_0:                 episode reward: 10.3500,                 loss: 0.0045
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 8667.0,                last time consumption/overall running time: 1935.3757s / 1139732.2386 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0056
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 8526.0,                last time consumption/overall running time: 2035.1609s / 1141767.3995 s
env0_first_0:                 episode reward: -14.8500,                 loss: 0.0067
env0_second_0:                 episode reward: 14.8500,                 loss: 0.0072
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 8818.15,                last time consumption/overall running time: 2024.7836s / 1143792.1832 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0048
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0052
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 7932.2,                last time consumption/overall running time: 1797.6390s / 1145589.8222 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0050
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0054
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 8983.1,                last time consumption/overall running time: 1803.8129s / 1147393.6351 s
env0_first_0:                 episode reward: -20.8500,                 loss: 0.0042
env0_second_0:                 episode reward: 20.8500,                 loss: 0.0044
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 8229.15,                last time consumption/overall running time: 1711.9094s / 1149105.5445 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0039
env0_second_0:                 episode reward: -4.7500,                 loss: 0.0038
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 6945.9,                last time consumption/overall running time: 1399.3683s / 1150504.9128 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0047
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0053
env1_first_0:                 episode reward: -15.3000,                 loss: nan
env1_second_0:                 episode reward: 15.3000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 9332.0,                last time consumption/overall running time: 1878.4186s / 1152383.3314 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0042
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0044
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 9317.9,                last time consumption/overall running time: 1903.5383s / 1154286.8697 s
env0_first_0:                 episode reward: -14.4500,                 loss: 0.0039
env0_second_0:                 episode reward: 14.4500,                 loss: 0.0042
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 9075.85,                last time consumption/overall running time: 1885.7525s / 1156172.6223 s
env0_first_0:                 episode reward: -18.4500,                 loss: 0.0043
env0_second_0:                 episode reward: 18.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 9134.2,                last time consumption/overall running time: 1941.9035s / 1158114.5257 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0041
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0049
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 9558.2,                last time consumption/overall running time: 2026.3781s / 1160140.9038 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0038
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0043
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 8819.0,                last time consumption/overall running time: 1773.4792s / 1161914.3830 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0040
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0049
env1_first_0:                 episode reward: -11.9500,                 loss: nan
env1_second_0:                 episode reward: 11.9500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 9272.0,                last time consumption/overall running time: 1875.2961s / 1163789.6791 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0048
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0053
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 9467.15,                last time consumption/overall running time: 1930.6883s / 1165720.3675 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0039
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0040
env1_first_0:                 episode reward: -10.2000,                 loss: nan
env1_second_0:                 episode reward: 10.2000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 9068.95,                last time consumption/overall running time: 1848.2624s / 1167568.6298 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0038
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0038
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 9389.6,                last time consumption/overall running time: 1944.9383s / 1169513.5682 s
env0_first_0:                 episode reward: -10.4000,                 loss: 0.0040
env0_second_0:                 episode reward: 10.4000,                 loss: 0.0041
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 8225.15,                last time consumption/overall running time: 1690.4613s / 1171204.0295 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0042
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0044
env1_first_0:                 episode reward: 16.2500,                 loss: nan
env1_second_0:                 episode reward: -16.2500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 9821.25,                last time consumption/overall running time: 2041.3165s / 1173245.3460 s
env0_first_0:                 episode reward: -16.3500,                 loss: 0.0043
env0_second_0:                 episode reward: 16.3500,                 loss: 0.0049
env1_first_0:                 episode reward: -21.7500,                 loss: nan
env1_second_0:                 episode reward: 21.7500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 9104.85,                last time consumption/overall running time: 1899.4489s / 1175144.7949 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.0038
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0037
env1_first_0:                 episode reward: -11.0000,                 loss: nan
env1_second_0:                 episode reward: 11.0000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 8774.4,                last time consumption/overall running time: 1777.4786s / 1176922.2735 s
env0_first_0:                 episode reward: -9.9000,                 loss: 0.0041
env0_second_0:                 episode reward: 9.9000,                 loss: 0.0042
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 8460.2,                last time consumption/overall running time: 1724.7058s / 1178646.9792 s
env0_first_0:                 episode reward: 7.0000,                 loss: 0.0039
env0_second_0:                 episode reward: -7.0000,                 loss: 0.0038
env1_first_0:                 episode reward: 15.6500,                 loss: nan
env1_second_0:                 episode reward: -15.6500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 8991.1,                last time consumption/overall running time: 1882.4443s / 1180529.4236 s
env0_first_0:                 episode reward: -17.5000,                 loss: 0.0041
env0_second_0:                 episode reward: 17.5000,                 loss: 0.0040
env1_first_0:                 episode reward: -20.6000,                 loss: nan
env1_second_0:                 episode reward: 20.6000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 7716.15,                last time consumption/overall running time: 1605.6559s / 1182135.0795 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0042
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0046
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 8653.0,                last time consumption/overall running time: 1795.9315s / 1183931.0110 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0035
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0035
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 8317.7,                last time consumption/overall running time: 1723.9331s / 1185654.9441 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0038
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0038
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 8758.9,                last time consumption/overall running time: 1784.9156s / 1187439.8597 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0034
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0036
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 8376.4,                last time consumption/overall running time: 1771.5273s / 1189211.3870 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0034
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0038
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 8651.5,                last time consumption/overall running time: 1786.6929s / 1190998.0799 s
env0_first_0:                 episode reward: 3.6000,                 loss: 0.0036
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0038
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 9098.45,                last time consumption/overall running time: 1872.4243s / 1192870.5042 s
env0_first_0:                 episode reward: -8.0500,                 loss: 0.0035
env0_second_0:                 episode reward: 8.0500,                 loss: 0.0036
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 8789.45,                last time consumption/overall running time: 1822.2693s / 1194692.7735 s
env0_first_0:                 episode reward: 11.7000,                 loss: 0.0049
env0_second_0:                 episode reward: -11.7000,                 loss: 0.0042
env1_first_0:                 episode reward: 12.6500,                 loss: nan
env1_second_0:                 episode reward: -12.6500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 8414.75,                last time consumption/overall running time: 1737.3742s / 1196430.1477 s
env0_first_0:                 episode reward: -21.3500,                 loss: 0.0050
env0_second_0:                 episode reward: 21.3500,                 loss: 0.0049
env1_first_0:                 episode reward: -29.2500,                 loss: nan
env1_second_0:                 episode reward: 29.2500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 8832.4,                last time consumption/overall running time: 1834.6999s / 1198264.8476 s
env0_first_0:                 episode reward: -10.7000,                 loss: 0.0042
env0_second_0:                 episode reward: 10.7000,                 loss: 0.0044
env1_first_0:                 episode reward: -11.5000,                 loss: nan
env1_second_0:                 episode reward: 11.5000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 9428.5,                last time consumption/overall running time: 1991.7847s / 1200256.6323 s
env0_first_0:                 episode reward: -8.8000,                 loss: 0.0035
env0_second_0:                 episode reward: 8.8000,                 loss: 0.0037
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 9016.65,                last time consumption/overall running time: 1827.6825s / 1202084.3148 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0040
env0_second_0:                 episode reward: -4.0000,                 loss: 0.0043
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 7729.35,                last time consumption/overall running time: 1623.4147s / 1203707.7296 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0042
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 7890.2,                last time consumption/overall running time: 1617.4470s / 1205325.1765 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0040
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0043
env1_first_0:                 episode reward: 12.8000,                 loss: nan
env1_second_0:                 episode reward: -12.8000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 9939.55,                last time consumption/overall running time: 2008.1886s / 1207333.3651 s
env0_first_0:                 episode reward: -11.2000,                 loss: 0.0027
env0_second_0:                 episode reward: 11.2000,                 loss: 0.0029
env1_first_0:                 episode reward: -9.3000,                 loss: nan
env1_second_0:                 episode reward: 9.3000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 8066.85,                last time consumption/overall running time: 1689.9502s / 1209023.3153 s
env0_first_0:                 episode reward: -8.8500,                 loss: 0.0051
env0_second_0:                 episode reward: 8.8500,                 loss: 0.0055
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 8510.55,                last time consumption/overall running time: 1734.1751s / 1210757.4904 s
env0_first_0:                 episode reward: -14.9000,                 loss: 0.0039
env0_second_0:                 episode reward: 14.9000,                 loss: 0.0046
env1_first_0:                 episode reward: -19.1000,                 loss: nan
env1_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 8783.15,                last time consumption/overall running time: 1797.7594s / 1212555.2497 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0044
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0049
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 6871.75,                last time consumption/overall running time: 1425.6229s / 1213980.8726 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0051
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0048
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 8176.85,                last time consumption/overall running time: 1719.0549s / 1215699.9275 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0053
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0045
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 9498.65,                last time consumption/overall running time: 1966.5430s / 1217666.4705 s
env0_first_0:                 episode reward: -8.8000,                 loss: 0.0035
env0_second_0:                 episode reward: 8.8000,                 loss: 0.0040
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 8221.65,                last time consumption/overall running time: 1682.2449s / 1219348.7154 s
env0_first_0:                 episode reward: -14.2000,                 loss: 0.0046
env0_second_0:                 episode reward: 14.2000,                 loss: 0.0050
env1_first_0:                 episode reward: -10.6500,                 loss: nan
env1_second_0:                 episode reward: 10.6500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 7921.85,                last time consumption/overall running time: 1623.5004s / 1220972.2158 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0041
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 9137.0,                last time consumption/overall running time: 1846.2596s / 1222818.4754 s
env0_first_0:                 episode reward: -8.2500,                 loss: 0.0050
env0_second_0:                 episode reward: 8.2500,                 loss: 0.0061
env1_first_0:                 episode reward: -18.5500,                 loss: nan
env1_second_0:                 episode reward: 18.5500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 8762.5,                last time consumption/overall running time: 1829.4740s / 1224647.9493 s
env0_first_0:                 episode reward: -24.3500,                 loss: 0.0043
env0_second_0:                 episode reward: 24.3500,                 loss: 0.0051
env1_first_0:                 episode reward: -15.1500,                 loss: nan
env1_second_0:                 episode reward: 15.1500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 9369.25,                last time consumption/overall running time: 1935.0141s / 1226582.9635 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0040
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0041
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 9495.65,                last time consumption/overall running time: 1952.2896s / 1228535.2531 s
env0_first_0:                 episode reward: -21.0000,                 loss: 0.0043
env0_second_0:                 episode reward: 21.0000,                 loss: 0.0046
env1_first_0:                 episode reward: -23.6500,                 loss: nan
env1_second_0:                 episode reward: 23.6500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 8953.75,                last time consumption/overall running time: 1816.3246s / 1230351.5777 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0039
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0050
env1_first_0:                 episode reward: -21.6500,                 loss: nan
env1_second_0:                 episode reward: 21.6500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 9393.75,                last time consumption/overall running time: 1891.8738s / 1232243.4515 s
env0_first_0:                 episode reward: -17.3500,                 loss: 0.0035
env0_second_0:                 episode reward: 17.3500,                 loss: 0.0037
env1_first_0:                 episode reward: -17.3500,                 loss: nan
env1_second_0:                 episode reward: 17.3500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 8617.75,                last time consumption/overall running time: 1784.1993s / 1234027.6508 s
env0_first_0:                 episode reward: -31.6000,                 loss: 0.0043
env0_second_0:                 episode reward: 31.6000,                 loss: 0.0047
env1_first_0:                 episode reward: -26.7500,                 loss: nan
env1_second_0:                 episode reward: 26.7500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 8543.6,                last time consumption/overall running time: 1731.1745s / 1235758.8253 s
env0_first_0:                 episode reward: -13.4500,                 loss: 0.0050
env0_second_0:                 episode reward: 13.4500,                 loss: 0.0056
env1_first_0:                 episode reward: -19.7000,                 loss: nan
env1_second_0:                 episode reward: 19.7000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 7815.45,                last time consumption/overall running time: 1641.2837s / 1237400.1090 s
env0_first_0:                 episode reward: -12.5500,                 loss: 0.0040
env0_second_0:                 episode reward: 12.5500,                 loss: 0.0050
env1_first_0:                 episode reward: -13.2000,                 loss: nan
env1_second_0:                 episode reward: 13.2000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 9060.5,                last time consumption/overall running time: 1918.0321s / 1239318.1411 s
env0_first_0:                 episode reward: -38.3500,                 loss: 0.0051
env0_second_0:                 episode reward: 38.3500,                 loss: 0.0054
env1_first_0:                 episode reward: -14.5500,                 loss: nan
env1_second_0:                 episode reward: 14.5500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 8442.4,                last time consumption/overall running time: 1731.0255s / 1241049.1666 s
env0_first_0:                 episode reward: -12.9500,                 loss: 0.0038
env0_second_0:                 episode reward: 12.9500,                 loss: 0.0045
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 9492.45,                last time consumption/overall running time: 2000.7036s / 1243049.8702 s
env0_first_0:                 episode reward: -23.2000,                 loss: 0.0049
env0_second_0:                 episode reward: 23.2000,                 loss: 0.0047
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 9361.95,                last time consumption/overall running time: 1920.0137s / 1244969.8839 s
env0_first_0:                 episode reward: -43.3500,                 loss: 0.0047
env0_second_0:                 episode reward: 43.3500,                 loss: 0.0049
env1_first_0:                 episode reward: -38.7500,                 loss: nan
env1_second_0:                 episode reward: 38.7500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 8975.05,                last time consumption/overall running time: 1951.3863s / 1246921.2702 s
env0_first_0:                 episode reward: -16.6500,                 loss: 0.0038
env0_second_0:                 episode reward: 16.6500,                 loss: 0.0050
env1_first_0:                 episode reward: -13.9500,                 loss: nan
env1_second_0:                 episode reward: 13.9500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 8871.05,                last time consumption/overall running time: 1936.2399s / 1248857.5101 s
env0_first_0:                 episode reward: -26.0000,                 loss: 0.0044
env0_second_0:                 episode reward: 26.0000,                 loss: 0.0051
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 9077.3,                last time consumption/overall running time: 1980.8761s / 1250838.3862 s
env0_first_0:                 episode reward: -39.5500,                 loss: 0.0060
env0_second_0:                 episode reward: 39.5500,                 loss: 0.0058
env1_first_0:                 episode reward: -11.3000,                 loss: nan
env1_second_0:                 episode reward: 11.3000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 8659.55,                last time consumption/overall running time: 1824.4945s / 1252662.8806 s
env0_first_0:                 episode reward: -9.6000,                 loss: 0.0034
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0041
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 9191.6,                last time consumption/overall running time: 2032.9693s / 1254695.8499 s
env0_first_0:                 episode reward: -22.6500,                 loss: 0.0043
env0_second_0:                 episode reward: 22.6500,                 loss: 0.0041
env1_first_0:                 episode reward: -22.1500,                 loss: nan
env1_second_0:                 episode reward: 22.1500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 7289.7,                last time consumption/overall running time: 1663.8352s / 1256359.6851 s
env0_first_0:                 episode reward: -14.9500,                 loss: 0.0043
env0_second_0:                 episode reward: 14.9500,                 loss: 0.0041
env1_first_0:                 episode reward: -25.1500,                 loss: nan
env1_second_0:                 episode reward: 25.1500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 7377.7,                last time consumption/overall running time: 1614.4820s / 1257974.1671 s
env0_first_0:                 episode reward: -34.9000,                 loss: 0.0050
env0_second_0:                 episode reward: 34.9000,                 loss: 0.0048
env1_first_0:                 episode reward: -36.8500,                 loss: nan
env1_second_0:                 episode reward: 36.8500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 8888.6,                last time consumption/overall running time: 1916.5615s / 1259890.7285 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0051
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0049
env1_first_0:                 episode reward: -25.3500,                 loss: nan
env1_second_0:                 episode reward: 25.3500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 8184.15,                last time consumption/overall running time: 1719.0764s / 1261609.8050 s
env0_first_0:                 episode reward: -13.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 13.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -24.1000,                 loss: nan
env1_second_0:                 episode reward: 24.1000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 8530.15,                last time consumption/overall running time: 1822.7588s / 1263432.5638 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0045
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0049
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 7932.55,                last time consumption/overall running time: 1710.7428s / 1265143.3066 s
env0_first_0:                 episode reward: -19.2000,                 loss: 0.0042
env0_second_0:                 episode reward: 19.2000,                 loss: 0.0043
env1_first_0:                 episode reward: -14.7500,                 loss: nan
env1_second_0:                 episode reward: 14.7500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 8486.35,                last time consumption/overall running time: 1789.5968s / 1266932.9034 s
env0_first_0:                 episode reward: -13.2000,                 loss: 0.0049
env0_second_0:                 episode reward: 13.2000,                 loss: 0.0046
env1_first_0:                 episode reward: -13.2000,                 loss: nan
env1_second_0:                 episode reward: 13.2000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 7928.55,                last time consumption/overall running time: 1712.2724s / 1268645.1758 s
env0_first_0:                 episode reward: -32.1500,                 loss: 0.0060
env0_second_0:                 episode reward: 32.1500,                 loss: 0.0060
env1_first_0:                 episode reward: -20.6000,                 loss: nan
env1_second_0:                 episode reward: 20.6000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 7906.05,                last time consumption/overall running time: 1730.8288s / 1270376.0046 s
env0_first_0:                 episode reward: -30.0500,                 loss: 0.0042
env0_second_0:                 episode reward: 30.0500,                 loss: 0.0044
env1_first_0:                 episode reward: -25.4500,                 loss: nan
env1_second_0:                 episode reward: 25.4500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 8181.4,                last time consumption/overall running time: 1707.5119s / 1272083.5165 s
env0_first_0:                 episode reward: -25.7000,                 loss: 0.0048
env0_second_0:                 episode reward: 25.7000,                 loss: 0.0047
env1_first_0:                 episode reward: -26.4000,                 loss: nan
env1_second_0:                 episode reward: 26.4000,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 9173.75,                last time consumption/overall running time: 2030.5535s / 1274114.0700 s
env0_first_0:                 episode reward: -15.5000,                 loss: 0.0037
env0_second_0:                 episode reward: 15.5000,                 loss: 0.0037
env1_first_0:                 episode reward: -18.5500,                 loss: nan
env1_second_0:                 episode reward: 18.5500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 8921.65,                last time consumption/overall running time: 1908.8290s / 1276022.8990 s
env0_first_0:                 episode reward: -31.5500,                 loss: 0.0054
env0_second_0:                 episode reward: 31.5500,                 loss: 0.0048
env1_first_0:                 episode reward: -27.2000,                 loss: nan
env1_second_0:                 episode reward: 27.2000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 8466.8,                last time consumption/overall running time: 1845.6439s / 1277868.5429 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0049
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0046
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 7199.45,                last time consumption/overall running time: 1569.1698s / 1279437.7127 s
env0_first_0:                 episode reward: -24.4500,                 loss: 0.0047
env0_second_0:                 episode reward: 24.4500,                 loss: 0.0055
env1_first_0:                 episode reward: -17.0500,                 loss: nan
env1_second_0:                 episode reward: 17.0500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 8111.7,                last time consumption/overall running time: 1816.0300s / 1281253.7427 s
env0_first_0:                 episode reward: -14.5500,                 loss: 0.0045
env0_second_0:                 episode reward: 14.5500,                 loss: 0.0044
env1_first_0:                 episode reward: -20.2000,                 loss: nan
env1_second_0:                 episode reward: 20.2000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 8560.7,                last time consumption/overall running time: 1857.6714s / 1283111.4141 s
env0_first_0:                 episode reward: -22.0000,                 loss: 0.0057
env0_second_0:                 episode reward: 22.0000,                 loss: 0.0053
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 8402.0,                last time consumption/overall running time: 1792.4957s / 1284903.9098 s
env0_first_0:                 episode reward: -17.7000,                 loss: 0.0051
env0_second_0:                 episode reward: 17.7000,                 loss: 0.0046
env1_first_0:                 episode reward: -15.1500,                 loss: nan
env1_second_0:                 episode reward: 15.1500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 8656.2,                last time consumption/overall running time: 1832.4908s / 1286736.4006 s
env0_first_0:                 episode reward: -26.9000,                 loss: 0.0050
env0_second_0:                 episode reward: 26.9000,                 loss: 0.0046
env1_first_0:                 episode reward: -14.8500,                 loss: nan
env1_second_0:                 episode reward: 14.8500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 7360.8,                last time consumption/overall running time: 1694.2046s / 1288430.6052 s
env0_first_0:                 episode reward: -14.5500,                 loss: 0.0045
env0_second_0:                 episode reward: 14.5500,                 loss: 0.0042
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 9068.7,                last time consumption/overall running time: 2078.5456s / 1290509.1508 s
env0_first_0:                 episode reward: -31.2000,                 loss: 0.0053
env0_second_0:                 episode reward: 31.2000,                 loss: 0.0052
env1_first_0:                 episode reward: -44.1000,                 loss: nan
env1_second_0:                 episode reward: 44.1000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 8893.9,                last time consumption/overall running time: 2067.9743s / 1292577.1250 s
env0_first_0:                 episode reward: -29.6500,                 loss: 0.0057
env0_second_0:                 episode reward: 29.6500,                 loss: 0.0058
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 8702.6,                last time consumption/overall running time: 1984.9213s / 1294562.0463 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0049
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 8017.6,                last time consumption/overall running time: 1829.4492s / 1296391.4955 s
env0_first_0:                 episode reward: -15.8000,                 loss: 0.0045
env0_second_0:                 episode reward: 15.8000,                 loss: 0.0043
env1_first_0:                 episode reward: -16.7500,                 loss: nan
env1_second_0:                 episode reward: 16.7500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 8813.35,                last time consumption/overall running time: 2025.2580s / 1298416.7535 s
env0_first_0:                 episode reward: -25.5500,                 loss: 0.0046
env0_second_0:                 episode reward: 25.5500,                 loss: 0.0045
env1_first_0:                 episode reward: -25.1000,                 loss: nan
env1_second_0:                 episode reward: 25.1000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 9075.15,                last time consumption/overall running time: 2082.3641s / 1300499.1176 s
env0_first_0:                 episode reward: -30.4000,                 loss: 0.0044
env0_second_0:                 episode reward: 30.4000,                 loss: 0.0043
env1_first_0:                 episode reward: -32.7000,                 loss: nan
env1_second_0:                 episode reward: 32.7000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 7723.1,                last time consumption/overall running time: 1769.1340s / 1302268.2516 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0046
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0045
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 8320.35,                last time consumption/overall running time: 1919.2871s / 1304187.5387 s
env0_first_0:                 episode reward: -25.5500,                 loss: 0.0055
env0_second_0:                 episode reward: 25.5500,                 loss: 0.0053
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 7668.45,                last time consumption/overall running time: 1771.4851s / 1305959.0238 s
env0_first_0:                 episode reward: -15.7000,                 loss: 0.0055
env0_second_0:                 episode reward: 15.7000,                 loss: 0.0057
env1_first_0:                 episode reward: -24.1000,                 loss: nan
env1_second_0:                 episode reward: 24.1000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 8287.1,                last time consumption/overall running time: 1909.6928s / 1307868.7166 s
env0_first_0:                 episode reward: -29.8000,                 loss: 0.0046
env0_second_0:                 episode reward: 29.8000,                 loss: 0.0045
env1_first_0:                 episode reward: -17.1500,                 loss: nan
env1_second_0:                 episode reward: 17.1500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 8044.35,                last time consumption/overall running time: 1853.8599s / 1309722.5764 s
env0_first_0:                 episode reward: -26.2500,                 loss: 0.0051
env0_second_0:                 episode reward: 26.2500,                 loss: 0.0050
env1_first_0:                 episode reward: -40.0500,                 loss: nan
env1_second_0:                 episode reward: 40.0500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 8172.9,                last time consumption/overall running time: 1876.6393s / 1311599.2157 s
env0_first_0:                 episode reward: -38.7000,                 loss: 0.0045
env0_second_0:                 episode reward: 38.7000,                 loss: 0.0042
env1_first_0:                 episode reward: -28.1000,                 loss: nan
env1_second_0:                 episode reward: 28.1000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 8021.9,                last time consumption/overall running time: 1830.2915s / 1313429.5072 s
env0_first_0:                 episode reward: -14.3500,                 loss: 0.0058
env0_second_0:                 episode reward: 14.3500,                 loss: 0.0053
env1_first_0:                 episode reward: -23.8500,                 loss: nan
env1_second_0:                 episode reward: 23.8500,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 8159.4,                last time consumption/overall running time: 1864.3140s / 1315293.8213 s
env0_first_0:                 episode reward: -17.1500,                 loss: 0.0055
env0_second_0:                 episode reward: 17.1500,                 loss: 0.0053
env1_first_0:                 episode reward: -29.3500,                 loss: nan
env1_second_0:                 episode reward: 29.3500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 8312.3,                last time consumption/overall running time: 1908.9501s / 1317202.7714 s
env0_first_0:                 episode reward: -22.1500,                 loss: 0.0058
env0_second_0:                 episode reward: 22.1500,                 loss: 0.0048
env1_first_0:                 episode reward: -22.0500,                 loss: nan
env1_second_0:                 episode reward: 22.0500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 8361.35,                last time consumption/overall running time: 1917.7789s / 1319120.5503 s
env0_first_0:                 episode reward: -22.7500,                 loss: 0.0046
env0_second_0:                 episode reward: 22.7500,                 loss: 0.0047
env1_first_0:                 episode reward: -19.6000,                 loss: nan
env1_second_0:                 episode reward: 19.6000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 8766.1,                last time consumption/overall running time: 2052.8735s / 1321173.4238 s
env0_first_0:                 episode reward: -15.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 15.1000,                 loss: 0.0045
env1_first_0:                 episode reward: -34.5500,                 loss: nan
env1_second_0:                 episode reward: 34.5500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 8786.7,                last time consumption/overall running time: 2025.1958s / 1323198.6196 s
env0_first_0:                 episode reward: -22.6000,                 loss: 0.0060
env0_second_0:                 episode reward: 22.6000,                 loss: 0.0049
env1_first_0:                 episode reward: -19.7000,                 loss: nan
env1_second_0:                 episode reward: 19.7000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 8126.3,                last time consumption/overall running time: 1863.6640s / 1325062.2835 s
env0_first_0:                 episode reward: -13.0000,                 loss: 0.0048
env0_second_0:                 episode reward: 13.0000,                 loss: 0.0052
env1_first_0:                 episode reward: -16.4000,                 loss: nan
env1_second_0:                 episode reward: 16.4000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 7873.35,                last time consumption/overall running time: 1802.5247s / 1326864.8082 s
env0_first_0:                 episode reward: -27.6500,                 loss: 0.0058
env0_second_0:                 episode reward: 27.6500,                 loss: 0.0059
env1_first_0:                 episode reward: -10.9500,                 loss: nan
env1_second_0:                 episode reward: 10.9500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 8032.6,                last time consumption/overall running time: 1847.2169s / 1328712.0251 s
env0_first_0:                 episode reward: -11.0000,                 loss: 0.0054
env0_second_0:                 episode reward: 11.0000,                 loss: 0.0061
env1_first_0:                 episode reward: -10.7500,                 loss: nan
env1_second_0:                 episode reward: 10.7500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 8537.95,                last time consumption/overall running time: 1976.5248s / 1330688.5498 s
env0_first_0:                 episode reward: -29.9500,                 loss: 0.0056
env0_second_0:                 episode reward: 29.9500,                 loss: 0.0057
env1_first_0:                 episode reward: -47.1500,                 loss: nan
env1_second_0:                 episode reward: 47.1500,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 8306.65,                last time consumption/overall running time: 1925.7800s / 1332614.3298 s
env0_first_0:                 episode reward: -24.9000,                 loss: 0.0051
env0_second_0:                 episode reward: 24.9000,                 loss: 0.0046
env1_first_0:                 episode reward: -22.0000,                 loss: nan
env1_second_0:                 episode reward: 22.0000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 9279.5,                last time consumption/overall running time: 2163.4636s / 1334777.7935 s
env0_first_0:                 episode reward: -8.6500,                 loss: 0.0049
env0_second_0:                 episode reward: 8.6500,                 loss: 0.0042
env1_first_0:                 episode reward: -23.5500,                 loss: nan
env1_second_0:                 episode reward: 23.5500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 8530.9,                last time consumption/overall running time: 2014.7771s / 1336792.5705 s
env0_first_0:                 episode reward: -21.3000,                 loss: 0.0056
env0_second_0:                 episode reward: 21.3000,                 loss: 0.0050
env1_first_0:                 episode reward: -27.0000,                 loss: nan
env1_second_0:                 episode reward: 27.0000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 8631.8,                last time consumption/overall running time: 2025.4037s / 1338817.9742 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0052
env1_first_0:                 episode reward: -18.1500,                 loss: nan
env1_second_0:                 episode reward: 18.1500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 8993.65,                last time consumption/overall running time: 2060.4107s / 1340878.3849 s
env0_first_0:                 episode reward: -31.4500,                 loss: 0.0055
env0_second_0:                 episode reward: 31.4500,                 loss: 0.0054
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 8285.8,                last time consumption/overall running time: 1899.9333s / 1342778.3182 s
env0_first_0:                 episode reward: -20.5000,                 loss: 0.0052
env0_second_0:                 episode reward: 20.5000,                 loss: 0.0049
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 8698.6,                last time consumption/overall running time: 2001.5337s / 1344779.8520 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0057
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0049
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 9083.6,                last time consumption/overall running time: 2085.8972s / 1346865.7492 s
env0_first_0:                 episode reward: -26.4500,                 loss: 0.0059
env0_second_0:                 episode reward: 26.4500,                 loss: 0.0060
env1_first_0:                 episode reward: -15.2500,                 loss: nan
env1_second_0:                 episode reward: 15.2500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 8557.95,                last time consumption/overall running time: 1981.9448s / 1348847.6940 s
env0_first_0:                 episode reward: -28.5500,                 loss: 0.0068
env0_second_0:                 episode reward: 28.5500,                 loss: 0.0060
env1_first_0:                 episode reward: -31.6000,                 loss: nan
env1_second_0:                 episode reward: 31.6000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 8849.65,                last time consumption/overall running time: 2036.8292s / 1350884.5231 s
env0_first_0:                 episode reward: -19.1000,                 loss: 0.0058
env0_second_0:                 episode reward: 19.1000,                 loss: 0.0051
env1_first_0:                 episode reward: -17.2500,                 loss: nan
env1_second_0:                 episode reward: 17.2500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 8822.15,                last time consumption/overall running time: 2041.1827s / 1352925.7059 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0056
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0051
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 9292.9,                last time consumption/overall running time: 2136.6915s / 1355062.3974 s
env0_first_0:                 episode reward: -22.4000,                 loss: 0.0051
env0_second_0:                 episode reward: 22.4000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 8486.05,                last time consumption/overall running time: 1959.2145s / 1357021.6118 s
env0_first_0:                 episode reward: -19.0000,                 loss: 0.0050
env0_second_0:                 episode reward: 19.0000,                 loss: 0.0044
env1_first_0:                 episode reward: -15.6000,                 loss: nan
env1_second_0:                 episode reward: 15.6000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 8424.45,                last time consumption/overall running time: 1951.7159s / 1358973.3277 s
env0_first_0:                 episode reward: -27.9500,                 loss: 0.0046
env0_second_0:                 episode reward: 27.9500,                 loss: 0.0049
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 8181.6,                last time consumption/overall running time: 1885.0736s / 1360858.4013 s
env0_first_0:                 episode reward: -14.2000,                 loss: 0.0048
env0_second_0:                 episode reward: 14.2000,                 loss: 0.0043
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 8876.05,                last time consumption/overall running time: 2043.6475s / 1362902.0488 s
env0_first_0:                 episode reward: -46.2500,                 loss: 0.0049
env0_second_0:                 episode reward: 46.2500,                 loss: 0.0048
env1_first_0:                 episode reward: -43.1500,                 loss: nan
env1_second_0:                 episode reward: 43.1500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 9046.15,                last time consumption/overall running time: 2088.7815s / 1364990.8303 s
env0_first_0:                 episode reward: -22.0500,                 loss: 0.0040
env0_second_0:                 episode reward: 22.0500,                 loss: 0.0038
env1_first_0:                 episode reward: -21.4000,                 loss: nan
env1_second_0:                 episode reward: 21.4000,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 8633.3,                last time consumption/overall running time: 1992.8749s / 1366983.7052 s
env0_first_0:                 episode reward: -22.6500,                 loss: 0.0040
env0_second_0:                 episode reward: 22.6500,                 loss: 0.0038
env1_first_0:                 episode reward: -26.0500,                 loss: nan
env1_second_0:                 episode reward: 26.0500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 8455.6,                last time consumption/overall running time: 1937.5507s / 1368921.2559 s
env0_first_0:                 episode reward: -27.7500,                 loss: 0.0044
env0_second_0:                 episode reward: 27.7500,                 loss: 0.0042
env1_first_0:                 episode reward: -35.1500,                 loss: nan
env1_second_0:                 episode reward: 35.1500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 8605.25,                last time consumption/overall running time: 1983.3455s / 1370904.6014 s
env0_first_0:                 episode reward: -26.3000,                 loss: 0.0044
env0_second_0:                 episode reward: 26.3000,                 loss: 0.0050
env1_first_0:                 episode reward: -23.0500,                 loss: nan
env1_second_0:                 episode reward: 23.0500,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 8228.9,                last time consumption/overall running time: 1894.5404s / 1372799.1418 s
env0_first_0:                 episode reward: -29.2500,                 loss: 0.0067
env0_second_0:                 episode reward: 29.2500,                 loss: 0.0056
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 7987.55,                last time consumption/overall running time: 1845.1418s / 1374644.2836 s
env0_first_0:                 episode reward: -24.5000,                 loss: 0.0071
env0_second_0:                 episode reward: 24.5000,                 loss: 0.0054
env1_first_0:                 episode reward: -31.4000,                 loss: nan
env1_second_0:                 episode reward: 31.4000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 8037.55,                last time consumption/overall running time: 1861.5381s / 1376505.8218 s
env0_first_0:                 episode reward: -11.7000,                 loss: 0.0048
env0_second_0:                 episode reward: 11.7000,                 loss: 0.0042
env1_first_0:                 episode reward: -14.9500,                 loss: nan
env1_second_0:                 episode reward: 14.9500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 8074.85,                last time consumption/overall running time: 1866.9626s / 1378372.7844 s
env0_first_0:                 episode reward: -20.0500,                 loss: 0.0047
env0_second_0:                 episode reward: 20.0500,                 loss: 0.0039
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 8775.05,                last time consumption/overall running time: 2047.3879s / 1380420.1722 s
env0_first_0:                 episode reward: -15.3500,                 loss: 0.0050
env0_second_0:                 episode reward: 15.3500,                 loss: 0.0047
env1_first_0:                 episode reward: -9.8000,                 loss: nan
env1_second_0:                 episode reward: 9.8000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 8056.2,                last time consumption/overall running time: 1869.2931s / 1382289.4653 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.0048
env0_second_0:                 episode reward: 11.4500,                 loss: 0.0049
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 8032.3,                last time consumption/overall running time: 1873.9948s / 1384163.4601 s
env0_first_0:                 episode reward: -13.5000,                 loss: 0.0048
env0_second_0:                 episode reward: 13.5000,                 loss: 0.0044
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 7786.85,                last time consumption/overall running time: 1811.3871s / 1385974.8472 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0048
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0052
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 8765.95,                last time consumption/overall running time: 2026.4521s / 1388001.2993 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0061
env0_second_0:                 episode reward: -3.9500,                 loss: 0.0059
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 7943.35,                last time consumption/overall running time: 1792.8022s / 1389794.1015 s
env0_first_0:                 episode reward: -11.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 11.8000,                 loss: 0.0048
env1_first_0:                 episode reward: -10.2500,                 loss: nan
env1_second_0:                 episode reward: 10.2500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 7528.75,                last time consumption/overall running time: 1686.9989s / 1391481.1004 s
env0_first_0:                 episode reward: -24.4000,                 loss: 0.0057
env0_second_0:                 episode reward: 24.4000,                 loss: 0.0049
env1_first_0:                 episode reward: -19.1000,                 loss: nan
env1_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 7305.8,                last time consumption/overall running time: 1631.5787s / 1393112.6791 s
env0_first_0:                 episode reward: -32.2000,                 loss: 0.0080
env0_second_0:                 episode reward: 32.2000,                 loss: 0.0068
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 7334.95,                last time consumption/overall running time: 1649.8564s / 1394762.5355 s
env0_first_0:                 episode reward: -10.7500,                 loss: 0.0051
env0_second_0:                 episode reward: 10.7500,                 loss: 0.0047
env1_first_0:                 episode reward: -11.3000,                 loss: nan
env1_second_0:                 episode reward: 11.3000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 7890.4,                last time consumption/overall running time: 1795.4588s / 1396557.9943 s
env0_first_0:                 episode reward: -25.3000,                 loss: 0.0046
env0_second_0:                 episode reward: 25.3000,                 loss: 0.0048
env1_first_0:                 episode reward: -22.4000,                 loss: nan
env1_second_0:                 episode reward: 22.4000,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 7066.65,                last time consumption/overall running time: 1616.4551s / 1398174.4494 s
env0_first_0:                 episode reward: -25.6000,                 loss: 0.0047
env0_second_0:                 episode reward: 25.6000,                 loss: 0.0049
env1_first_0:                 episode reward: -26.4500,                 loss: nan
env1_second_0:                 episode reward: 26.4500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 7289.8,                last time consumption/overall running time: 1638.4415s / 1399812.8909 s
env0_first_0:                 episode reward: -18.3500,                 loss: 0.0057
env0_second_0:                 episode reward: 18.3500,                 loss: 0.0054
env1_first_0:                 episode reward: -16.7500,                 loss: nan
env1_second_0:                 episode reward: 16.7500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 8335.45,                last time consumption/overall running time: 1851.5391s / 1401664.4300 s
env0_first_0:                 episode reward: -10.0500,                 loss: 0.0049
env0_second_0:                 episode reward: 10.0500,                 loss: 0.0046
env1_first_0:                 episode reward: -11.8500,                 loss: nan
env1_second_0:                 episode reward: 11.8500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 7962.85,                last time consumption/overall running time: 1788.8265s / 1403453.2565 s
env0_first_0:                 episode reward: -43.2000,                 loss: 0.0077
env0_second_0:                 episode reward: 43.2000,                 loss: 0.0071
env1_first_0:                 episode reward: -23.8000,                 loss: nan
env1_second_0:                 episode reward: 23.8000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 7977.1,                last time consumption/overall running time: 1795.7392s / 1405248.9956 s
env0_first_0:                 episode reward: -23.7000,                 loss: 0.0071
env0_second_0:                 episode reward: 23.7000,                 loss: 0.0068
env1_first_0:                 episode reward: -35.7000,                 loss: nan
env1_second_0:                 episode reward: 35.7000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 7789.8,                last time consumption/overall running time: 1753.9991s / 1407002.9947 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.0052
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0051
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 7827.1,                last time consumption/overall running time: 1745.0650s / 1408748.0597 s
env0_first_0:                 episode reward: -14.9000,                 loss: 0.0063
env0_second_0:                 episode reward: 14.9000,                 loss: 0.0058
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 8205.45,                last time consumption/overall running time: 1832.3453s / 1410580.4051 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0062
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0061
env1_first_0:                 episode reward: -14.7500,                 loss: nan
env1_second_0:                 episode reward: 14.7500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 8100.45,                last time consumption/overall running time: 1848.8775s / 1412429.2826 s
env0_first_0:                 episode reward: -20.8000,                 loss: 0.0063
env0_second_0:                 episode reward: 20.8000,                 loss: 0.0062
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 8807.9,                last time consumption/overall running time: 2002.8165s / 1414432.0990 s
env0_first_0:                 episode reward: -14.7500,                 loss: 0.0067
env0_second_0:                 episode reward: 14.7500,                 loss: 0.0068
env1_first_0:                 episode reward: -15.3000,                 loss: nan
env1_second_0:                 episode reward: 15.3000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 8301.9,                last time consumption/overall running time: 1857.9786s / 1416290.0776 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0048
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0048
env1_first_0:                 episode reward: -9.8000,                 loss: nan
env1_second_0:                 episode reward: 9.8000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 8464.45,                last time consumption/overall running time: 1893.5176s / 1418183.5952 s
env0_first_0:                 episode reward: -54.7000,                 loss: 0.0063
env0_second_0:                 episode reward: 54.7000,                 loss: 0.0061
env1_first_0:                 episode reward: -27.5000,                 loss: nan
env1_second_0:                 episode reward: 27.5000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 7006.85,                last time consumption/overall running time: 1561.5312s / 1419745.1264 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0062
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0057
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 7998.5,                last time consumption/overall running time: 1644.3291s / 1421389.4555 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0052
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0050
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 6548.75,                last time consumption/overall running time: 1474.4381s / 1422863.8935 s
env0_first_0:                 episode reward: -17.6000,                 loss: 0.0059
env0_second_0:                 episode reward: 17.6000,                 loss: 0.0056
env1_first_0:                 episode reward: -23.5500,                 loss: nan
env1_second_0:                 episode reward: 23.5500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 8384.0,                last time consumption/overall running time: 1811.7581s / 1424675.6517 s
env0_first_0:                 episode reward: -14.4500,                 loss: 0.0055
env0_second_0:                 episode reward: 14.4500,                 loss: 0.0052
env1_first_0:                 episode reward: -10.9000,                 loss: nan
env1_second_0:                 episode reward: 10.9000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 7597.05,                last time consumption/overall running time: 1672.5527s / 1426348.2044 s
env0_first_0:                 episode reward: -8.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 8.9000,                 loss: 0.0047
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 7558.5,                last time consumption/overall running time: 1710.5722s / 1428058.7766 s
env0_first_0:                 episode reward: -29.7000,                 loss: 0.0057
env0_second_0:                 episode reward: 29.7000,                 loss: 0.0059
env1_first_0:                 episode reward: -17.1000,                 loss: nan
env1_second_0:                 episode reward: 17.1000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 7250.85,                last time consumption/overall running time: 1587.7392s / 1429646.5157 s
env0_first_0:                 episode reward: -13.6500,                 loss: 0.0055
env0_second_0:                 episode reward: 13.6500,                 loss: 0.0051
env1_first_0:                 episode reward: -17.9000,                 loss: nan
env1_second_0:                 episode reward: 17.9000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 8254.4,                last time consumption/overall running time: 1856.2025s / 1431502.7183 s
env0_first_0:                 episode reward: -21.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 21.0500,                 loss: 0.0051
env1_first_0:                 episode reward: -23.5500,                 loss: nan
env1_second_0:                 episode reward: 23.5500,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 8431.7,                last time consumption/overall running time: 1870.3239s / 1433373.0422 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.0050
env0_second_0:                 episode reward: 8.5000,                 loss: 0.0053
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 8531.7,                last time consumption/overall running time: 1876.5421s / 1435249.5843 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.0060
env0_second_0:                 episode reward: 12.0500,                 loss: 0.0053
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 7192.7,                last time consumption/overall running time: 1561.5497s / 1436811.1340 s
env0_first_0:                 episode reward: -19.9500,                 loss: 0.0082
env0_second_0:                 episode reward: 19.9500,                 loss: 0.0061
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 8396.5,                last time consumption/overall running time: 1849.1978s / 1438660.3318 s
env0_first_0:                 episode reward: -36.0000,                 loss: 0.0065
env0_second_0:                 episode reward: 36.0000,                 loss: 0.0057
env1_first_0:                 episode reward: -36.1500,                 loss: nan
env1_second_0:                 episode reward: 36.1500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 8096.6,                last time consumption/overall running time: 1747.3969s / 1440407.7287 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0051
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0048
env1_first_0:                 episode reward: -13.8500,                 loss: nan
env1_second_0:                 episode reward: 13.8500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 7948.4,                last time consumption/overall running time: 1767.2204s / 1442174.9491 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0052
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0047
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 7896.25,                last time consumption/overall running time: 1738.0720s / 1443913.0211 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0064
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0062
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 8622.55,                last time consumption/overall running time: 1839.4981s / 1445752.5191 s
env0_first_0:                 episode reward: -11.9500,                 loss: 0.0061
env0_second_0:                 episode reward: 11.9500,                 loss: 0.0051
env1_first_0:                 episode reward: -12.1000,                 loss: nan
env1_second_0:                 episode reward: 12.1000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 7524.8,                last time consumption/overall running time: 1638.6335s / 1447391.1526 s
env0_first_0:                 episode reward: -14.2000,                 loss: 0.0063
env0_second_0:                 episode reward: 14.2000,                 loss: 0.0063
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 6737.35,                last time consumption/overall running time: 1494.4063s / 1448885.5589 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0050
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0048
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 7765.35,                last time consumption/overall running time: 1684.0085s / 1450569.5674 s
env0_first_0:                 episode reward: 15.3500,                 loss: 0.0050
env0_second_0:                 episode reward: -15.3500,                 loss: 0.0048
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 8027.3,                last time consumption/overall running time: 1773.6537s / 1452343.2211 s
env0_first_0:                 episode reward: -28.0500,                 loss: 0.0059
env0_second_0:                 episode reward: 28.0500,                 loss: 0.0056
env1_first_0:                 episode reward: -22.8000,                 loss: nan
env1_second_0:                 episode reward: 22.8000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 8056.15,                last time consumption/overall running time: 1722.4552s / 1454065.6762 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0054
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0052
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 7627.55,                last time consumption/overall running time: 1684.6466s / 1455750.3228 s
env0_first_0:                 episode reward: -13.2000,                 loss: 0.0047
env0_second_0:                 episode reward: 13.2000,                 loss: 0.0048
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 6532.05,                last time consumption/overall running time: 1449.2172s / 1457199.5400 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0053
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0052
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 8013.4,                last time consumption/overall running time: 1783.2880s / 1458982.8280 s
env0_first_0:                 episode reward: -9.9500,                 loss: 0.0056
env0_second_0:                 episode reward: 9.9500,                 loss: 0.0055
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 7861.55,                last time consumption/overall running time: 1688.4823s / 1460671.3103 s
env0_first_0:                 episode reward: -9.5500,                 loss: 0.0054
env0_second_0:                 episode reward: 9.5500,                 loss: 0.0053
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 7961.8,                last time consumption/overall running time: 1790.6119s / 1462461.9222 s
env0_first_0:                 episode reward: -22.7500,                 loss: 0.0063
env0_second_0:                 episode reward: 22.7500,                 loss: 0.0062
env1_first_0:                 episode reward: -29.4500,                 loss: nan
env1_second_0:                 episode reward: 29.4500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 8035.45,                last time consumption/overall running time: 1836.5542s / 1464298.4764 s
env0_first_0:                 episode reward: -11.2000,                 loss: 0.0051
env0_second_0:                 episode reward: 11.2000,                 loss: 0.0052
env1_first_0:                 episode reward: -13.2500,                 loss: nan
env1_second_0:                 episode reward: 13.2500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 6538.05,                last time consumption/overall running time: 1484.9032s / 1465783.3796 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0045
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0047
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 8254.45,                last time consumption/overall running time: 1845.3290s / 1467628.7086 s
env0_first_0:                 episode reward: -9.5000,                 loss: 0.0057
env0_second_0:                 episode reward: 9.5000,                 loss: 0.0055
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 6728.5,                last time consumption/overall running time: 1500.4949s / 1469129.2034 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0057
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0055
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 8324.65,                last time consumption/overall running time: 1860.6179s / 1470989.8213 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.0056
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0055
env1_first_0:                 episode reward: -13.8500,                 loss: nan
env1_second_0:                 episode reward: 13.8500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 8614.65,                last time consumption/overall running time: 1870.7707s / 1472860.5920 s
env0_first_0:                 episode reward: -28.0000,                 loss: 0.0064
env0_second_0:                 episode reward: 28.0000,                 loss: 0.0060
env1_first_0:                 episode reward: -39.1500,                 loss: nan
env1_second_0:                 episode reward: 39.1500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 7852.05,                last time consumption/overall running time: 1647.2023s / 1474507.7943 s
env0_first_0:                 episode reward: 8.0000,                 loss: 0.0064
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0061
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 7904.6,                last time consumption/overall running time: 1704.5880s / 1476212.3824 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0060
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0059
env1_first_0:                 episode reward: 19.4000,                 loss: nan
env1_second_0:                 episode reward: -19.4000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 8111.75,                last time consumption/overall running time: 1762.7530s / 1477975.1354 s
env0_first_0:                 episode reward: -17.4500,                 loss: 0.0059
env0_second_0:                 episode reward: 17.4500,                 loss: 0.0055
env1_first_0:                 episode reward: -17.7000,                 loss: nan
env1_second_0:                 episode reward: 17.7000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 7760.35,                last time consumption/overall running time: 1682.3009s / 1479657.4363 s
env0_first_0:                 episode reward: -18.9500,                 loss: 0.0064
env0_second_0:                 episode reward: 18.9500,                 loss: 0.0058
env1_first_0:                 episode reward: -39.4000,                 loss: nan
env1_second_0:                 episode reward: 39.4000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 8369.6,                last time consumption/overall running time: 1808.6895s / 1481466.1258 s
env0_first_0:                 episode reward: -27.8000,                 loss: 0.0069
env0_second_0:                 episode reward: 27.8000,                 loss: 0.0066
env1_first_0:                 episode reward: -26.8500,                 loss: nan
env1_second_0:                 episode reward: 26.8500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 7749.65,                last time consumption/overall running time: 1713.1053s / 1483179.2311 s
env0_first_0:                 episode reward: -20.2500,                 loss: 0.0071
env0_second_0:                 episode reward: 20.2500,                 loss: 0.0060
env1_first_0:                 episode reward: -22.2000,                 loss: nan
env1_second_0:                 episode reward: 22.2000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 7606.55,                last time consumption/overall running time: 1697.0207s / 1484876.2518 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0076
env0_second_0:                 episode reward: -7.6500,                 loss: 0.0065
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 8158.7,                last time consumption/overall running time: 1810.6274s / 1486686.8792 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0064
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0054
env1_first_0:                 episode reward: -11.6000,                 loss: nan
env1_second_0:                 episode reward: 11.6000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 7941.65,                last time consumption/overall running time: 1742.7864s / 1488429.6655 s
env0_first_0:                 episode reward: 11.3500,                 loss: 0.0078
env0_second_0:                 episode reward: -11.3500,                 loss: 0.0062
env1_first_0:                 episode reward: 12.0000,                 loss: nan
env1_second_0:                 episode reward: -12.0000,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 6937.6,                last time consumption/overall running time: 1520.1624s / 1489949.8279 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0048
env0_second_0:                 episode reward: -5.7000,                 loss: 0.0039
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 7834.75,                last time consumption/overall running time: 1702.0411s / 1491651.8690 s
env0_first_0:                 episode reward: -12.5500,                 loss: 0.0052
env0_second_0:                 episode reward: 12.5500,                 loss: 0.0043
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 7476.1,                last time consumption/overall running time: 1672.5428s / 1493324.4118 s
env0_first_0:                 episode reward: -10.7000,                 loss: 0.0069
env0_second_0:                 episode reward: 10.7000,                 loss: 0.0054
env1_first_0:                 episode reward: -9.3000,                 loss: nan
env1_second_0:                 episode reward: 9.3000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 7441.55,                last time consumption/overall running time: 1638.8020s / 1494963.2139 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0061
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0053
env1_first_0:                 episode reward: 11.3500,                 loss: nan
env1_second_0:                 episode reward: -11.3500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 6386.35,                last time consumption/overall running time: 1381.8364s / 1496345.0503 s
env0_first_0:                 episode reward: 3.4500,                 loss: 0.0064
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0051
env1_first_0:                 episode reward: 11.0000,                 loss: nan
env1_second_0:                 episode reward: -11.0000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 7057.35,                last time consumption/overall running time: 1548.8686s / 1497893.9189 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0085
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0060
env1_first_0:                 episode reward: 12.4000,                 loss: nan
env1_second_0:                 episode reward: -12.4000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 7380.85,                last time consumption/overall running time: 1657.1661s / 1499551.0850 s
env0_first_0:                 episode reward: -27.7500,                 loss: 0.0073
env0_second_0:                 episode reward: 27.7500,                 loss: 0.0056
env1_first_0:                 episode reward: -16.6000,                 loss: nan
env1_second_0:                 episode reward: 16.6000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 7907.7,                last time consumption/overall running time: 1687.4830s / 1501238.5680 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0065
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0050
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 7851.8,                last time consumption/overall running time: 1699.4503s / 1502938.0183 s
env0_first_0:                 episode reward: -15.7500,                 loss: 0.0073
env0_second_0:                 episode reward: 15.7500,                 loss: 0.0049
env1_first_0:                 episode reward: -24.5500,                 loss: nan
env1_second_0:                 episode reward: 24.5500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 7198.0,                last time consumption/overall running time: 1514.8589s / 1504452.8771 s
env0_first_0:                 episode reward: -10.3000,                 loss: 0.0156
env0_second_0:                 episode reward: 10.3000,                 loss: 0.0058
env1_first_0:                 episode reward: -16.5500,                 loss: nan
env1_second_0:                 episode reward: 16.5500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 7180.65,                last time consumption/overall running time: 1591.5108s / 1506044.3879 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0074
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0054
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 7955.35,                last time consumption/overall running time: 1697.1166s / 1507741.5044 s
env0_first_0:                 episode reward: -23.1000,                 loss: 0.0064
env0_second_0:                 episode reward: 23.1000,                 loss: 0.0056
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 7869.65,                last time consumption/overall running time: 1728.3503s / 1509469.8548 s
env0_first_0:                 episode reward: -9.6500,                 loss: 0.0059
env0_second_0:                 episode reward: 9.6500,                 loss: 0.0044
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 8221.05,                last time consumption/overall running time: 1813.3280s / 1511283.1827 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0071
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0044
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 8151.65,                last time consumption/overall running time: 1784.0247s / 1513067.2075 s
env0_first_0:                 episode reward: -19.3500,                 loss: 0.0073
env0_second_0:                 episode reward: 19.3500,                 loss: 0.0051
env1_first_0:                 episode reward: -22.1000,                 loss: nan
env1_second_0:                 episode reward: 22.1000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 7782.85,                last time consumption/overall running time: 1715.6116s / 1514782.8191 s
env0_first_0:                 episode reward: 5.9500,                 loss: 0.0067
env0_second_0:                 episode reward: -5.9500,                 loss: 0.0056
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 8503.05,                last time consumption/overall running time: 1878.9634s / 1516661.7825 s
env0_first_0:                 episode reward: 16.5500,                 loss: 0.0059
env0_second_0:                 episode reward: -16.5500,                 loss: 0.0055
env1_first_0:                 episode reward: 22.7000,                 loss: nan
env1_second_0:                 episode reward: -22.7000,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 8116.5,                last time consumption/overall running time: 1808.1141s / 1518469.8966 s
env0_first_0:                 episode reward: 10.2500,                 loss: 0.0055
env0_second_0:                 episode reward: -10.2500,                 loss: 0.0047
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 7617.65,                last time consumption/overall running time: 1712.7807s / 1520182.6772 s
env0_first_0:                 episode reward: 9.4500,                 loss: 0.0063
env0_second_0:                 episode reward: -9.4500,                 loss: 0.0057
env1_first_0:                 episode reward: 29.0500,                 loss: nan
env1_second_0:                 episode reward: -29.0500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 7933.4,                last time consumption/overall running time: 1823.2854s / 1522005.9626 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0050
env0_second_0:                 episode reward: -5.4500,                 loss: 0.0043
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 8547.9,                last time consumption/overall running time: 1860.9761s / 1523866.9387 s
env0_first_0:                 episode reward: 43.6500,                 loss: 0.0069
env0_second_0:                 episode reward: -43.6500,                 loss: 0.0067
env1_first_0:                 episode reward: 45.4000,                 loss: nan
env1_second_0:                 episode reward: -45.4000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 7527.8,                last time consumption/overall running time: 1650.6824s / 1525517.6211 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0056
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0055
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 8801.0,                last time consumption/overall running time: 1907.2496s / 1527424.8706 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0060
env1_first_0:                 episode reward: 24.5000,                 loss: nan
env1_second_0:                 episode reward: -24.5000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 7307.6,                last time consumption/overall running time: 1599.5114s / 1529024.3821 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0050
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0047
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 7240.2,                last time consumption/overall running time: 1620.5569s / 1530644.9390 s
env0_first_0:                 episode reward: -15.1500,                 loss: 0.0039
env0_second_0:                 episode reward: 15.1500,                 loss: 0.0036
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 7734.4,                last time consumption/overall running time: 1715.2003s / 1532360.1393 s
env0_first_0:                 episode reward: -9.7000,                 loss: 0.0041
env0_second_0:                 episode reward: 9.7000,                 loss: 0.0039
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 5685.6,                last time consumption/overall running time: 1254.9466s / 1533615.0859 s
env0_first_0:                 episode reward: -20.3000,                 loss: 0.0045
env0_second_0:                 episode reward: 20.3000,                 loss: 0.0038
env1_first_0:                 episode reward: -12.0500,                 loss: nan
env1_second_0:                 episode reward: 12.0500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 8094.6,                last time consumption/overall running time: 1742.8827s / 1535357.9686 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0038
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0034
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 7997.5,                last time consumption/overall running time: 1789.2200s / 1537147.1885 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0044
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0042
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 7135.05,                last time consumption/overall running time: 1623.5886s / 1538770.7772 s
env0_first_0:                 episode reward: -16.5500,                 loss: 0.0044
env0_second_0:                 episode reward: 16.5500,                 loss: 0.0043
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 8910.95,                last time consumption/overall running time: 2050.0638s / 1540820.8409 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0042
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 7904.6,                last time consumption/overall running time: 1829.0515s / 1542649.8924 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0045
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0042
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 7615.9,                last time consumption/overall running time: 1757.5062s / 1544407.3986 s
env0_first_0:                 episode reward: -39.0000,                 loss: 0.0141
env0_second_0:                 episode reward: 39.0000,                 loss: 0.0052
env1_first_0:                 episode reward: -28.2000,                 loss: nan
env1_second_0:                 episode reward: 28.2000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 7665.1,                last time consumption/overall running time: 1761.9906s / 1546169.3892 s
env0_first_0:                 episode reward: -12.1500,                 loss: 0.0045
env0_second_0:                 episode reward: 12.1500,                 loss: 0.0039
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 8475.8,                last time consumption/overall running time: 1963.3952s / 1548132.7844 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0041
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0037
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 8514.35,                last time consumption/overall running time: 1949.2625s / 1550082.0470 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0033
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0030
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 8181.4,                last time consumption/overall running time: 1762.1166s / 1551844.1635 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0033
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0030
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 8301.4,                last time consumption/overall running time: 1839.6399s / 1553683.8034 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0030
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0029
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 8259.55,                last time consumption/overall running time: 1820.9067s / 1555504.7101 s
env0_first_0:                 episode reward: -19.4500,                 loss: 0.0046
env0_second_0:                 episode reward: 19.4500,                 loss: 0.0045
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 8207.75,                last time consumption/overall running time: 1819.3967s / 1557324.1069 s
env0_first_0:                 episode reward: 14.4500,                 loss: 0.0041
env0_second_0:                 episode reward: -14.4500,                 loss: 0.0039
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 8446.55,                last time consumption/overall running time: 1874.2463s / 1559198.3532 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0036
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0033
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 8079.5,                last time consumption/overall running time: 1742.5977s / 1560940.9509 s
env0_first_0:                 episode reward: 9.8500,                 loss: 0.0043
env0_second_0:                 episode reward: -9.8500,                 loss: 0.0039
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 7958.65,                last time consumption/overall running time: 1732.5393s / 1562673.4902 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0036
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0033
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 8107.6,                last time consumption/overall running time: 1775.1271s / 1564448.6173 s
env0_first_0:                 episode reward: -19.8500,                 loss: 0.0053
env0_second_0:                 episode reward: 19.8500,                 loss: 0.0051
env1_first_0:                 episode reward: -21.3500,                 loss: nan
env1_second_0:                 episode reward: 21.3500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 7872.1,                last time consumption/overall running time: 1717.0404s / 1566165.6577 s
env0_first_0:                 episode reward: -8.0500,                 loss: 0.0036
env0_second_0:                 episode reward: 8.0500,                 loss: 0.0033
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 6779.85,                last time consumption/overall running time: 1472.9937s / 1567638.6514 s
env0_first_0:                 episode reward: 6.1000,                 loss: 0.0043
env0_second_0:                 episode reward: -6.1000,                 loss: 0.0038
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 7843.9,                last time consumption/overall running time: 1706.0099s / 1569344.6613 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.0051
env0_second_0:                 episode reward: 8.5000,                 loss: 0.0048
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 8582.6,                last time consumption/overall running time: 1868.3850s / 1571213.0463 s
env0_first_0:                 episode reward: 9.6000,                 loss: 0.0044
env0_second_0:                 episode reward: -9.6000,                 loss: 0.0038
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 7892.1,                last time consumption/overall running time: 1728.6140s / 1572941.6603 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0036
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 7345.15,                last time consumption/overall running time: 1626.8890s / 1574568.5493 sLoad tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: -4.8000,                 loss: 0.0039
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0037
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 7942.85,                last time consumption/overall running time: 1740.0858s / 1576308.6351 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0050
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0044
env1_first_0:                 episode reward: -20.0000,                 loss: nan
env1_second_0:                 episode reward: 20.0000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 7351.85,                last time consumption/overall running time: 1603.5542s / 1577912.1893 s
env0_first_0:                 episode reward: -26.7500,                 loss: 0.0061
env0_second_0:                 episode reward: 26.7500,                 loss: 0.0056
env1_first_0:                 episode reward: -22.7000,                 loss: nan
env1_second_0:                 episode reward: 22.7000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 6633.85,                last time consumption/overall running time: 1449.4979s / 1579361.6872 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0046
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0042
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 7104.85,                last time consumption/overall running time: 1580.0589s / 1580941.7462 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0044
env0_second_0:                 episode reward: -4.1500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 7464.4,                last time consumption/overall running time: 1637.2119s / 1582578.9580 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0051
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0047
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 7151.5,                last time consumption/overall running time: 1579.4815s / 1584158.4395 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0039
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 6691.4,                last time consumption/overall running time: 1467.6921s / 1585626.1316 s
env0_first_0:                 episode reward: -13.0500,                 loss: 0.0054
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -15.6500,                 loss: nan
env1_second_0:                 episode reward: 15.6500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 7392.2,                last time consumption/overall running time: 1642.2967s / 1587268.4282 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0087
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0065
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 6688.55,                last time consumption/overall running time: 1540.6864s / 1588809.1147 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0049
env0_second_0:                 episode reward: 7.9000,                 loss: 0.0039
env1_first_0:                 episode reward: -10.3000,                 loss: nan
env1_second_0:                 episode reward: 10.3000,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 7745.8,                last time consumption/overall running time: 1913.0701s / 1590722.1848 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0040
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 6051.7,                last time consumption/overall running time: 1551.0549s / 1592273.2396 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0051
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0042
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
