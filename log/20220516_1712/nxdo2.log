pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 42.0, (1,), float32) action space: Discrete(6)
random seed: 122
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7fabbb5d3dd8>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.0, 'eps_decay': 10000}, 'num_process': 1, 'batch_size': 640, 'max_episodes': 50100, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 2.0, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220516171217/mdp_arbitrary_mdp_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220516171217/mdp_arbitrary_mdp_nxdo2.
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 42.0, (1,), float32) action space: Discrete(6)
random seed: 123
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Episode: 1/50100 (0.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6783s / 0.6783 s
agent0:                 episode reward: -2.0950,                 loss: nan
agent1:                 episode reward: 2.0950,                 loss: nan
Episode: 21/50100 (0.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 0.1103s / 0.7887 s
agent0:                 episode reward: -0.4583,                 loss: nan
agent1:                 episode reward: 0.4583,                 loss: nan
Episode: 41/50100 (0.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 0.1268s / 0.9154 s
agent0:                 episode reward: -0.1795,                 loss: nan
agent1:                 episode reward: 0.1795,                 loss: nan
Episode: 61/50100 (0.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 0.1124s / 1.0278 s
agent0:                 episode reward: -0.2358,                 loss: nan
agent1:                 episode reward: 0.2358,                 loss: nan
Episode: 81/50100 (0.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 0.1051s / 1.1329 s
agent0:                 episode reward: -0.4889,                 loss: nan
agent1:                 episode reward: 0.4889,                 loss: nan
Episode: 101/50100 (0.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0995s / 1.2324 s
agent0:                 episode reward: -0.3044,                 loss: nan
agent1:                 episode reward: 0.3044,                 loss: nan
Episode: 121/50100 (0.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 0.4794s / 1.7119 s
agent0:                 episode reward: -0.2682,                 loss: 0.4525
agent1:                 episode reward: 0.2682,                 loss: nan
Episode: 141/50100 (0.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6356s / 2.3475 s
agent0:                 episode reward: -0.1607,                 loss: 0.4385
agent1:                 episode reward: 0.1607,                 loss: nan
Episode: 161/50100 (0.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6382s / 2.9857 s
agent0:                 episode reward: -0.2851,                 loss: 0.4331
agent1:                 episode reward: 0.2851,                 loss: nan
Episode: 181/50100 (0.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6350s / 3.6207 s
agent0:                 episode reward: -0.1307,                 loss: 0.4286
agent1:                 episode reward: 0.1307,                 loss: nan
Episode: 201/50100 (0.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6440s / 4.2647 s
agent0:                 episode reward: -0.9395,                 loss: 0.4266
agent1:                 episode reward: 0.9395,                 loss: nan
Episode: 221/50100 (0.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6402s / 4.9050 s
agent0:                 episode reward: -0.1843,                 loss: 0.4236
agent1:                 episode reward: 0.1843,                 loss: nan
Episode: 241/50100 (0.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6463s / 5.5513 s
agent0:                 episode reward: 0.0839,                 loss: 0.4257
agent1:                 episode reward: -0.0839,                 loss: nan
Episode: 261/50100 (0.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6411s / 6.1924 s
agent0:                 episode reward: 0.1973,                 loss: 0.4251
agent1:                 episode reward: -0.1973,                 loss: nan
Episode: 281/50100 (0.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6541s / 6.8464 s
agent0:                 episode reward: 0.3676,                 loss: 0.4213
agent1:                 episode reward: -0.3676,                 loss: nan
Episode: 301/50100 (0.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6446s / 7.4910 s
agent0:                 episode reward: 0.0832,                 loss: 0.4090
agent1:                 episode reward: -0.0832,                 loss: nan
Episode: 321/50100 (0.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6452s / 8.1363 s
agent0:                 episode reward: 0.2797,                 loss: 0.4046
agent1:                 episode reward: -0.2797,                 loss: nan
Episode: 341/50100 (0.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6513s / 8.7876 s
agent0:                 episode reward: 0.3483,                 loss: 0.4030
agent1:                 episode reward: -0.3483,                 loss: nan
Episode: 361/50100 (0.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6518s / 9.4394 s
agent0:                 episode reward: 0.0781,                 loss: 0.4030
agent1:                 episode reward: -0.0781,                 loss: nan
Episode: 381/50100 (0.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6490s / 10.0883 s
agent0:                 episode reward: -0.2343,                 loss: 0.4020
agent1:                 episode reward: 0.2343,                 loss: nan
Episode: 401/50100 (0.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 0.5392s / 10.6275 s
agent0:                 episode reward: 0.5097,                 loss: 0.4043
agent1:                 episode reward: -0.5097,                 loss: nan
Score delta: 2.0471298915330784, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/291_0.
Episode: 421/50100 (0.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0951s / 10.7226 s
agent0:                 episode reward: 0.1748,                 loss: nan
agent1:                 episode reward: -0.1748,                 loss: nan
Episode: 441/50100 (0.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0985s / 10.8211 s
agent0:                 episode reward: 0.1680,                 loss: nan
agent1:                 episode reward: -0.1680,                 loss: nan
Episode: 461/50100 (0.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0987s / 10.9198 s
agent0:                 episode reward: 0.1533,                 loss: nan
agent1:                 episode reward: -0.1533,                 loss: nan
Episode: 481/50100 (0.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0983s / 11.0181 s
agent0:                 episode reward: 0.2285,                 loss: nan
agent1:                 episode reward: -0.2285,                 loss: nan
Episode: 501/50100 (1.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0955s / 11.1135 s
agent0:                 episode reward: 0.0960,                 loss: nan
agent1:                 episode reward: -0.0960,                 loss: nan
Episode: 521/50100 (1.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 0.5784s / 11.6919 s
agent0:                 episode reward: -0.2255,                 loss: nan
agent1:                 episode reward: 0.2255,                 loss: 0.4395
Episode: 541/50100 (1.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6579s / 12.3498 s
agent0:                 episode reward: -0.3523,                 loss: nan
agent1:                 episode reward: 0.3523,                 loss: 0.4043
Episode: 561/50100 (1.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6582s / 13.0080 s
agent0:                 episode reward: 0.0052,                 loss: nan
agent1:                 episode reward: -0.0052,                 loss: 0.3896
Episode: 581/50100 (1.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6676s / 13.6755 s
agent0:                 episode reward: -0.0003,                 loss: nan
agent1:                 episode reward: 0.0003,                 loss: 0.3752
Episode: 601/50100 (1.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6626s / 14.3381 s
agent0:                 episode reward: 0.0114,                 loss: nan
agent1:                 episode reward: -0.0114,                 loss: 0.3515
Episode: 621/50100 (1.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6595s / 14.9976 s
agent0:                 episode reward: 0.0074,                 loss: nan
agent1:                 episode reward: -0.0074,                 loss: 0.3233
Episode: 641/50100 (1.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6640s / 15.6616 s
agent0:                 episode reward: -0.0282,                 loss: nan
agent1:                 episode reward: 0.0282,                 loss: 0.2972
Episode: 661/50100 (1.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6555s / 16.3172 s
agent0:                 episode reward: 0.2775,                 loss: nan
agent1:                 episode reward: -0.2775,                 loss: 0.2762
Episode: 681/50100 (1.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6597s / 16.9769 s
agent0:                 episode reward: -0.1444,                 loss: nan
agent1:                 episode reward: 0.1444,                 loss: 0.3269
Episode: 701/50100 (1.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6677s / 17.6446 s
agent0:                 episode reward: 0.0259,                 loss: nan
agent1:                 episode reward: -0.0259,                 loss: 0.3528
Episode: 721/50100 (1.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6676s / 18.3121 s
agent0:                 episode reward: -0.3064,                 loss: nan
agent1:                 episode reward: 0.3064,                 loss: 0.3528
Episode: 741/50100 (1.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6866s / 18.9987 s
agent0:                 episode reward: -0.9081,                 loss: 0.4046
agent1:                 episode reward: 0.9081,                 loss: 0.3504
Score delta: 2.326840634629603, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/514_1.
Episode: 761/50100 (1.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6583s / 19.6570 s
agent0:                 episode reward: -0.2668,                 loss: 0.4075
agent1:                 episode reward: 0.2668,                 loss: nan
Episode: 781/50100 (1.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6690s / 20.3260 s
agent0:                 episode reward: -0.0006,                 loss: 0.3966
agent1:                 episode reward: 0.0006,                 loss: nan
Episode: 801/50100 (1.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6693s / 20.9954 s
agent0:                 episode reward: -0.5389,                 loss: 0.3853
agent1:                 episode reward: 0.5389,                 loss: nan
Episode: 821/50100 (1.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6689s / 21.6643 s
agent0:                 episode reward: -0.5873,                 loss: 0.3869
agent1:                 episode reward: 0.5873,                 loss: nan
Episode: 841/50100 (1.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6713s / 22.3355 s
agent0:                 episode reward: -0.5037,                 loss: 0.3867
agent1:                 episode reward: 0.5037,                 loss: nan
Episode: 861/50100 (1.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6675s / 23.0030 s
agent0:                 episode reward: -0.5301,                 loss: 0.3874
agent1:                 episode reward: 0.5301,                 loss: nan
Episode: 881/50100 (1.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6757s / 23.6787 s
agent0:                 episode reward: -0.8553,                 loss: 0.3893
agent1:                 episode reward: 0.8553,                 loss: nan
Episode: 901/50100 (1.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6765s / 24.3552 s
agent0:                 episode reward: -1.0603,                 loss: 0.3883
agent1:                 episode reward: 1.0603,                 loss: nan
Episode: 921/50100 (1.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6719s / 25.0271 s
agent0:                 episode reward: -0.8477,                 loss: 0.3901
agent1:                 episode reward: 0.8477,                 loss: nan
Episode: 941/50100 (1.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6809s / 25.7080 s
agent0:                 episode reward: -0.6369,                 loss: 0.3993
agent1:                 episode reward: 0.6369,                 loss: nan
Episode: 961/50100 (1.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6752s / 26.3832 s
agent0:                 episode reward: -0.4612,                 loss: 0.4164
agent1:                 episode reward: 0.4612,                 loss: nan
Episode: 981/50100 (1.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6772s / 27.0604 s
agent0:                 episode reward: -0.5481,                 loss: 0.4168
agent1:                 episode reward: 0.5481,                 loss: nan
Episode: 1001/50100 (1.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6774s / 27.7378 s
agent0:                 episode reward: -0.4413,                 loss: 0.4190
agent1:                 episode reward: 0.4413,                 loss: nan
Episode: 1021/50100 (2.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6888s / 28.4266 s
agent0:                 episode reward: -0.6880,                 loss: 0.4177
agent1:                 episode reward: 0.6880,                 loss: nan
Episode: 1041/50100 (2.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6895s / 29.1161 s
agent0:                 episode reward: -0.5404,                 loss: 0.4184
agent1:                 episode reward: 0.5404,                 loss: nan
Episode: 1061/50100 (2.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6905s / 29.8066 s
agent0:                 episode reward: -0.4919,                 loss: 0.4184
agent1:                 episode reward: 0.4919,                 loss: nan
Episode: 1081/50100 (2.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6848s / 30.4914 s
agent0:                 episode reward: -0.4680,                 loss: 0.4183
agent1:                 episode reward: 0.4680,                 loss: nan
Episode: 1101/50100 (2.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6904s / 31.1818 s
agent0:                 episode reward: -0.3862,                 loss: 0.4178
agent1:                 episode reward: 0.3862,                 loss: nan
Episode: 1121/50100 (2.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6971s / 31.8789 s
agent0:                 episode reward: 0.4473,                 loss: 0.4295
agent1:                 episode reward: -0.4473,                 loss: nan
Episode: 1141/50100 (2.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6858s / 32.5646 s
agent0:                 episode reward: -0.1930,                 loss: 0.4287
agent1:                 episode reward: 0.1930,                 loss: nan
Episode: 1161/50100 (2.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6890s / 33.2537 s
agent0:                 episode reward: -0.5204,                 loss: 0.4293
agent1:                 episode reward: 0.5204,                 loss: nan
Episode: 1181/50100 (2.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6861s / 33.9397 s
agent0:                 episode reward: 0.1180,                 loss: 0.4292
agent1:                 episode reward: -0.1180,                 loss: nan
Episode: 1201/50100 (2.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6994s / 34.6391 s
agent0:                 episode reward: -0.8792,                 loss: 0.4282
agent1:                 episode reward: 0.8792,                 loss: nan
Episode: 1221/50100 (2.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7012s / 35.3404 s
agent0:                 episode reward: -0.3285,                 loss: 0.4277
agent1:                 episode reward: 0.3285,                 loss: nan
Episode: 1241/50100 (2.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6983s / 36.0387 s
agent0:                 episode reward: -0.0077,                 loss: 0.4279
agent1:                 episode reward: 0.0077,                 loss: nan
Episode: 1261/50100 (2.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7129s / 36.7516 s
agent0:                 episode reward: 0.2299,                 loss: 0.4284
agent1:                 episode reward: -0.2299,                 loss: 0.3487
Score delta: 2.0890509879661407, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/1046_0.
Episode: 1281/50100 (2.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6682s / 37.4198 s
agent0:                 episode reward: -0.0478,                 loss: nan
agent1:                 episode reward: 0.0478,                 loss: 0.3495
Episode: 1301/50100 (2.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6876s / 38.1074 s
agent0:                 episode reward: 0.2162,                 loss: nan
agent1:                 episode reward: -0.2162,                 loss: 0.3513
Episode: 1321/50100 (2.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7661s / 38.8735 s
agent0:                 episode reward: -0.4610,                 loss: 0.4299
agent1:                 episode reward: 0.4610,                 loss: 0.3486
Score delta: 2.9444067731889647, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/1102_1.
Episode: 1341/50100 (2.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7870s / 39.6605 s
agent0:                 episode reward: 0.0124,                 loss: 0.4439
agent1:                 episode reward: -0.0124,                 loss: nan
Episode: 1361/50100 (2.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7841s / 40.4446 s
agent0:                 episode reward: -0.4297,                 loss: 0.4449
agent1:                 episode reward: 0.4297,                 loss: nan
Episode: 1381/50100 (2.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7835s / 41.2281 s
agent0:                 episode reward: -1.1888,                 loss: 0.4433
agent1:                 episode reward: 1.1888,                 loss: nan
Episode: 1401/50100 (2.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7851s / 42.0132 s
agent0:                 episode reward: 0.2232,                 loss: 0.4435
agent1:                 episode reward: -0.2232,                 loss: nan
Episode: 1421/50100 (2.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7809s / 42.7941 s
agent0:                 episode reward: -0.2076,                 loss: 0.4436
agent1:                 episode reward: 0.2076,                 loss: nan
Episode: 1441/50100 (2.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8211s / 43.6152 s
agent0:                 episode reward: 0.1363,                 loss: 0.4435
agent1:                 episode reward: -0.1363,                 loss: nan
Episode: 1461/50100 (2.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7863s / 44.4016 s
agent0:                 episode reward: -0.2637,                 loss: 0.4422
agent1:                 episode reward: 0.2637,                 loss: nan
Episode: 1481/50100 (2.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7836s / 45.1851 s
agent0:                 episode reward: -0.2290,                 loss: 0.4422
agent1:                 episode reward: 0.2290,                 loss: nan
Episode: 1501/50100 (2.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8011s / 45.9862 s
agent0:                 episode reward: 0.2576,                 loss: 0.4440
agent1:                 episode reward: -0.2576,                 loss: nan
Episode: 1521/50100 (3.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7982s / 46.7843 s
agent0:                 episode reward: 0.0040,                 loss: 0.4441
agent1:                 episode reward: -0.0040,                 loss: nan
Episode: 1541/50100 (3.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7920s / 47.5763 s
agent0:                 episode reward: 0.1928,                 loss: 0.4441
agent1:                 episode reward: -0.1928,                 loss: nan
Episode: 1561/50100 (3.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7827s / 48.3590 s
agent0:                 episode reward: 0.2800,                 loss: 0.4418
agent1:                 episode reward: -0.2800,                 loss: nan
Episode: 1581/50100 (3.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7932s / 49.1522 s
agent0:                 episode reward: 0.3516,                 loss: 0.4410
agent1:                 episode reward: -0.3516,                 loss: nan
Episode: 1601/50100 (3.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7999s / 49.9521 s
agent0:                 episode reward: -0.3835,                 loss: 0.4402
agent1:                 episode reward: 0.3835,                 loss: nan
Episode: 1621/50100 (3.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7949s / 50.7470 s
agent0:                 episode reward: -0.0746,                 loss: 0.4395
agent1:                 episode reward: 0.0746,                 loss: nan
Episode: 1641/50100 (3.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7906s / 51.5376 s
agent0:                 episode reward: 0.4456,                 loss: 0.4386
agent1:                 episode reward: -0.4456,                 loss: nan
Episode: 1661/50100 (3.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8261s / 52.3637 s
agent0:                 episode reward: 0.0292,                 loss: 0.4371
agent1:                 episode reward: -0.0292,                 loss: 0.3537
Score delta: 2.1868098894006227, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/1435_0.
Episode: 1681/50100 (3.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8558s / 53.2195 s
agent0:                 episode reward: -0.6995,                 loss: 0.4360
agent1:                 episode reward: 0.6995,                 loss: 0.3524
Score delta: 2.4420754666023057, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/1456_1.
Episode: 1701/50100 (3.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7898s / 54.0092 s
agent0:                 episode reward: -0.8751,                 loss: 0.3935
agent1:                 episode reward: 0.8751,                 loss: nan
Episode: 1721/50100 (3.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8027s / 54.8119 s
agent0:                 episode reward: 0.1550,                 loss: 0.3849
agent1:                 episode reward: -0.1550,                 loss: nan
Episode: 1741/50100 (3.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8012s / 55.6131 s
agent0:                 episode reward: -0.0066,                 loss: 0.3810
agent1:                 episode reward: 0.0066,                 loss: nan
Episode: 1761/50100 (3.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8106s / 56.4237 s
agent0:                 episode reward: 0.1134,                 loss: 0.3780
agent1:                 episode reward: -0.1134,                 loss: nan
Episode: 1781/50100 (3.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8035s / 57.2272 s
agent0:                 episode reward: -0.3421,                 loss: 0.3758
agent1:                 episode reward: 0.3421,                 loss: nan
Episode: 1801/50100 (3.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8154s / 58.0426 s
agent0:                 episode reward: -0.1700,                 loss: 0.3745
agent1:                 episode reward: 0.1700,                 loss: nan
Episode: 1821/50100 (3.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8156s / 58.8582 s
agent0:                 episode reward: -0.3153,                 loss: 0.3741
agent1:                 episode reward: 0.3153,                 loss: nan
Episode: 1841/50100 (3.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7878s / 59.6460 s
agent0:                 episode reward: 0.2152,                 loss: 0.3720
agent1:                 episode reward: -0.2152,                 loss: nan
Episode: 1861/50100 (3.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7892s / 60.4352 s
agent0:                 episode reward: -0.1548,                 loss: 0.3819
agent1:                 episode reward: 0.1548,                 loss: nan
Episode: 1881/50100 (3.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8119s / 61.2471 s
agent0:                 episode reward: 0.3356,                 loss: 0.3841
agent1:                 episode reward: -0.3356,                 loss: nan
Episode: 1901/50100 (3.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7921s / 62.0393 s
agent0:                 episode reward: 0.3984,                 loss: 0.3823
agent1:                 episode reward: -0.3984,                 loss: nan
Episode: 1921/50100 (3.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7884s / 62.8276 s
agent0:                 episode reward: -0.5331,                 loss: 0.3836
agent1:                 episode reward: 0.5331,                 loss: nan
Episode: 1941/50100 (3.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8082s / 63.6359 s
agent0:                 episode reward: -0.2844,                 loss: 0.3834
agent1:                 episode reward: 0.2844,                 loss: nan
Episode: 1961/50100 (3.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8134s / 64.4492 s
agent0:                 episode reward: 0.2693,                 loss: 0.3793
agent1:                 episode reward: -0.2693,                 loss: nan
Episode: 1981/50100 (3.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8048s / 65.2541 s
agent0:                 episode reward: 0.6734,                 loss: 0.3806
agent1:                 episode reward: -0.6734,                 loss: nan
Episode: 2001/50100 (3.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8087s / 66.0627 s
agent0:                 episode reward: 0.1504,                 loss: 0.3793
agent1:                 episode reward: -0.1504,                 loss: nan
Episode: 2021/50100 (4.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8086s / 66.8713 s
agent0:                 episode reward: -0.4512,                 loss: 0.3945
agent1:                 episode reward: 0.4512,                 loss: nan
Episode: 2041/50100 (4.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7957s / 67.6670 s
agent0:                 episode reward: 0.5575,                 loss: 0.4126
agent1:                 episode reward: -0.5575,                 loss: nan
Episode: 2061/50100 (4.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8064s / 68.4734 s
agent0:                 episode reward: -0.2269,                 loss: 0.4101
agent1:                 episode reward: 0.2269,                 loss: nan
Episode: 2081/50100 (4.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8060s / 69.2794 s
agent0:                 episode reward: 0.1704,                 loss: 0.4110
agent1:                 episode reward: -0.1704,                 loss: nan
Episode: 2101/50100 (4.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8165s / 70.0959 s
agent0:                 episode reward: 0.2059,                 loss: 0.4089
agent1:                 episode reward: -0.2059,                 loss: nan
Episode: 2121/50100 (4.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8105s / 70.9064 s
agent0:                 episode reward: 0.3845,                 loss: 0.4089
agent1:                 episode reward: -0.3845,                 loss: nan
Episode: 2141/50100 (4.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8064s / 71.7128 s
agent0:                 episode reward: -0.3746,                 loss: 0.4075
agent1:                 episode reward: 0.3746,                 loss: nan
Episode: 2161/50100 (4.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8515s / 72.5643 s
agent0:                 episode reward: -0.0435,                 loss: 0.4129
agent1:                 episode reward: 0.0435,                 loss: 0.3535
Score delta: 2.1658025657339324, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/1938_0.
Episode: 2181/50100 (4.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7715s / 73.3358 s
agent0:                 episode reward: -0.4897,                 loss: nan
agent1:                 episode reward: 0.4897,                 loss: 0.3511
Episode: 2201/50100 (4.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8968s / 74.2326 s
agent0:                 episode reward: -0.2875,                 loss: 0.4317
agent1:                 episode reward: 0.2875,                 loss: 0.3495
Score delta: 2.128236376523888, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/1970_1.
Episode: 2221/50100 (4.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8152s / 75.0479 s
agent0:                 episode reward: -0.2609,                 loss: 0.4003
agent1:                 episode reward: 0.2609,                 loss: nan
Episode: 2241/50100 (4.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8387s / 75.8866 s
agent0:                 episode reward: -0.0750,                 loss: 0.3697
agent1:                 episode reward: 0.0750,                 loss: nan
Episode: 2261/50100 (4.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8327s / 76.7193 s
agent0:                 episode reward: -0.1852,                 loss: 0.3649
agent1:                 episode reward: 0.1852,                 loss: nan
Episode: 2281/50100 (4.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8364s / 77.5557 s
agent0:                 episode reward: 0.2183,                 loss: 0.3630
agent1:                 episode reward: -0.2183,                 loss: nan
Episode: 2301/50100 (4.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8327s / 78.3884 s
agent0:                 episode reward: -0.1913,                 loss: 0.3604
agent1:                 episode reward: 0.1913,                 loss: nan
Episode: 2321/50100 (4.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8300s / 79.2183 s
agent0:                 episode reward: -0.0082,                 loss: 0.3594
agent1:                 episode reward: 0.0082,                 loss: nan
Episode: 2341/50100 (4.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9143s / 80.1326 s
agent0:                 episode reward: 0.3001,                 loss: 0.3586
agent1:                 episode reward: -0.3001,                 loss: 0.3717
Score delta: 2.0914282646190205, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/2126_0.
Episode: 2361/50100 (4.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7503s / 80.8829 s
agent0:                 episode reward: -0.2122,                 loss: nan
agent1:                 episode reward: 0.2122,                 loss: 0.3461
Episode: 2381/50100 (4.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9030s / 81.7859 s
agent0:                 episode reward: -0.5552,                 loss: 0.4355
agent1:                 episode reward: 0.5552,                 loss: 0.3340
Score delta: 2.023305122466842, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/2159_1.
Episode: 2401/50100 (4.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8425s / 82.6284 s
agent0:                 episode reward: -0.1845,                 loss: 0.4219
agent1:                 episode reward: 0.1845,                 loss: nan
Episode: 2421/50100 (4.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8495s / 83.4779 s
agent0:                 episode reward: 0.1956,                 loss: 0.4026
agent1:                 episode reward: -0.1956,                 loss: nan
Episode: 2441/50100 (4.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8464s / 84.3243 s
agent0:                 episode reward: -0.2584,                 loss: 0.3800
agent1:                 episode reward: 0.2584,                 loss: nan
Episode: 2461/50100 (4.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8503s / 85.1747 s
agent0:                 episode reward: 0.1616,                 loss: 0.3741
agent1:                 episode reward: -0.1616,                 loss: nan
Episode: 2481/50100 (4.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8482s / 86.0229 s
agent0:                 episode reward: -0.3459,                 loss: 0.3705
agent1:                 episode reward: 0.3459,                 loss: nan
Episode: 2501/50100 (4.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8297s / 86.8526 s
agent0:                 episode reward: -0.5034,                 loss: 0.3710
agent1:                 episode reward: 0.5034,                 loss: nan
Episode: 2521/50100 (5.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8473s / 87.6999 s
agent0:                 episode reward: -0.1897,                 loss: 0.3686
agent1:                 episode reward: 0.1897,                 loss: nan
Episode: 2541/50100 (5.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8428s / 88.5428 s
agent0:                 episode reward: 0.1083,                 loss: 0.3678
agent1:                 episode reward: -0.1083,                 loss: nan
Episode: 2561/50100 (5.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8426s / 89.3853 s
agent0:                 episode reward: 0.0980,                 loss: 0.3645
agent1:                 episode reward: -0.0980,                 loss: nan
Episode: 2581/50100 (5.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8368s / 90.2221 s
agent0:                 episode reward: 0.2686,                 loss: 0.3680
agent1:                 episode reward: -0.2686,                 loss: nan
Episode: 2601/50100 (5.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8808s / 91.1029 s
agent0:                 episode reward: 0.5264,                 loss: 0.3807
agent1:                 episode reward: -0.5264,                 loss: 0.3501
Score delta: 2.1529294763738496, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/2374_0.
Episode: 2621/50100 (5.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7600s / 91.8629 s
agent0:                 episode reward: -0.2155,                 loss: nan
agent1:                 episode reward: 0.2155,                 loss: 0.3497
Episode: 2641/50100 (5.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7649s / 92.6278 s
agent0:                 episode reward: -0.0968,                 loss: nan
agent1:                 episode reward: 0.0968,                 loss: 0.3524
Episode: 2661/50100 (5.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7568s / 93.3847 s
agent0:                 episode reward: -0.1119,                 loss: nan
agent1:                 episode reward: 0.1119,                 loss: 0.3488
Episode: 2681/50100 (5.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7753s / 94.1600 s
agent0:                 episode reward: -0.2163,                 loss: nan
agent1:                 episode reward: 0.2163,                 loss: 0.3452
Episode: 2701/50100 (5.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7668s / 94.9268 s
agent0:                 episode reward: 0.2400,                 loss: nan
agent1:                 episode reward: -0.2400,                 loss: 0.3430
Episode: 2721/50100 (5.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9054s / 95.8322 s
agent0:                 episode reward: -0.4689,                 loss: 0.4042
agent1:                 episode reward: 0.4689,                 loss: 0.3425
Score delta: 2.0194292394644044, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/2505_1.
Episode: 2741/50100 (5.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8605s / 96.6927 s
agent0:                 episode reward: 0.3961,                 loss: 0.4050
agent1:                 episode reward: -0.3961,                 loss: nan
Episode: 2761/50100 (5.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8538s / 97.5465 s
agent0:                 episode reward: 0.4185,                 loss: 0.4033
agent1:                 episode reward: -0.4185,                 loss: nan
Episode: 2781/50100 (5.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8666s / 98.4131 s
agent0:                 episode reward: 0.3290,                 loss: 0.4030
agent1:                 episode reward: -0.3290,                 loss: nan
Episode: 2801/50100 (5.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9424s / 99.3555 s
agent0:                 episode reward: 0.5105,                 loss: 0.4020
agent1:                 episode reward: -0.5105,                 loss: 0.3739
Score delta: 2.1056916487720985, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/2579_0.
Episode: 2821/50100 (5.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7815s / 100.1370 s
agent0:                 episode reward: -0.0385,                 loss: nan
agent1:                 episode reward: 0.0385,                 loss: 0.3543
Episode: 2841/50100 (5.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7839s / 100.9209 s
agent0:                 episode reward: 1.0408,                 loss: nan
agent1:                 episode reward: -1.0408,                 loss: 0.3472
Episode: 2861/50100 (5.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7799s / 101.7008 s
agent0:                 episode reward: 0.2359,                 loss: nan
agent1:                 episode reward: -0.2359,                 loss: 0.3447
Episode: 2881/50100 (5.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7850s / 102.4858 s
agent0:                 episode reward: -0.2966,                 loss: nan
agent1:                 episode reward: 0.2966,                 loss: 0.3427
Episode: 2901/50100 (5.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7780s / 103.2638 s
agent0:                 episode reward: -0.0660,                 loss: nan
agent1:                 episode reward: 0.0660,                 loss: 0.3391
Episode: 2921/50100 (5.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7817s / 104.0456 s
agent0:                 episode reward: 0.0370,                 loss: nan
agent1:                 episode reward: -0.0370,                 loss: 0.3374
Episode: 2941/50100 (5.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7879s / 104.8334 s
agent0:                 episode reward: -0.1546,                 loss: nan
agent1:                 episode reward: 0.1546,                 loss: 0.3365
Episode: 2961/50100 (5.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7742s / 105.6077 s
agent0:                 episode reward: 0.1061,                 loss: nan
agent1:                 episode reward: -0.1061,                 loss: 0.3363
Episode: 2981/50100 (5.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7796s / 106.3872 s
agent0:                 episode reward: 0.0273,                 loss: nan
agent1:                 episode reward: -0.0273,                 loss: 0.3110
Episode: 3001/50100 (5.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7916s / 107.1789 s
agent0:                 episode reward: -0.1500,                 loss: nan
agent1:                 episode reward: 0.1500,                 loss: 0.2965
Episode: 3021/50100 (6.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7941s / 107.9730 s
agent0:                 episode reward: 0.5295,                 loss: nan
agent1:                 episode reward: -0.5295,                 loss: 0.2952
Episode: 3041/50100 (6.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7892s / 108.7621 s
agent0:                 episode reward: -0.5331,                 loss: nan
agent1:                 episode reward: 0.5331,                 loss: 0.2955
Episode: 3061/50100 (6.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9360s / 109.6981 s
agent0:                 episode reward: -0.2569,                 loss: 0.4008
agent1:                 episode reward: 0.2569,                 loss: 0.2930
Score delta: 2.006439844031845, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/2848_1.
Episode: 3081/50100 (6.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8574s / 110.5556 s
agent0:                 episode reward: 0.1668,                 loss: 0.4044
agent1:                 episode reward: -0.1668,                 loss: nan
Episode: 3101/50100 (6.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8610s / 111.4166 s
agent0:                 episode reward: 0.1498,                 loss: 0.4022
agent1:                 episode reward: -0.1498,                 loss: nan
Episode: 3121/50100 (6.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8661s / 112.2827 s
agent0:                 episode reward: -0.4542,                 loss: 0.4044
agent1:                 episode reward: 0.4542,                 loss: nan
Episode: 3141/50100 (6.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8670s / 113.1497 s
agent0:                 episode reward: -0.2503,                 loss: 0.4040
agent1:                 episode reward: 0.2503,                 loss: nan
Episode: 3161/50100 (6.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8571s / 114.0068 s
agent0:                 episode reward: 0.0683,                 loss: 0.4137
agent1:                 episode reward: -0.0683,                 loss: nan
Episode: 3181/50100 (6.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8673s / 114.8741 s
agent0:                 episode reward: 0.0649,                 loss: 0.4144
agent1:                 episode reward: -0.0649,                 loss: nan
Episode: 3201/50100 (6.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8626s / 115.7367 s
agent0:                 episode reward: 0.5159,                 loss: 0.4152
agent1:                 episode reward: -0.5159,                 loss: nan
Episode: 3221/50100 (6.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8621s / 116.5988 s
agent0:                 episode reward: -0.2988,                 loss: 0.4161
agent1:                 episode reward: 0.2988,                 loss: nan
Episode: 3241/50100 (6.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9565s / 117.5553 s
agent0:                 episode reward: 0.7034,                 loss: 0.4147
agent1:                 episode reward: -0.7034,                 loss: 0.3724
Score delta: 2.1804223687982756, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3017_0.
Episode: 3261/50100 (6.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7820s / 118.3373 s
agent0:                 episode reward: -0.1995,                 loss: nan
agent1:                 episode reward: 0.1995,                 loss: 0.3686
Episode: 3281/50100 (6.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7821s / 119.1194 s
agent0:                 episode reward: 0.0045,                 loss: nan
agent1:                 episode reward: -0.0045,                 loss: 0.3654
Episode: 3301/50100 (6.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7870s / 119.9065 s
agent0:                 episode reward: -0.1285,                 loss: nan
agent1:                 episode reward: 0.1285,                 loss: 0.3736
Episode: 3321/50100 (6.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8009s / 120.7073 s
agent0:                 episode reward: -0.1878,                 loss: nan
agent1:                 episode reward: 0.1878,                 loss: 0.3842
Episode: 3341/50100 (6.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7903s / 121.4977 s
agent0:                 episode reward: -0.2656,                 loss: nan
agent1:                 episode reward: 0.2656,                 loss: 0.3753
Episode: 3361/50100 (6.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9615s / 122.4592 s
agent0:                 episode reward: -0.2526,                 loss: 0.4451
agent1:                 episode reward: 0.2526,                 loss: 0.3723
Score delta: 2.531395825592278, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3146_1.
Episode: 3381/50100 (6.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8650s / 123.3242 s
agent0:                 episode reward: 0.0230,                 loss: 0.4227
agent1:                 episode reward: -0.0230,                 loss: nan
Episode: 3401/50100 (6.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8734s / 124.1976 s
agent0:                 episode reward: -0.1771,                 loss: 0.4155
agent1:                 episode reward: 0.1771,                 loss: nan
Episode: 3421/50100 (6.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8560s / 125.0537 s
agent0:                 episode reward: -0.4126,                 loss: 0.4129
agent1:                 episode reward: 0.4126,                 loss: nan
Episode: 3441/50100 (6.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8586s / 125.9123 s
agent0:                 episode reward: -0.5557,                 loss: 0.4094
agent1:                 episode reward: 0.5557,                 loss: nan
Episode: 3461/50100 (6.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8809s / 126.7932 s
agent0:                 episode reward: 0.8612,                 loss: 0.3799
agent1:                 episode reward: -0.8612,                 loss: nan
Episode: 3481/50100 (6.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9635s / 127.7567 s
agent0:                 episode reward: -0.3982,                 loss: 0.3738
agent1:                 episode reward: 0.3982,                 loss: 0.3703
Score delta: 2.084939585876851, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3250_0.
Episode: 3501/50100 (6.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8060s / 128.5627 s
agent0:                 episode reward: 0.4822,                 loss: nan
agent1:                 episode reward: -0.4822,                 loss: 0.3685
Episode: 3521/50100 (7.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8054s / 129.3681 s
agent0:                 episode reward: 0.4438,                 loss: nan
agent1:                 episode reward: -0.4438,                 loss: 0.3661
Episode: 3541/50100 (7.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8049s / 130.1730 s
agent0:                 episode reward: -0.1133,                 loss: nan
agent1:                 episode reward: 0.1133,                 loss: 0.3658
Episode: 3561/50100 (7.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7948s / 130.9678 s
agent0:                 episode reward: -0.1133,                 loss: nan
agent1:                 episode reward: 0.1133,                 loss: 0.3638
Episode: 3581/50100 (7.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9943s / 131.9621 s
agent0:                 episode reward: -0.6463,                 loss: nan
agent1:                 episode reward: 0.6463,                 loss: 0.3709
Score delta: 2.273753077827257, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3369_1.
Episode: 3601/50100 (7.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8639s / 132.8260 s
agent0:                 episode reward: -0.4201,                 loss: 0.4266
agent1:                 episode reward: 0.4201,                 loss: nan
Episode: 3621/50100 (7.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8768s / 133.7028 s
agent0:                 episode reward: -0.1835,                 loss: 0.4151
agent1:                 episode reward: 0.1835,                 loss: nan
Episode: 3641/50100 (7.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8850s / 134.5878 s
agent0:                 episode reward: -0.4174,                 loss: 0.4129
agent1:                 episode reward: 0.4174,                 loss: nan
Episode: 3661/50100 (7.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8736s / 135.4614 s
agent0:                 episode reward: 0.0432,                 loss: 0.4098
agent1:                 episode reward: -0.0432,                 loss: nan
Episode: 3681/50100 (7.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8826s / 136.3440 s
agent0:                 episode reward: 0.4130,                 loss: 0.4095
agent1:                 episode reward: -0.4130,                 loss: nan
Episode: 3701/50100 (7.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8780s / 137.2221 s
agent0:                 episode reward: 0.0688,                 loss: 0.4081
agent1:                 episode reward: -0.0688,                 loss: nan
Episode: 3721/50100 (7.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0110s / 138.2331 s
agent0:                 episode reward: -0.0041,                 loss: 0.4082
agent1:                 episode reward: 0.0041,                 loss: 0.3706
Score delta: 2.230387171207416, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3497_0.
Episode: 3741/50100 (7.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7974s / 139.0305 s
agent0:                 episode reward: 0.2171,                 loss: nan
agent1:                 episode reward: -0.2171,                 loss: 0.3685
Episode: 3761/50100 (7.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8099s / 139.8404 s
agent0:                 episode reward: 0.2010,                 loss: nan
agent1:                 episode reward: -0.2010,                 loss: 0.3663
Episode: 3781/50100 (7.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0024s / 140.8428 s
agent0:                 episode reward: -0.2929,                 loss: 0.3999
agent1:                 episode reward: 0.2929,                 loss: 0.3638
Score delta: 2.485791918399633, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3566_1.
Episode: 3801/50100 (7.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8968s / 141.7396 s
agent0:                 episode reward: -0.0566,                 loss: 0.4072
agent1:                 episode reward: 0.0566,                 loss: nan
Episode: 3821/50100 (7.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0298s / 142.7694 s
agent0:                 episode reward: -0.2183,                 loss: 0.4167
agent1:                 episode reward: 0.2183,                 loss: 0.3711
Score delta: 2.6007823274472557, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3594_0.
Episode: 3841/50100 (7.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8003s / 143.5697 s
agent0:                 episode reward: -0.3361,                 loss: nan
agent1:                 episode reward: 0.3361,                 loss: 0.3676
Episode: 3861/50100 (7.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0248s / 144.5945 s
agent0:                 episode reward: -0.6548,                 loss: nan
agent1:                 episode reward: 0.6548,                 loss: 0.3656
Score delta: 2.088423525281637, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3649_1.
Episode: 3881/50100 (7.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8906s / 145.4851 s
agent0:                 episode reward: 0.4983,                 loss: 0.4027
agent1:                 episode reward: -0.4983,                 loss: nan
Episode: 3901/50100 (7.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0339s / 146.5190 s
agent0:                 episode reward: 0.1802,                 loss: 0.4095
agent1:                 episode reward: -0.1802,                 loss: 0.3720
Score delta: 2.1009117224815226, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3670_0.
Episode: 3921/50100 (7.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8110s / 147.3301 s
agent0:                 episode reward: -0.0355,                 loss: nan
agent1:                 episode reward: 0.0355,                 loss: 0.3750
Episode: 3941/50100 (7.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8100s / 148.1401 s
agent0:                 episode reward: 0.0730,                 loss: nan
agent1:                 episode reward: -0.0730,                 loss: 0.3671
Episode: 3961/50100 (7.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0917s / 149.2317 s
agent0:                 episode reward: -0.3491,                 loss: 0.4293
agent1:                 episode reward: 0.3491,                 loss: 0.3649
Score delta: 2.042099945194838, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/3736_1.
Episode: 3981/50100 (7.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8699s / 150.1016 s
agent0:                 episode reward: -0.5190,                 loss: 0.4175
agent1:                 episode reward: 0.5190,                 loss: nan
Episode: 4001/50100 (7.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8824s / 150.9840 s
agent0:                 episode reward: -0.0970,                 loss: 0.4123
agent1:                 episode reward: 0.0970,                 loss: nan
Episode: 4021/50100 (8.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9089s / 151.8929 s
agent0:                 episode reward: 0.0761,                 loss: 0.4130
agent1:                 episode reward: -0.0761,                 loss: nan
Episode: 4041/50100 (8.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9049s / 152.7978 s
agent0:                 episode reward: -0.1450,                 loss: 0.4094
agent1:                 episode reward: 0.1450,                 loss: nan
Episode: 4061/50100 (8.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8851s / 153.6829 s
agent0:                 episode reward: -0.5132,                 loss: 0.4086
agent1:                 episode reward: 0.5132,                 loss: nan
Episode: 4081/50100 (8.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8970s / 154.5799 s
agent0:                 episode reward: -0.4644,                 loss: 0.4088
agent1:                 episode reward: 0.4644,                 loss: nan
Episode: 4101/50100 (8.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8883s / 155.4682 s
agent0:                 episode reward: -0.4144,                 loss: 0.3690
agent1:                 episode reward: 0.4144,                 loss: nan
Episode: 4121/50100 (8.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8751s / 156.3432 s
agent0:                 episode reward: -0.4937,                 loss: 0.3576
agent1:                 episode reward: 0.4937,                 loss: nan
Episode: 4141/50100 (8.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8642s / 157.2074 s
agent0:                 episode reward: -0.4684,                 loss: 0.3532
agent1:                 episode reward: 0.4684,                 loss: nan
Episode: 4161/50100 (8.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8815s / 158.0889 s
agent0:                 episode reward: 0.2520,                 loss: 0.3533
agent1:                 episode reward: -0.2520,                 loss: nan
Episode: 4181/50100 (8.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8976s / 158.9865 s
agent0:                 episode reward: -0.4346,                 loss: 0.3544
agent1:                 episode reward: 0.4346,                 loss: nan
Episode: 4201/50100 (8.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9139s / 159.9004 s
agent0:                 episode reward: -0.4029,                 loss: 0.3517
agent1:                 episode reward: 0.4029,                 loss: nan
Episode: 4221/50100 (8.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8902s / 160.7906 s
agent0:                 episode reward: -0.4749,                 loss: 0.3485
agent1:                 episode reward: 0.4749,                 loss: nan
Episode: 4241/50100 (8.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9186s / 161.7091 s
agent0:                 episode reward: 0.1412,                 loss: 0.3498
agent1:                 episode reward: -0.1412,                 loss: nan
Episode: 4261/50100 (8.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9083s / 162.6174 s
agent0:                 episode reward: -0.4351,                 loss: 0.3732
agent1:                 episode reward: 0.4351,                 loss: nan
Episode: 4281/50100 (8.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8961s / 163.5135 s
agent0:                 episode reward: -0.2075,                 loss: 0.3890
agent1:                 episode reward: 0.2075,                 loss: nan
Episode: 4301/50100 (8.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9393s / 164.4528 s
agent0:                 episode reward: 0.6037,                 loss: 0.3848
agent1:                 episode reward: -0.6037,                 loss: nan
Episode: 4321/50100 (8.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9019s / 165.3547 s
agent0:                 episode reward: -0.4428,                 loss: 0.3854
agent1:                 episode reward: 0.4428,                 loss: nan
Episode: 4341/50100 (8.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8930s / 166.2477 s
agent0:                 episode reward: 0.0844,                 loss: 0.3853
agent1:                 episode reward: -0.0844,                 loss: nan
Episode: 4361/50100 (8.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8985s / 167.1462 s
agent0:                 episode reward: 0.0677,                 loss: 0.3850
agent1:                 episode reward: -0.0677,                 loss: nan
Episode: 4381/50100 (8.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1223s / 168.2685 s
agent0:                 episode reward: 0.5100,                 loss: 0.3855
agent1:                 episode reward: -0.5100,                 loss: 0.3650
Score delta: 2.1347558252016383, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/4165_0.
Episode: 4401/50100 (8.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8075s / 169.0760 s
agent0:                 episode reward: -0.3010,                 loss: nan
agent1:                 episode reward: 0.3010,                 loss: 0.3650
Episode: 4421/50100 (8.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8151s / 169.8912 s
agent0:                 episode reward: -0.3488,                 loss: nan
agent1:                 episode reward: 0.3488,                 loss: 0.3635
Episode: 4441/50100 (8.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8199s / 170.7110 s
agent0:                 episode reward: -0.6054,                 loss: nan
agent1:                 episode reward: 0.6054,                 loss: 0.3621
Episode: 4461/50100 (8.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1614s / 171.8724 s
agent0:                 episode reward: -0.5253,                 loss: 0.4077
agent1:                 episode reward: 0.5253,                 loss: 0.3578
Score delta: 2.1135839774479406, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/4231_1.
Episode: 4481/50100 (8.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9053s / 172.7777 s
agent0:                 episode reward: 0.4515,                 loss: 0.4073
agent1:                 episode reward: -0.4515,                 loss: nan
Episode: 4501/50100 (8.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9161s / 173.6938 s
agent0:                 episode reward: 0.1191,                 loss: 0.4264
agent1:                 episode reward: -0.1191,                 loss: nan
Episode: 4521/50100 (9.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9118s / 174.6056 s
agent0:                 episode reward: -0.2124,                 loss: 0.4241
agent1:                 episode reward: 0.2124,                 loss: nan
Episode: 4541/50100 (9.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8914s / 175.4970 s
agent0:                 episode reward: -0.2152,                 loss: 0.4244
agent1:                 episode reward: 0.2152,                 loss: nan
Episode: 4561/50100 (9.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9001s / 176.3971 s
agent0:                 episode reward: 0.2466,                 loss: 0.4242
agent1:                 episode reward: -0.2466,                 loss: nan
Episode: 4581/50100 (9.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9006s / 177.2978 s
agent0:                 episode reward: -0.2898,                 loss: 0.4235
agent1:                 episode reward: 0.2898,                 loss: nan
Episode: 4601/50100 (9.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9131s / 178.2109 s
agent0:                 episode reward: -0.3159,                 loss: 0.4256
agent1:                 episode reward: 0.3159,                 loss: nan
Episode: 4621/50100 (9.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9109s / 179.1218 s
agent0:                 episode reward: -0.2596,                 loss: 0.4245
agent1:                 episode reward: 0.2596,                 loss: nan
Episode: 4641/50100 (9.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9003s / 180.0221 s
agent0:                 episode reward: 0.1362,                 loss: 0.4257
agent1:                 episode reward: -0.1362,                 loss: nan
Episode: 4661/50100 (9.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9197s / 180.9418 s
agent0:                 episode reward: -0.2278,                 loss: 0.4321
agent1:                 episode reward: 0.2278,                 loss: nan
Episode: 4681/50100 (9.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9478s / 181.8896 s
agent0:                 episode reward: 0.1540,                 loss: 0.4345
agent1:                 episode reward: -0.1540,                 loss: nan
Episode: 4701/50100 (9.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9205s / 182.8101 s
agent0:                 episode reward: -0.2348,                 loss: 0.4339
agent1:                 episode reward: 0.2348,                 loss: nan
Episode: 4721/50100 (9.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9134s / 183.7234 s
agent0:                 episode reward: 0.2294,                 loss: 0.4335
agent1:                 episode reward: -0.2294,                 loss: nan
Episode: 4741/50100 (9.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8967s / 184.6201 s
agent0:                 episode reward: -0.1904,                 loss: 0.4344
agent1:                 episode reward: 0.1904,                 loss: nan
Episode: 4761/50100 (9.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9214s / 185.5415 s
agent0:                 episode reward: 0.2197,                 loss: 0.4337
agent1:                 episode reward: -0.2197,                 loss: nan
Episode: 4781/50100 (9.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9135s / 186.4551 s
agent0:                 episode reward: -0.1511,                 loss: 0.4332
agent1:                 episode reward: 0.1511,                 loss: nan
Episode: 4801/50100 (9.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9099s / 187.3650 s
agent0:                 episode reward: -0.4423,                 loss: 0.4325
agent1:                 episode reward: 0.4423,                 loss: nan
Episode: 4821/50100 (9.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9206s / 188.2855 s
agent0:                 episode reward: 0.1850,                 loss: 0.4345
agent1:                 episode reward: -0.1850,                 loss: nan
Episode: 4841/50100 (9.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9297s / 189.2152 s
agent0:                 episode reward: 0.2024,                 loss: 0.4345
agent1:                 episode reward: -0.2024,                 loss: nan
Episode: 4861/50100 (9.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9289s / 190.1441 s
agent0:                 episode reward: -0.3167,                 loss: 0.4356
agent1:                 episode reward: 0.3167,                 loss: nan
Episode: 4881/50100 (9.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9381s / 191.0822 s
agent0:                 episode reward: -0.1282,                 loss: 0.4358
agent1:                 episode reward: 0.1282,                 loss: nan
Episode: 4901/50100 (9.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1335s / 192.2157 s
agent0:                 episode reward: 1.0035,                 loss: 0.4362
agent1:                 episode reward: -1.0035,                 loss: 0.3715
Score delta: 2.064202912720258, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/4678_0.
Episode: 4921/50100 (9.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8310s / 193.0467 s
agent0:                 episode reward: 0.5805,                 loss: nan
agent1:                 episode reward: -0.5805,                 loss: 0.3695
Episode: 4941/50100 (9.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8206s / 193.8673 s
agent0:                 episode reward: -0.3216,                 loss: nan
agent1:                 episode reward: 0.3216,                 loss: 0.3668
Episode: 4961/50100 (9.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8123s / 194.6795 s
agent0:                 episode reward: -0.4623,                 loss: nan
agent1:                 episode reward: 0.4623,                 loss: 0.3761
Episode: 4981/50100 (9.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8193s / 195.4988 s
agent0:                 episode reward: -0.1456,                 loss: nan
agent1:                 episode reward: 0.1456,                 loss: 0.3782
Episode: 5001/50100 (9.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8080s / 196.3068 s
agent0:                 episode reward: -0.1426,                 loss: nan
agent1:                 episode reward: 0.1426,                 loss: 0.3693
Episode: 5021/50100 (10.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8122s / 197.1190 s
agent0:                 episode reward: -0.0730,                 loss: nan
agent1:                 episode reward: 0.0730,                 loss: 0.3653
Episode: 5041/50100 (10.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8231s / 197.9421 s
agent0:                 episode reward: -0.1316,                 loss: nan
agent1:                 episode reward: 0.1316,                 loss: 0.3615
Episode: 5061/50100 (10.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8341s / 198.7762 s
agent0:                 episode reward: 0.2936,                 loss: nan
agent1:                 episode reward: -0.2936,                 loss: 0.3596
Episode: 5081/50100 (10.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8389s / 199.6151 s
agent0:                 episode reward: 0.0897,                 loss: nan
agent1:                 episode reward: -0.0897,                 loss: 0.3594
Episode: 5101/50100 (10.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8351s / 200.4502 s
agent0:                 episode reward: 0.2081,                 loss: nan
agent1:                 episode reward: -0.2081,                 loss: 0.3572
Episode: 5121/50100 (10.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1637s / 201.6139 s
agent0:                 episode reward: -0.3214,                 loss: 0.4215
agent1:                 episode reward: 0.3214,                 loss: 0.3567
Score delta: 2.1606414242402976, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/4896_1.
Episode: 5141/50100 (10.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9315s / 202.5454 s
agent0:                 episode reward: 0.1022,                 loss: 0.4214
agent1:                 episode reward: -0.1022,                 loss: nan
Episode: 5161/50100 (10.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9276s / 203.4729 s
agent0:                 episode reward: 0.6678,                 loss: 0.4210
agent1:                 episode reward: -0.6678,                 loss: nan
Episode: 5181/50100 (10.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1826s / 204.6555 s
agent0:                 episode reward: 0.5214,                 loss: 0.4205
agent1:                 episode reward: -0.5214,                 loss: 0.3642
Score delta: 2.036213332576584, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/4967_0.
Episode: 5201/50100 (10.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8383s / 205.4938 s
agent0:                 episode reward: -0.3211,                 loss: nan
agent1:                 episode reward: 0.3211,                 loss: 0.3783
Episode: 5221/50100 (10.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8412s / 206.3350 s
agent0:                 episode reward: 0.4400,                 loss: nan
agent1:                 episode reward: -0.4400,                 loss: 0.3729
Episode: 5241/50100 (10.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8510s / 207.1860 s
agent0:                 episode reward: 0.2598,                 loss: nan
agent1:                 episode reward: -0.2598,                 loss: 0.3682
Episode: 5261/50100 (10.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1797s / 208.3658 s
agent0:                 episode reward: -0.9085,                 loss: 0.3901
agent1:                 episode reward: 0.9085,                 loss: 0.3680
Score delta: 2.4221036902881763, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/5038_1.
Episode: 5281/50100 (10.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9383s / 209.3040 s
agent0:                 episode reward: 0.2634,                 loss: 0.4125
agent1:                 episode reward: -0.2634,                 loss: nan
Episode: 5301/50100 (10.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9591s / 210.2631 s
agent0:                 episode reward: -0.9076,                 loss: 0.4293
agent1:                 episode reward: 0.9076,                 loss: nan
Episode: 5321/50100 (10.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9377s / 211.2009 s
agent0:                 episode reward: -0.2244,                 loss: 0.4269
agent1:                 episode reward: 0.2244,                 loss: nan
Episode: 5341/50100 (10.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9401s / 212.1409 s
agent0:                 episode reward: 0.3054,                 loss: 0.4271
agent1:                 episode reward: -0.3054,                 loss: nan
Episode: 5361/50100 (10.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9112s / 213.0522 s
agent0:                 episode reward: -0.1298,                 loss: 0.4285
agent1:                 episode reward: 0.1298,                 loss: nan
Episode: 5381/50100 (10.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9249s / 213.9770 s
agent0:                 episode reward: -0.5712,                 loss: 0.4269
agent1:                 episode reward: 0.5712,                 loss: nan
Episode: 5401/50100 (10.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9328s / 214.9099 s
agent0:                 episode reward: 0.4001,                 loss: 0.4272
agent1:                 episode reward: -0.4001,                 loss: nan
Episode: 5421/50100 (10.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9360s / 215.8459 s
agent0:                 episode reward: 0.3093,                 loss: 0.4276
agent1:                 episode reward: -0.3093,                 loss: nan
Episode: 5441/50100 (10.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9454s / 216.7913 s
agent0:                 episode reward: -0.0420,                 loss: 0.4288
agent1:                 episode reward: 0.0420,                 loss: nan
Episode: 5461/50100 (10.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9412s / 217.7325 s
agent0:                 episode reward: -0.2687,                 loss: 0.4385
agent1:                 episode reward: 0.2687,                 loss: nan
Episode: 5481/50100 (10.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9350s / 218.6675 s
agent0:                 episode reward: -0.1157,                 loss: 0.4379
agent1:                 episode reward: 0.1157,                 loss: nan
Episode: 5501/50100 (10.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9631s / 219.6307 s
agent0:                 episode reward: 0.2327,                 loss: 0.4354
agent1:                 episode reward: -0.2327,                 loss: nan
Episode: 5521/50100 (11.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9457s / 220.5764 s
agent0:                 episode reward: 0.3585,                 loss: 0.4363
agent1:                 episode reward: -0.3585,                 loss: nan
Episode: 5541/50100 (11.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9427s / 221.5190 s
agent0:                 episode reward: -0.0465,                 loss: 0.4358
agent1:                 episode reward: 0.0465,                 loss: nan
Episode: 5561/50100 (11.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9542s / 222.4732 s
agent0:                 episode reward: -0.1022,                 loss: 0.4360
agent1:                 episode reward: 0.1022,                 loss: nan
Episode: 5581/50100 (11.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9513s / 223.4245 s
agent0:                 episode reward: -0.2895,                 loss: 0.4364
agent1:                 episode reward: 0.2895,                 loss: nan
Episode: 5601/50100 (11.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9508s / 224.3753 s
agent0:                 episode reward: 0.3281,                 loss: 0.4362
agent1:                 episode reward: -0.3281,                 loss: nan
Episode: 5621/50100 (11.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1515s / 225.5268 s
agent0:                 episode reward: -0.4066,                 loss: 0.4357
agent1:                 episode reward: 0.4066,                 loss: 0.3616
Score delta: 2.0207482145492452, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/5391_0.
Episode: 5641/50100 (11.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8531s / 226.3799 s
agent0:                 episode reward: -0.2163,                 loss: nan
agent1:                 episode reward: 0.2163,                 loss: 0.3575
Episode: 5661/50100 (11.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8454s / 227.2253 s
agent0:                 episode reward: -0.1277,                 loss: nan
agent1:                 episode reward: 0.1277,                 loss: 0.3533
Episode: 5681/50100 (11.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2222s / 228.4475 s
agent0:                 episode reward: -0.1094,                 loss: 0.4299
agent1:                 episode reward: 0.1094,                 loss: 0.3497
Score delta: 2.032251071723695, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/5459_1.
Episode: 5701/50100 (11.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9622s / 229.4097 s
agent0:                 episode reward: 0.0041,                 loss: 0.4306
agent1:                 episode reward: -0.0041,                 loss: nan
Episode: 5721/50100 (11.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2067s / 230.6164 s
agent0:                 episode reward: 0.3893,                 loss: 0.4285
agent1:                 episode reward: -0.3893,                 loss: 0.3588
Score delta: 2.1149135724724752, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/5497_0.
Episode: 5741/50100 (11.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2268s / 231.8432 s
agent0:                 episode reward: -0.5309,                 loss: 0.4128
agent1:                 episode reward: 0.5309,                 loss: 0.3613
Score delta: 2.000933368682753, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/5519_1.
Episode: 5761/50100 (11.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9292s / 232.7725 s
agent0:                 episode reward: 0.1882,                 loss: 0.4135
agent1:                 episode reward: -0.1882,                 loss: nan
Episode: 5781/50100 (11.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9540s / 233.7265 s
agent0:                 episode reward: 0.3883,                 loss: 0.4127
agent1:                 episode reward: -0.3883,                 loss: nan
Episode: 5801/50100 (11.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9527s / 234.6792 s
agent0:                 episode reward: 0.2269,                 loss: 0.4147
agent1:                 episode reward: -0.2269,                 loss: nan
Episode: 5821/50100 (11.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9611s / 235.6403 s
agent0:                 episode reward: -0.2744,                 loss: 0.4143
agent1:                 episode reward: 0.2744,                 loss: nan
Episode: 5841/50100 (11.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9491s / 236.5894 s
agent0:                 episode reward: 0.3873,                 loss: 0.4134
agent1:                 episode reward: -0.3873,                 loss: nan
Episode: 5861/50100 (11.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9642s / 237.5535 s
agent0:                 episode reward: 0.1456,                 loss: 0.4146
agent1:                 episode reward: -0.1456,                 loss: nan
Episode: 5881/50100 (11.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2138s / 238.7673 s
agent0:                 episode reward: 0.2312,                 loss: 0.4409
agent1:                 episode reward: -0.2312,                 loss: 0.3724
Score delta: 2.031027570797767, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/5653_0.
Episode: 5901/50100 (11.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8592s / 239.6265 s
agent0:                 episode reward: -0.1179,                 loss: nan
agent1:                 episode reward: 0.1179,                 loss: 0.3883
Episode: 5921/50100 (11.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8575s / 240.4840 s
agent0:                 episode reward: -0.4953,                 loss: nan
agent1:                 episode reward: 0.4953,                 loss: 0.3783
Episode: 5941/50100 (11.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8567s / 241.3407 s
agent0:                 episode reward: -0.4526,                 loss: nan
agent1:                 episode reward: 0.4526,                 loss: 0.3723
Episode: 5961/50100 (11.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8454s / 242.1861 s
agent0:                 episode reward: -0.0962,                 loss: nan
agent1:                 episode reward: 0.0962,                 loss: 0.3704
Episode: 5981/50100 (11.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8470s / 243.0330 s
agent0:                 episode reward: -0.3172,                 loss: nan
agent1:                 episode reward: 0.3172,                 loss: 0.3665
Episode: 6001/50100 (11.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8523s / 243.8853 s
agent0:                 episode reward: -0.5415,                 loss: nan
agent1:                 episode reward: 0.5415,                 loss: 0.3661
Episode: 6021/50100 (12.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8651s / 244.7504 s
agent0:                 episode reward: 0.0606,                 loss: nan
agent1:                 episode reward: -0.0606,                 loss: 0.3633
Episode: 6041/50100 (12.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8524s / 245.6028 s
agent0:                 episode reward: 0.2027,                 loss: nan
agent1:                 episode reward: -0.2027,                 loss: 0.3626
Episode: 6061/50100 (12.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8517s / 246.4545 s
agent0:                 episode reward: -0.1770,                 loss: nan
agent1:                 episode reward: 0.1770,                 loss: 0.3594
Episode: 6081/50100 (12.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8541s / 247.3085 s
agent0:                 episode reward: -0.3376,                 loss: nan
agent1:                 episode reward: 0.3376,                 loss: 0.3417
Episode: 6101/50100 (12.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8475s / 248.1560 s
agent0:                 episode reward: 0.3700,                 loss: nan
agent1:                 episode reward: -0.3700,                 loss: 0.3404
Episode: 6121/50100 (12.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8613s / 249.0173 s
agent0:                 episode reward: -0.1995,                 loss: nan
agent1:                 episode reward: 0.1995,                 loss: 0.3404
Episode: 6141/50100 (12.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8659s / 249.8832 s
agent0:                 episode reward: -0.0956,                 loss: nan
agent1:                 episode reward: 0.0956,                 loss: 0.3376
Episode: 6161/50100 (12.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8545s / 250.7377 s
agent0:                 episode reward: 0.0645,                 loss: nan
agent1:                 episode reward: -0.0645,                 loss: 0.3370
Episode: 6181/50100 (12.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8822s / 251.6199 s
agent0:                 episode reward: 0.2010,                 loss: nan
agent1:                 episode reward: -0.2010,                 loss: 0.3378
Episode: 6201/50100 (12.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8615s / 252.4814 s
agent0:                 episode reward: -0.4220,                 loss: nan
agent1:                 episode reward: 0.4220,                 loss: 0.3388
Episode: 6221/50100 (12.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8543s / 253.3357 s
agent0:                 episode reward: -0.4526,                 loss: nan
agent1:                 episode reward: 0.4526,                 loss: 0.3527
Episode: 6241/50100 (12.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8699s / 254.2055 s
agent0:                 episode reward: -0.4393,                 loss: nan
agent1:                 episode reward: 0.4393,                 loss: 0.3864
Episode: 6261/50100 (12.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8644s / 255.0699 s
agent0:                 episode reward: -0.0903,                 loss: nan
agent1:                 episode reward: 0.0903,                 loss: 0.3838
Episode: 6281/50100 (12.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8652s / 255.9351 s
agent0:                 episode reward: -0.6147,                 loss: nan
agent1:                 episode reward: 0.6147,                 loss: 0.3831
Episode: 6301/50100 (12.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2808s / 257.2159 s
agent0:                 episode reward: -0.3487,                 loss: 0.3825
agent1:                 episode reward: 0.3487,                 loss: 0.3819
Score delta: 2.1310885935290087, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/6074_1.
Episode: 6321/50100 (12.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9926s / 258.2084 s
agent0:                 episode reward: 0.4418,                 loss: 0.3816
agent1:                 episode reward: -0.4418,                 loss: nan
Episode: 6341/50100 (12.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9755s / 259.1839 s
agent0:                 episode reward: 0.0841,                 loss: 0.3821
agent1:                 episode reward: -0.0841,                 loss: nan
Episode: 6361/50100 (12.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9874s / 260.1714 s
agent0:                 episode reward: -0.1319,                 loss: 0.3798
agent1:                 episode reward: 0.1319,                 loss: nan
Episode: 6381/50100 (12.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9848s / 261.1562 s
agent0:                 episode reward: -0.6219,                 loss: 0.3803
agent1:                 episode reward: 0.6219,                 loss: nan
Episode: 6401/50100 (12.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9657s / 262.1218 s
agent0:                 episode reward: 0.0039,                 loss: 0.3798
agent1:                 episode reward: -0.0039,                 loss: nan
Episode: 6421/50100 (12.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9603s / 263.0822 s
agent0:                 episode reward: -0.3592,                 loss: 0.3788
agent1:                 episode reward: 0.3592,                 loss: nan
Episode: 6441/50100 (12.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9689s / 264.0511 s
agent0:                 episode reward: 0.1347,                 loss: 0.3791
agent1:                 episode reward: -0.1347,                 loss: nan
Episode: 6461/50100 (12.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2936s / 265.3447 s
agent0:                 episode reward: 0.3769,                 loss: 0.3954
agent1:                 episode reward: -0.3769,                 loss: 0.3641
Score delta: 2.1806243680800534, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/6247_0.
Episode: 6481/50100 (12.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8727s / 266.2175 s
agent0:                 episode reward: 0.3237,                 loss: nan
agent1:                 episode reward: -0.3237,                 loss: 0.3642
Episode: 6501/50100 (12.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8510s / 267.0684 s
agent0:                 episode reward: 0.4341,                 loss: nan
agent1:                 episode reward: -0.4341,                 loss: 0.3585
Episode: 6521/50100 (13.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1770s / 268.2454 s
agent0:                 episode reward: -0.5348,                 loss: 0.3847
agent1:                 episode reward: 0.5348,                 loss: 0.3576
Score delta: 2.0607000923397187, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/6304_1.
Episode: 6541/50100 (13.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9572s / 269.2026 s
agent0:                 episode reward: -0.0092,                 loss: 0.3825
agent1:                 episode reward: 0.0092,                 loss: nan
Episode: 6561/50100 (13.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9625s / 270.1650 s
agent0:                 episode reward: 0.0492,                 loss: 0.3817
agent1:                 episode reward: -0.0492,                 loss: nan
Episode: 6581/50100 (13.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9744s / 271.1395 s
agent0:                 episode reward: -0.3667,                 loss: 0.3778
agent1:                 episode reward: 0.3667,                 loss: nan
Episode: 6601/50100 (13.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9918s / 272.1313 s
agent0:                 episode reward: 0.0126,                 loss: 0.3781
agent1:                 episode reward: -0.0126,                 loss: nan
Episode: 6621/50100 (13.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9690s / 273.1003 s
agent0:                 episode reward: -0.1529,                 loss: 0.3788
agent1:                 episode reward: 0.1529,                 loss: nan
Episode: 6641/50100 (13.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9773s / 274.0776 s
agent0:                 episode reward: -0.1364,                 loss: 0.3793
agent1:                 episode reward: 0.1364,                 loss: nan
Episode: 6661/50100 (13.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2597s / 275.3373 s
agent0:                 episode reward: 0.0209,                 loss: 0.3792
agent1:                 episode reward: -0.0209,                 loss: 0.3518
Score delta: 2.0852657853090646, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/6435_0.
Episode: 6681/50100 (13.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8586s / 276.1960 s
agent0:                 episode reward: 0.1425,                 loss: nan
agent1:                 episode reward: -0.1425,                 loss: 0.3494
Episode: 6701/50100 (13.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8631s / 277.0591 s
agent0:                 episode reward: -0.1696,                 loss: nan
agent1:                 episode reward: 0.1696,                 loss: 0.3425
Episode: 6721/50100 (13.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8597s / 277.9188 s
agent0:                 episode reward: 0.0448,                 loss: nan
agent1:                 episode reward: -0.0448,                 loss: 0.3267
Episode: 6741/50100 (13.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8660s / 278.7848 s
agent0:                 episode reward: 0.3274,                 loss: nan
agent1:                 episode reward: -0.3274,                 loss: 0.3230
Episode: 6761/50100 (13.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2544s / 280.0392 s
agent0:                 episode reward: -0.5388,                 loss: 0.3960
agent1:                 episode reward: 0.5388,                 loss: 0.3220
Score delta: 2.5967122958285302, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/6545_1.
Episode: 6781/50100 (13.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9659s / 281.0051 s
agent0:                 episode reward: -0.2661,                 loss: 0.3943
agent1:                 episode reward: 0.2661,                 loss: nan
Episode: 6801/50100 (13.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9607s / 281.9658 s
agent0:                 episode reward: -0.0755,                 loss: 0.4373
agent1:                 episode reward: 0.0755,                 loss: nan
Episode: 6821/50100 (13.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9693s / 282.9350 s
agent0:                 episode reward: -0.0737,                 loss: 0.4342
agent1:                 episode reward: 0.0737,                 loss: nan
Episode: 6841/50100 (13.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9871s / 283.9221 s
agent0:                 episode reward: -0.1308,                 loss: 0.4360
agent1:                 episode reward: 0.1308,                 loss: nan
Episode: 6861/50100 (13.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9846s / 284.9067 s
agent0:                 episode reward: -0.5589,                 loss: 0.4351
agent1:                 episode reward: 0.5589,                 loss: nan
Episode: 6881/50100 (13.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3289s / 286.2356 s
agent0:                 episode reward: 0.5083,                 loss: 0.4354
agent1:                 episode reward: -0.5083,                 loss: 0.3236
Score delta: 2.659141838804775, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/6662_0.
Episode: 6901/50100 (13.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8821s / 287.1176 s
agent0:                 episode reward: -0.0525,                 loss: nan
agent1:                 episode reward: 0.0525,                 loss: 0.3190
Episode: 6921/50100 (13.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3352s / 288.4529 s
agent0:                 episode reward: -0.3110,                 loss: 0.4301
agent1:                 episode reward: 0.3110,                 loss: 0.3218
Score delta: 2.0757622710649017, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/6698_1.
Episode: 6941/50100 (13.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0142s / 289.4671 s
agent0:                 episode reward: 0.1219,                 loss: 0.4333
agent1:                 episode reward: -0.1219,                 loss: nan
Episode: 6961/50100 (13.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9976s / 290.4647 s
agent0:                 episode reward: -0.0298,                 loss: 0.4310
agent1:                 episode reward: 0.0298,                 loss: nan
Episode: 6981/50100 (13.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0106s / 291.4753 s
agent0:                 episode reward: -0.1842,                 loss: 0.4303
agent1:                 episode reward: 0.1842,                 loss: nan
Episode: 7001/50100 (13.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9977s / 292.4729 s
agent0:                 episode reward: -0.2052,                 loss: 0.4404
agent1:                 episode reward: 0.2052,                 loss: nan
Episode: 7021/50100 (14.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0106s / 293.4835 s
agent0:                 episode reward: -0.1006,                 loss: 0.4418
agent1:                 episode reward: 0.1006,                 loss: nan
Episode: 7041/50100 (14.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9960s / 294.4796 s
agent0:                 episode reward: -0.3098,                 loss: 0.4414
agent1:                 episode reward: 0.3098,                 loss: nan
Episode: 7061/50100 (14.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9844s / 295.4639 s
agent0:                 episode reward: -0.3653,                 loss: 0.4415
agent1:                 episode reward: 0.3653,                 loss: nan
Episode: 7081/50100 (14.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9818s / 296.4457 s
agent0:                 episode reward: 0.1571,                 loss: 0.4405
agent1:                 episode reward: -0.1571,                 loss: nan
Episode: 7101/50100 (14.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9928s / 297.4385 s
agent0:                 episode reward: 0.3336,                 loss: 0.4406
agent1:                 episode reward: -0.3336,                 loss: nan
Episode: 7121/50100 (14.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3038s / 298.7423 s
agent0:                 episode reward: 0.2804,                 loss: 0.4410
agent1:                 episode reward: -0.2804,                 loss: 0.3653
Score delta: 2.017741196432929, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/6892_0.
Episode: 7141/50100 (14.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8921s / 299.6344 s
agent0:                 episode reward: 0.0651,                 loss: nan
agent1:                 episode reward: -0.0651,                 loss: 0.3596
Episode: 7161/50100 (14.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8980s / 300.5324 s
agent0:                 episode reward: -0.1766,                 loss: nan
agent1:                 episode reward: 0.1766,                 loss: 0.3596
Episode: 7181/50100 (14.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9238s / 301.4562 s
agent0:                 episode reward: -0.0368,                 loss: nan
agent1:                 episode reward: 0.0368,                 loss: 0.3730
Episode: 7201/50100 (14.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9107s / 302.3669 s
agent0:                 episode reward: -0.4526,                 loss: nan
agent1:                 episode reward: 0.4526,                 loss: 0.3632
Episode: 7221/50100 (14.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9052s / 303.2721 s
agent0:                 episode reward: -0.4371,                 loss: nan
agent1:                 episode reward: 0.4371,                 loss: 0.3613
Episode: 7241/50100 (14.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4272s / 304.6993 s
agent0:                 episode reward: -0.2296,                 loss: 0.4211
agent1:                 episode reward: 0.2296,                 loss: 0.3567
Score delta: 2.2322619615160915, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/7010_1.
Episode: 7261/50100 (14.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0080s / 305.7073 s
agent0:                 episode reward: 0.2015,                 loss: 0.4179
agent1:                 episode reward: -0.2015,                 loss: nan
Episode: 7281/50100 (14.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0063s / 306.7135 s
agent0:                 episode reward: 0.0868,                 loss: 0.4029
agent1:                 episode reward: -0.0868,                 loss: nan
Episode: 7301/50100 (14.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0288s / 307.7423 s
agent0:                 episode reward: 0.3790,                 loss: 0.3886
agent1:                 episode reward: -0.3790,                 loss: nan
Episode: 7321/50100 (14.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0189s / 308.7612 s
agent0:                 episode reward: 0.0924,                 loss: 0.3849
agent1:                 episode reward: -0.0924,                 loss: nan
Episode: 7341/50100 (14.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0219s / 309.7831 s
agent0:                 episode reward: 0.2136,                 loss: 0.3853
agent1:                 episode reward: -0.2136,                 loss: nan
Episode: 7361/50100 (14.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0254s / 310.8084 s
agent0:                 episode reward: -0.5200,                 loss: 0.3870
agent1:                 episode reward: 0.5200,                 loss: nan
Episode: 7381/50100 (14.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9935s / 311.8019 s
agent0:                 episode reward: 0.4229,                 loss: 0.3862
agent1:                 episode reward: -0.4229,                 loss: nan
Episode: 7401/50100 (14.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0087s / 312.8107 s
agent0:                 episode reward: -0.8447,                 loss: 0.3837
agent1:                 episode reward: 0.8447,                 loss: nan
Episode: 7421/50100 (14.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0076s / 313.8182 s
agent0:                 episode reward: -0.5680,                 loss: 0.3842
agent1:                 episode reward: 0.5680,                 loss: nan
Episode: 7441/50100 (14.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0127s / 314.8309 s
agent0:                 episode reward: -0.1092,                 loss: 0.3891
agent1:                 episode reward: 0.1092,                 loss: nan
Episode: 7461/50100 (14.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0081s / 315.8389 s
agent0:                 episode reward: 0.0110,                 loss: 0.4051
agent1:                 episode reward: -0.0110,                 loss: nan
Episode: 7481/50100 (14.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0153s / 316.8542 s
agent0:                 episode reward: -0.2421,                 loss: 0.4040
agent1:                 episode reward: 0.2421,                 loss: nan
Episode: 7501/50100 (14.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0229s / 317.8772 s
agent0:                 episode reward: -0.1296,                 loss: 0.4046
agent1:                 episode reward: 0.1296,                 loss: nan
Episode: 7521/50100 (15.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0420s / 318.9192 s
agent0:                 episode reward: 0.3266,                 loss: 0.4041
agent1:                 episode reward: -0.3266,                 loss: nan
Episode: 7541/50100 (15.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0035s / 319.9227 s
agent0:                 episode reward: -0.9292,                 loss: 0.4022
agent1:                 episode reward: 0.9292,                 loss: nan
Episode: 7561/50100 (15.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0266s / 320.9493 s
agent0:                 episode reward: 0.3563,                 loss: 0.4036
agent1:                 episode reward: -0.3563,                 loss: nan
Episode: 7581/50100 (15.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0295s / 321.9787 s
agent0:                 episode reward: -0.2565,                 loss: 0.4038
agent1:                 episode reward: 0.2565,                 loss: nan
Episode: 7601/50100 (15.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0204s / 322.9991 s
agent0:                 episode reward: -0.3122,                 loss: 0.4026
agent1:                 episode reward: 0.3122,                 loss: nan
Episode: 7621/50100 (15.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0226s / 324.0218 s
agent0:                 episode reward: -0.0325,                 loss: 0.4358
agent1:                 episode reward: 0.0325,                 loss: nan
Episode: 7641/50100 (15.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0571s / 325.0789 s
agent0:                 episode reward: -0.1573,                 loss: 0.4352
agent1:                 episode reward: 0.1573,                 loss: nan
Episode: 7661/50100 (15.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0178s / 326.0967 s
agent0:                 episode reward: 0.2762,                 loss: 0.4354
agent1:                 episode reward: -0.2762,                 loss: nan
Episode: 7681/50100 (15.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0288s / 327.1255 s
agent0:                 episode reward: 0.2192,                 loss: 0.4353
agent1:                 episode reward: -0.2192,                 loss: nan
Episode: 7701/50100 (15.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0228s / 328.1482 s
agent0:                 episode reward: -0.3745,                 loss: 0.4343
agent1:                 episode reward: 0.3745,                 loss: nan
Episode: 7721/50100 (15.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0204s / 329.1686 s
agent0:                 episode reward: 0.0658,                 loss: 0.4362
agent1:                 episode reward: -0.0658,                 loss: nan
Episode: 7741/50100 (15.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0385s / 330.2071 s
agent0:                 episode reward: -0.2058,                 loss: 0.4365
agent1:                 episode reward: 0.2058,                 loss: nan
Episode: 7761/50100 (15.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0443s / 331.2514 s
agent0:                 episode reward: 0.5325,                 loss: 0.4333
agent1:                 episode reward: -0.5325,                 loss: nan
Episode: 7781/50100 (15.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0546s / 332.3060 s
agent0:                 episode reward: 0.4010,                 loss: 0.4409
agent1:                 episode reward: -0.4010,                 loss: nan
Episode: 7801/50100 (15.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0216s / 333.3276 s
agent0:                 episode reward: -0.2345,                 loss: 0.4414
agent1:                 episode reward: 0.2345,                 loss: nan
Episode: 7821/50100 (15.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0286s / 334.3562 s
agent0:                 episode reward: 0.3939,                 loss: 0.4413
agent1:                 episode reward: -0.3939,                 loss: nan
Episode: 7841/50100 (15.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0388s / 335.3950 s
agent0:                 episode reward: 0.4625,                 loss: 0.4415
agent1:                 episode reward: -0.4625,                 loss: nan
Episode: 7861/50100 (15.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0416s / 336.4366 s
agent0:                 episode reward: 0.1488,                 loss: 0.4410
agent1:                 episode reward: -0.1488,                 loss: nan
Episode: 7881/50100 (15.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0251s / 337.4617 s
agent0:                 episode reward: -0.2955,                 loss: 0.4425
agent1:                 episode reward: 0.2955,                 loss: nan
Episode: 7901/50100 (15.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0390s / 338.5007 s
agent0:                 episode reward: -0.1393,                 loss: 0.4415
agent1:                 episode reward: 0.1393,                 loss: nan
Episode: 7921/50100 (15.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0209s / 339.5216 s
agent0:                 episode reward: 0.0596,                 loss: 0.4408
agent1:                 episode reward: -0.0596,                 loss: nan
Episode: 7941/50100 (15.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0293s / 340.5510 s
agent0:                 episode reward: -0.0942,                 loss: 0.4440
agent1:                 episode reward: 0.0942,                 loss: nan
Episode: 7961/50100 (15.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0256s / 341.5766 s
agent0:                 episode reward: 0.7407,                 loss: 0.4462
agent1:                 episode reward: -0.7407,                 loss: nan
Episode: 7981/50100 (15.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0223s / 342.5989 s
agent0:                 episode reward: -0.1063,                 loss: 0.4458
agent1:                 episode reward: 0.1063,                 loss: nan
Episode: 8001/50100 (15.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0436s / 343.6425 s
agent0:                 episode reward: 0.0025,                 loss: 0.4461
agent1:                 episode reward: -0.0025,                 loss: nan
Episode: 8021/50100 (16.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0339s / 344.6764 s
agent0:                 episode reward: -0.2949,                 loss: 0.4441
agent1:                 episode reward: 0.2949,                 loss: nan
Episode: 8041/50100 (16.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0129s / 345.6893 s
agent0:                 episode reward: 0.1363,                 loss: 0.4444
agent1:                 episode reward: -0.1363,                 loss: nan
Episode: 8061/50100 (16.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0380s / 346.7272 s
agent0:                 episode reward: -0.2793,                 loss: 0.4450
agent1:                 episode reward: 0.2793,                 loss: nan
Episode: 8081/50100 (16.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0550s / 347.7823 s
agent0:                 episode reward: 0.3157,                 loss: 0.4450
agent1:                 episode reward: -0.3157,                 loss: nan
Episode: 8101/50100 (16.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0630s / 348.8453 s
agent0:                 episode reward: -0.0667,                 loss: 0.4456
agent1:                 episode reward: 0.0667,                 loss: nan
Episode: 8121/50100 (16.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0725s / 349.9178 s
agent0:                 episode reward: 0.3544,                 loss: 0.4436
agent1:                 episode reward: -0.3544,                 loss: nan
Episode: 8141/50100 (16.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0611s / 350.9789 s
agent0:                 episode reward: -0.0638,                 loss: 0.4423
agent1:                 episode reward: 0.0638,                 loss: nan
Episode: 8161/50100 (16.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0722s / 352.0511 s
agent0:                 episode reward: 0.2035,                 loss: 0.4433
agent1:                 episode reward: -0.2035,                 loss: nan
Episode: 8181/50100 (16.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0790s / 353.1301 s
agent0:                 episode reward: -0.1586,                 loss: 0.4444
agent1:                 episode reward: 0.1586,                 loss: nan
Episode: 8201/50100 (16.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1001s / 354.2302 s
agent0:                 episode reward: 0.0228,                 loss: 0.4429
agent1:                 episode reward: -0.0228,                 loss: nan
Episode: 8221/50100 (16.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0651s / 355.2953 s
agent0:                 episode reward: 0.3130,                 loss: 0.4432
agent1:                 episode reward: -0.3130,                 loss: nan
Episode: 8241/50100 (16.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0687s / 356.3640 s
agent0:                 episode reward: 0.1035,                 loss: 0.4426
agent1:                 episode reward: -0.1035,                 loss: nan
Episode: 8261/50100 (16.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0884s / 357.4524 s
agent0:                 episode reward: 0.0705,                 loss: 0.4424
agent1:                 episode reward: -0.0705,                 loss: nan
Episode: 8281/50100 (16.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0829s / 358.5352 s
agent0:                 episode reward: -0.1996,                 loss: 0.4374
agent1:                 episode reward: 0.1996,                 loss: nan
Episode: 8301/50100 (16.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0783s / 359.6136 s
agent0:                 episode reward: 0.1502,                 loss: 0.4343
agent1:                 episode reward: -0.1502,                 loss: nan
Episode: 8321/50100 (16.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0927s / 360.7063 s
agent0:                 episode reward: -0.0881,                 loss: 0.4310
agent1:                 episode reward: 0.0881,                 loss: nan
Episode: 8341/50100 (16.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4889s / 362.1952 s
agent0:                 episode reward: 0.3211,                 loss: 0.4311
agent1:                 episode reward: -0.3211,                 loss: 0.3639
Score delta: 2.0659351545647846, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/8125_0.
Episode: 8361/50100 (16.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8952s / 363.0904 s
agent0:                 episode reward: -0.2312,                 loss: nan
agent1:                 episode reward: 0.2312,                 loss: 0.3636
Episode: 8381/50100 (16.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8824s / 363.9729 s
agent0:                 episode reward: 0.3712,                 loss: nan
agent1:                 episode reward: -0.3712,                 loss: 0.3605
Episode: 8401/50100 (16.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8902s / 364.8630 s
agent0:                 episode reward: 0.2194,                 loss: nan
agent1:                 episode reward: -0.2194,                 loss: 0.3584
Episode: 8421/50100 (16.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8981s / 365.7611 s
agent0:                 episode reward: 0.1488,                 loss: nan
agent1:                 episode reward: -0.1488,                 loss: 0.3574
Episode: 8441/50100 (16.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8684s / 366.6295 s
agent0:                 episode reward: -0.1402,                 loss: nan
agent1:                 episode reward: 0.1402,                 loss: 0.3544
Episode: 8461/50100 (16.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3160s / 367.9455 s
agent0:                 episode reward: -0.6483,                 loss: 0.4410
agent1:                 episode reward: 0.6483,                 loss: 0.3615
Score delta: 2.045452886725754, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/8248_1.
Episode: 8481/50100 (16.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0716s / 369.0171 s
agent0:                 episode reward: -0.5790,                 loss: 0.4365
agent1:                 episode reward: 0.5790,                 loss: nan
Episode: 8501/50100 (16.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0847s / 370.1018 s
agent0:                 episode reward: 0.3724,                 loss: 0.4345
agent1:                 episode reward: -0.3724,                 loss: nan
Episode: 8521/50100 (17.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0948s / 371.1967 s
agent0:                 episode reward: -0.1054,                 loss: 0.4342
agent1:                 episode reward: 0.1054,                 loss: nan
Episode: 8541/50100 (17.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0718s / 372.2684 s
agent0:                 episode reward: 0.1663,                 loss: 0.4352
agent1:                 episode reward: -0.1663,                 loss: nan
Episode: 8561/50100 (17.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0677s / 373.3361 s
agent0:                 episode reward: -0.1014,                 loss: 0.4348
agent1:                 episode reward: 0.1014,                 loss: nan
Episode: 8581/50100 (17.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0908s / 374.4269 s
agent0:                 episode reward: 0.1536,                 loss: 0.4468
agent1:                 episode reward: -0.1536,                 loss: nan
Episode: 8601/50100 (17.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0818s / 375.5087 s
agent0:                 episode reward: 0.0937,                 loss: 0.4456
agent1:                 episode reward: -0.0937,                 loss: nan
Episode: 8621/50100 (17.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0675s / 376.5762 s
agent0:                 episode reward: 0.2165,                 loss: 0.4449
agent1:                 episode reward: -0.2165,                 loss: nan
Episode: 8641/50100 (17.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0914s / 377.6676 s
agent0:                 episode reward: 0.0442,                 loss: 0.4442
agent1:                 episode reward: -0.0442,                 loss: nan
Episode: 8661/50100 (17.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0789s / 378.7466 s
agent0:                 episode reward: 0.0791,                 loss: 0.4447
agent1:                 episode reward: -0.0791,                 loss: nan
Episode: 8681/50100 (17.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0715s / 379.8181 s
agent0:                 episode reward: 0.0553,                 loss: 0.4441
agent1:                 episode reward: -0.0553,                 loss: nan
Episode: 8701/50100 (17.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0865s / 380.9046 s
agent0:                 episode reward: 0.2702,                 loss: 0.4449
agent1:                 episode reward: -0.2702,                 loss: nan
Episode: 8721/50100 (17.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0783s / 381.9829 s
agent0:                 episode reward: -0.0872,                 loss: 0.4437
agent1:                 episode reward: 0.0872,                 loss: nan
Episode: 8741/50100 (17.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0861s / 383.0690 s
agent0:                 episode reward: -0.1284,                 loss: 0.4438
agent1:                 episode reward: 0.1284,                 loss: nan
Episode: 8761/50100 (17.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0654s / 384.1344 s
agent0:                 episode reward: 0.5585,                 loss: 0.4441
agent1:                 episode reward: -0.5585,                 loss: nan
Episode: 8781/50100 (17.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0740s / 385.2085 s
agent0:                 episode reward: 0.0130,                 loss: 0.4438
agent1:                 episode reward: -0.0130,                 loss: nan
Episode: 8801/50100 (17.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1177s / 386.3262 s
agent0:                 episode reward: -0.0573,                 loss: 0.4426
agent1:                 episode reward: 0.0573,                 loss: nan
Episode: 8821/50100 (17.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0905s / 387.4167 s
agent0:                 episode reward: -0.1284,                 loss: 0.4440
agent1:                 episode reward: 0.1284,                 loss: nan
Episode: 8841/50100 (17.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0895s / 388.5062 s
agent0:                 episode reward: 0.0768,                 loss: 0.4432
agent1:                 episode reward: -0.0768,                 loss: nan
Episode: 8861/50100 (17.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1011s / 389.6073 s
agent0:                 episode reward: 0.0224,                 loss: 0.4421
agent1:                 episode reward: -0.0224,                 loss: nan
Episode: 8881/50100 (17.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0625s / 390.6699 s
agent0:                 episode reward: 0.0082,                 loss: 0.4420
agent1:                 episode reward: -0.0082,                 loss: nan
Episode: 8901/50100 (17.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0928s / 391.7626 s
agent0:                 episode reward: -0.4182,                 loss: 0.4406
agent1:                 episode reward: 0.4182,                 loss: nan
Episode: 8921/50100 (17.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1335s / 392.8962 s
agent0:                 episode reward: -0.4521,                 loss: 0.4361
agent1:                 episode reward: 0.4521,                 loss: nan
Episode: 8941/50100 (17.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1291s / 394.0252 s
agent0:                 episode reward: -0.1263,                 loss: 0.4367
agent1:                 episode reward: 0.1263,                 loss: nan
Episode: 8961/50100 (17.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1189s / 395.1441 s
agent0:                 episode reward: -0.0377,                 loss: 0.4367
agent1:                 episode reward: 0.0377,                 loss: nan
Episode: 8981/50100 (17.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1119s / 396.2561 s
agent0:                 episode reward: -0.1036,                 loss: 0.4357
agent1:                 episode reward: 0.1036,                 loss: nan
Episode: 9001/50100 (17.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5034s / 397.7595 s
agent0:                 episode reward: 0.5277,                 loss: 0.4362
agent1:                 episode reward: -0.5277,                 loss: 0.3030
Score delta: 2.303795762179779, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/8786_0.
Episode: 9021/50100 (18.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8684s / 398.6279 s
agent0:                 episode reward: -0.5552,                 loss: nan
agent1:                 episode reward: 0.5552,                 loss: 0.2987
Episode: 9041/50100 (18.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8878s / 399.5157 s
agent0:                 episode reward: -0.4385,                 loss: nan
agent1:                 episode reward: 0.4385,                 loss: 0.2984
Episode: 9061/50100 (18.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8854s / 400.4010 s
agent0:                 episode reward: -0.6043,                 loss: nan
agent1:                 episode reward: 0.6043,                 loss: 0.2996
Episode: 9081/50100 (18.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4199s / 401.8209 s
agent0:                 episode reward: -1.0065,                 loss: 0.3889
agent1:                 episode reward: 1.0065,                 loss: 0.3021
Score delta: 2.5814033725892616, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/8861_1.
Episode: 9101/50100 (18.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1118s / 402.9328 s
agent0:                 episode reward: 0.3871,                 loss: 0.3902
agent1:                 episode reward: -0.3871,                 loss: nan
Episode: 9121/50100 (18.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1477s / 404.0804 s
agent0:                 episode reward: 0.0543,                 loss: 0.3904
agent1:                 episode reward: -0.0543,                 loss: nan
Episode: 9141/50100 (18.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1137s / 405.1941 s
agent0:                 episode reward: -0.1175,                 loss: 0.4090
agent1:                 episode reward: 0.1175,                 loss: nan
Episode: 9161/50100 (18.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1150s / 406.3092 s
agent0:                 episode reward: -0.2981,                 loss: 0.4361
agent1:                 episode reward: 0.2981,                 loss: nan
Episode: 9181/50100 (18.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1583s / 407.4675 s
agent0:                 episode reward: 0.1504,                 loss: 0.4367
agent1:                 episode reward: -0.1504,                 loss: nan
Episode: 9201/50100 (18.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1152s / 408.5827 s
agent0:                 episode reward: 0.0663,                 loss: 0.4348
agent1:                 episode reward: -0.0663,                 loss: nan
Episode: 9221/50100 (18.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1213s / 409.7040 s
agent0:                 episode reward: -0.0078,                 loss: 0.4370
agent1:                 episode reward: 0.0078,                 loss: nan
Episode: 9241/50100 (18.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1316s / 410.8356 s
agent0:                 episode reward: -0.4303,                 loss: 0.4335
agent1:                 episode reward: 0.4303,                 loss: nan
Episode: 9261/50100 (18.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0919s / 411.9275 s
agent0:                 episode reward: 0.0101,                 loss: 0.4350
agent1:                 episode reward: -0.0101,                 loss: nan
Episode: 9281/50100 (18.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1030s / 413.0305 s
agent0:                 episode reward: -0.2414,                 loss: 0.4345
agent1:                 episode reward: 0.2414,                 loss: nan
Episode: 9301/50100 (18.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1126s / 414.1431 s
agent0:                 episode reward: 0.1046,                 loss: 0.4351
agent1:                 episode reward: -0.1046,                 loss: nan
Episode: 9321/50100 (18.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1188s / 415.2619 s
agent0:                 episode reward: -0.1025,                 loss: 0.4478
agent1:                 episode reward: 0.1025,                 loss: nan
Episode: 9341/50100 (18.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1142s / 416.3761 s
agent0:                 episode reward: -0.0991,                 loss: 0.4462
agent1:                 episode reward: 0.0991,                 loss: nan
Episode: 9361/50100 (18.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1175s / 417.4936 s
agent0:                 episode reward: 0.3345,                 loss: 0.4463
agent1:                 episode reward: -0.3345,                 loss: nan
Episode: 9381/50100 (18.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1139s / 418.6076 s
agent0:                 episode reward: 0.4311,                 loss: 0.4456
agent1:                 episode reward: -0.4311,                 loss: nan
Episode: 9401/50100 (18.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3352s / 419.9428 s
agent0:                 episode reward: -0.4493,                 loss: 0.4488
agent1:                 episode reward: 0.4493,                 loss: 0.2983
Score delta: 2.010113657175976, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/9170_0.
Episode: 9421/50100 (18.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8972s / 420.8400 s
agent0:                 episode reward: -0.4994,                 loss: nan
agent1:                 episode reward: 0.4994,                 loss: 0.2985
Episode: 9441/50100 (18.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8907s / 421.7306 s
agent0:                 episode reward: -0.0494,                 loss: nan
agent1:                 episode reward: 0.0494,                 loss: 0.2982
Episode: 9461/50100 (18.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8948s / 422.6255 s
agent0:                 episode reward: -0.3689,                 loss: nan
agent1:                 episode reward: 0.3689,                 loss: 0.2983
Episode: 9481/50100 (18.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9069s / 423.5323 s
agent0:                 episode reward: 0.0810,                 loss: nan
agent1:                 episode reward: -0.0810,                 loss: 0.3633
Episode: 9501/50100 (18.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5161s / 425.0484 s
agent0:                 episode reward: -0.4358,                 loss: 0.4278
agent1:                 episode reward: 0.4358,                 loss: 0.3602
Score delta: 2.069989985036668, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/9274_1.
Episode: 9521/50100 (19.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1604s / 426.2088 s
agent0:                 episode reward: 0.2061,                 loss: 0.4257
agent1:                 episode reward: -0.2061,                 loss: nan
Episode: 9541/50100 (19.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1570s / 427.3658 s
agent0:                 episode reward: 0.0871,                 loss: 0.4263
agent1:                 episode reward: -0.0871,                 loss: nan
Episode: 9561/50100 (19.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1681s / 428.5339 s
agent0:                 episode reward: 0.1254,                 loss: 0.4258
agent1:                 episode reward: -0.1254,                 loss: nan
Episode: 9581/50100 (19.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1085s / 429.6424 s
agent0:                 episode reward: -0.1823,                 loss: 0.4359
agent1:                 episode reward: 0.1823,                 loss: nan
Episode: 9601/50100 (19.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1373s / 430.7797 s
agent0:                 episode reward: 0.1799,                 loss: 0.4467
agent1:                 episode reward: -0.1799,                 loss: nan
Episode: 9621/50100 (19.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1666s / 431.9463 s
agent0:                 episode reward: -0.2194,                 loss: 0.4473
agent1:                 episode reward: 0.2194,                 loss: nan
Episode: 9641/50100 (19.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1437s / 433.0899 s
agent0:                 episode reward: -0.0113,                 loss: 0.4467
agent1:                 episode reward: 0.0113,                 loss: nan
Episode: 9661/50100 (19.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1685s / 434.2585 s
agent0:                 episode reward: 0.2683,                 loss: 0.4468
agent1:                 episode reward: -0.2683,                 loss: nan
Episode: 9681/50100 (19.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1620s / 435.4205 s
agent0:                 episode reward: 0.1983,                 loss: 0.4448
agent1:                 episode reward: -0.1983,                 loss: nan
Episode: 9701/50100 (19.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1426s / 436.5631 s
agent0:                 episode reward: 0.2191,                 loss: 0.4457
agent1:                 episode reward: -0.2191,                 loss: nan
Episode: 9721/50100 (19.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1464s / 437.7095 s
agent0:                 episode reward: 0.0246,                 loss: 0.4455
agent1:                 episode reward: -0.0246,                 loss: nan
Episode: 9741/50100 (19.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1365s / 438.8459 s
agent0:                 episode reward: -0.1764,                 loss: 0.4450
agent1:                 episode reward: 0.1764,                 loss: nan
Episode: 9761/50100 (19.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1308s / 439.9767 s
agent0:                 episode reward: 0.3987,                 loss: 0.4464
agent1:                 episode reward: -0.3987,                 loss: nan
Episode: 9781/50100 (19.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1436s / 441.1203 s
agent0:                 episode reward: -0.0443,                 loss: 0.4472
agent1:                 episode reward: 0.0443,                 loss: nan
Episode: 9801/50100 (19.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1335s / 442.2538 s
agent0:                 episode reward: 0.5415,                 loss: 0.4464
agent1:                 episode reward: -0.5415,                 loss: nan
Episode: 9821/50100 (19.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1499s / 443.4038 s
agent0:                 episode reward: -0.3498,                 loss: 0.4463
agent1:                 episode reward: 0.3498,                 loss: nan
Episode: 9841/50100 (19.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1507s / 444.5545 s
agent0:                 episode reward: -0.4918,                 loss: 0.4456
agent1:                 episode reward: 0.4918,                 loss: nan
Episode: 9861/50100 (19.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1239s / 445.6784 s
agent0:                 episode reward: -0.0438,                 loss: 0.4446
agent1:                 episode reward: 0.0438,                 loss: nan
Episode: 9881/50100 (19.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1232s / 446.8016 s
agent0:                 episode reward: 0.0662,                 loss: 0.4452
agent1:                 episode reward: -0.0662,                 loss: nan
Episode: 9901/50100 (19.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4428s / 448.2444 s
agent0:                 episode reward: 0.1550,                 loss: 0.4478
agent1:                 episode reward: -0.1550,                 loss: 0.3731
Score delta: 2.308337039079625, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/9675_0.
Episode: 9921/50100 (19.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8906s / 449.1350 s
agent0:                 episode reward: -0.4569,                 loss: nan
agent1:                 episode reward: 0.4569,                 loss: 0.3687
Episode: 9941/50100 (19.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4766s / 450.6116 s
agent0:                 episode reward: -0.3940,                 loss: 0.3915
agent1:                 episode reward: 0.3940,                 loss: 0.3682
Score delta: 2.078000098272364, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/9718_1.
Episode: 9961/50100 (19.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1354s / 451.7470 s
agent0:                 episode reward: 0.1066,                 loss: 0.4123
agent1:                 episode reward: -0.1066,                 loss: nan
Episode: 9981/50100 (19.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1357s / 452.8827 s
agent0:                 episode reward: -0.0636,                 loss: 0.4238
agent1:                 episode reward: 0.0636,                 loss: nan
Episode: 10001/50100 (19.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1486s / 454.0313 s
agent0:                 episode reward: -0.4944,                 loss: 0.4218
agent1:                 episode reward: 0.4944,                 loss: nan
Episode: 10021/50100 (20.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5419s / 455.5732 s
agent0:                 episode reward: 0.6753,                 loss: 0.4220
agent1:                 episode reward: -0.6753,                 loss: 0.3555
Score delta: 2.013717626341159, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/9805_0.
Episode: 10041/50100 (20.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9146s / 456.4878 s
agent0:                 episode reward: 0.9338,                 loss: nan
agent1:                 episode reward: -0.9338,                 loss: 0.3549
Episode: 10061/50100 (20.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8999s / 457.3876 s
agent0:                 episode reward: -0.0433,                 loss: nan
agent1:                 episode reward: 0.0433,                 loss: 0.3556
Episode: 10081/50100 (20.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9120s / 458.2997 s
agent0:                 episode reward: -0.3674,                 loss: nan
agent1:                 episode reward: 0.3674,                 loss: 0.3526
Episode: 10101/50100 (20.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8980s / 459.1977 s
agent0:                 episode reward: -0.4347,                 loss: nan
agent1:                 episode reward: 0.4347,                 loss: 0.3521
Episode: 10121/50100 (20.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9055s / 460.1032 s
agent0:                 episode reward: -0.3363,                 loss: nan
agent1:                 episode reward: 0.3363,                 loss: 0.3524
Episode: 10141/50100 (20.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9161s / 461.0192 s
agent0:                 episode reward: -0.2946,                 loss: nan
agent1:                 episode reward: 0.2946,                 loss: 0.3277
Episode: 10161/50100 (20.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4672s / 462.4865 s
agent0:                 episode reward: -0.6331,                 loss: 0.4470
agent1:                 episode reward: 0.6331,                 loss: 0.3254
Score delta: 2.187882171814094, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/9943_1.
Episode: 10181/50100 (20.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1611s / 463.6476 s
agent0:                 episode reward: 0.2693,                 loss: 0.4449
agent1:                 episode reward: -0.2693,                 loss: nan
Episode: 10201/50100 (20.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1605s / 464.8081 s
agent0:                 episode reward: -0.1827,                 loss: 0.4429
agent1:                 episode reward: 0.1827,                 loss: nan
Episode: 10221/50100 (20.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1463s / 465.9544 s
agent0:                 episode reward: -0.1801,                 loss: 0.4441
agent1:                 episode reward: 0.1801,                 loss: nan
Episode: 10241/50100 (20.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1643s / 467.1187 s
agent0:                 episode reward: 0.2662,                 loss: 0.4437
agent1:                 episode reward: -0.2662,                 loss: nan
Episode: 10261/50100 (20.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1625s / 468.2811 s
agent0:                 episode reward: 0.3665,                 loss: 0.4437
agent1:                 episode reward: -0.3665,                 loss: nan
Episode: 10281/50100 (20.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1571s / 469.4383 s
agent0:                 episode reward: -0.0405,                 loss: 0.4454
agent1:                 episode reward: 0.0405,                 loss: nan
Episode: 10301/50100 (20.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1507s / 470.5889 s
agent0:                 episode reward: 0.3414,                 loss: 0.4452
agent1:                 episode reward: -0.3414,                 loss: nan
Episode: 10321/50100 (20.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1642s / 471.7532 s
agent0:                 episode reward: 0.2247,                 loss: 0.4436
agent1:                 episode reward: -0.2247,                 loss: nan
Episode: 10341/50100 (20.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1505s / 472.9037 s
agent0:                 episode reward: -0.0131,                 loss: 0.4436
agent1:                 episode reward: 0.0131,                 loss: nan
Episode: 10361/50100 (20.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1489s / 474.0525 s
agent0:                 episode reward: -0.0809,                 loss: 0.4447
agent1:                 episode reward: 0.0809,                 loss: nan
Episode: 10381/50100 (20.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1811s / 475.2336 s
agent0:                 episode reward: -0.2192,                 loss: 0.4437
agent1:                 episode reward: 0.2192,                 loss: nan
Episode: 10401/50100 (20.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1741s / 476.4078 s
agent0:                 episode reward: -0.1413,                 loss: 0.4434
agent1:                 episode reward: 0.1413,                 loss: nan
Episode: 10421/50100 (20.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1546s / 477.5624 s
agent0:                 episode reward: -0.2594,                 loss: 0.4433
agent1:                 episode reward: 0.2594,                 loss: nan
Episode: 10441/50100 (20.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5465s / 479.1089 s
agent0:                 episode reward: 0.5720,                 loss: 0.4347
agent1:                 episode reward: -0.5720,                 loss: 0.3630
Score delta: 2.23059886669882, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/10222_0.
Episode: 10461/50100 (20.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9179s / 480.0268 s
agent0:                 episode reward: 0.4138,                 loss: nan
agent1:                 episode reward: -0.4138,                 loss: 0.3621
Episode: 10481/50100 (20.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9104s / 480.9372 s
agent0:                 episode reward: 0.1956,                 loss: nan
agent1:                 episode reward: -0.1956,                 loss: 0.3602
Episode: 10501/50100 (20.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9114s / 481.8486 s
agent0:                 episode reward: 0.3139,                 loss: nan
agent1:                 episode reward: -0.3139,                 loss: 0.3579
Episode: 10521/50100 (21.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9001s / 482.7487 s
agent0:                 episode reward: 0.2328,                 loss: nan
agent1:                 episode reward: -0.2328,                 loss: 0.3567
Episode: 10541/50100 (21.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4737s / 484.2225 s
agent0:                 episode reward: -0.5715,                 loss: 0.4430
agent1:                 episode reward: 0.5715,                 loss: 0.3557
Score delta: 2.1387403207141906, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/10322_1.
Episode: 10561/50100 (21.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1620s / 485.3844 s
agent0:                 episode reward: -0.3182,                 loss: 0.4429
agent1:                 episode reward: 0.3182,                 loss: nan
Episode: 10581/50100 (21.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1650s / 486.5494 s
agent0:                 episode reward: 0.0355,                 loss: 0.4412
agent1:                 episode reward: -0.0355,                 loss: nan
Episode: 10601/50100 (21.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1636s / 487.7130 s
agent0:                 episode reward: -0.2288,                 loss: 0.4415
agent1:                 episode reward: 0.2288,                 loss: nan
Episode: 10621/50100 (21.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1670s / 488.8800 s
agent0:                 episode reward: 0.1679,                 loss: 0.4427
agent1:                 episode reward: -0.1679,                 loss: nan
Episode: 10641/50100 (21.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1703s / 490.0503 s
agent0:                 episode reward: 0.2201,                 loss: 0.4414
agent1:                 episode reward: -0.2201,                 loss: nan
Episode: 10661/50100 (21.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1626s / 491.2130 s
agent0:                 episode reward: -0.1850,                 loss: 0.4411
agent1:                 episode reward: 0.1850,                 loss: nan
Episode: 10681/50100 (21.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1713s / 492.3843 s
agent0:                 episode reward: 0.5055,                 loss: 0.4406
agent1:                 episode reward: -0.5055,                 loss: nan
Episode: 10701/50100 (21.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1746s / 493.5589 s
agent0:                 episode reward: 0.3871,                 loss: 0.4440
agent1:                 episode reward: -0.3871,                 loss: nan
Episode: 10721/50100 (21.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1640s / 494.7229 s
agent0:                 episode reward: 0.2923,                 loss: 0.4439
agent1:                 episode reward: -0.2923,                 loss: nan
Episode: 10741/50100 (21.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4726s / 496.1955 s
agent0:                 episode reward: 0.1179,                 loss: 0.4409
agent1:                 episode reward: -0.1179,                 loss: 0.3546
Score delta: 2.0203482551975602, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/10516_0.
Episode: 10761/50100 (21.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9288s / 497.1243 s
agent0:                 episode reward: -0.0516,                 loss: nan
agent1:                 episode reward: 0.0516,                 loss: 0.3603
Episode: 10781/50100 (21.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9381s / 498.0625 s
agent0:                 episode reward: -0.1296,                 loss: nan
agent1:                 episode reward: 0.1296,                 loss: 0.3389
Episode: 10801/50100 (21.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9382s / 499.0006 s
agent0:                 episode reward: -0.2723,                 loss: nan
agent1:                 episode reward: 0.2723,                 loss: 0.3342
Episode: 10821/50100 (21.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9260s / 499.9266 s
agent0:                 episode reward: -0.0999,                 loss: nan
agent1:                 episode reward: 0.0999,                 loss: 0.3335
Episode: 10841/50100 (21.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9241s / 500.8507 s
agent0:                 episode reward: 0.1395,                 loss: nan
agent1:                 episode reward: -0.1395,                 loss: 0.3342
Episode: 10861/50100 (21.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9274s / 501.7781 s
agent0:                 episode reward: -0.2459,                 loss: nan
agent1:                 episode reward: 0.2459,                 loss: 0.3314
Episode: 10881/50100 (21.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9249s / 502.7030 s
agent0:                 episode reward: -0.1635,                 loss: nan
agent1:                 episode reward: 0.1635,                 loss: 0.3318
Episode: 10901/50100 (21.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9254s / 503.6283 s
agent0:                 episode reward: -0.2310,                 loss: nan
agent1:                 episode reward: 0.2310,                 loss: 0.3306
Episode: 10921/50100 (21.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9451s / 504.5734 s
agent0:                 episode reward: 0.0380,                 loss: nan
agent1:                 episode reward: -0.0380,                 loss: 0.3321
Episode: 10941/50100 (21.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5867s / 506.1601 s
agent0:                 episode reward: -0.4589,                 loss: 0.4010
agent1:                 episode reward: 0.4589,                 loss: 0.3853
Score delta: 2.048714322937327, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/10717_1.
Episode: 10961/50100 (21.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1797s / 507.3398 s
agent0:                 episode reward: -0.4512,                 loss: 0.4008
agent1:                 episode reward: 0.4512,                 loss: nan
Episode: 10981/50100 (21.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1742s / 508.5140 s
agent0:                 episode reward: -0.3665,                 loss: 0.3991
agent1:                 episode reward: 0.3665,                 loss: nan
Episode: 11001/50100 (21.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1786s / 509.6926 s
agent0:                 episode reward: -0.0519,                 loss: 0.3974
agent1:                 episode reward: 0.0519,                 loss: nan
Episode: 11021/50100 (21.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1905s / 510.8831 s
agent0:                 episode reward: 0.0898,                 loss: 0.3997
agent1:                 episode reward: -0.0898,                 loss: nan
Episode: 11041/50100 (22.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1842s / 512.0673 s
agent0:                 episode reward: -0.5185,                 loss: 0.3999
agent1:                 episode reward: 0.5185,                 loss: nan
Episode: 11061/50100 (22.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1745s / 513.2418 s
agent0:                 episode reward: 0.0720,                 loss: 0.4169
agent1:                 episode reward: -0.0720,                 loss: nan
Episode: 11081/50100 (22.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1865s / 514.4283 s
agent0:                 episode reward: 0.1037,                 loss: 0.4440
agent1:                 episode reward: -0.1037,                 loss: nan
Episode: 11101/50100 (22.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1866s / 515.6149 s
agent0:                 episode reward: -0.2822,                 loss: 0.4440
agent1:                 episode reward: 0.2822,                 loss: nan
Episode: 11121/50100 (22.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1828s / 516.7977 s
agent0:                 episode reward: -0.1728,                 loss: 0.4432
agent1:                 episode reward: 0.1728,                 loss: nan
Episode: 11141/50100 (22.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1935s / 517.9912 s
agent0:                 episode reward: 0.2331,                 loss: 0.4423
agent1:                 episode reward: -0.2331,                 loss: nan
Episode: 11161/50100 (22.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1904s / 519.1815 s
agent0:                 episode reward: -0.2180,                 loss: 0.4414
agent1:                 episode reward: 0.2180,                 loss: nan
Episode: 11181/50100 (22.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1838s / 520.3653 s
agent0:                 episode reward: 1.0004,                 loss: 0.4423
agent1:                 episode reward: -1.0004,                 loss: nan
Episode: 11201/50100 (22.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4419s / 521.8072 s
agent0:                 episode reward: 0.1724,                 loss: 0.4431
agent1:                 episode reward: -0.1724,                 loss: 0.3860
Score delta: 2.135074321743871, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/10970_0.
Episode: 11221/50100 (22.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9183s / 522.7255 s
agent0:                 episode reward: 0.0340,                 loss: nan
agent1:                 episode reward: -0.0340,                 loss: 0.3863
Episode: 11241/50100 (22.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9499s / 523.6754 s
agent0:                 episode reward: -0.2092,                 loss: nan
agent1:                 episode reward: 0.2092,                 loss: 0.3849
Episode: 11261/50100 (22.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9379s / 524.6133 s
agent0:                 episode reward: -0.5089,                 loss: nan
agent1:                 episode reward: 0.5089,                 loss: 0.3837
Episode: 11281/50100 (22.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9262s / 525.5395 s
agent0:                 episode reward: -0.0854,                 loss: nan
agent1:                 episode reward: 0.0854,                 loss: 0.3842
Episode: 11301/50100 (22.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6538s / 527.1933 s
agent0:                 episode reward: -0.3066,                 loss: 0.4308
agent1:                 episode reward: 0.3066,                 loss: 0.3850
Score delta: 2.2390055540931386, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/11074_1.
Episode: 11321/50100 (22.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1931s / 528.3864 s
agent0:                 episode reward: -0.5217,                 loss: 0.4250
agent1:                 episode reward: 0.5217,                 loss: nan
Episode: 11341/50100 (22.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1874s / 529.5738 s
agent0:                 episode reward: 0.0473,                 loss: 0.4475
agent1:                 episode reward: -0.0473,                 loss: nan
Episode: 11361/50100 (22.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1945s / 530.7683 s
agent0:                 episode reward: -0.2283,                 loss: 0.4492
agent1:                 episode reward: 0.2283,                 loss: nan
Episode: 11381/50100 (22.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1888s / 531.9571 s
agent0:                 episode reward: 0.2025,                 loss: 0.4471
agent1:                 episode reward: -0.2025,                 loss: nan
Episode: 11401/50100 (22.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6526s / 533.6097 s
agent0:                 episode reward: 0.1880,                 loss: 0.4464
agent1:                 episode reward: -0.1880,                 loss: 0.3644
Score delta: 2.029281822290395, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/11185_0.
Episode: 11421/50100 (22.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9342s / 534.5439 s
agent0:                 episode reward: -0.6997,                 loss: nan
agent1:                 episode reward: 0.6997,                 loss: 0.3623
Episode: 11441/50100 (22.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9384s / 535.4823 s
agent0:                 episode reward: -0.6492,                 loss: nan
agent1:                 episode reward: 0.6492,                 loss: 0.3613
Episode: 11461/50100 (22.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4810s / 536.9633 s
agent0:                 episode reward: -0.3625,                 loss: 0.3950
agent1:                 episode reward: 0.3625,                 loss: 0.3573
Score delta: 2.219642624185684, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/11248_1.
Episode: 11481/50100 (22.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2077s / 538.1710 s
agent0:                 episode reward: -0.0352,                 loss: 0.4004
agent1:                 episode reward: 0.0352,                 loss: nan
Episode: 11501/50100 (22.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1998s / 539.3707 s
agent0:                 episode reward: 0.2932,                 loss: 0.3992
agent1:                 episode reward: -0.2932,                 loss: nan
Episode: 11521/50100 (22.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2229s / 540.5936 s
agent0:                 episode reward: -0.1697,                 loss: 0.4006
agent1:                 episode reward: 0.1697,                 loss: nan
Episode: 11541/50100 (23.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1859s / 541.7796 s
agent0:                 episode reward: 0.1849,                 loss: 0.3998
agent1:                 episode reward: -0.1849,                 loss: nan
Episode: 11561/50100 (23.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1938s / 542.9734 s
agent0:                 episode reward: 0.1000,                 loss: 0.4198
agent1:                 episode reward: -0.1000,                 loss: nan
Episode: 11581/50100 (23.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2096s / 544.1830 s
agent0:                 episode reward: -0.1679,                 loss: 0.4459
agent1:                 episode reward: 0.1679,                 loss: nan
Episode: 11601/50100 (23.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2285s / 545.4115 s
agent0:                 episode reward: -0.1718,                 loss: 0.4449
agent1:                 episode reward: 0.1718,                 loss: nan
Episode: 11621/50100 (23.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2131s / 546.6246 s
agent0:                 episode reward: -0.4346,                 loss: 0.4438
agent1:                 episode reward: 0.4346,                 loss: nan
Episode: 11641/50100 (23.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2002s / 547.8248 s
agent0:                 episode reward: 0.5431,                 loss: 0.4446
agent1:                 episode reward: -0.5431,                 loss: nan
Episode: 11661/50100 (23.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2004s / 549.0252 s
agent0:                 episode reward: 0.5009,                 loss: 0.4449
agent1:                 episode reward: -0.5009,                 loss: nan
Episode: 11681/50100 (23.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7640s / 550.7892 s
agent0:                 episode reward: 0.3542,                 loss: 0.4446
agent1:                 episode reward: -0.3542,                 loss: 0.3663
Score delta: 2.169676218743935, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/11467_0.
Episode: 11701/50100 (23.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9894s / 551.7786 s
agent0:                 episode reward: -0.5690,                 loss: nan
agent1:                 episode reward: 0.5690,                 loss: 0.3668
Episode: 11721/50100 (23.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0070s / 552.7856 s
agent0:                 episode reward: 0.4442,                 loss: nan
agent1:                 episode reward: -0.4442,                 loss: 0.3657
Episode: 11741/50100 (23.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0551s / 553.8408 s
agent0:                 episode reward: -0.1969,                 loss: nan
agent1:                 episode reward: 0.1969,                 loss: 0.3615
Episode: 11761/50100 (23.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1054s / 554.9462 s
agent0:                 episode reward: -0.0786,                 loss: nan
agent1:                 episode reward: 0.0786,                 loss: 0.3604
Episode: 11781/50100 (23.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0714s / 556.0176 s
agent0:                 episode reward: 0.2288,                 loss: nan
agent1:                 episode reward: -0.2288,                 loss: 0.3587
Episode: 11801/50100 (23.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0579s / 557.0754 s
agent0:                 episode reward: -0.3170,                 loss: nan
agent1:                 episode reward: 0.3170,                 loss: 0.3564
Episode: 11821/50100 (23.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0479s / 558.1234 s
agent0:                 episode reward: -0.0769,                 loss: nan
agent1:                 episode reward: 0.0769,                 loss: 0.3536
Episode: 11841/50100 (23.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0523s / 559.1757 s
agent0:                 episode reward: 0.5462,                 loss: nan
agent1:                 episode reward: -0.5462,                 loss: 0.3582
Episode: 11861/50100 (23.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0601s / 560.2358 s
agent0:                 episode reward: -0.1971,                 loss: nan
agent1:                 episode reward: 0.1971,                 loss: 0.3598
Episode: 11881/50100 (23.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0891s / 561.3249 s
agent0:                 episode reward: -0.3031,                 loss: nan
agent1:                 episode reward: 0.3031,                 loss: 0.3502
Episode: 11901/50100 (23.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0680s / 562.3929 s
agent0:                 episode reward: 0.0190,                 loss: nan
agent1:                 episode reward: -0.0190,                 loss: 0.3463
Episode: 11921/50100 (23.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8492s / 564.2421 s
agent0:                 episode reward: -0.4622,                 loss: 0.4246
agent1:                 episode reward: 0.4622,                 loss: 0.3444
Score delta: 2.0663005759925572, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/11707_1.
Episode: 11941/50100 (23.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3869s / 565.6290 s
agent0:                 episode reward: -0.5355,                 loss: 0.4252
agent1:                 episode reward: 0.5355,                 loss: nan
Episode: 11961/50100 (23.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3918s / 567.0208 s
agent0:                 episode reward: -0.0544,                 loss: 0.4217
agent1:                 episode reward: 0.0544,                 loss: nan
Episode: 11981/50100 (23.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3942s / 568.4151 s
agent0:                 episode reward: 0.4954,                 loss: 0.3992
agent1:                 episode reward: -0.4954,                 loss: nan
Episode: 12001/50100 (23.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8502s / 570.2652 s
agent0:                 episode reward: 0.2011,                 loss: 0.4009
agent1:                 episode reward: -0.2011,                 loss: 0.3725
Score delta: 2.1631061214075067, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/11770_0.
Episode: 12021/50100 (23.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0947s / 571.3599 s
agent0:                 episode reward: -0.2286,                 loss: nan
agent1:                 episode reward: 0.2286,                 loss: 0.3672
Episode: 12041/50100 (24.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0670s / 572.4269 s
agent0:                 episode reward: 0.0724,                 loss: nan
agent1:                 episode reward: -0.0724,                 loss: 0.3662
Episode: 12061/50100 (24.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0735s / 573.5005 s
agent0:                 episode reward: -0.0340,                 loss: nan
agent1:                 episode reward: 0.0340,                 loss: 0.3651
Episode: 12081/50100 (24.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0898s / 574.5902 s
agent0:                 episode reward: -0.1147,                 loss: nan
agent1:                 episode reward: 0.1147,                 loss: 0.3706
Episode: 12101/50100 (24.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9070s / 576.4972 s
agent0:                 episode reward: -0.4171,                 loss: 0.4466
agent1:                 episode reward: 0.4171,                 loss: 0.3615
Score delta: 2.0630998776827782, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/11883_1.
Episode: 12121/50100 (24.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3557s / 577.8529 s
agent0:                 episode reward: 0.3779,                 loss: 0.4469
agent1:                 episode reward: -0.3779,                 loss: nan
Episode: 12141/50100 (24.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3229s / 579.1759 s
agent0:                 episode reward: -0.2090,                 loss: 0.4447
agent1:                 episode reward: 0.2090,                 loss: nan
Episode: 12161/50100 (24.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3299s / 580.5057 s
agent0:                 episode reward: 0.3040,                 loss: 0.4464
agent1:                 episode reward: -0.3040,                 loss: nan
Episode: 12181/50100 (24.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3225s / 581.8282 s
agent0:                 episode reward: 0.4039,                 loss: 0.4457
agent1:                 episode reward: -0.4039,                 loss: nan
Episode: 12201/50100 (24.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3412s / 583.1694 s
agent0:                 episode reward: -0.0312,                 loss: 0.4446
agent1:                 episode reward: 0.0312,                 loss: nan
Episode: 12221/50100 (24.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3368s / 584.5062 s
agent0:                 episode reward: 0.0208,                 loss: 0.4469
agent1:                 episode reward: -0.0208,                 loss: nan
Episode: 12241/50100 (24.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3478s / 585.8540 s
agent0:                 episode reward: -0.0298,                 loss: 0.4453
agent1:                 episode reward: 0.0298,                 loss: nan
Episode: 12261/50100 (24.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3281s / 587.1821 s
agent0:                 episode reward: 0.3201,                 loss: 0.4452
agent1:                 episode reward: -0.3201,                 loss: nan
Episode: 12281/50100 (24.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3568s / 588.5389 s
agent0:                 episode reward: -0.8843,                 loss: 0.4460
agent1:                 episode reward: 0.8843,                 loss: nan
Episode: 12301/50100 (24.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3485s / 589.8874 s
agent0:                 episode reward: -0.3770,                 loss: 0.4466
agent1:                 episode reward: 0.3770,                 loss: nan
Episode: 12321/50100 (24.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3491s / 591.2365 s
agent0:                 episode reward: -0.3870,                 loss: 0.4462
agent1:                 episode reward: 0.3870,                 loss: nan
Episode: 12341/50100 (24.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3338s / 592.5703 s
agent0:                 episode reward: -0.2972,                 loss: 0.4468
agent1:                 episode reward: 0.2972,                 loss: nan
Episode: 12361/50100 (24.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3437s / 593.9140 s
agent0:                 episode reward: -0.0205,                 loss: 0.4450
agent1:                 episode reward: 0.0205,                 loss: nan
Episode: 12381/50100 (24.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3489s / 595.2628 s
agent0:                 episode reward: 0.2095,                 loss: 0.4459
agent1:                 episode reward: -0.2095,                 loss: nan
Episode: 12401/50100 (24.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3472s / 596.6101 s
agent0:                 episode reward: 0.3738,                 loss: 0.4461
agent1:                 episode reward: -0.3738,                 loss: nan
Episode: 12421/50100 (24.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3509s / 597.9609 s
agent0:                 episode reward: 0.1879,                 loss: 0.4369
agent1:                 episode reward: -0.1879,                 loss: nan
Episode: 12441/50100 (24.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3516s / 599.3125 s
agent0:                 episode reward: 0.1273,                 loss: 0.4324
agent1:                 episode reward: -0.1273,                 loss: nan
Episode: 12461/50100 (24.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3609s / 600.6734 s
agent0:                 episode reward: -0.0610,                 loss: 0.4320
agent1:                 episode reward: 0.0610,                 loss: nan
Episode: 12481/50100 (24.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3571s / 602.0305 s
agent0:                 episode reward: -0.2061,                 loss: 0.4334
agent1:                 episode reward: 0.2061,                 loss: nan
Episode: 12501/50100 (24.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3677s / 603.3982 s
agent0:                 episode reward: 0.2165,                 loss: 0.4334
agent1:                 episode reward: -0.2165,                 loss: nan
Episode: 12521/50100 (24.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3471s / 604.7453 s
agent0:                 episode reward: 0.1205,                 loss: 0.4315
agent1:                 episode reward: -0.1205,                 loss: nan
Episode: 12541/50100 (25.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3730s / 606.1184 s
agent0:                 episode reward: -0.1257,                 loss: 0.4320
agent1:                 episode reward: 0.1257,                 loss: nan
Episode: 12561/50100 (25.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3895s / 607.5079 s
agent0:                 episode reward: 0.0275,                 loss: 0.4320
agent1:                 episode reward: -0.0275,                 loss: nan
Episode: 12581/50100 (25.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3725s / 608.8804 s
agent0:                 episode reward: -0.2580,                 loss: 0.4273
agent1:                 episode reward: 0.2580,                 loss: nan
Episode: 12601/50100 (25.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3524s / 610.2328 s
agent0:                 episode reward: -0.3787,                 loss: 0.4195
agent1:                 episode reward: 0.3787,                 loss: nan
Episode: 12621/50100 (25.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3516s / 611.5844 s
agent0:                 episode reward: 0.0927,                 loss: 0.4221
agent1:                 episode reward: -0.0927,                 loss: nan
Episode: 12641/50100 (25.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3672s / 612.9516 s
agent0:                 episode reward: -0.3388,                 loss: 0.4227
agent1:                 episode reward: 0.3388,                 loss: nan
Episode: 12661/50100 (25.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3539s / 614.3056 s
agent0:                 episode reward: -0.0677,                 loss: 0.4222
agent1:                 episode reward: 0.0677,                 loss: nan
Episode: 12681/50100 (25.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3476s / 615.6532 s
agent0:                 episode reward: 0.4616,                 loss: 0.4204
agent1:                 episode reward: -0.4616,                 loss: nan
Episode: 12701/50100 (25.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3637s / 617.0169 s
agent0:                 episode reward: 0.4924,                 loss: 0.4193
agent1:                 episode reward: -0.4924,                 loss: nan
Episode: 12721/50100 (25.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3677s / 618.3846 s
agent0:                 episode reward: 0.4487,                 loss: 0.4202
agent1:                 episode reward: -0.4487,                 loss: nan
Episode: 12741/50100 (25.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3732s / 619.7578 s
agent0:                 episode reward: -0.4316,                 loss: 0.4207
agent1:                 episode reward: 0.4316,                 loss: nan
Episode: 12761/50100 (25.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3644s / 621.1222 s
agent0:                 episode reward: -0.0378,                 loss: 0.4183
agent1:                 episode reward: 0.0378,                 loss: nan
Episode: 12781/50100 (25.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3672s / 622.4894 s
agent0:                 episode reward: 0.3439,                 loss: 0.4177
agent1:                 episode reward: -0.3439,                 loss: nan
Episode: 12801/50100 (25.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3596s / 623.8490 s
agent0:                 episode reward: 0.0172,                 loss: 0.4196
agent1:                 episode reward: -0.0172,                 loss: nan
Episode: 12821/50100 (25.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3667s / 625.2157 s
agent0:                 episode reward: -0.4615,                 loss: 0.4202
agent1:                 episode reward: 0.4615,                 loss: nan
Episode: 12841/50100 (25.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3656s / 626.5813 s
agent0:                 episode reward: 0.4093,                 loss: 0.4184
agent1:                 episode reward: -0.4093,                 loss: nan
Episode: 12861/50100 (25.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3810s / 627.9622 s
agent0:                 episode reward: 0.1388,                 loss: 0.4178
agent1:                 episode reward: -0.1388,                 loss: nan
Episode: 12881/50100 (25.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8350s / 629.7972 s
agent0:                 episode reward: 0.8424,                 loss: 0.4184
agent1:                 episode reward: -0.8424,                 loss: 0.3627
Score delta: 2.0391098749470697, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/12653_0.
Episode: 12901/50100 (25.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0460s / 630.8432 s
agent0:                 episode reward: -0.0236,                 loss: nan
agent1:                 episode reward: 0.0236,                 loss: 0.3594
Episode: 12921/50100 (25.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0490s / 631.8922 s
agent0:                 episode reward: -0.1052,                 loss: nan
agent1:                 episode reward: 0.1052,                 loss: 0.3575
Episode: 12941/50100 (25.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0202s / 633.9124 s
agent0:                 episode reward: 0.1997,                 loss: 0.4460
agent1:                 episode reward: -0.1997,                 loss: 0.3589
Score delta: 2.0396466785354184, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/12715_1.
Episode: 12961/50100 (25.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3557s / 635.2682 s
agent0:                 episode reward: -0.1829,                 loss: 0.4426
agent1:                 episode reward: 0.1829,                 loss: nan
Episode: 12981/50100 (25.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3645s / 636.6326 s
agent0:                 episode reward: -0.1530,                 loss: 0.4471
agent1:                 episode reward: 0.1530,                 loss: nan
Episode: 13001/50100 (25.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4098s / 638.0424 s
agent0:                 episode reward: 0.0062,                 loss: 0.4483
agent1:                 episode reward: -0.0062,                 loss: nan
Episode: 13021/50100 (25.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3910s / 639.4334 s
agent0:                 episode reward: 0.0353,                 loss: 0.4481
agent1:                 episode reward: -0.0353,                 loss: nan
Episode: 13041/50100 (26.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4072s / 640.8407 s
agent0:                 episode reward: 0.1440,                 loss: 0.4489
agent1:                 episode reward: -0.1440,                 loss: nan
Episode: 13061/50100 (26.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4040s / 642.2446 s
agent0:                 episode reward: -0.1774,                 loss: 0.4479
agent1:                 episode reward: 0.1774,                 loss: nan
Episode: 13081/50100 (26.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4050s / 643.6496 s
agent0:                 episode reward: 0.1551,                 loss: 0.4479
agent1:                 episode reward: -0.1551,                 loss: nan
Episode: 13101/50100 (26.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1159s / 645.7655 s
agent0:                 episode reward: 0.6935,                 loss: 0.4479
agent1:                 episode reward: -0.6935,                 loss: 0.3556
Score delta: 2.0132516282767923, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/12885_0.
Episode: 13121/50100 (26.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0711s / 646.8366 s
agent0:                 episode reward: 0.0622,                 loss: nan
agent1:                 episode reward: -0.0622,                 loss: 0.3500
Episode: 13141/50100 (26.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0657s / 647.9022 s
agent0:                 episode reward: 0.1287,                 loss: nan
agent1:                 episode reward: -0.1287,                 loss: 0.3474
Episode: 13161/50100 (26.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0588s / 648.9611 s
agent0:                 episode reward: 0.1691,                 loss: nan
agent1:                 episode reward: -0.1691,                 loss: 0.3462
Episode: 13181/50100 (26.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0528s / 650.0139 s
agent0:                 episode reward: -0.4460,                 loss: nan
agent1:                 episode reward: 0.4460,                 loss: 0.3490
Episode: 13201/50100 (26.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0604s / 651.0743 s
agent0:                 episode reward: -0.0830,                 loss: nan
agent1:                 episode reward: 0.0830,                 loss: 0.3328
Episode: 13221/50100 (26.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0719s / 652.1463 s
agent0:                 episode reward: 0.5220,                 loss: nan
agent1:                 episode reward: -0.5220,                 loss: 0.3308
Episode: 13241/50100 (26.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0649s / 653.2112 s
agent0:                 episode reward: -0.3288,                 loss: nan
agent1:                 episode reward: 0.3288,                 loss: 0.3288
Episode: 13261/50100 (26.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0619s / 654.2731 s
agent0:                 episode reward: 0.5680,                 loss: nan
agent1:                 episode reward: -0.5680,                 loss: 0.3282
Episode: 13281/50100 (26.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0617s / 655.3348 s
agent0:                 episode reward: -0.1375,                 loss: nan
agent1:                 episode reward: 0.1375,                 loss: 0.3281
Episode: 13301/50100 (26.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0394s / 656.3743 s
agent0:                 episode reward: -0.2852,                 loss: nan
agent1:                 episode reward: 0.2852,                 loss: 0.3280
Episode: 13321/50100 (26.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0305s / 657.4047 s
agent0:                 episode reward: -0.1500,                 loss: nan
agent1:                 episode reward: 0.1500,                 loss: 0.3279
Episode: 13341/50100 (26.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8411s / 659.2458 s
agent0:                 episode reward: -0.7579,                 loss: 0.4285
agent1:                 episode reward: 0.7579,                 loss: 0.3286
Score delta: 2.05786794980402, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/13125_1.
Episode: 13361/50100 (26.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3469s / 660.5927 s
agent0:                 episode reward: -0.0455,                 loss: 0.4256
agent1:                 episode reward: 0.0455,                 loss: nan
Episode: 13381/50100 (26.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3714s / 661.9641 s
agent0:                 episode reward: -0.0282,                 loss: 0.4162
agent1:                 episode reward: 0.0282,                 loss: nan
Episode: 13401/50100 (26.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3617s / 663.3258 s
agent0:                 episode reward: 0.0237,                 loss: 0.3924
agent1:                 episode reward: -0.0237,                 loss: nan
Episode: 13421/50100 (26.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3490s / 664.6749 s
agent0:                 episode reward: -0.3761,                 loss: 0.3913
agent1:                 episode reward: 0.3761,                 loss: nan
Episode: 13441/50100 (26.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3905s / 666.0654 s
agent0:                 episode reward: -0.7413,                 loss: 0.3926
agent1:                 episode reward: 0.7413,                 loss: nan
Episode: 13461/50100 (26.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3683s / 667.4337 s
agent0:                 episode reward: -0.1737,                 loss: 0.3910
agent1:                 episode reward: 0.1737,                 loss: nan
Episode: 13481/50100 (26.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3821s / 668.8158 s
agent0:                 episode reward: -0.2764,                 loss: 0.3915
agent1:                 episode reward: 0.2764,                 loss: nan
Episode: 13501/50100 (26.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3909s / 670.2067 s
agent0:                 episode reward: -0.0321,                 loss: 0.3900
agent1:                 episode reward: 0.0321,                 loss: nan
Episode: 13521/50100 (26.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3957s / 671.6024 s
agent0:                 episode reward: -0.0186,                 loss: 0.3895
agent1:                 episode reward: 0.0186,                 loss: nan
Episode: 13541/50100 (27.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4039s / 673.0063 s
agent0:                 episode reward: -0.0710,                 loss: 0.3899
agent1:                 episode reward: 0.0710,                 loss: nan
Episode: 13561/50100 (27.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4089s / 674.4153 s
agent0:                 episode reward: -0.5587,                 loss: 0.3872
agent1:                 episode reward: 0.5587,                 loss: nan
Episode: 13581/50100 (27.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1674s / 676.5826 s
agent0:                 episode reward: 0.5957,                 loss: 0.3881
agent1:                 episode reward: -0.5957,                 loss: nan
Score delta: 2.3403754846019504, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/13369_0.
Episode: 13601/50100 (27.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0615s / 677.6441 s
agent0:                 episode reward: 0.1589,                 loss: nan
agent1:                 episode reward: -0.1589,                 loss: 0.3494
Episode: 13621/50100 (27.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0731s / 678.7172 s
agent0:                 episode reward: -0.1913,                 loss: nan
agent1:                 episode reward: 0.1913,                 loss: 0.3398
Episode: 13641/50100 (27.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0822s / 680.7994 s
agent0:                 episode reward: -0.7744,                 loss: 0.4356
agent1:                 episode reward: 0.7744,                 loss: 0.3422
Score delta: 2.1057633893638936, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/13414_1.
Episode: 13661/50100 (27.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3999s / 682.1993 s
agent0:                 episode reward: -0.1635,                 loss: 0.4285
agent1:                 episode reward: 0.1635,                 loss: nan
Episode: 13681/50100 (27.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4260s / 683.6253 s
agent0:                 episode reward: -0.3760,                 loss: 0.4262
agent1:                 episode reward: 0.3760,                 loss: nan
Episode: 13701/50100 (27.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4109s / 685.0363 s
agent0:                 episode reward: -0.2896,                 loss: 0.4253
agent1:                 episode reward: 0.2896,                 loss: nan
Episode: 13721/50100 (27.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4150s / 686.4513 s
agent0:                 episode reward: 0.2527,                 loss: 0.4237
agent1:                 episode reward: -0.2527,                 loss: nan
Episode: 13741/50100 (27.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3984s / 687.8497 s
agent0:                 episode reward: -0.2378,                 loss: 0.4245
agent1:                 episode reward: 0.2378,                 loss: nan
Episode: 13761/50100 (27.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4058s / 689.2555 s
agent0:                 episode reward: -0.0957,                 loss: 0.4166
agent1:                 episode reward: 0.0957,                 loss: nan
Episode: 13781/50100 (27.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1016s / 691.3571 s
agent0:                 episode reward: 0.3824,                 loss: 0.4021
agent1:                 episode reward: -0.3824,                 loss: 0.3232
Score delta: 2.275822069327693, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/13562_0.
Episode: 13801/50100 (27.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0884s / 692.4455 s
agent0:                 episode reward: -0.0556,                 loss: nan
agent1:                 episode reward: 0.0556,                 loss: 0.3200
Episode: 13821/50100 (27.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0821s / 693.5277 s
agent0:                 episode reward: -0.3653,                 loss: nan
agent1:                 episode reward: 0.3653,                 loss: 0.3196
Episode: 13841/50100 (27.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0607s / 694.5884 s
agent0:                 episode reward: -0.4920,                 loss: nan
agent1:                 episode reward: 0.4920,                 loss: 0.3215
Episode: 13861/50100 (27.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0606s / 695.6490 s
agent0:                 episode reward: -0.3222,                 loss: nan
agent1:                 episode reward: 0.3222,                 loss: 0.3185
Episode: 13881/50100 (27.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0747s / 696.7237 s
agent0:                 episode reward: -0.4026,                 loss: nan
agent1:                 episode reward: 0.4026,                 loss: 0.3184
Episode: 13901/50100 (27.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0707s / 698.7945 s
agent0:                 episode reward: -0.6873,                 loss: 0.3927
agent1:                 episode reward: 0.6873,                 loss: 0.3181
Score delta: 2.795520964532115, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/13679_1.
Episode: 13921/50100 (27.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4317s / 700.2262 s
agent0:                 episode reward: 0.0880,                 loss: 0.3934
agent1:                 episode reward: -0.0880,                 loss: nan
Episode: 13941/50100 (27.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4158s / 701.6420 s
agent0:                 episode reward: -0.2970,                 loss: 0.3904
agent1:                 episode reward: 0.2970,                 loss: nan
Episode: 13961/50100 (27.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4233s / 703.0652 s
agent0:                 episode reward: 0.0672,                 loss: 0.3883
agent1:                 episode reward: -0.0672,                 loss: nan
Episode: 13981/50100 (27.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4270s / 704.4923 s
agent0:                 episode reward: 0.0857,                 loss: 0.3888
agent1:                 episode reward: -0.0857,                 loss: nan
Episode: 14001/50100 (27.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4631s / 705.9553 s
agent0:                 episode reward: -0.3435,                 loss: 0.3903
agent1:                 episode reward: 0.3435,                 loss: nan
Episode: 14021/50100 (27.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4693s / 707.4246 s
agent0:                 episode reward: -0.2242,                 loss: 0.3882
agent1:                 episode reward: 0.2242,                 loss: nan
Episode: 14041/50100 (28.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2056s / 709.6302 s
agent0:                 episode reward: 0.5508,                 loss: 0.3905
agent1:                 episode reward: -0.5508,                 loss: 0.3248
Score delta: 2.380592576091311, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/13827_0.
Episode: 14061/50100 (28.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0953s / 710.7255 s
agent0:                 episode reward: 0.3425,                 loss: nan
agent1:                 episode reward: -0.3425,                 loss: 0.3226
Episode: 14081/50100 (28.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1227s / 712.8482 s
agent0:                 episode reward: -0.6346,                 loss: 0.4247
agent1:                 episode reward: 0.6346,                 loss: 0.3047
Score delta: 2.8145750967608967, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/13857_1.
Episode: 14101/50100 (28.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4755s / 714.3237 s
agent0:                 episode reward: -0.2833,                 loss: 0.4231
agent1:                 episode reward: 0.2833,                 loss: nan
Episode: 14121/50100 (28.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4642s / 715.7879 s
agent0:                 episode reward: -0.2999,                 loss: 0.4236
agent1:                 episode reward: 0.2999,                 loss: nan
Episode: 14141/50100 (28.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4578s / 717.2457 s
agent0:                 episode reward: 0.2933,                 loss: 0.4229
agent1:                 episode reward: -0.2933,                 loss: nan
Episode: 14161/50100 (28.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4515s / 718.6972 s
agent0:                 episode reward: -0.0439,                 loss: 0.4222
agent1:                 episode reward: 0.0439,                 loss: nan
Episode: 14181/50100 (28.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4531s / 720.1503 s
agent0:                 episode reward: -0.4486,                 loss: 0.4237
agent1:                 episode reward: 0.4486,                 loss: nan
Episode: 14201/50100 (28.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4909s / 721.6412 s
agent0:                 episode reward: 0.0391,                 loss: 0.4242
agent1:                 episode reward: -0.0391,                 loss: nan
Episode: 14221/50100 (28.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4643s / 723.1056 s
agent0:                 episode reward: -0.3201,                 loss: 0.4225
agent1:                 episode reward: 0.3201,                 loss: nan
Episode: 14241/50100 (28.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4597s / 724.5652 s
agent0:                 episode reward: -0.1807,                 loss: 0.4312
agent1:                 episode reward: 0.1807,                 loss: nan
Episode: 14261/50100 (28.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4397s / 726.0049 s
agent0:                 episode reward: -0.3576,                 loss: 0.4475
agent1:                 episode reward: 0.3576,                 loss: nan
Episode: 14281/50100 (28.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4462s / 727.4511 s
agent0:                 episode reward: -0.6406,                 loss: 0.4473
agent1:                 episode reward: 0.6406,                 loss: nan
Episode: 14301/50100 (28.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4826s / 728.9337 s
agent0:                 episode reward: -0.0583,                 loss: 0.4486
agent1:                 episode reward: 0.0583,                 loss: nan
Episode: 14321/50100 (28.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4448s / 730.3785 s
agent0:                 episode reward: 0.0857,                 loss: 0.4475
agent1:                 episode reward: -0.0857,                 loss: nan
Episode: 14341/50100 (28.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4350s / 731.8135 s
agent0:                 episode reward: -0.1118,                 loss: 0.4480
agent1:                 episode reward: 0.1118,                 loss: nan
Episode: 14361/50100 (28.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4445s / 733.2580 s
agent0:                 episode reward: 0.1198,                 loss: 0.4480
agent1:                 episode reward: -0.1198,                 loss: nan
Episode: 14381/50100 (28.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4351s / 734.6931 s
agent0:                 episode reward: -0.1604,                 loss: 0.4478
agent1:                 episode reward: 0.1604,                 loss: nan
Episode: 14401/50100 (28.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4391s / 736.1323 s
agent0:                 episode reward: -0.4110,                 loss: 0.4470
agent1:                 episode reward: 0.4110,                 loss: nan
Episode: 14421/50100 (28.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4472s / 737.5795 s
agent0:                 episode reward: 0.1389,                 loss: 0.4485
agent1:                 episode reward: -0.1389,                 loss: nan
Episode: 14441/50100 (28.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4560s / 739.0355 s
agent0:                 episode reward: -0.1008,                 loss: 0.4478
agent1:                 episode reward: 0.1008,                 loss: nan
Episode: 14461/50100 (28.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4252s / 740.4607 s
agent0:                 episode reward: -0.2788,                 loss: 0.4474
agent1:                 episode reward: 0.2788,                 loss: nan
Episode: 14481/50100 (28.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4620s / 741.9226 s
agent0:                 episode reward: 0.2136,                 loss: 0.4470
agent1:                 episode reward: -0.2136,                 loss: nan
Episode: 14501/50100 (28.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4560s / 743.3787 s
agent0:                 episode reward: 0.2592,                 loss: 0.4471
agent1:                 episode reward: -0.2592,                 loss: nan
Episode: 14521/50100 (28.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4387s / 744.8174 s
agent0:                 episode reward: 0.4186,                 loss: 0.4474
agent1:                 episode reward: -0.4186,                 loss: nan
Episode: 14541/50100 (29.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4518s / 746.2692 s
agent0:                 episode reward: -0.5394,                 loss: 0.4475
agent1:                 episode reward: 0.5394,                 loss: nan
Episode: 14561/50100 (29.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4320s / 747.7012 s
agent0:                 episode reward: 0.0622,                 loss: 0.4482
agent1:                 episode reward: -0.0622,                 loss: nan
Episode: 14581/50100 (29.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4740s / 749.1752 s
agent0:                 episode reward: -0.0296,                 loss: 0.4417
agent1:                 episode reward: 0.0296,                 loss: nan
Episode: 14601/50100 (29.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4338s / 750.6090 s
agent0:                 episode reward: 0.0633,                 loss: 0.4400
agent1:                 episode reward: -0.0633,                 loss: nan
Episode: 14621/50100 (29.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4790s / 752.0881 s
agent0:                 episode reward: 0.4350,                 loss: 0.4382
agent1:                 episode reward: -0.4350,                 loss: nan
Episode: 14641/50100 (29.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4718s / 753.5599 s
agent0:                 episode reward: 0.1328,                 loss: 0.4395
agent1:                 episode reward: -0.1328,                 loss: nan
Episode: 14661/50100 (29.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4632s / 755.0231 s
agent0:                 episode reward: 0.0007,                 loss: 0.4391
agent1:                 episode reward: -0.0007,                 loss: nan
Episode: 14681/50100 (29.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4930s / 756.5160 s
agent0:                 episode reward: -0.1407,                 loss: 0.4372
agent1:                 episode reward: 0.1407,                 loss: nan
Episode: 14701/50100 (29.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4854s / 758.0015 s
agent0:                 episode reward: 0.1786,                 loss: 0.4377
agent1:                 episode reward: -0.1786,                 loss: nan
Episode: 14721/50100 (29.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5091s / 759.5106 s
agent0:                 episode reward: -0.6180,                 loss: 0.4388
agent1:                 episode reward: 0.6180,                 loss: nan
Episode: 14741/50100 (29.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5457s / 761.0563 s
agent0:                 episode reward: 0.3752,                 loss: 0.4312
agent1:                 episode reward: -0.3752,                 loss: nan
Episode: 14761/50100 (29.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5068s / 762.5632 s
agent0:                 episode reward: -0.1056,                 loss: 0.4198
agent1:                 episode reward: 0.1056,                 loss: nan
Episode: 14781/50100 (29.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4873s / 764.0505 s
agent0:                 episode reward: 0.2428,                 loss: 0.4209
agent1:                 episode reward: -0.2428,                 loss: nan
Episode: 14801/50100 (29.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4808s / 765.5313 s
agent0:                 episode reward: 0.0900,                 loss: 0.4189
agent1:                 episode reward: -0.0900,                 loss: nan
Episode: 14821/50100 (29.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4879s / 767.0192 s
agent0:                 episode reward: -0.0864,                 loss: 0.4173
agent1:                 episode reward: 0.0864,                 loss: nan
Episode: 14841/50100 (29.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4846s / 768.5038 s
agent0:                 episode reward: -0.1314,                 loss: 0.4193
agent1:                 episode reward: 0.1314,                 loss: nan
Episode: 14861/50100 (29.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4600s / 769.9639 s
agent0:                 episode reward: -0.1068,                 loss: 0.4174
agent1:                 episode reward: 0.1068,                 loss: nan
Episode: 14881/50100 (29.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4814s / 771.4452 s
agent0:                 episode reward: 0.2942,                 loss: 0.4180
agent1:                 episode reward: -0.2942,                 loss: nan
Episode: 14901/50100 (29.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4910s / 772.9362 s
agent0:                 episode reward: 0.1208,                 loss: 0.4196
agent1:                 episode reward: -0.1208,                 loss: nan
Episode: 14921/50100 (29.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5221s / 774.4584 s
agent0:                 episode reward: 0.1893,                 loss: 0.4097
agent1:                 episode reward: -0.1893,                 loss: nan
Episode: 14941/50100 (29.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5157s / 775.9740 s
agent0:                 episode reward: 0.1901,                 loss: 0.4094
agent1:                 episode reward: -0.1901,                 loss: nan
Episode: 14961/50100 (29.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5172s / 777.4912 s
agent0:                 episode reward: 0.3260,                 loss: 0.4108
agent1:                 episode reward: -0.3260,                 loss: nan
Episode: 14981/50100 (29.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5019s / 778.9931 s
agent0:                 episode reward: -0.1604,                 loss: 0.4100
agent1:                 episode reward: 0.1604,                 loss: nan
Episode: 15001/50100 (29.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4880s / 780.4812 s
agent0:                 episode reward: 0.0649,                 loss: 0.4081
agent1:                 episode reward: -0.0649,                 loss: nan
Episode: 15021/50100 (29.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4940s / 781.9752 s
agent0:                 episode reward: 0.1018,                 loss: 0.4081
agent1:                 episode reward: -0.1018,                 loss: nan
Episode: 15041/50100 (30.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4649s / 783.4401 s
agent0:                 episode reward: -0.1247,                 loss: 0.4086
agent1:                 episode reward: 0.1247,                 loss: nan
Episode: 15061/50100 (30.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4812s / 784.9213 s
agent0:                 episode reward: 0.3817,                 loss: 0.4092
agent1:                 episode reward: -0.3817,                 loss: nan
Episode: 15081/50100 (30.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4809s / 786.4022 s
agent0:                 episode reward: 0.2139,                 loss: 0.4149
agent1:                 episode reward: -0.2139,                 loss: nan
Episode: 15101/50100 (30.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4654s / 787.8675 s
agent0:                 episode reward: -0.4336,                 loss: 0.4157
agent1:                 episode reward: 0.4336,                 loss: nan
Episode: 15121/50100 (30.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4928s / 789.3603 s
agent0:                 episode reward: 0.0982,                 loss: 0.4178
agent1:                 episode reward: -0.0982,                 loss: nan
Episode: 15141/50100 (30.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5076s / 790.8680 s
agent0:                 episode reward: 0.4951,                 loss: 0.4165
agent1:                 episode reward: -0.4951,                 loss: nan
Episode: 15161/50100 (30.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5166s / 792.3846 s
agent0:                 episode reward: -0.3569,                 loss: 0.4157
agent1:                 episode reward: 0.3569,                 loss: nan
Episode: 15181/50100 (30.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5326s / 793.9171 s
agent0:                 episode reward: 0.2256,                 loss: 0.4161
agent1:                 episode reward: -0.2256,                 loss: nan
Episode: 15201/50100 (30.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5164s / 795.4335 s
agent0:                 episode reward: -0.2038,                 loss: 0.4167
agent1:                 episode reward: 0.2038,                 loss: nan
Episode: 15221/50100 (30.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5145s / 796.9480 s
agent0:                 episode reward: 0.1202,                 loss: 0.4166
agent1:                 episode reward: -0.1202,                 loss: nan
Episode: 15241/50100 (30.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5124s / 798.4604 s
agent0:                 episode reward: 0.1700,                 loss: 0.4244
agent1:                 episode reward: -0.1700,                 loss: nan
Episode: 15261/50100 (30.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0873s / 800.5477 s
agent0:                 episode reward: -0.0685,                 loss: 0.4387
agent1:                 episode reward: 0.0685,                 loss: 0.3219
Score delta: 2.043668196488036, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/15039_0.
Episode: 15281/50100 (30.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0742s / 801.6219 s
agent0:                 episode reward: -0.2409,                 loss: nan
agent1:                 episode reward: 0.2409,                 loss: 0.3177
Episode: 15301/50100 (30.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0991s / 802.7211 s
agent0:                 episode reward: -0.1125,                 loss: nan
agent1:                 episode reward: 0.1125,                 loss: 0.3186
Episode: 15321/50100 (30.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0921s / 803.8132 s
agent0:                 episode reward: -0.3717,                 loss: nan
agent1:                 episode reward: 0.3717,                 loss: 0.3173
Episode: 15341/50100 (30.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0764s / 804.8896 s
agent0:                 episode reward: -0.1578,                 loss: nan
agent1:                 episode reward: 0.1578,                 loss: 0.3160
Episode: 15361/50100 (30.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0731s / 805.9627 s
agent0:                 episode reward: -0.0075,                 loss: nan
agent1:                 episode reward: 0.0075,                 loss: 0.3172
Episode: 15381/50100 (30.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0579s / 807.0206 s
agent0:                 episode reward: -0.1978,                 loss: nan
agent1:                 episode reward: 0.1978,                 loss: 0.3165
Episode: 15401/50100 (30.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0335s / 808.0541 s
agent0:                 episode reward: -0.2604,                 loss: nan
agent1:                 episode reward: 0.2604,                 loss: 0.3283
Episode: 15421/50100 (30.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0411s / 809.0951 s
agent0:                 episode reward: -0.5342,                 loss: nan
agent1:                 episode reward: 0.5342,                 loss: 0.3563
Episode: 15441/50100 (30.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0303s / 810.1254 s
agent0:                 episode reward: 0.1135,                 loss: nan
agent1:                 episode reward: -0.1135,                 loss: 0.3522
Episode: 15461/50100 (30.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0451s / 811.1706 s
agent0:                 episode reward: -0.4680,                 loss: nan
agent1:                 episode reward: 0.4680,                 loss: 0.3524
Episode: 15481/50100 (30.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0436s / 812.2141 s
agent0:                 episode reward: -0.3200,                 loss: nan
agent1:                 episode reward: 0.3200,                 loss: 0.3503
Episode: 15501/50100 (30.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0594s / 813.2735 s
agent0:                 episode reward: -0.2874,                 loss: nan
agent1:                 episode reward: 0.2874,                 loss: 0.3506
Episode: 15521/50100 (30.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0847s / 814.3582 s
agent0:                 episode reward: -0.2928,                 loss: nan
agent1:                 episode reward: 0.2928,                 loss: 0.3513
Episode: 15541/50100 (31.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0778s / 815.4360 s
agent0:                 episode reward: -0.0613,                 loss: nan
agent1:                 episode reward: 0.0613,                 loss: 0.3506
Episode: 15561/50100 (31.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1229s / 817.5589 s
agent0:                 episode reward: -0.3543,                 loss: 0.4354
agent1:                 episode reward: 0.3543,                 loss: 0.3499
Score delta: 2.2078749977142693, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/15336_1.
Episode: 15581/50100 (31.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4632s / 819.0220 s
agent0:                 episode reward: -0.0156,                 loss: 0.4366
agent1:                 episode reward: 0.0156,                 loss: nan
Episode: 15601/50100 (31.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4854s / 820.5075 s
agent0:                 episode reward: -0.4934,                 loss: 0.4340
agent1:                 episode reward: 0.4934,                 loss: nan
Episode: 15621/50100 (31.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4805s / 821.9879 s
agent0:                 episode reward: -0.3731,                 loss: 0.4342
agent1:                 episode reward: 0.3731,                 loss: nan
Episode: 15641/50100 (31.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2174s / 824.2053 s
agent0:                 episode reward: 0.4895,                 loss: 0.4336
agent1:                 episode reward: -0.4895,                 loss: 0.3536
Score delta: 2.1137688534059116, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/15427_0.
Episode: 15661/50100 (31.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0783s / 825.2837 s
agent0:                 episode reward: -0.5769,                 loss: nan
agent1:                 episode reward: 0.5769,                 loss: 0.3495
Episode: 15681/50100 (31.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0748s / 826.3585 s
agent0:                 episode reward: -0.2518,                 loss: nan
agent1:                 episode reward: 0.2518,                 loss: 0.3169
Episode: 15701/50100 (31.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0660s / 827.4245 s
agent0:                 episode reward: -0.5306,                 loss: nan
agent1:                 episode reward: 0.5306,                 loss: 0.3151
Episode: 15721/50100 (31.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0814s / 828.5060 s
agent0:                 episode reward: 0.1031,                 loss: nan
agent1:                 episode reward: -0.1031,                 loss: 0.3138
Episode: 15741/50100 (31.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0691s / 829.5751 s
agent0:                 episode reward: -0.0617,                 loss: nan
agent1:                 episode reward: 0.0617,                 loss: 0.3110
Episode: 15761/50100 (31.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0847s / 830.6597 s
agent0:                 episode reward: -0.5509,                 loss: nan
agent1:                 episode reward: 0.5509,                 loss: 0.3128
Episode: 15781/50100 (31.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1843s / 832.8440 s
agent0:                 episode reward: -0.2026,                 loss: 0.3952
agent1:                 episode reward: 0.2026,                 loss: 0.3086
Score delta: 2.0877910498419916, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/15553_1.
Episode: 15801/50100 (31.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4958s / 834.3398 s
agent0:                 episode reward: 0.1300,                 loss: 0.3896
agent1:                 episode reward: -0.1300,                 loss: nan
Episode: 15821/50100 (31.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4977s / 835.8375 s
agent0:                 episode reward: -0.0127,                 loss: 0.3918
agent1:                 episode reward: 0.0127,                 loss: nan
Episode: 15841/50100 (31.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5012s / 837.3388 s
agent0:                 episode reward: 0.0251,                 loss: 0.4292
agent1:                 episode reward: -0.0251,                 loss: nan
Episode: 15861/50100 (31.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2328s / 839.5715 s
agent0:                 episode reward: 0.5616,                 loss: 0.4350
agent1:                 episode reward: -0.5616,                 loss: 0.3761
Score delta: 2.2422074554221867, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/15647_0.
Episode: 15881/50100 (31.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0869s / 840.6584 s
agent0:                 episode reward: -1.0011,                 loss: nan
agent1:                 episode reward: 1.0011,                 loss: 0.3704
Episode: 15901/50100 (31.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2758s / 842.9342 s
agent0:                 episode reward: -0.2350,                 loss: 0.3751
agent1:                 episode reward: 0.2350,                 loss: 0.3712
Score delta: 2.0480771075310273, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/15670_1.
Episode: 15921/50100 (31.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4992s / 844.4334 s
agent0:                 episode reward: 0.1376,                 loss: 0.3751
agent1:                 episode reward: -0.1376,                 loss: nan
Episode: 15941/50100 (31.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5015s / 845.9348 s
agent0:                 episode reward: -0.4511,                 loss: 0.3742
agent1:                 episode reward: 0.4511,                 loss: nan
Episode: 15961/50100 (31.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1444s / 848.0792 s
agent0:                 episode reward: 0.5739,                 loss: 0.3724
agent1:                 episode reward: -0.5739,                 loss: 0.3243
Score delta: 2.126993371374712, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/15742_0.
Episode: 15981/50100 (31.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0797s / 849.1589 s
agent0:                 episode reward: -0.5464,                 loss: nan
agent1:                 episode reward: 0.5464,                 loss: 0.3202
Episode: 16001/50100 (31.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0771s / 850.2360 s
agent0:                 episode reward: 0.4045,                 loss: nan
agent1:                 episode reward: -0.4045,                 loss: 0.3549
Episode: 16021/50100 (31.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1663s / 852.4023 s
agent0:                 episode reward: -0.2845,                 loss: 0.3977
agent1:                 episode reward: 0.2845,                 loss: 0.3644
Score delta: 2.205401726757801, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/15799_1.
Episode: 16041/50100 (32.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5247s / 853.9270 s
agent0:                 episode reward: 0.0755,                 loss: 0.3916
agent1:                 episode reward: -0.0755,                 loss: nan
Episode: 16061/50100 (32.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5389s / 855.4659 s
agent0:                 episode reward: -0.0508,                 loss: 0.3889
agent1:                 episode reward: 0.0508,                 loss: nan
Episode: 16081/50100 (32.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5369s / 857.0028 s
agent0:                 episode reward: -1.0969,                 loss: 0.4143
agent1:                 episode reward: 1.0969,                 loss: nan
Episode: 16101/50100 (32.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5321s / 858.5349 s
agent0:                 episode reward: -0.2585,                 loss: 0.4309
agent1:                 episode reward: 0.2585,                 loss: nan
Episode: 16121/50100 (32.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5303s / 860.0652 s
agent0:                 episode reward: -0.2134,                 loss: 0.4295
agent1:                 episode reward: 0.2134,                 loss: nan
Episode: 16141/50100 (32.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5371s / 861.6023 s
agent0:                 episode reward: -0.0534,                 loss: 0.4303
agent1:                 episode reward: 0.0534,                 loss: nan
Episode: 16161/50100 (32.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5288s / 863.1311 s
agent0:                 episode reward: -0.8356,                 loss: 0.4307
agent1:                 episode reward: 0.8356,                 loss: nan
Episode: 16181/50100 (32.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5192s / 864.6504 s
agent0:                 episode reward: 0.1492,                 loss: 0.4304
agent1:                 episode reward: -0.1492,                 loss: nan
Episode: 16201/50100 (32.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5192s / 866.1696 s
agent0:                 episode reward: 0.2773,                 loss: 0.4313
agent1:                 episode reward: -0.2773,                 loss: nan
Episode: 16221/50100 (32.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5306s / 867.7002 s
agent0:                 episode reward: -0.4655,                 loss: 0.4304
agent1:                 episode reward: 0.4655,                 loss: nan
Episode: 16241/50100 (32.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5401s / 869.2402 s
agent0:                 episode reward: -0.1101,                 loss: 0.4317
agent1:                 episode reward: 0.1101,                 loss: nan
Episode: 16261/50100 (32.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5321s / 870.7723 s
agent0:                 episode reward: -0.1697,                 loss: 0.4432
agent1:                 episode reward: 0.1697,                 loss: nan
Episode: 16281/50100 (32.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5315s / 872.3038 s
agent0:                 episode reward: -0.1830,                 loss: 0.4441
agent1:                 episode reward: 0.1830,                 loss: nan
Episode: 16301/50100 (32.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5198s / 873.8237 s
agent0:                 episode reward: -0.1083,                 loss: 0.4441
agent1:                 episode reward: 0.1083,                 loss: nan
Episode: 16321/50100 (32.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5394s / 875.3630 s
agent0:                 episode reward: -0.1483,                 loss: 0.4429
agent1:                 episode reward: 0.1483,                 loss: nan
Episode: 16341/50100 (32.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5278s / 876.8909 s
agent0:                 episode reward: 0.3249,                 loss: 0.4443
agent1:                 episode reward: -0.3249,                 loss: nan
Episode: 16361/50100 (32.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5424s / 878.4333 s
agent0:                 episode reward: -0.3995,                 loss: 0.4441
agent1:                 episode reward: 0.3995,                 loss: nan
Episode: 16381/50100 (32.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5399s / 879.9732 s
agent0:                 episode reward: 0.0519,                 loss: 0.4431
agent1:                 episode reward: -0.0519,                 loss: nan
Episode: 16401/50100 (32.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5466s / 881.5197 s
agent0:                 episode reward: -0.0771,                 loss: 0.4429
agent1:                 episode reward: 0.0771,                 loss: nan
Episode: 16421/50100 (32.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5585s / 883.0783 s
agent0:                 episode reward: -0.4501,                 loss: 0.4447
agent1:                 episode reward: 0.4501,                 loss: nan
Episode: 16441/50100 (32.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5477s / 884.6260 s
agent0:                 episode reward: -0.2172,                 loss: 0.4452
agent1:                 episode reward: 0.2172,                 loss: nan
Episode: 16461/50100 (32.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5443s / 886.1703 s
agent0:                 episode reward: -0.0295,                 loss: 0.4459
agent1:                 episode reward: 0.0295,                 loss: nan
Episode: 16481/50100 (32.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5427s / 887.7130 s
agent0:                 episode reward: 0.4131,                 loss: 0.4453
agent1:                 episode reward: -0.4131,                 loss: nan
Episode: 16501/50100 (32.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8891s / 889.6021 s
agent0:                 episode reward: 0.0651,                 loss: 0.4473
agent1:                 episode reward: -0.0651,                 loss: 0.3652
Score delta: 2.4460769517903254, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/16270_0.
Episode: 16521/50100 (32.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2531s / 891.8552 s
agent0:                 episode reward: -0.3071,                 loss: 0.4053
agent1:                 episode reward: 0.3071,                 loss: 0.3635
Score delta: 2.1063136743169797, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/16294_1.
Episode: 16541/50100 (33.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5537s / 893.4089 s
agent0:                 episode reward: -0.5053,                 loss: 0.4048
agent1:                 episode reward: 0.5053,                 loss: nan
Episode: 16561/50100 (33.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5624s / 894.9713 s
agent0:                 episode reward: -0.9581,                 loss: 0.4042
agent1:                 episode reward: 0.9581,                 loss: nan
Episode: 16581/50100 (33.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5459s / 896.5172 s
agent0:                 episode reward: -0.4243,                 loss: 0.4046
agent1:                 episode reward: 0.4243,                 loss: nan
Episode: 16601/50100 (33.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5403s / 898.0575 s
agent0:                 episode reward: -0.1380,                 loss: 0.4200
agent1:                 episode reward: 0.1380,                 loss: nan
Episode: 16621/50100 (33.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5561s / 899.6136 s
agent0:                 episode reward: -0.4529,                 loss: 0.4469
agent1:                 episode reward: 0.4529,                 loss: nan
Episode: 16641/50100 (33.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5695s / 901.1831 s
agent0:                 episode reward: -0.0863,                 loss: 0.4460
agent1:                 episode reward: 0.0863,                 loss: nan
Episode: 16661/50100 (33.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5787s / 902.7618 s
agent0:                 episode reward: 0.2535,                 loss: 0.4460
agent1:                 episode reward: -0.2535,                 loss: nan
Episode: 16681/50100 (33.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5823s / 904.3440 s
agent0:                 episode reward: 0.0455,                 loss: 0.4456
agent1:                 episode reward: -0.0455,                 loss: nan
Episode: 16701/50100 (33.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5754s / 905.9195 s
agent0:                 episode reward: -0.4278,                 loss: 0.4452
agent1:                 episode reward: 0.4278,                 loss: nan
Episode: 16721/50100 (33.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5905s / 907.5099 s
agent0:                 episode reward: -0.3228,                 loss: 0.4452
agent1:                 episode reward: 0.3228,                 loss: nan
Episode: 16741/50100 (33.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5667s / 909.0766 s
agent0:                 episode reward: 0.2843,                 loss: 0.4442
agent1:                 episode reward: -0.2843,                 loss: nan
Episode: 16761/50100 (33.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5681s / 910.6448 s
agent0:                 episode reward: 0.0176,                 loss: 0.4446
agent1:                 episode reward: -0.0176,                 loss: nan
Episode: 16781/50100 (33.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5654s / 912.2102 s
agent0:                 episode reward: 0.1076,                 loss: 0.4505
agent1:                 episode reward: -0.1076,                 loss: nan
Episode: 16801/50100 (33.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5850s / 913.7952 s
agent0:                 episode reward: -0.1501,                 loss: 0.4495
agent1:                 episode reward: 0.1501,                 loss: nan
Episode: 16821/50100 (33.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5758s / 915.3709 s
agent0:                 episode reward: -0.1800,                 loss: 0.4495
agent1:                 episode reward: 0.1800,                 loss: nan
Episode: 16841/50100 (33.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5562s / 916.9271 s
agent0:                 episode reward: 0.1705,                 loss: 0.4481
agent1:                 episode reward: -0.1705,                 loss: nan
Episode: 16861/50100 (33.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6007s / 918.5278 s
agent0:                 episode reward: 0.3075,                 loss: 0.4485
agent1:                 episode reward: -0.3075,                 loss: nan
Episode: 16881/50100 (33.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6590s / 920.1868 s
agent0:                 episode reward: 0.0949,                 loss: 0.4474
agent1:                 episode reward: -0.0949,                 loss: nan
Episode: 16901/50100 (33.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6018s / 921.7886 s
agent0:                 episode reward: -0.0153,                 loss: 0.4466
agent1:                 episode reward: 0.0153,                 loss: nan
Episode: 16921/50100 (33.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6051s / 923.3937 s
agent0:                 episode reward: -0.1159,                 loss: 0.4476
agent1:                 episode reward: 0.1159,                 loss: nan
Episode: 16941/50100 (33.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6106s / 925.0042 s
agent0:                 episode reward: -0.6271,                 loss: 0.4452
agent1:                 episode reward: 0.6271,                 loss: nan
Episode: 16961/50100 (33.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6161s / 926.6204 s
agent0:                 episode reward: 0.2013,                 loss: 0.4454
agent1:                 episode reward: -0.2013,                 loss: nan
Episode: 16981/50100 (33.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5777s / 928.1981 s
agent0:                 episode reward: 0.1909,                 loss: 0.4456
agent1:                 episode reward: -0.1909,                 loss: nan
Episode: 17001/50100 (33.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5704s / 929.7685 s
agent0:                 episode reward: -0.0459,                 loss: 0.4451
agent1:                 episode reward: 0.0459,                 loss: nan
Episode: 17021/50100 (33.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0755s / 931.8440 s
agent0:                 episode reward: 0.2562,                 loss: 0.4448
agent1:                 episode reward: -0.2562,                 loss: 0.3869
Score delta: 2.1210658772755777, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/16796_0.
Episode: 17041/50100 (34.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0685s / 932.9125 s
agent0:                 episode reward: 0.3688,                 loss: nan
agent1:                 episode reward: -0.3688,                 loss: 0.3871
Episode: 17061/50100 (34.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0998s / 934.0123 s
agent0:                 episode reward: -0.2828,                 loss: nan
agent1:                 episode reward: 0.2828,                 loss: 0.3885
Episode: 17081/50100 (34.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1007s / 935.1129 s
agent0:                 episode reward: -0.5407,                 loss: nan
agent1:                 episode reward: 0.5407,                 loss: 0.3874
Episode: 17101/50100 (34.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1037s / 936.2166 s
agent0:                 episode reward: -0.1712,                 loss: nan
agent1:                 episode reward: 0.1712,                 loss: 0.3890
Episode: 17121/50100 (34.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0788s / 937.2954 s
agent0:                 episode reward: 0.5992,                 loss: nan
agent1:                 episode reward: -0.5992,                 loss: 0.3872
Episode: 17141/50100 (34.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0203s / 939.3157 s
agent0:                 episode reward: -0.4504,                 loss: 0.3775
agent1:                 episode reward: 0.4504,                 loss: 0.3814
Score delta: 2.1003460945526795, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/16924_1.
Episode: 17161/50100 (34.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5687s / 940.8844 s
agent0:                 episode reward: 0.3973,                 loss: 0.3753
agent1:                 episode reward: -0.3973,                 loss: nan
Episode: 17181/50100 (34.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5824s / 942.4669 s
agent0:                 episode reward: -0.2984,                 loss: 0.3748
agent1:                 episode reward: 0.2984,                 loss: nan
Episode: 17201/50100 (34.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5902s / 944.0571 s
agent0:                 episode reward: 0.0505,                 loss: 0.3756
agent1:                 episode reward: -0.0505,                 loss: nan
Episode: 17221/50100 (34.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5895s / 945.6465 s
agent0:                 episode reward: -0.3238,                 loss: 0.3738
agent1:                 episode reward: 0.3238,                 loss: nan
Episode: 17241/50100 (34.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5855s / 947.2321 s
agent0:                 episode reward: -0.4698,                 loss: 0.3954
agent1:                 episode reward: 0.4698,                 loss: nan
Episode: 17261/50100 (34.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5885s / 948.8205 s
agent0:                 episode reward: -0.3119,                 loss: 0.3940
agent1:                 episode reward: 0.3119,                 loss: nan
Episode: 17281/50100 (34.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6098s / 950.4303 s
agent0:                 episode reward: 0.0379,                 loss: 0.3952
agent1:                 episode reward: -0.0379,                 loss: nan
Episode: 17301/50100 (34.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5995s / 952.0298 s
agent0:                 episode reward: -0.2449,                 loss: 0.3938
agent1:                 episode reward: 0.2449,                 loss: nan
Episode: 17321/50100 (34.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6177s / 953.6475 s
agent0:                 episode reward: -0.4293,                 loss: 0.3937
agent1:                 episode reward: 0.4293,                 loss: nan
Episode: 17341/50100 (34.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6187s / 955.2662 s
agent0:                 episode reward: 0.1561,                 loss: 0.3917
agent1:                 episode reward: -0.1561,                 loss: nan
Episode: 17361/50100 (34.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6194s / 956.8856 s
agent0:                 episode reward: -0.1617,                 loss: 0.3927
agent1:                 episode reward: 0.1617,                 loss: nan
Episode: 17381/50100 (34.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6156s / 958.5012 s
agent0:                 episode reward: -0.0365,                 loss: 0.3903
agent1:                 episode reward: 0.0365,                 loss: nan
Episode: 17401/50100 (34.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6279s / 960.1290 s
agent0:                 episode reward: 0.0270,                 loss: 0.4095
agent1:                 episode reward: -0.0270,                 loss: nan
Episode: 17421/50100 (34.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6424s / 961.7715 s
agent0:                 episode reward: 0.1193,                 loss: 0.4213
agent1:                 episode reward: -0.1193,                 loss: nan
Episode: 17441/50100 (34.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2348s / 964.0063 s
agent0:                 episode reward: 0.1340,                 loss: 0.4193
agent1:                 episode reward: -0.1340,                 loss: 0.3210
Score delta: 2.0506868745981643, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/17218_0.
Episode: 17461/50100 (34.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0406s / 966.0468 s
agent0:                 episode reward: -0.6093,                 loss: 0.3799
agent1:                 episode reward: 0.6093,                 loss: 0.3181
Score delta: 2.5535397543898517, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/17247_1.
Episode: 17481/50100 (34.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6451s / 967.6919 s
agent0:                 episode reward: 0.4389,                 loss: 0.3742
agent1:                 episode reward: -0.4389,                 loss: nan
Episode: 17501/50100 (34.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6528s / 969.3447 s
agent0:                 episode reward: -0.3025,                 loss: 0.3778
agent1:                 episode reward: 0.3025,                 loss: nan
Episode: 17521/50100 (34.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6415s / 970.9862 s
agent0:                 episode reward: -0.1499,                 loss: 0.3742
agent1:                 episode reward: 0.1499,                 loss: nan
Episode: 17541/50100 (35.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6461s / 972.6323 s
agent0:                 episode reward: -0.2420,                 loss: 0.3741
agent1:                 episode reward: 0.2420,                 loss: nan
Episode: 17561/50100 (35.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6414s / 974.2737 s
agent0:                 episode reward: 0.1680,                 loss: 0.3751
agent1:                 episode reward: -0.1680,                 loss: nan
Episode: 17581/50100 (35.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6421s / 975.9158 s
agent0:                 episode reward: 0.0043,                 loss: 0.3765
agent1:                 episode reward: -0.0043,                 loss: nan
Episode: 17601/50100 (35.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6457s / 977.5616 s
agent0:                 episode reward: -0.7189,                 loss: 0.3908
agent1:                 episode reward: 0.7189,                 loss: nan
Episode: 17621/50100 (35.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6605s / 979.2220 s
agent0:                 episode reward: -0.0945,                 loss: 0.3939
agent1:                 episode reward: 0.0945,                 loss: nan
Episode: 17641/50100 (35.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6741s / 980.8961 s
agent0:                 episode reward: -0.0998,                 loss: 0.3919
agent1:                 episode reward: 0.0998,                 loss: nan
Episode: 17661/50100 (35.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6497s / 982.5458 s
agent0:                 episode reward: -0.0510,                 loss: 0.3935
agent1:                 episode reward: 0.0510,                 loss: nan
Episode: 17681/50100 (35.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6690s / 984.2148 s
agent0:                 episode reward: 0.0978,                 loss: 0.3933
agent1:                 episode reward: -0.0978,                 loss: nan
Episode: 17701/50100 (35.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6739s / 985.8887 s
agent0:                 episode reward: 0.3915,                 loss: 0.3927
agent1:                 episode reward: -0.3915,                 loss: nan
Episode: 17721/50100 (35.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6813s / 987.5700 s
agent0:                 episode reward: -0.2321,                 loss: 0.3921
agent1:                 episode reward: 0.2321,                 loss: nan
Episode: 17741/50100 (35.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6842s / 989.2542 s
agent0:                 episode reward: -0.0552,                 loss: 0.3886
agent1:                 episode reward: 0.0552,                 loss: nan
Episode: 17761/50100 (35.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6785s / 990.9327 s
agent0:                 episode reward: -0.6334,                 loss: 0.4086
agent1:                 episode reward: 0.6334,                 loss: nan
Episode: 17781/50100 (35.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6764s / 992.6091 s
agent0:                 episode reward: 0.1098,                 loss: 0.4260
agent1:                 episode reward: -0.1098,                 loss: nan
Episode: 17801/50100 (35.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6735s / 994.2826 s
agent0:                 episode reward: -0.2457,                 loss: 0.4250
agent1:                 episode reward: 0.2457,                 loss: nan
Episode: 17821/50100 (35.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6814s / 995.9639 s
agent0:                 episode reward: 0.1099,                 loss: 0.4245
agent1:                 episode reward: -0.1099,                 loss: nan
Episode: 17841/50100 (35.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6835s / 997.6475 s
agent0:                 episode reward: -0.0776,                 loss: 0.4260
agent1:                 episode reward: 0.0776,                 loss: nan
Episode: 17861/50100 (35.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6502s / 999.2977 s
agent0:                 episode reward: -0.2673,                 loss: 0.4247
agent1:                 episode reward: 0.2673,                 loss: nan
Episode: 17881/50100 (35.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6431s / 1000.9408 s
agent0:                 episode reward: 0.3566,                 loss: 0.4242
agent1:                 episode reward: -0.3566,                 loss: nan
Episode: 17901/50100 (35.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3171s / 1003.2579 s
agent0:                 episode reward: 0.4889,                 loss: 0.4256
agent1:                 episode reward: -0.4889,                 loss: 0.3611
Score delta: 2.2216837989609006, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/17683_0.
Episode: 17921/50100 (35.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0912s / 1004.3491 s
agent0:                 episode reward: -0.1863,                 loss: nan
agent1:                 episode reward: 0.1863,                 loss: 0.3601
Episode: 17941/50100 (35.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0974s / 1005.4465 s
agent0:                 episode reward: -0.1856,                 loss: nan
agent1:                 episode reward: 0.1856,                 loss: 0.3583
Episode: 17961/50100 (35.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0764s / 1006.5229 s
agent0:                 episode reward: -0.2022,                 loss: nan
agent1:                 episode reward: 0.2022,                 loss: 0.3561
Episode: 17981/50100 (35.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0878s / 1007.6107 s
agent0:                 episode reward: -0.2647,                 loss: nan
agent1:                 episode reward: 0.2647,                 loss: 0.3555
Episode: 18001/50100 (35.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1086s / 1008.7193 s
agent0:                 episode reward: -0.1811,                 loss: nan
agent1:                 episode reward: 0.1811,                 loss: 0.3549
Episode: 18021/50100 (35.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1111s / 1009.8304 s
agent0:                 episode reward: -0.4402,                 loss: nan
agent1:                 episode reward: 0.4402,                 loss: 0.3523
Episode: 18041/50100 (36.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1110s / 1010.9414 s
agent0:                 episode reward: -0.4833,                 loss: nan
agent1:                 episode reward: 0.4833,                 loss: 0.3431
Episode: 18061/50100 (36.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3844s / 1013.3258 s
agent0:                 episode reward: -0.4029,                 loss: 0.4062
agent1:                 episode reward: 0.4029,                 loss: 0.3310
Score delta: 2.1851846988953554, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/17834_1.
Episode: 18081/50100 (36.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6294s / 1014.9552 s
agent0:                 episode reward: -0.1815,                 loss: 0.4345
agent1:                 episode reward: 0.1815,                 loss: nan
Episode: 18101/50100 (36.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2372s / 1017.1924 s
agent0:                 episode reward: 0.5634,                 loss: 0.4493
agent1:                 episode reward: -0.5634,                 loss: 0.3676
Score delta: 2.112630398712512, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/17880_0.
Episode: 18121/50100 (36.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0983s / 1018.2907 s
agent0:                 episode reward: 0.0270,                 loss: nan
agent1:                 episode reward: -0.0270,                 loss: 0.3640
Episode: 18141/50100 (36.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0851s / 1019.3758 s
agent0:                 episode reward: 0.1428,                 loss: nan
agent1:                 episode reward: -0.1428,                 loss: 0.3615
Episode: 18161/50100 (36.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1018s / 1020.4777 s
agent0:                 episode reward: 0.2067,                 loss: nan
agent1:                 episode reward: -0.2067,                 loss: 0.3591
Episode: 18181/50100 (36.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0942s / 1021.5718 s
agent0:                 episode reward: -0.3862,                 loss: nan
agent1:                 episode reward: 0.3862,                 loss: 0.3600
Episode: 18201/50100 (36.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1048s / 1022.6766 s
agent0:                 episode reward: -0.1794,                 loss: nan
agent1:                 episode reward: 0.1794,                 loss: 0.3571
Episode: 18221/50100 (36.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 2.4371s / 1025.1137 s
agent0:                 episode reward: -0.3881,                 loss: 0.4281
agent1:                 episode reward: 0.3881,                 loss: 0.3570
Score delta: 2.0173980787257446, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/17991_1.
Episode: 18241/50100 (36.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6554s / 1026.7691 s
agent0:                 episode reward: -0.0269,                 loss: 0.4275
agent1:                 episode reward: 0.0269,                 loss: nan
Episode: 18261/50100 (36.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6588s / 1028.4280 s
agent0:                 episode reward: -0.2368,                 loss: 0.4259
agent1:                 episode reward: 0.2368,                 loss: nan
Episode: 18281/50100 (36.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6537s / 1030.0817 s
agent0:                 episode reward: 0.0443,                 loss: 0.4284
agent1:                 episode reward: -0.0443,                 loss: nan
Episode: 18301/50100 (36.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6501s / 1031.7318 s
agent0:                 episode reward: -0.2823,                 loss: 0.4267
agent1:                 episode reward: 0.2823,                 loss: nan
Episode: 18321/50100 (36.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6739s / 1033.4056 s
agent0:                 episode reward: -0.1449,                 loss: 0.4258
agent1:                 episode reward: 0.1449,                 loss: nan
Episode: 18341/50100 (36.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6913s / 1035.0969 s
agent0:                 episode reward: -0.4624,                 loss: 0.4260
agent1:                 episode reward: 0.4624,                 loss: nan
Episode: 18361/50100 (36.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6823s / 1036.7792 s
agent0:                 episode reward: -0.1076,                 loss: 0.4450
agent1:                 episode reward: 0.1076,                 loss: nan
Episode: 18381/50100 (36.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7023s / 1038.4815 s
agent0:                 episode reward: 0.0778,                 loss: 0.4502
agent1:                 episode reward: -0.0778,                 loss: nan
Episode: 18401/50100 (36.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6949s / 1040.1764 s
agent0:                 episode reward: 0.1380,                 loss: 0.4502
agent1:                 episode reward: -0.1380,                 loss: nan
Episode: 18421/50100 (36.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6960s / 1041.8724 s
agent0:                 episode reward: -0.0658,                 loss: 0.4499
agent1:                 episode reward: 0.0658,                 loss: nan
Episode: 18441/50100 (36.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7040s / 1043.5764 s
agent0:                 episode reward: 0.0593,                 loss: 0.4495
agent1:                 episode reward: -0.0593,                 loss: nan
Episode: 18461/50100 (36.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6877s / 1045.2641 s
agent0:                 episode reward: -0.3236,                 loss: 0.4498
agent1:                 episode reward: 0.3236,                 loss: nan
Episode: 18481/50100 (36.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6837s / 1046.9478 s
agent0:                 episode reward: -0.5149,                 loss: 0.4501
agent1:                 episode reward: 0.5149,                 loss: nan
Episode: 18501/50100 (36.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6862s / 1048.6340 s
agent0:                 episode reward: 0.1811,                 loss: 0.4492
agent1:                 episode reward: -0.1811,                 loss: nan
Episode: 18521/50100 (36.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6795s / 1050.3135 s
agent0:                 episode reward: 0.1680,                 loss: 0.4499
agent1:                 episode reward: -0.1680,                 loss: nan
Episode: 18541/50100 (37.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6861s / 1051.9997 s
agent0:                 episode reward: -0.4764,                 loss: 0.4494
agent1:                 episode reward: 0.4764,                 loss: nan
Episode: 18561/50100 (37.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6922s / 1053.6919 s
agent0:                 episode reward: -0.2971,                 loss: 0.4488
agent1:                 episode reward: 0.2971,                 loss: nan
Episode: 18581/50100 (37.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6911s / 1055.3829 s
agent0:                 episode reward: 0.5984,                 loss: 0.4487
agent1:                 episode reward: -0.5984,                 loss: nan
Episode: 18601/50100 (37.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6807s / 1057.0637 s
agent0:                 episode reward: -0.1529,                 loss: 0.4489
agent1:                 episode reward: 0.1529,                 loss: nan
Episode: 18621/50100 (37.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6868s / 1058.7505 s
agent0:                 episode reward: -0.1761,                 loss: 0.4490
agent1:                 episode reward: 0.1761,                 loss: nan
Episode: 18641/50100 (37.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6944s / 1060.4449 s
agent0:                 episode reward: 0.1472,                 loss: 0.4479
agent1:                 episode reward: -0.1472,                 loss: nan
Episode: 18661/50100 (37.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6898s / 1062.1347 s
agent0:                 episode reward: 0.2127,                 loss: 0.4487
agent1:                 episode reward: -0.2127,                 loss: nan
Episode: 18681/50100 (37.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6918s / 1063.8266 s
agent0:                 episode reward: 0.2202,                 loss: 0.4487
agent1:                 episode reward: -0.2202,                 loss: nan
Episode: 18701/50100 (37.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6874s / 1065.5140 s
agent0:                 episode reward: -0.6511,                 loss: 0.4501
agent1:                 episode reward: 0.6511,                 loss: nan
Episode: 18721/50100 (37.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6966s / 1067.2106 s
agent0:                 episode reward: 0.0371,                 loss: 0.4484
agent1:                 episode reward: -0.0371,                 loss: nan
Episode: 18741/50100 (37.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6927s / 1068.9033 s
agent0:                 episode reward: 0.3118,                 loss: 0.4480
agent1:                 episode reward: -0.3118,                 loss: nan
Episode: 18761/50100 (37.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7059s / 1070.6092 s
agent0:                 episode reward: 0.1942,                 loss: 0.4484
agent1:                 episode reward: -0.1942,                 loss: nan
Episode: 18781/50100 (37.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7102s / 1072.3194 s
agent0:                 episode reward: -0.2208,                 loss: 0.4479
agent1:                 episode reward: 0.2208,                 loss: nan
Episode: 18801/50100 (37.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7019s / 1074.0213 s
agent0:                 episode reward: -0.1118,                 loss: 0.4495
agent1:                 episode reward: 0.1118,                 loss: nan
Episode: 18821/50100 (37.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6993s / 1075.7206 s
agent0:                 episode reward: 0.0717,                 loss: 0.4484
agent1:                 episode reward: -0.0717,                 loss: nan
Episode: 18841/50100 (37.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7226s / 1077.4432 s
agent0:                 episode reward: 0.1275,                 loss: 0.4478
agent1:                 episode reward: -0.1275,                 loss: nan
Episode: 18861/50100 (37.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7346s / 1079.1777 s
agent0:                 episode reward: -0.3717,                 loss: 0.4407
agent1:                 episode reward: 0.3717,                 loss: nan
Episode: 18881/50100 (37.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7307s / 1080.9084 s
agent0:                 episode reward: 0.2782,                 loss: 0.4361
agent1:                 episode reward: -0.2782,                 loss: nan
Episode: 18901/50100 (37.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3012s / 1083.2096 s
agent0:                 episode reward: 0.6073,                 loss: 0.4359
agent1:                 episode reward: -0.6073,                 loss: 0.3586
Score delta: 2.4237224572277105, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/18676_0.
Episode: 18921/50100 (37.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1502s / 1084.3598 s
agent0:                 episode reward: -0.0309,                 loss: nan
agent1:                 episode reward: 0.0309,                 loss: 0.3554
Episode: 18941/50100 (37.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1513s / 1085.5111 s
agent0:                 episode reward: -0.3199,                 loss: nan
agent1:                 episode reward: 0.3199,                 loss: 0.3550
Episode: 18961/50100 (37.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1448s / 1086.6558 s
agent0:                 episode reward: 0.4017,                 loss: nan
agent1:                 episode reward: -0.4017,                 loss: 0.3435
Episode: 18981/50100 (37.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1788s / 1088.8347 s
agent0:                 episode reward: -0.8054,                 loss: 0.4370
agent1:                 episode reward: 0.8054,                 loss: 0.3413
Score delta: 2.2505618335790496, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/18765_1.
Episode: 19001/50100 (37.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7290s / 1090.5637 s
agent0:                 episode reward: -0.2027,                 loss: 0.4361
agent1:                 episode reward: 0.2027,                 loss: nan
Episode: 19021/50100 (37.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7119s / 1092.2756 s
agent0:                 episode reward: 0.2280,                 loss: 0.4356
agent1:                 episode reward: -0.2280,                 loss: nan
Episode: 19041/50100 (38.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7047s / 1093.9802 s
agent0:                 episode reward: 0.5857,                 loss: 0.4352
agent1:                 episode reward: -0.5857,                 loss: nan
Episode: 19061/50100 (38.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7168s / 1095.6970 s
agent0:                 episode reward: 0.7348,                 loss: 0.4367
agent1:                 episode reward: -0.7348,                 loss: nan
Episode: 19081/50100 (38.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7220s / 1097.4190 s
agent0:                 episode reward: -0.0988,                 loss: 0.4362
agent1:                 episode reward: 0.0988,                 loss: nan
Episode: 19101/50100 (38.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7152s / 1099.1342 s
agent0:                 episode reward: 0.3923,                 loss: 0.4361
agent1:                 episode reward: -0.3923,                 loss: nan
Episode: 19121/50100 (38.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7208s / 1100.8550 s
agent0:                 episode reward: 0.3382,                 loss: 0.4176
agent1:                 episode reward: -0.3382,                 loss: nan
Episode: 19141/50100 (38.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7261s / 1102.5811 s
agent0:                 episode reward: 0.0967,                 loss: 0.4146
agent1:                 episode reward: -0.0967,                 loss: nan
Episode: 19161/50100 (38.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7249s / 1104.3060 s
agent0:                 episode reward: 0.3464,                 loss: 0.4137
agent1:                 episode reward: -0.3464,                 loss: nan
Episode: 19181/50100 (38.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1151s / 1106.4211 s
agent0:                 episode reward: 0.9507,                 loss: 0.4161
agent1:                 episode reward: -0.9507,                 loss: 0.3390
Score delta: 2.1068535767940095, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/18954_0.
Episode: 19201/50100 (38.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0947s / 1107.5159 s
agent0:                 episode reward: 0.1139,                 loss: nan
agent1:                 episode reward: -0.1139,                 loss: 0.3379
Episode: 19221/50100 (38.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1209s / 1108.6368 s
agent0:                 episode reward: 0.4847,                 loss: nan
agent1:                 episode reward: -0.4847,                 loss: 0.3386
Episode: 19241/50100 (38.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1213s / 1109.7581 s
agent0:                 episode reward: 0.0719,                 loss: nan
agent1:                 episode reward: -0.0719,                 loss: 0.3372
Episode: 19261/50100 (38.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1090s / 1110.8671 s
agent0:                 episode reward: -0.4550,                 loss: nan
agent1:                 episode reward: 0.4550,                 loss: 0.3356
Episode: 19281/50100 (38.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1063s / 1111.9734 s
agent0:                 episode reward: -0.2607,                 loss: nan
agent1:                 episode reward: 0.2607,                 loss: 0.3342
Episode: 19301/50100 (38.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1166s / 1113.0900 s
agent0:                 episode reward: -0.2043,                 loss: nan
agent1:                 episode reward: 0.2043,                 loss: 0.2919
Episode: 19321/50100 (38.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1394s / 1114.2293 s
agent0:                 episode reward: -0.0374,                 loss: nan
agent1:                 episode reward: 0.0374,                 loss: 0.2895
Episode: 19341/50100 (38.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 2.4103s / 1116.6396 s
agent0:                 episode reward: 0.0657,                 loss: 0.4402
agent1:                 episode reward: -0.0657,                 loss: 0.2892
Score delta: 2.3665217197726465, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/19118_1.
Episode: 19361/50100 (38.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7233s / 1118.3629 s
agent0:                 episode reward: 0.3658,                 loss: 0.4392
agent1:                 episode reward: -0.3658,                 loss: nan
Episode: 19381/50100 (38.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7266s / 1120.0896 s
agent0:                 episode reward: -0.3465,                 loss: 0.4397
agent1:                 episode reward: 0.3465,                 loss: nan
Episode: 19401/50100 (38.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7419s / 1121.8315 s
agent0:                 episode reward: -0.0928,                 loss: 0.4395
agent1:                 episode reward: 0.0928,                 loss: nan
Episode: 19421/50100 (38.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7549s / 1123.5864 s
agent0:                 episode reward: 0.3019,                 loss: 0.4396
agent1:                 episode reward: -0.3019,                 loss: nan
Episode: 19441/50100 (38.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7431s / 1125.3295 s
agent0:                 episode reward: 0.4037,                 loss: 0.4356
agent1:                 episode reward: -0.4037,                 loss: nan
Episode: 19461/50100 (38.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7474s / 1127.0770 s
agent0:                 episode reward: -0.6099,                 loss: 0.4238
agent1:                 episode reward: 0.6099,                 loss: nan
Episode: 19481/50100 (38.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7501s / 1128.8270 s
agent0:                 episode reward: -0.1102,                 loss: 0.4219
agent1:                 episode reward: 0.1102,                 loss: nan
Episode: 19501/50100 (38.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7414s / 1130.5684 s
agent0:                 episode reward: -0.0908,                 loss: 0.4220
agent1:                 episode reward: 0.0908,                 loss: nan
Episode: 19521/50100 (38.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7694s / 1132.3378 s
agent0:                 episode reward: 0.0470,                 loss: 0.4234
agent1:                 episode reward: -0.0470,                 loss: nan
Episode: 19541/50100 (39.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7403s / 1134.0781 s
agent0:                 episode reward: -0.6478,                 loss: 0.4239
agent1:                 episode reward: 0.6478,                 loss: nan
Episode: 19561/50100 (39.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7507s / 1135.8289 s
agent0:                 episode reward: 0.0588,                 loss: 0.4236
agent1:                 episode reward: -0.0588,                 loss: nan
Episode: 19581/50100 (39.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1516s / 1137.9805 s
agent0:                 episode reward: 0.0425,                 loss: 0.4219
agent1:                 episode reward: -0.0425,                 loss: 0.3441
Score delta: 2.0915919411557953, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/19354_0.
Episode: 19601/50100 (39.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1091s / 1139.0896 s
agent0:                 episode reward: 0.0023,                 loss: nan
agent1:                 episode reward: -0.0023,                 loss: 0.3417
Episode: 19621/50100 (39.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1182s / 1140.2077 s
agent0:                 episode reward: -0.4945,                 loss: nan
agent1:                 episode reward: 0.4945,                 loss: 0.3399
Episode: 19641/50100 (39.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1609s / 1142.3687 s
agent0:                 episode reward: -0.6428,                 loss: 0.4235
agent1:                 episode reward: 0.6428,                 loss: 0.3392
Score delta: 2.3954354960344038, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/19424_1.
Episode: 19661/50100 (39.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7535s / 1144.1221 s
agent0:                 episode reward: -0.1389,                 loss: 0.4237
agent1:                 episode reward: 0.1389,                 loss: nan
Episode: 19681/50100 (39.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7665s / 1145.8886 s
agent0:                 episode reward: 0.2018,                 loss: 0.4285
agent1:                 episode reward: -0.2018,                 loss: nan
Episode: 19701/50100 (39.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8099s / 1147.6985 s
agent0:                 episode reward: -0.2128,                 loss: 0.4323
agent1:                 episode reward: 0.2128,                 loss: nan
Episode: 19721/50100 (39.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7663s / 1149.4648 s
agent0:                 episode reward: -0.0237,                 loss: 0.4340
agent1:                 episode reward: 0.0237,                 loss: nan
Episode: 19741/50100 (39.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7553s / 1151.2201 s
agent0:                 episode reward: 0.3887,                 loss: 0.4331
agent1:                 episode reward: -0.3887,                 loss: nan
Episode: 19761/50100 (39.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7592s / 1152.9793 s
agent0:                 episode reward: 0.2914,                 loss: 0.4333
agent1:                 episode reward: -0.2914,                 loss: nan
Episode: 19781/50100 (39.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7780s / 1154.7574 s
agent0:                 episode reward: 0.0916,                 loss: 0.4318
agent1:                 episode reward: -0.0916,                 loss: nan
Episode: 19801/50100 (39.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7682s / 1156.5255 s
agent0:                 episode reward: 0.3843,                 loss: 0.4320
agent1:                 episode reward: -0.3843,                 loss: nan
Episode: 19821/50100 (39.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0456s / 1158.5712 s
agent0:                 episode reward: -0.1091,                 loss: 0.4276
agent1:                 episode reward: 0.1091,                 loss: 0.3612
Score delta: 2.215534899010516, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/19590_0.
Episode: 19841/50100 (39.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1699s / 1160.7411 s
agent0:                 episode reward: -0.5924,                 loss: 0.4491
agent1:                 episode reward: 0.5924,                 loss: 0.3601
Score delta: 2.329148987728509, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/19625_1.
Episode: 19861/50100 (39.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7731s / 1162.5141 s
agent0:                 episode reward: 0.1760,                 loss: 0.4483
agent1:                 episode reward: -0.1760,                 loss: nan
Episode: 19881/50100 (39.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7925s / 1164.3066 s
agent0:                 episode reward: 0.2535,                 loss: 0.4503
agent1:                 episode reward: -0.2535,                 loss: nan
Episode: 19901/50100 (39.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8042s / 1166.1109 s
agent0:                 episode reward: -0.6845,                 loss: 0.4517
agent1:                 episode reward: 0.6845,                 loss: nan
Episode: 19921/50100 (39.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8069s / 1167.9178 s
agent0:                 episode reward: 0.0819,                 loss: 0.4519
agent1:                 episode reward: -0.0819,                 loss: nan
Episode: 19941/50100 (39.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7915s / 1169.7093 s
agent0:                 episode reward: 0.1594,                 loss: 0.4507
agent1:                 episode reward: -0.1594,                 loss: nan
Episode: 19961/50100 (39.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7872s / 1171.4965 s
agent0:                 episode reward: -0.0771,                 loss: 0.4505
agent1:                 episode reward: 0.0771,                 loss: nan
Episode: 19981/50100 (39.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7768s / 1173.2733 s
agent0:                 episode reward: 0.0616,                 loss: 0.4514
agent1:                 episode reward: -0.0616,                 loss: nan
Episode: 20001/50100 (39.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7782s / 1175.0515 s
agent0:                 episode reward: 0.1949,                 loss: 0.4509
agent1:                 episode reward: -0.1949,                 loss: nan
Episode: 20021/50100 (39.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8003s / 1176.8519 s
agent0:                 episode reward: -0.0523,                 loss: 0.4520
agent1:                 episode reward: 0.0523,                 loss: nan
Episode: 20041/50100 (40.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8313s / 1178.6832 s
agent0:                 episode reward: 0.0967,                 loss: 0.4511
agent1:                 episode reward: -0.0967,                 loss: nan
Episode: 20061/50100 (40.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8513s / 1180.5345 s
agent0:                 episode reward: 0.5708,                 loss: 0.4500
agent1:                 episode reward: -0.5708,                 loss: nan
Episode: 20081/50100 (40.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2400s / 1182.7745 s
agent0:                 episode reward: -0.1058,                 loss: 0.4517
agent1:                 episode reward: 0.1058,                 loss: 0.3455
Score delta: 2.185969409786193, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/19851_0.
Episode: 20101/50100 (40.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1553s / 1183.9298 s
agent0:                 episode reward: -0.0086,                 loss: nan
agent1:                 episode reward: 0.0086,                 loss: 0.3215
Episode: 20121/50100 (40.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 2.5130s / 1186.4428 s
agent0:                 episode reward: -0.4433,                 loss: 0.4378
agent1:                 episode reward: 0.4433,                 loss: 0.3196
Score delta: 2.064988343539036, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/19897_1.
Episode: 20141/50100 (40.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7832s / 1188.2260 s
agent0:                 episode reward: -0.0312,                 loss: 0.4373
agent1:                 episode reward: 0.0312,                 loss: nan
Episode: 20161/50100 (40.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7961s / 1190.0221 s
agent0:                 episode reward: 0.5486,                 loss: 0.4375
agent1:                 episode reward: -0.5486,                 loss: nan
Episode: 20181/50100 (40.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7833s / 1191.8053 s
agent0:                 episode reward: 0.1323,                 loss: 0.4357
agent1:                 episode reward: -0.1323,                 loss: nan
Episode: 20201/50100 (40.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8004s / 1193.6058 s
agent0:                 episode reward: -0.1224,                 loss: 0.4362
agent1:                 episode reward: 0.1224,                 loss: nan
Episode: 20221/50100 (40.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7993s / 1195.4051 s
agent0:                 episode reward: -0.0147,                 loss: 0.4370
agent1:                 episode reward: 0.0147,                 loss: nan
Episode: 20241/50100 (40.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8151s / 1197.2202 s
agent0:                 episode reward: -0.2801,                 loss: 0.4368
agent1:                 episode reward: 0.2801,                 loss: nan
Episode: 20261/50100 (40.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8144s / 1199.0346 s
agent0:                 episode reward: 0.2045,                 loss: 0.4266
agent1:                 episode reward: -0.2045,                 loss: nan
Episode: 20281/50100 (40.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8157s / 1200.8503 s
agent0:                 episode reward: 0.1024,                 loss: 0.4155
agent1:                 episode reward: -0.1024,                 loss: nan
Episode: 20301/50100 (40.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8004s / 1202.6506 s
agent0:                 episode reward: 0.3410,                 loss: 0.4151
agent1:                 episode reward: -0.3410,                 loss: nan
Episode: 20321/50100 (40.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8283s / 1204.4789 s
agent0:                 episode reward: 0.3536,                 loss: 0.4157
agent1:                 episode reward: -0.3536,                 loss: nan
Episode: 20341/50100 (40.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8361s / 1206.3150 s
agent0:                 episode reward: 0.4522,                 loss: 0.4132
agent1:                 episode reward: -0.4522,                 loss: nan
Episode: 20361/50100 (40.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 2.6369s / 1208.9519 s
agent0:                 episode reward: 0.3990,                 loss: 0.4136
agent1:                 episode reward: -0.3990,                 loss: 0.3573
Score delta: 2.1146410703952947, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/20143_0.
Episode: 20381/50100 (40.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1376s / 1210.0894 s
agent0:                 episode reward: 0.0292,                 loss: nan
agent1:                 episode reward: -0.0292,                 loss: 0.3520
Episode: 20401/50100 (40.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1511s / 1211.2406 s
agent0:                 episode reward: -0.2167,                 loss: nan
agent1:                 episode reward: 0.2167,                 loss: 0.3494
Episode: 20421/50100 (40.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1465s / 1212.3871 s
agent0:                 episode reward: 0.2363,                 loss: nan
agent1:                 episode reward: -0.2363,                 loss: 0.3463
Episode: 20441/50100 (40.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1384s / 1213.5254 s
agent0:                 episode reward: 0.1680,                 loss: nan
agent1:                 episode reward: -0.1680,                 loss: 0.3443
Episode: 20461/50100 (40.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1355s / 1214.6609 s
agent0:                 episode reward: -0.0621,                 loss: nan
agent1:                 episode reward: 0.0621,                 loss: 0.3440
Episode: 20481/50100 (40.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1322s / 1215.7931 s
agent0:                 episode reward: -0.1860,                 loss: nan
agent1:                 episode reward: 0.1860,                 loss: 0.3422
Episode: 20501/50100 (40.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1371s / 1216.9302 s
agent0:                 episode reward: 0.2444,                 loss: nan
agent1:                 episode reward: -0.2444,                 loss: 0.3548
Episode: 20521/50100 (40.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1406s / 1218.0707 s
agent0:                 episode reward: -0.5923,                 loss: nan
agent1:                 episode reward: 0.5923,                 loss: 0.3450
Episode: 20541/50100 (41.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1387s / 1219.2095 s
agent0:                 episode reward: -0.0152,                 loss: nan
agent1:                 episode reward: 0.0152,                 loss: 0.3435
Episode: 20561/50100 (41.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1398s / 1220.3492 s
agent0:                 episode reward: 0.4328,                 loss: nan
agent1:                 episode reward: -0.4328,                 loss: 0.3423
Episode: 20581/50100 (41.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1297s / 1221.4789 s
agent0:                 episode reward: -0.1063,                 loss: nan
agent1:                 episode reward: 0.1063,                 loss: 0.3424
Episode: 20601/50100 (41.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2014s / 1223.6803 s
agent0:                 episode reward: -0.4025,                 loss: 0.4297
agent1:                 episode reward: 0.4025,                 loss: 0.3403
Score delta: 2.040863782010333, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/20385_1.
Episode: 20621/50100 (41.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8264s / 1225.5067 s
agent0:                 episode reward: -0.0877,                 loss: 0.4297
agent1:                 episode reward: 0.0877,                 loss: nan
Episode: 20641/50100 (41.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8014s / 1227.3081 s
agent0:                 episode reward: -0.2079,                 loss: 0.4275
agent1:                 episode reward: 0.2079,                 loss: nan
Episode: 20661/50100 (41.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8063s / 1229.1144 s
agent0:                 episode reward: 0.0390,                 loss: 0.4311
agent1:                 episode reward: -0.0390,                 loss: nan
Episode: 20681/50100 (41.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8109s / 1230.9253 s
agent0:                 episode reward: 0.0614,                 loss: 0.4484
agent1:                 episode reward: -0.0614,                 loss: nan
Episode: 20701/50100 (41.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8525s / 1232.7778 s
agent0:                 episode reward: -0.2707,                 loss: 0.4477
agent1:                 episode reward: 0.2707,                 loss: nan
Episode: 20721/50100 (41.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8433s / 1234.6211 s
agent0:                 episode reward: -0.0037,                 loss: 0.4470
agent1:                 episode reward: 0.0037,                 loss: nan
Episode: 20741/50100 (41.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8523s / 1236.4734 s
agent0:                 episode reward: -0.6250,                 loss: 0.4473
agent1:                 episode reward: 0.6250,                 loss: nan
Episode: 20761/50100 (41.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8398s / 1238.3132 s
agent0:                 episode reward: -0.1947,                 loss: 0.4461
agent1:                 episode reward: 0.1947,                 loss: nan
Episode: 20781/50100 (41.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8610s / 1240.1742 s
agent0:                 episode reward: 0.1358,                 loss: 0.4466
agent1:                 episode reward: -0.1358,                 loss: nan
Episode: 20801/50100 (41.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8506s / 1242.0248 s
agent0:                 episode reward: -0.4110,                 loss: 0.4456
agent1:                 episode reward: 0.4110,                 loss: nan
Episode: 20821/50100 (41.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9067s / 1243.9315 s
agent0:                 episode reward: -0.2169,                 loss: 0.4473
agent1:                 episode reward: 0.2169,                 loss: nan
Episode: 20841/50100 (41.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8666s / 1245.7981 s
agent0:                 episode reward: -0.5757,                 loss: 0.4470
agent1:                 episode reward: 0.5757,                 loss: nan
Episode: 20861/50100 (41.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8166s / 1247.6147 s
agent0:                 episode reward: -0.6413,                 loss: 0.4464
agent1:                 episode reward: 0.6413,                 loss: nan
Episode: 20881/50100 (41.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8254s / 1249.4401 s
agent0:                 episode reward: -0.0546,                 loss: 0.4464
agent1:                 episode reward: 0.0546,                 loss: nan
Episode: 20901/50100 (41.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 2.4757s / 1251.9158 s
agent0:                 episode reward: 0.6078,                 loss: 0.4470
agent1:                 episode reward: -0.6078,                 loss: 0.3844
Score delta: 2.066672842903251, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/20681_0.
Episode: 20921/50100 (41.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1389s / 1253.0548 s
agent0:                 episode reward: -0.2185,                 loss: nan
agent1:                 episode reward: 0.2185,                 loss: 0.3824
Episode: 20941/50100 (41.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1511s / 1254.2059 s
agent0:                 episode reward: 0.3143,                 loss: nan
agent1:                 episode reward: -0.3143,                 loss: 0.3815
Episode: 20961/50100 (41.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1528s / 1255.3587 s
agent0:                 episode reward: -0.0014,                 loss: nan
agent1:                 episode reward: 0.0014,                 loss: 0.3786
Episode: 20981/50100 (41.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1493s / 1256.5080 s
agent0:                 episode reward: 0.1324,                 loss: nan
agent1:                 episode reward: -0.1324,                 loss: 0.3639
Episode: 21001/50100 (41.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1406s / 1257.6486 s
agent0:                 episode reward: -0.0474,                 loss: nan
agent1:                 episode reward: 0.0474,                 loss: 0.3611
Episode: 21021/50100 (41.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1418s / 1258.7904 s
agent0:                 episode reward: -0.0866,                 loss: nan
agent1:                 episode reward: 0.0866,                 loss: 0.3627
Episode: 21041/50100 (41.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1618s / 1259.9522 s
agent0:                 episode reward: -0.4552,                 loss: nan
agent1:                 episode reward: 0.4552,                 loss: 0.3631
Episode: 21061/50100 (42.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1470s / 1261.0992 s
agent0:                 episode reward: -0.2763,                 loss: nan
agent1:                 episode reward: 0.2763,                 loss: 0.3627
Episode: 21081/50100 (42.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1470s / 1262.2461 s
agent0:                 episode reward: 0.1799,                 loss: nan
agent1:                 episode reward: -0.1799,                 loss: 0.3621
Episode: 21101/50100 (42.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1486s / 1263.3947 s
agent0:                 episode reward: -0.6273,                 loss: nan
agent1:                 episode reward: 0.6273,                 loss: 0.3602
Episode: 21121/50100 (42.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2045s / 1265.5992 s
agent0:                 episode reward: -0.5883,                 loss: 0.4476
agent1:                 episode reward: 0.5883,                 loss: 0.3591
Score delta: 2.35682150225537, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/20906_1.
Episode: 21141/50100 (42.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8430s / 1267.4422 s
agent0:                 episode reward: -0.3003,                 loss: 0.4436
agent1:                 episode reward: 0.3003,                 loss: nan
Episode: 21161/50100 (42.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8630s / 1269.3052 s
agent0:                 episode reward: 0.2106,                 loss: 0.4416
agent1:                 episode reward: -0.2106,                 loss: nan
Episode: 21181/50100 (42.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8591s / 1271.1644 s
agent0:                 episode reward: 0.3653,                 loss: 0.4421
agent1:                 episode reward: -0.3653,                 loss: nan
Episode: 21201/50100 (42.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8638s / 1273.0282 s
agent0:                 episode reward: -0.5972,                 loss: 0.4417
agent1:                 episode reward: 0.5972,                 loss: nan
Episode: 21221/50100 (42.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8405s / 1274.8686 s
agent0:                 episode reward: 0.0507,                 loss: 0.4434
agent1:                 episode reward: -0.0507,                 loss: nan
Episode: 21241/50100 (42.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8354s / 1276.7041 s
agent0:                 episode reward: 0.6739,                 loss: 0.4475
agent1:                 episode reward: -0.6739,                 loss: nan
Episode: 21261/50100 (42.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8441s / 1278.5482 s
agent0:                 episode reward: -0.3493,                 loss: 0.4470
agent1:                 episode reward: 0.3493,                 loss: nan
Episode: 21281/50100 (42.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8563s / 1280.4045 s
agent0:                 episode reward: -0.1665,                 loss: 0.4480
agent1:                 episode reward: 0.1665,                 loss: nan
Episode: 21301/50100 (42.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8475s / 1282.2520 s
agent0:                 episode reward: 0.1826,                 loss: 0.4480
agent1:                 episode reward: -0.1826,                 loss: nan
Episode: 21321/50100 (42.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8449s / 1284.0969 s
agent0:                 episode reward: 0.0784,                 loss: 0.4463
agent1:                 episode reward: -0.0784,                 loss: nan
Episode: 21341/50100 (42.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8420s / 1285.9389 s
agent0:                 episode reward: -0.1356,                 loss: 0.4465
agent1:                 episode reward: 0.1356,                 loss: nan
Episode: 21361/50100 (42.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8528s / 1287.7917 s
agent0:                 episode reward: 0.3387,                 loss: 0.4473
agent1:                 episode reward: -0.3387,                 loss: nan
Episode: 21381/50100 (42.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8538s / 1289.6455 s
agent0:                 episode reward: 0.2401,                 loss: 0.4470
agent1:                 episode reward: -0.2401,                 loss: nan
Episode: 21401/50100 (42.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8894s / 1291.5349 s
agent0:                 episode reward: 0.1183,                 loss: 0.4450
agent1:                 episode reward: -0.1183,                 loss: nan
Episode: 21421/50100 (42.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9062s / 1293.4411 s
agent0:                 episode reward: 0.2135,                 loss: 0.4451
agent1:                 episode reward: -0.2135,                 loss: nan
Episode: 21441/50100 (42.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8973s / 1295.3384 s
agent0:                 episode reward: -0.3025,                 loss: 0.4441
agent1:                 episode reward: 0.3025,                 loss: nan
Episode: 21461/50100 (42.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8619s / 1297.2003 s
agent0:                 episode reward: -0.2539,                 loss: 0.4438
agent1:                 episode reward: 0.2539,                 loss: nan
Episode: 21481/50100 (42.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8721s / 1299.0724 s
agent0:                 episode reward: -0.5603,                 loss: 0.4445
agent1:                 episode reward: 0.5603,                 loss: nan
Episode: 21501/50100 (42.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8588s / 1300.9312 s
agent0:                 episode reward: 0.1389,                 loss: 0.4451
agent1:                 episode reward: -0.1389,                 loss: nan
Episode: 21521/50100 (42.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8710s / 1302.8022 s
agent0:                 episode reward: -0.3303,                 loss: 0.4436
agent1:                 episode reward: 0.3303,                 loss: nan
Episode: 21541/50100 (42.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8847s / 1304.6869 s
agent0:                 episode reward: -0.0425,                 loss: 0.4441
agent1:                 episode reward: 0.0425,                 loss: nan
Episode: 21561/50100 (43.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8667s / 1306.5537 s
agent0:                 episode reward: 0.0652,                 loss: 0.4431
agent1:                 episode reward: -0.0652,                 loss: nan
Episode: 21581/50100 (43.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8742s / 1308.4279 s
agent0:                 episode reward: -0.0276,                 loss: 0.4405
agent1:                 episode reward: 0.0276,                 loss: nan
Episode: 21601/50100 (43.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8875s / 1310.3154 s
agent0:                 episode reward: 0.1341,                 loss: 0.4412
agent1:                 episode reward: -0.1341,                 loss: nan
Episode: 21621/50100 (43.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8667s / 1312.1821 s
agent0:                 episode reward: -0.0262,                 loss: 0.4413
agent1:                 episode reward: 0.0262,                 loss: nan
Episode: 21641/50100 (43.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8747s / 1314.0568 s
agent0:                 episode reward: 0.4575,                 loss: 0.4420
agent1:                 episode reward: -0.4575,                 loss: nan
Episode: 21661/50100 (43.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8805s / 1315.9373 s
agent0:                 episode reward: -0.5019,                 loss: 0.4415
agent1:                 episode reward: 0.5019,                 loss: nan
Episode: 21681/50100 (43.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8789s / 1317.8162 s
agent0:                 episode reward: -0.0305,                 loss: 0.4418
agent1:                 episode reward: 0.0305,                 loss: nan
Episode: 21701/50100 (43.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8667s / 1319.6829 s
agent0:                 episode reward: 0.2031,                 loss: 0.4425
agent1:                 episode reward: -0.2031,                 loss: nan
Episode: 21721/50100 (43.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8869s / 1321.5698 s
agent0:                 episode reward: 0.5376,                 loss: 0.4403
agent1:                 episode reward: -0.5376,                 loss: nan
Episode: 21741/50100 (43.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8917s / 1323.4615 s
agent0:                 episode reward: 0.4124,                 loss: 0.4383
agent1:                 episode reward: -0.4124,                 loss: nan
Episode: 21761/50100 (43.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8970s / 1325.3585 s
agent0:                 episode reward: 0.5629,                 loss: 0.4380
agent1:                 episode reward: -0.5629,                 loss: nan
Episode: 21781/50100 (43.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2022s / 1327.5607 s
agent0:                 episode reward: -0.1461,                 loss: 0.4391
agent1:                 episode reward: 0.1461,                 loss: 0.3418
Score delta: 2.0739608242683323, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/21552_0.
Episode: 21801/50100 (43.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 2.7839s / 1330.3446 s
agent0:                 episode reward: -0.0611,                 loss: 0.4296
agent1:                 episode reward: 0.0611,                 loss: 0.3413
Score delta: 2.060330376932846, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/21573_1.
Episode: 21821/50100 (43.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9020s / 1332.2466 s
agent0:                 episode reward: 0.1288,                 loss: 0.4296
agent1:                 episode reward: -0.1288,                 loss: nan
Episode: 21841/50100 (43.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8834s / 1334.1300 s
agent0:                 episode reward: -0.0392,                 loss: 0.4286
agent1:                 episode reward: 0.0392,                 loss: nan
Episode: 21861/50100 (43.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8928s / 1336.0228 s
agent0:                 episode reward: -1.0589,                 loss: 0.4274
agent1:                 episode reward: 1.0589,                 loss: nan
Episode: 21881/50100 (43.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 2.6278s / 1338.6506 s
agent0:                 episode reward: 0.5456,                 loss: 0.4280
agent1:                 episode reward: -0.5456,                 loss: 0.3090
Score delta: 2.01801491615967, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/21663_0.
Episode: 21901/50100 (43.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1533s / 1339.8039 s
agent0:                 episode reward: -0.1326,                 loss: nan
agent1:                 episode reward: 0.1326,                 loss: 0.3117
Episode: 21921/50100 (43.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1696s / 1340.9735 s
agent0:                 episode reward: -0.2916,                 loss: nan
agent1:                 episode reward: 0.2916,                 loss: 0.3085
Episode: 21941/50100 (43.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1772s / 1342.1507 s
agent0:                 episode reward: 0.0666,                 loss: nan
agent1:                 episode reward: -0.0666,                 loss: 0.3069
Episode: 21961/50100 (43.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1762s / 1343.3268 s
agent0:                 episode reward: 0.1158,                 loss: nan
agent1:                 episode reward: -0.1158,                 loss: 0.3066
Episode: 21981/50100 (43.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1711s / 1344.4979 s
agent0:                 episode reward: -0.1330,                 loss: nan
agent1:                 episode reward: 0.1330,                 loss: 0.3065
Episode: 22001/50100 (43.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1777s / 1345.6755 s
agent0:                 episode reward: -0.3122,                 loss: nan
agent1:                 episode reward: 0.3122,                 loss: 0.3070
Episode: 22021/50100 (43.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1777s / 1346.8532 s
agent0:                 episode reward: -0.3680,                 loss: nan
agent1:                 episode reward: 0.3680,                 loss: 0.3257
Episode: 22041/50100 (43.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1642s / 1348.0174 s
agent0:                 episode reward: -0.3388,                 loss: nan
agent1:                 episode reward: 0.3388,                 loss: 0.3697
Episode: 22061/50100 (44.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1546s / 1349.1720 s
agent0:                 episode reward: -0.3168,                 loss: nan
agent1:                 episode reward: 0.3168,                 loss: 0.3688
Episode: 22081/50100 (44.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1562s / 1350.3282 s
agent0:                 episode reward: -0.5548,                 loss: nan
agent1:                 episode reward: 0.5548,                 loss: 0.3677
Episode: 22101/50100 (44.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1643s / 1351.4925 s
agent0:                 episode reward: -0.3124,                 loss: nan
agent1:                 episode reward: 0.3124,                 loss: 0.3681
Episode: 22121/50100 (44.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1627s / 1352.6552 s
agent0:                 episode reward: -0.6397,                 loss: nan
agent1:                 episode reward: 0.6397,                 loss: 0.3655
Episode: 22141/50100 (44.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1572s / 1353.8124 s
agent0:                 episode reward: -0.0485,                 loss: nan
agent1:                 episode reward: 0.0485,                 loss: 0.3670
Episode: 22161/50100 (44.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1666s / 1354.9789 s
agent0:                 episode reward: -0.6110,                 loss: nan
agent1:                 episode reward: 0.6110,                 loss: 0.3682
Episode: 22181/50100 (44.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1814s / 1356.1603 s
agent0:                 episode reward: -0.2989,                 loss: nan
agent1:                 episode reward: 0.2989,                 loss: 0.3671
Episode: 22201/50100 (44.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 2.6519s / 1358.8122 s
agent0:                 episode reward: -0.3747,                 loss: 0.4519
agent1:                 episode reward: 0.3747,                 loss: 0.3994
Score delta: 2.348059762067001, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/21976_1.
Episode: 22221/50100 (44.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9700s / 1360.7822 s
agent0:                 episode reward: -0.2415,                 loss: 0.4505
agent1:                 episode reward: 0.2415,                 loss: nan
Episode: 22241/50100 (44.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9566s / 1362.7388 s
agent0:                 episode reward: -0.7860,                 loss: 0.4523
agent1:                 episode reward: 0.7860,                 loss: nan
Episode: 22261/50100 (44.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9204s / 1364.6591 s
agent0:                 episode reward: -0.3914,                 loss: 0.4523
agent1:                 episode reward: 0.3914,                 loss: nan
Episode: 22281/50100 (44.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8964s / 1366.5555 s
agent0:                 episode reward: -0.1319,                 loss: 0.4522
agent1:                 episode reward: 0.1319,                 loss: nan
Episode: 22301/50100 (44.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8974s / 1368.4530 s
agent0:                 episode reward: -0.4908,                 loss: 0.4515
agent1:                 episode reward: 0.4908,                 loss: nan
Episode: 22321/50100 (44.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8897s / 1370.3426 s
agent0:                 episode reward: -0.4762,                 loss: 0.4515
agent1:                 episode reward: 0.4762,                 loss: nan
Episode: 22341/50100 (44.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8937s / 1372.2364 s
agent0:                 episode reward: 0.0160,                 loss: 0.4511
agent1:                 episode reward: -0.0160,                 loss: nan
Episode: 22361/50100 (44.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8950s / 1374.1314 s
agent0:                 episode reward: 0.1714,                 loss: 0.4514
agent1:                 episode reward: -0.1714,                 loss: nan
Episode: 22381/50100 (44.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9081s / 1376.0395 s
agent0:                 episode reward: 0.4057,                 loss: 0.4505
agent1:                 episode reward: -0.4057,                 loss: nan
Episode: 22401/50100 (44.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9317s / 1377.9712 s
agent0:                 episode reward: -0.2910,                 loss: 0.4506
agent1:                 episode reward: 0.2910,                 loss: nan
Episode: 22421/50100 (44.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9265s / 1379.8977 s
agent0:                 episode reward: -0.1033,                 loss: 0.4497
agent1:                 episode reward: 0.1033,                 loss: nan
Episode: 22441/50100 (44.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9249s / 1381.8226 s
agent0:                 episode reward: -0.9514,                 loss: 0.4483
agent1:                 episode reward: 0.9514,                 loss: nan
Episode: 22461/50100 (44.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9682s / 1383.7908 s
agent0:                 episode reward: -0.2480,                 loss: 0.4481
agent1:                 episode reward: 0.2480,                 loss: nan
Episode: 22481/50100 (44.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9484s / 1385.7393 s
agent0:                 episode reward: -0.0697,                 loss: 0.4462
agent1:                 episode reward: 0.0697,                 loss: nan
Episode: 22501/50100 (44.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9545s / 1387.6938 s
agent0:                 episode reward: -0.1764,                 loss: 0.4458
agent1:                 episode reward: 0.1764,                 loss: nan
Episode: 22521/50100 (44.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9667s / 1389.6604 s
agent0:                 episode reward: 0.4571,                 loss: 0.4474
agent1:                 episode reward: -0.4571,                 loss: nan
Episode: 22541/50100 (44.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9333s / 1391.5937 s
agent0:                 episode reward: -0.0773,                 loss: 0.4481
agent1:                 episode reward: 0.0773,                 loss: nan
Episode: 22561/50100 (45.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9491s / 1393.5428 s
agent0:                 episode reward: -0.0610,                 loss: 0.4416
agent1:                 episode reward: 0.0610,                 loss: nan
Episode: 22581/50100 (45.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9215s / 1395.4644 s
agent0:                 episode reward: -0.4384,                 loss: 0.4350
agent1:                 episode reward: 0.4384,                 loss: nan
Episode: 22601/50100 (45.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9219s / 1397.3862 s
agent0:                 episode reward: -0.6216,                 loss: 0.4341
agent1:                 episode reward: 0.6216,                 loss: nan
Episode: 22621/50100 (45.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9135s / 1399.2998 s
agent0:                 episode reward: -0.0164,                 loss: 0.4340
agent1:                 episode reward: 0.0164,                 loss: nan
Episode: 22641/50100 (45.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9238s / 1401.2236 s
agent0:                 episode reward: -0.0752,                 loss: 0.4337
agent1:                 episode reward: 0.0752,                 loss: nan
Episode: 22661/50100 (45.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9264s / 1403.1500 s
agent0:                 episode reward: 0.2140,                 loss: 0.4345
agent1:                 episode reward: -0.2140,                 loss: nan
Episode: 22681/50100 (45.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9241s / 1405.0741 s
agent0:                 episode reward: -0.0139,                 loss: 0.4328
agent1:                 episode reward: 0.0139,                 loss: nan
Episode: 22701/50100 (45.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9345s / 1407.0086 s
agent0:                 episode reward: 0.5896,                 loss: 0.4339
agent1:                 episode reward: -0.5896,                 loss: nan
Episode: 22721/50100 (45.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3317s / 1409.3403 s
agent0:                 episode reward: 0.5037,                 loss: 0.4337
agent1:                 episode reward: -0.5037,                 loss: 0.3624
Score delta: 2.408544457474798, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/22495_0.
Episode: 22741/50100 (45.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1822s / 1410.5225 s
agent0:                 episode reward: -0.0695,                 loss: nan
agent1:                 episode reward: 0.0695,                 loss: 0.3609
Episode: 22761/50100 (45.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1599s / 1411.6824 s
agent0:                 episode reward: -0.5565,                 loss: nan
agent1:                 episode reward: 0.5565,                 loss: 0.3593
Episode: 22781/50100 (45.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3101s / 1413.9924 s
agent0:                 episode reward: -0.6437,                 loss: 0.4292
agent1:                 episode reward: 0.6437,                 loss: 0.3578
Score delta: 2.0054435993665023, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/22565_1.
Episode: 22801/50100 (45.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9483s / 1416.9407 s
agent0:                 episode reward: 0.6708,                 loss: 0.4424
agent1:                 episode reward: -0.6708,                 loss: 0.3606
Score delta: 2.02109381286835, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/22588_0.
Episode: 22821/50100 (45.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2041s / 1418.1448 s
agent0:                 episode reward: -0.1379,                 loss: nan
agent1:                 episode reward: 0.1379,                 loss: 0.3687
Episode: 22841/50100 (45.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1836s / 1419.3284 s
agent0:                 episode reward: -0.3614,                 loss: nan
agent1:                 episode reward: 0.3614,                 loss: 0.3646
Episode: 22861/50100 (45.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1820s / 1420.5104 s
agent0:                 episode reward: -0.3917,                 loss: nan
agent1:                 episode reward: 0.3917,                 loss: 0.3633
Episode: 22881/50100 (45.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1779s / 1421.6883 s
agent0:                 episode reward: -0.0509,                 loss: nan
agent1:                 episode reward: 0.0509,                 loss: 0.3612
Episode: 22901/50100 (45.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1831s / 1422.8714 s
agent0:                 episode reward: -0.1114,                 loss: nan
agent1:                 episode reward: 0.1114,                 loss: 0.3607
Episode: 22921/50100 (45.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3342s / 1425.2057 s
agent0:                 episode reward: -0.3020,                 loss: 0.4304
agent1:                 episode reward: 0.3020,                 loss: 0.3472
Score delta: 2.105940917736571, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/22705_1.
Episode: 22941/50100 (45.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9759s / 1427.1816 s
agent0:                 episode reward: 0.1562,                 loss: 0.4298
agent1:                 episode reward: -0.1562,                 loss: nan
Episode: 22961/50100 (45.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9587s / 1429.1404 s
agent0:                 episode reward: -0.3301,                 loss: 0.4304
agent1:                 episode reward: 0.3301,                 loss: nan
Episode: 22981/50100 (45.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9675s / 1431.1078 s
agent0:                 episode reward: -0.0144,                 loss: 0.4281
agent1:                 episode reward: 0.0144,                 loss: nan
Episode: 23001/50100 (45.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9446s / 1433.0525 s
agent0:                 episode reward: 0.0483,                 loss: 0.4294
agent1:                 episode reward: -0.0483,                 loss: nan
Episode: 23021/50100 (45.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9598s / 1435.0122 s
agent0:                 episode reward: -0.0715,                 loss: 0.4289
agent1:                 episode reward: 0.0715,                 loss: nan
Episode: 23041/50100 (45.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9523s / 1436.9645 s
agent0:                 episode reward: -0.7290,                 loss: 0.4279
agent1:                 episode reward: 0.7290,                 loss: nan
Episode: 23061/50100 (46.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9646s / 1438.9292 s
agent0:                 episode reward: -0.1379,                 loss: 0.4292
agent1:                 episode reward: 0.1379,                 loss: nan
Episode: 23081/50100 (46.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9571s / 1440.8863 s
agent0:                 episode reward: -0.0239,                 loss: 0.4376
agent1:                 episode reward: 0.0239,                 loss: nan
Episode: 23101/50100 (46.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9679s / 1442.8542 s
agent0:                 episode reward: -0.1007,                 loss: 0.4465
agent1:                 episode reward: 0.1007,                 loss: nan
Episode: 23121/50100 (46.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9700s / 1444.8241 s
agent0:                 episode reward: 0.0563,                 loss: 0.4471
agent1:                 episode reward: -0.0563,                 loss: nan
Episode: 23141/50100 (46.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9589s / 1446.7830 s
agent0:                 episode reward: -0.6486,                 loss: 0.4470
agent1:                 episode reward: 0.6486,                 loss: nan
Episode: 23161/50100 (46.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9527s / 1448.7357 s
agent0:                 episode reward: 0.2128,                 loss: 0.4464
agent1:                 episode reward: -0.2128,                 loss: nan
Episode: 23181/50100 (46.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9605s / 1450.6962 s
agent0:                 episode reward: -0.2088,                 loss: 0.4461
agent1:                 episode reward: 0.2088,                 loss: nan
Episode: 23201/50100 (46.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9765s / 1452.6727 s
agent0:                 episode reward: 0.1798,                 loss: 0.4463
agent1:                 episode reward: -0.1798,                 loss: nan
Episode: 23221/50100 (46.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9631s / 1454.6358 s
agent0:                 episode reward: -0.1606,                 loss: 0.4467
agent1:                 episode reward: 0.1606,                 loss: nan
Episode: 23241/50100 (46.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9891s / 1456.6249 s
agent0:                 episode reward: 0.1795,                 loss: 0.4450
agent1:                 episode reward: -0.1795,                 loss: nan
Episode: 23261/50100 (46.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9695s / 1458.5944 s
agent0:                 episode reward: -0.2447,                 loss: 0.4490
agent1:                 episode reward: 0.2447,                 loss: nan
Episode: 23281/50100 (46.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9652s / 1460.5595 s
agent0:                 episode reward: 0.4467,                 loss: 0.4494
agent1:                 episode reward: -0.4467,                 loss: nan
Episode: 23301/50100 (46.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0496s / 1462.6091 s
agent0:                 episode reward: 0.1957,                 loss: 0.4499
agent1:                 episode reward: -0.1957,                 loss: nan
Episode: 23321/50100 (46.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9780s / 1464.5871 s
agent0:                 episode reward: 0.1031,                 loss: 0.4490
agent1:                 episode reward: -0.1031,                 loss: nan
Episode: 23341/50100 (46.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9804s / 1466.5675 s
agent0:                 episode reward: 0.4413,                 loss: 0.4500
agent1:                 episode reward: -0.4413,                 loss: nan
Episode: 23361/50100 (46.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0196s / 1468.5871 s
agent0:                 episode reward: -0.3187,                 loss: 0.4489
agent1:                 episode reward: 0.3187,                 loss: nan
Episode: 23381/50100 (46.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0279s / 1470.6151 s
agent0:                 episode reward: -0.6579,                 loss: 0.4485
agent1:                 episode reward: 0.6579,                 loss: nan
Episode: 23401/50100 (46.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0002s / 1472.6153 s
agent0:                 episode reward: 0.0036,                 loss: 0.4494
agent1:                 episode reward: -0.0036,                 loss: nan
Episode: 23421/50100 (46.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9621s / 1475.5774 s
agent0:                 episode reward: 0.4456,                 loss: 0.4482
agent1:                 episode reward: -0.4456,                 loss: 0.3375
Score delta: 2.082496107839462, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/23207_0.
Episode: 23441/50100 (46.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1974s / 1476.7748 s
agent0:                 episode reward: -0.3136,                 loss: nan
agent1:                 episode reward: 0.3136,                 loss: 0.3277
Episode: 23461/50100 (46.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1968s / 1477.9716 s
agent0:                 episode reward: -0.1241,                 loss: nan
agent1:                 episode reward: 0.1241,                 loss: 0.3266
Episode: 23481/50100 (46.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1980s / 1479.1696 s
agent0:                 episode reward: 0.1087,                 loss: nan
agent1:                 episode reward: -0.1087,                 loss: 0.3227
Episode: 23501/50100 (46.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2015s / 1480.3711 s
agent0:                 episode reward: 0.3992,                 loss: nan
agent1:                 episode reward: -0.3992,                 loss: 0.3239
Episode: 23521/50100 (46.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 2.5373s / 1482.9084 s
agent0:                 episode reward: -0.7830,                 loss: 0.4325
agent1:                 episode reward: 0.7830,                 loss: 0.3210
Score delta: 2.08851326074398, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/23302_1.
Episode: 23541/50100 (46.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9606s / 1484.8689 s
agent0:                 episode reward: -0.4947,                 loss: 0.4281
agent1:                 episode reward: 0.4947,                 loss: nan
Episode: 23561/50100 (47.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9787s / 1486.8476 s
agent0:                 episode reward: -0.2296,                 loss: 0.4286
agent1:                 episode reward: 0.2296,                 loss: nan
Episode: 23581/50100 (47.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9791s / 1488.8267 s
agent0:                 episode reward: 0.2698,                 loss: 0.4268
agent1:                 episode reward: -0.2698,                 loss: nan
Episode: 23601/50100 (47.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9666s / 1490.7933 s
agent0:                 episode reward: -0.2797,                 loss: 0.4283
agent1:                 episode reward: 0.2797,                 loss: nan
Episode: 23621/50100 (47.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9692s / 1492.7625 s
agent0:                 episode reward: -0.0393,                 loss: 0.4267
agent1:                 episode reward: 0.0393,                 loss: nan
Episode: 23641/50100 (47.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9722s / 1494.7348 s
agent0:                 episode reward: 0.2299,                 loss: 0.4261
agent1:                 episode reward: -0.2299,                 loss: nan
Episode: 23661/50100 (47.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9901s / 1496.7249 s
agent0:                 episode reward: 0.1415,                 loss: 0.4280
agent1:                 episode reward: -0.1415,                 loss: nan
Episode: 23681/50100 (47.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0007s / 1498.7256 s
agent0:                 episode reward: -0.6051,                 loss: 0.4072
agent1:                 episode reward: 0.6051,                 loss: nan
Episode: 23701/50100 (47.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9994s / 1500.7250 s
agent0:                 episode reward: -0.3269,                 loss: 0.3960
agent1:                 episode reward: 0.3269,                 loss: nan
Episode: 23721/50100 (47.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9725s / 1502.6974 s
agent0:                 episode reward: -0.5307,                 loss: 0.3939
agent1:                 episode reward: 0.5307,                 loss: nan
Episode: 23741/50100 (47.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9804s / 1504.6779 s
agent0:                 episode reward: -0.4154,                 loss: 0.3916
agent1:                 episode reward: 0.4154,                 loss: nan
Episode: 23761/50100 (47.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9811s / 1506.6590 s
agent0:                 episode reward: -0.3000,                 loss: 0.3944
agent1:                 episode reward: 0.3000,                 loss: nan
Episode: 23781/50100 (47.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8502s / 1509.5092 s
agent0:                 episode reward: 0.4416,                 loss: 0.3943
agent1:                 episode reward: -0.4416,                 loss: 0.3236
Score delta: 2.3039549425315284, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/23566_0.
Episode: 23801/50100 (47.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2086s / 1510.7178 s
agent0:                 episode reward: -0.4114,                 loss: nan
agent1:                 episode reward: 0.4114,                 loss: 0.3260
Episode: 23821/50100 (47.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9274s / 1513.6452 s
agent0:                 episode reward: 0.1532,                 loss: 0.4387
agent1:                 episode reward: -0.1532,                 loss: 0.3212
Score delta: 2.3752866566370807, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/23593_1.
Episode: 23841/50100 (47.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9936s / 1515.6387 s
agent0:                 episode reward: -0.0258,                 loss: 0.4387
agent1:                 episode reward: 0.0258,                 loss: nan
Episode: 23861/50100 (47.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9829s / 1517.6216 s
agent0:                 episode reward: -0.1333,                 loss: 0.4380
agent1:                 episode reward: 0.1333,                 loss: nan
Episode: 23881/50100 (47.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9936s / 1519.6152 s
agent0:                 episode reward: -0.2827,                 loss: 0.4277
agent1:                 episode reward: 0.2827,                 loss: nan
Episode: 23901/50100 (47.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9993s / 1521.6144 s
agent0:                 episode reward: 0.3103,                 loss: 0.4252
agent1:                 episode reward: -0.3103,                 loss: nan
Episode: 23921/50100 (47.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0002s / 1523.6146 s
agent0:                 episode reward: 0.3512,                 loss: 0.4260
agent1:                 episode reward: -0.3512,                 loss: nan
Episode: 23941/50100 (47.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0096s / 1525.6242 s
agent0:                 episode reward: -0.0381,                 loss: 0.4249
agent1:                 episode reward: 0.0381,                 loss: nan
Episode: 23961/50100 (47.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9848s / 1527.6091 s
agent0:                 episode reward: -0.1159,                 loss: 0.4249
agent1:                 episode reward: 0.1159,                 loss: nan
Episode: 23981/50100 (47.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9857s / 1529.5947 s
agent0:                 episode reward: 0.1895,                 loss: 0.4263
agent1:                 episode reward: -0.1895,                 loss: nan
Episode: 24001/50100 (47.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9853s / 1531.5801 s
agent0:                 episode reward: -0.0447,                 loss: 0.4241
agent1:                 episode reward: 0.0447,                 loss: nan
Episode: 24021/50100 (47.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9749s / 1533.5550 s
agent0:                 episode reward: 0.0636,                 loss: 0.4254
agent1:                 episode reward: -0.0636,                 loss: nan
Episode: 24041/50100 (47.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9801s / 1535.5351 s
agent0:                 episode reward: 0.3756,                 loss: 0.4203
agent1:                 episode reward: -0.3756,                 loss: nan
Episode: 24061/50100 (48.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0011s / 1537.5362 s
agent0:                 episode reward: -0.1495,                 loss: 0.4197
agent1:                 episode reward: 0.1495,                 loss: nan
Episode: 24081/50100 (48.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0004s / 1539.5366 s
agent0:                 episode reward: -0.0632,                 loss: 0.4192
agent1:                 episode reward: 0.0632,                 loss: nan
Episode: 24101/50100 (48.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0006s / 1541.5372 s
agent0:                 episode reward: 0.1482,                 loss: 0.4173
agent1:                 episode reward: -0.1482,                 loss: nan
Episode: 24121/50100 (48.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0086s / 1543.5457 s
agent0:                 episode reward: 0.0487,                 loss: 0.4192
agent1:                 episode reward: -0.0487,                 loss: nan
Episode: 24141/50100 (48.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0139s / 1545.5596 s
agent0:                 episode reward: 0.2104,                 loss: 0.4185
agent1:                 episode reward: -0.2104,                 loss: nan
Episode: 24161/50100 (48.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9829s / 1547.5425 s
agent0:                 episode reward: -0.2959,                 loss: 0.4190
agent1:                 episode reward: 0.2959,                 loss: nan
Episode: 24181/50100 (48.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 2.5259s / 1550.0684 s
agent0:                 episode reward: 0.7655,                 loss: 0.4164
agent1:                 episode reward: -0.7655,                 loss: 0.3662
Score delta: 2.0647738134472915, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/23957_0.
Episode: 24201/50100 (48.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2010s / 1551.2695 s
agent0:                 episode reward: -0.1293,                 loss: nan
agent1:                 episode reward: 0.1293,                 loss: 0.3707
Episode: 24221/50100 (48.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9631s / 1554.2326 s
agent0:                 episode reward: -0.3608,                 loss: 0.3940
agent1:                 episode reward: 0.3608,                 loss: 0.3619
Score delta: 2.170134108336896, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/23991_1.
Episode: 24241/50100 (48.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0393s / 1556.2719 s
agent0:                 episode reward: -0.1804,                 loss: 0.4236
agent1:                 episode reward: 0.1804,                 loss: nan
Episode: 24261/50100 (48.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0073s / 1558.2792 s
agent0:                 episode reward: -0.3174,                 loss: 0.4366
agent1:                 episode reward: 0.3174,                 loss: nan
Episode: 24281/50100 (48.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0091s / 1560.2883 s
agent0:                 episode reward: -0.1612,                 loss: 0.4356
agent1:                 episode reward: 0.1612,                 loss: nan
Episode: 24301/50100 (48.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0159s / 1562.3042 s
agent0:                 episode reward: -0.5395,                 loss: 0.4344
agent1:                 episode reward: 0.5395,                 loss: nan
Episode: 24321/50100 (48.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9871s / 1564.2913 s
agent0:                 episode reward: -0.0188,                 loss: 0.4352
agent1:                 episode reward: 0.0188,                 loss: nan
Episode: 24341/50100 (48.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0091s / 1566.3004 s
agent0:                 episode reward: -0.1077,                 loss: 0.4348
agent1:                 episode reward: 0.1077,                 loss: nan
Episode: 24361/50100 (48.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0043s / 1568.3047 s
agent0:                 episode reward: 0.1862,                 loss: 0.4353
agent1:                 episode reward: -0.1862,                 loss: nan
Episode: 24381/50100 (48.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0152s / 1570.3199 s
agent0:                 episode reward: 0.3497,                 loss: 0.4335
agent1:                 episode reward: -0.3497,                 loss: nan
Episode: 24401/50100 (48.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 2.5086s / 1572.8285 s
agent0:                 episode reward: -0.3692,                 loss: 0.4370
agent1:                 episode reward: 0.3692,                 loss: 0.3844
Score delta: 2.164478595730553, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/24176_0.
Episode: 24421/50100 (48.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 2.7466s / 1575.5751 s
agent0:                 episode reward: -0.3487,                 loss: 0.4260
agent1:                 episode reward: 0.3487,                 loss: 0.3840
Score delta: 2.094457268719781, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/24197_1.
Episode: 24441/50100 (48.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9877s / 1577.5629 s
agent0:                 episode reward: -0.6427,                 loss: 0.4100
agent1:                 episode reward: 0.6427,                 loss: nan
Episode: 24461/50100 (48.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9927s / 1579.5555 s
agent0:                 episode reward: 0.0812,                 loss: 0.4083
agent1:                 episode reward: -0.0812,                 loss: nan
Episode: 24481/50100 (48.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9826s / 1581.5381 s
agent0:                 episode reward: -0.6209,                 loss: 0.4066
agent1:                 episode reward: 0.6209,                 loss: nan
Episode: 24501/50100 (48.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9711s / 1583.5092 s
agent0:                 episode reward: 0.0068,                 loss: 0.4087
agent1:                 episode reward: -0.0068,                 loss: nan
Episode: 24521/50100 (48.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 2.5989s / 1586.1081 s
agent0:                 episode reward: -0.1999,                 loss: 0.4082
agent1:                 episode reward: 0.1999,                 loss: 0.3255
Score delta: 2.039576947058092, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/24296_0.
Episode: 24541/50100 (48.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3713s / 1588.4794 s
agent0:                 episode reward: -0.8785,                 loss: 0.4184
agent1:                 episode reward: 0.8785,                 loss: 0.3259
Score delta: 2.089270939135103, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/24328_1.
Episode: 24561/50100 (49.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0348s / 1590.5142 s
agent0:                 episode reward: -0.1409,                 loss: 0.4116
agent1:                 episode reward: 0.1409,                 loss: nan
Episode: 24581/50100 (49.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0064s / 1592.5207 s
agent0:                 episode reward: -0.1050,                 loss: 0.4070
agent1:                 episode reward: 0.1050,                 loss: nan
Episode: 24601/50100 (49.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0462s / 1594.5668 s
agent0:                 episode reward: -0.6474,                 loss: 0.4087
agent1:                 episode reward: 0.6474,                 loss: nan
Episode: 24621/50100 (49.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0159s / 1596.5827 s
agent0:                 episode reward: -0.3033,                 loss: 0.4254
agent1:                 episode reward: 0.3033,                 loss: nan
Episode: 24641/50100 (49.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0000s / 1598.5827 s
agent0:                 episode reward: -0.5273,                 loss: 0.4547
agent1:                 episode reward: 0.5273,                 loss: nan
Episode: 24661/50100 (49.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9937s / 1600.5765 s
agent0:                 episode reward: -0.4454,                 loss: 0.4540
agent1:                 episode reward: 0.4454,                 loss: nan
Episode: 24681/50100 (49.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9890s / 1602.5655 s
agent0:                 episode reward: -0.0001,                 loss: 0.4536
agent1:                 episode reward: 0.0001,                 loss: nan
Episode: 24701/50100 (49.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0195s / 1604.5849 s
agent0:                 episode reward: -0.0973,                 loss: 0.4530
agent1:                 episode reward: 0.0973,                 loss: nan
Episode: 24721/50100 (49.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0079s / 1606.5929 s
agent0:                 episode reward: -0.6372,                 loss: 0.4535
agent1:                 episode reward: 0.6372,                 loss: nan
Episode: 24741/50100 (49.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9866s / 1608.5795 s
agent0:                 episode reward: 0.0789,                 loss: 0.4529
agent1:                 episode reward: -0.0789,                 loss: nan
Episode: 24761/50100 (49.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9974s / 1610.5769 s
agent0:                 episode reward: 0.3211,                 loss: 0.4535
agent1:                 episode reward: -0.3211,                 loss: nan
Episode: 24781/50100 (49.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 2.7570s / 1613.3339 s
agent0:                 episode reward: 0.3255,                 loss: 0.4539
agent1:                 episode reward: -0.3255,                 loss: 0.3561
Score delta: 2.1759441826236925, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/24562_0.
Episode: 24801/50100 (49.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 2.4579s / 1615.7917 s
agent0:                 episode reward: -0.8748,                 loss: 0.4326
agent1:                 episode reward: 0.8748,                 loss: 0.3521
Score delta: 2.07664317569361, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/24585_1.
Episode: 24821/50100 (49.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0254s / 1617.8171 s
agent0:                 episode reward: 0.1228,                 loss: 0.4408
agent1:                 episode reward: -0.1228,                 loss: nan
Episode: 24841/50100 (49.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0084s / 1619.8255 s
agent0:                 episode reward: 0.0730,                 loss: 0.4426
agent1:                 episode reward: -0.0730,                 loss: nan
Episode: 24861/50100 (49.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9996s / 1621.8251 s
agent0:                 episode reward: 0.0053,                 loss: 0.4426
agent1:                 episode reward: -0.0053,                 loss: nan
Episode: 24881/50100 (49.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0083s / 1623.8334 s
agent0:                 episode reward: 0.3282,                 loss: 0.4430
agent1:                 episode reward: -0.3282,                 loss: nan
Episode: 24901/50100 (49.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0003s / 1625.8337 s
agent0:                 episode reward: 0.0618,                 loss: 0.4446
agent1:                 episode reward: -0.0618,                 loss: nan
Episode: 24921/50100 (49.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9946s / 1627.8284 s
agent0:                 episode reward: -0.0151,                 loss: 0.4435
agent1:                 episode reward: 0.0151,                 loss: nan
Episode: 24941/50100 (49.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0063s / 1629.8347 s
agent0:                 episode reward: -0.2715,                 loss: 0.4433
agent1:                 episode reward: 0.2715,                 loss: nan
Episode: 24961/50100 (49.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9975s / 1631.8323 s
agent0:                 episode reward: -0.0405,                 loss: 0.4435
agent1:                 episode reward: 0.0405,                 loss: nan
Episode: 24981/50100 (49.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9896s / 1633.8219 s
agent0:                 episode reward: 0.5851,                 loss: 0.4473
agent1:                 episode reward: -0.5851,                 loss: nan
Episode: 25001/50100 (49.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0066s / 1635.8285 s
agent0:                 episode reward: 0.2816,                 loss: 0.4509
agent1:                 episode reward: -0.2816,                 loss: nan
Episode: 25021/50100 (49.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0110s / 1637.8395 s
agent0:                 episode reward: 0.1022,                 loss: 0.4516
agent1:                 episode reward: -0.1022,                 loss: nan
Episode: 25041/50100 (49.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0279s / 1639.8674 s
agent0:                 episode reward: -0.3004,                 loss: 0.4513
agent1:                 episode reward: 0.3004,                 loss: nan
Episode: 25061/50100 (50.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0101s / 1641.8775 s
agent0:                 episode reward: 0.3053,                 loss: 0.4514
agent1:                 episode reward: -0.3053,                 loss: nan
Episode: 25081/50100 (50.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9939s / 1643.8714 s
agent0:                 episode reward: 0.0344,                 loss: 0.4506
agent1:                 episode reward: -0.0344,                 loss: nan
Episode: 25101/50100 (50.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9914s / 1645.8628 s
agent0:                 episode reward: -0.1342,                 loss: 0.4511
agent1:                 episode reward: 0.1342,                 loss: nan
Episode: 25121/50100 (50.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9807s / 1647.8435 s
agent0:                 episode reward: 0.4062,                 loss: 0.4512
agent1:                 episode reward: -0.4062,                 loss: nan
Episode: 25141/50100 (50.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0072s / 1649.8507 s
agent0:                 episode reward: 0.0510,                 loss: 0.4519
agent1:                 episode reward: -0.0510,                 loss: nan
Episode: 25161/50100 (50.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9889s / 1651.8395 s
agent0:                 episode reward: 0.3103,                 loss: 0.4532
agent1:                 episode reward: -0.3103,                 loss: nan
Episode: 25181/50100 (50.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9922s / 1653.8318 s
agent0:                 episode reward: -0.2279,                 loss: 0.4520
agent1:                 episode reward: 0.2279,                 loss: nan
Episode: 25201/50100 (50.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9839s / 1655.8157 s
agent0:                 episode reward: 0.9235,                 loss: 0.4520
agent1:                 episode reward: -0.9235,                 loss: nan
Episode: 25221/50100 (50.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3354s / 1658.1511 s
agent0:                 episode reward: -0.6105,                 loss: 0.4516
agent1:                 episode reward: 0.6105,                 loss: 0.3809
Score delta: 2.036049401076707, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/24991_0.
Episode: 25241/50100 (50.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 2.7238s / 1660.8749 s
agent0:                 episode reward: -1.2087,                 loss: 0.4265
agent1:                 episode reward: 1.2087,                 loss: 0.3820
Score delta: 2.1607717985555657, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/25018_1.
Episode: 25261/50100 (50.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0168s / 1662.8916 s
agent0:                 episode reward: 0.0934,                 loss: 0.4260
agent1:                 episode reward: -0.0934,                 loss: nan
Episode: 25281/50100 (50.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9919s / 1664.8835 s
agent0:                 episode reward: -0.0208,                 loss: 0.4251
agent1:                 episode reward: 0.0208,                 loss: nan
Episode: 25301/50100 (50.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0041s / 1666.8876 s
agent0:                 episode reward: -0.2422,                 loss: 0.4257
agent1:                 episode reward: 0.2422,                 loss: nan
Episode: 25321/50100 (50.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0023s / 1668.8899 s
agent0:                 episode reward: 0.0658,                 loss: 0.4255
agent1:                 episode reward: -0.0658,                 loss: nan
Episode: 25341/50100 (50.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0196s / 1670.9095 s
agent0:                 episode reward: -0.4215,                 loss: 0.4331
agent1:                 episode reward: 0.4215,                 loss: nan
Episode: 25361/50100 (50.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9856s / 1672.8951 s
agent0:                 episode reward: -0.3386,                 loss: 0.4432
agent1:                 episode reward: 0.3386,                 loss: nan
Episode: 25381/50100 (50.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9879s / 1674.8830 s
agent0:                 episode reward: -0.5021,                 loss: 0.4423
agent1:                 episode reward: 0.5021,                 loss: nan
Episode: 25401/50100 (50.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9913s / 1676.8743 s
agent0:                 episode reward: -0.3336,                 loss: 0.4432
agent1:                 episode reward: 0.3336,                 loss: nan
Episode: 25421/50100 (50.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0154s / 1679.8897 s
agent0:                 episode reward: 0.3688,                 loss: 0.4434
agent1:                 episode reward: -0.3688,                 loss: 0.3697
Score delta: 2.1452738567685583, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/25206_0.
Episode: 25441/50100 (50.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2123s / 1681.1020 s
agent0:                 episode reward: -0.5393,                 loss: nan
agent1:                 episode reward: 0.5393,                 loss: 0.3651
Episode: 25461/50100 (50.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 2.5339s / 1683.6358 s
agent0:                 episode reward: -0.1590,                 loss: 0.4097
agent1:                 episode reward: 0.1590,                 loss: 0.3615
Score delta: 2.0892222165224132, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/25243_1.
Episode: 25481/50100 (50.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9855s / 1685.6214 s
agent0:                 episode reward: -0.3347,                 loss: 0.4061
agent1:                 episode reward: 0.3347,                 loss: nan
Episode: 25501/50100 (50.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0100s / 1687.6314 s
agent0:                 episode reward: -0.3833,                 loss: 0.4073
agent1:                 episode reward: 0.3833,                 loss: nan
Episode: 25521/50100 (50.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9919s / 1689.6233 s
agent0:                 episode reward: 0.4223,                 loss: 0.4075
agent1:                 episode reward: -0.4223,                 loss: nan
Episode: 25541/50100 (50.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9988s / 1691.6221 s
agent0:                 episode reward: -0.1572,                 loss: 0.4074
agent1:                 episode reward: 0.1572,                 loss: nan
Episode: 25561/50100 (51.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9868s / 1693.6089 s
agent0:                 episode reward: 0.3592,                 loss: 0.4075
agent1:                 episode reward: -0.3592,                 loss: nan
Episode: 25581/50100 (51.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9900s / 1695.5990 s
agent0:                 episode reward: -0.1776,                 loss: 0.4051
agent1:                 episode reward: 0.1776,                 loss: nan
Episode: 25601/50100 (51.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9988s / 1697.5978 s
agent0:                 episode reward: -0.1707,                 loss: 0.4063
agent1:                 episode reward: 0.1707,                 loss: nan
Episode: 25621/50100 (51.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9869s / 1699.5847 s
agent0:                 episode reward: -0.0010,                 loss: 0.4045
agent1:                 episode reward: 0.0010,                 loss: nan
Episode: 25641/50100 (51.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9896s / 1701.5742 s
agent0:                 episode reward: -0.1030,                 loss: 0.4040
agent1:                 episode reward: 0.1030,                 loss: nan
Episode: 25661/50100 (51.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9956s / 1703.5698 s
agent0:                 episode reward: 0.0282,                 loss: 0.4044
agent1:                 episode reward: -0.0282,                 loss: nan
Episode: 25681/50100 (51.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0113s / 1705.5811 s
agent0:                 episode reward: -0.4897,                 loss: 0.4047
agent1:                 episode reward: 0.4897,                 loss: nan
Episode: 25701/50100 (51.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9928s / 1707.5739 s
agent0:                 episode reward: 0.1536,                 loss: 0.4056
agent1:                 episode reward: -0.1536,                 loss: nan
Episode: 25721/50100 (51.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9922s / 1709.5661 s
agent0:                 episode reward: 0.1120,                 loss: 0.4277
agent1:                 episode reward: -0.1120,                 loss: nan
Episode: 25741/50100 (51.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9931s / 1711.5592 s
agent0:                 episode reward: -0.3794,                 loss: 0.4288
agent1:                 episode reward: 0.3794,                 loss: nan
Episode: 25761/50100 (51.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9946s / 1713.5538 s
agent0:                 episode reward: -0.3821,                 loss: 0.4275
agent1:                 episode reward: 0.3821,                 loss: nan
Episode: 25781/50100 (51.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8396s / 1716.3934 s
agent0:                 episode reward: 0.2354,                 loss: 0.4280
agent1:                 episode reward: -0.2354,                 loss: 0.3673
Score delta: 2.212414236932708, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/25563_0.
Episode: 25801/50100 (51.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2039s / 1717.5973 s
agent0:                 episode reward: 0.0668,                 loss: nan
agent1:                 episode reward: -0.0668,                 loss: 0.3734
Episode: 25821/50100 (51.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2181s / 1718.8153 s
agent0:                 episode reward: -0.4814,                 loss: nan
agent1:                 episode reward: 0.4814,                 loss: 0.3608
Episode: 25841/50100 (51.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 2.3105s / 1721.1258 s
agent0:                 episode reward: -0.1856,                 loss: nan
agent1:                 episode reward: 0.1856,                 loss: 0.3561
Score delta: 2.1370570911471405, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/25629_1.
Episode: 25861/50100 (51.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0010s / 1723.1267 s
agent0:                 episode reward: -0.5417,                 loss: 0.4478
agent1:                 episode reward: 0.5417,                 loss: nan
Episode: 25881/50100 (51.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9832s / 1725.1099 s
agent0:                 episode reward: -0.2673,                 loss: 0.4456
agent1:                 episode reward: 0.2673,                 loss: nan
Episode: 25901/50100 (51.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9861s / 1727.0961 s
agent0:                 episode reward: -0.2725,                 loss: 0.4461
agent1:                 episode reward: 0.2725,                 loss: nan
Episode: 25921/50100 (51.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9980s / 1729.0941 s
agent0:                 episode reward: -0.1239,                 loss: 0.4458
agent1:                 episode reward: 0.1239,                 loss: nan
Episode: 25941/50100 (51.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9878s / 1731.0819 s
agent0:                 episode reward: 0.4844,                 loss: 0.4483
agent1:                 episode reward: -0.4844,                 loss: nan
Episode: 25961/50100 (51.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0035s / 1733.0854 s
agent0:                 episode reward: -0.7704,                 loss: 0.4540
agent1:                 episode reward: 0.7704,                 loss: nan
Episode: 25981/50100 (51.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9898s / 1735.0751 s
agent0:                 episode reward: 0.1644,                 loss: 0.4528
agent1:                 episode reward: -0.1644,                 loss: nan
Episode: 26001/50100 (51.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9777s / 1737.0529 s
agent0:                 episode reward: -0.0794,                 loss: 0.4525
agent1:                 episode reward: 0.0794,                 loss: nan
Episode: 26021/50100 (51.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9931s / 1739.0459 s
agent0:                 episode reward: 0.1387,                 loss: 0.4526
agent1:                 episode reward: -0.1387,                 loss: nan
Episode: 26041/50100 (51.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9811s / 1741.0270 s
agent0:                 episode reward: -0.3274,                 loss: 0.4529
agent1:                 episode reward: 0.3274,                 loss: nan
Episode: 26061/50100 (52.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9963s / 1743.0233 s
agent0:                 episode reward: -0.0827,                 loss: 0.4528
agent1:                 episode reward: 0.0827,                 loss: nan
Episode: 26081/50100 (52.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9742s / 1744.9975 s
agent0:                 episode reward: -0.0976,                 loss: 0.4521
agent1:                 episode reward: 0.0976,                 loss: nan
Episode: 26101/50100 (52.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9898s / 1746.9873 s
agent0:                 episode reward: 0.1316,                 loss: 0.4526
agent1:                 episode reward: -0.1316,                 loss: nan
Episode: 26121/50100 (52.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9878s / 1748.9750 s
agent0:                 episode reward: 0.1800,                 loss: 0.4510
agent1:                 episode reward: -0.1800,                 loss: nan
Episode: 26141/50100 (52.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9900s / 1750.9650 s
agent0:                 episode reward: -0.3988,                 loss: 0.4509
agent1:                 episode reward: 0.3988,                 loss: nan
Episode: 26161/50100 (52.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9845s / 1752.9496 s
agent0:                 episode reward: 0.0842,                 loss: 0.4510
agent1:                 episode reward: -0.0842,                 loss: nan
Episode: 26181/50100 (52.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9934s / 1754.9430 s
agent0:                 episode reward: -0.2753,                 loss: 0.4501
agent1:                 episode reward: 0.2753,                 loss: nan
Episode: 26201/50100 (52.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9916s / 1756.9346 s
agent0:                 episode reward: -0.4122,                 loss: 0.4506
agent1:                 episode reward: 0.4122,                 loss: nan
Episode: 26221/50100 (52.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0051s / 1758.9396 s
agent0:                 episode reward: 0.2725,                 loss: 0.4511
agent1:                 episode reward: -0.2725,                 loss: nan
Episode: 26241/50100 (52.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0095s / 1760.9491 s
agent0:                 episode reward: 0.0231,                 loss: 0.4505
agent1:                 episode reward: -0.0231,                 loss: nan
Episode: 26261/50100 (52.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9920s / 1762.9411 s
agent0:                 episode reward: 0.2873,                 loss: 0.4498
agent1:                 episode reward: -0.2873,                 loss: nan
Episode: 26281/50100 (52.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9903s / 1764.9313 s
agent0:                 episode reward: 0.1703,                 loss: 0.4412
agent1:                 episode reward: -0.1703,                 loss: nan
Episode: 26301/50100 (52.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0072s / 1766.9385 s
agent0:                 episode reward: -0.0669,                 loss: 0.4384
agent1:                 episode reward: 0.0669,                 loss: nan
Episode: 26321/50100 (52.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0141s / 1768.9526 s
agent0:                 episode reward: 0.2660,                 loss: 0.4379
agent1:                 episode reward: -0.2660,                 loss: nan
Episode: 26341/50100 (52.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0161s / 1770.9687 s
agent0:                 episode reward: -0.1150,                 loss: 0.4367
agent1:                 episode reward: 0.1150,                 loss: nan
Episode: 26361/50100 (52.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 2.6225s / 1773.5912 s
agent0:                 episode reward: 0.2665,                 loss: 0.4380
agent1:                 episode reward: -0.2665,                 loss: 0.3189
Score delta: 2.2162858577072684, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/26134_0.
Episode: 26381/50100 (52.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2319s / 1774.8231 s
agent0:                 episode reward: -0.1946,                 loss: nan
agent1:                 episode reward: 0.1946,                 loss: 0.3152
Episode: 26401/50100 (52.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2208s / 1776.0439 s
agent0:                 episode reward: -0.4527,                 loss: nan
agent1:                 episode reward: 0.4527,                 loss: 0.3168
Episode: 26421/50100 (52.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9053s / 1778.9492 s
agent0:                 episode reward: -0.9275,                 loss: 0.3866
agent1:                 episode reward: 0.9275,                 loss: 0.3122
Score delta: 2.2270469883079995, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/26194_1.
Episode: 26441/50100 (52.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9894s / 1780.9386 s
agent0:                 episode reward: -0.3699,                 loss: 0.3805
agent1:                 episode reward: 0.3699,                 loss: nan
Episode: 26461/50100 (52.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0071s / 1782.9457 s
agent0:                 episode reward: 0.1865,                 loss: 0.3822
agent1:                 episode reward: -0.1865,                 loss: nan
Episode: 26481/50100 (52.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9787s / 1784.9245 s
agent0:                 episode reward: -0.1478,                 loss: 0.3826
agent1:                 episode reward: 0.1478,                 loss: nan
Episode: 26501/50100 (52.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9823s / 1786.9068 s
agent0:                 episode reward: -0.3799,                 loss: 0.3797
agent1:                 episode reward: 0.3799,                 loss: nan
Episode: 26521/50100 (52.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0087s / 1788.9155 s
agent0:                 episode reward: -0.1874,                 loss: 0.3828
agent1:                 episode reward: 0.1874,                 loss: nan
Episode: 26541/50100 (52.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0100s / 1790.9255 s
agent0:                 episode reward: -0.0998,                 loss: 0.3800
agent1:                 episode reward: 0.0998,                 loss: nan
Episode: 26561/50100 (53.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0147s / 1792.9401 s
agent0:                 episode reward: -0.5765,                 loss: 0.3812
agent1:                 episode reward: 0.5765,                 loss: nan
Episode: 26581/50100 (53.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0212s / 1794.9613 s
agent0:                 episode reward: -0.3128,                 loss: 0.3808
agent1:                 episode reward: 0.3128,                 loss: nan
Episode: 26601/50100 (53.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0007s / 1796.9621 s
agent0:                 episode reward: 0.1162,                 loss: 0.3784
agent1:                 episode reward: -0.1162,                 loss: nan
Episode: 26621/50100 (53.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0053s / 1798.9673 s
agent0:                 episode reward: -0.5376,                 loss: 0.3787
agent1:                 episode reward: 0.5376,                 loss: nan
Episode: 26641/50100 (53.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0000s / 1800.9674 s
agent0:                 episode reward: 0.5234,                 loss: 0.3792
agent1:                 episode reward: -0.5234,                 loss: nan
Episode: 26661/50100 (53.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9988s / 1802.9661 s
agent0:                 episode reward: 0.1029,                 loss: 0.3790
agent1:                 episode reward: -0.1029,                 loss: nan
Episode: 26681/50100 (53.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9994s / 1804.9656 s
agent0:                 episode reward: -0.2901,                 loss: 0.4307
agent1:                 episode reward: 0.2901,                 loss: nan
Episode: 26701/50100 (53.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9946s / 1806.9602 s
agent0:                 episode reward: -0.2494,                 loss: 0.4294
agent1:                 episode reward: 0.2494,                 loss: nan
Episode: 26721/50100 (53.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9960s / 1808.9562 s
agent0:                 episode reward: 0.3187,                 loss: 0.4287
agent1:                 episode reward: -0.3187,                 loss: nan
Episode: 26741/50100 (53.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0203s / 1810.9765 s
agent0:                 episode reward: -0.3140,                 loss: 0.4286
agent1:                 episode reward: 0.3140,                 loss: nan
Episode: 26761/50100 (53.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0290s / 1813.0055 s
agent0:                 episode reward: -0.0147,                 loss: 0.4273
agent1:                 episode reward: 0.0147,                 loss: nan
Episode: 26781/50100 (53.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9803s / 1814.9858 s
agent0:                 episode reward: 0.2277,                 loss: 0.4274
agent1:                 episode reward: -0.2277,                 loss: nan
Episode: 26801/50100 (53.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0077s / 1816.9935 s
agent0:                 episode reward: 0.3627,                 loss: 0.4282
agent1:                 episode reward: -0.3627,                 loss: nan
Episode: 26821/50100 (53.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9875s / 1818.9810 s
agent0:                 episode reward: -0.0012,                 loss: 0.4279
agent1:                 episode reward: 0.0012,                 loss: nan
Episode: 26841/50100 (53.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0179s / 1820.9990 s
agent0:                 episode reward: -0.3530,                 loss: 0.4495
agent1:                 episode reward: 0.3530,                 loss: nan
Episode: 26861/50100 (53.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0241s / 1823.0230 s
agent0:                 episode reward: -0.1128,                 loss: 0.4561
agent1:                 episode reward: 0.1128,                 loss: nan
Episode: 26881/50100 (53.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0066s / 1825.0296 s
agent0:                 episode reward: 0.1115,                 loss: 0.4557
agent1:                 episode reward: -0.1115,                 loss: nan
Episode: 26901/50100 (53.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0138s / 1827.0434 s
agent0:                 episode reward: 0.1041,                 loss: 0.4551
agent1:                 episode reward: -0.1041,                 loss: nan
Episode: 26921/50100 (53.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0200s / 1829.0634 s
agent0:                 episode reward: -0.4666,                 loss: 0.4542
agent1:                 episode reward: 0.4666,                 loss: nan
Episode: 26941/50100 (53.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0094s / 1831.0729 s
agent0:                 episode reward: 0.2185,                 loss: 0.4549
agent1:                 episode reward: -0.2185,                 loss: nan
Episode: 26961/50100 (53.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0231s / 1833.0960 s
agent0:                 episode reward: 0.0730,                 loss: 0.4543
agent1:                 episode reward: -0.0730,                 loss: nan
Episode: 26981/50100 (53.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0105s / 1835.1065 s
agent0:                 episode reward: 0.3909,                 loss: 0.4543
agent1:                 episode reward: -0.3909,                 loss: nan
Episode: 27001/50100 (53.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0217s / 1837.1282 s
agent0:                 episode reward: 0.0375,                 loss: 0.4541
agent1:                 episode reward: -0.0375,                 loss: nan
Episode: 27021/50100 (53.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0205s / 1839.1488 s
agent0:                 episode reward: 0.2285,                 loss: 0.4550
agent1:                 episode reward: -0.2285,                 loss: nan
Episode: 27041/50100 (53.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0386s / 1841.1874 s
agent0:                 episode reward: 0.0140,                 loss: 0.4544
agent1:                 episode reward: -0.0140,                 loss: nan
Episode: 27061/50100 (54.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0345s / 1843.2218 s
agent0:                 episode reward: 0.2367,                 loss: 0.4546
agent1:                 episode reward: -0.2367,                 loss: nan
Episode: 27081/50100 (54.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0638s / 1845.2856 s
agent0:                 episode reward: -0.4769,                 loss: 0.4540
agent1:                 episode reward: 0.4769,                 loss: nan
Episode: 27101/50100 (54.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0335s / 1847.3191 s
agent0:                 episode reward: -0.0895,                 loss: 0.4543
agent1:                 episode reward: 0.0895,                 loss: nan
Episode: 27121/50100 (54.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0083s / 1849.3274 s
agent0:                 episode reward: -0.3259,                 loss: 0.4538
agent1:                 episode reward: 0.3259,                 loss: nan
Episode: 27141/50100 (54.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1211s / 1852.4485 s
agent0:                 episode reward: 0.5630,                 loss: 0.4536
agent1:                 episode reward: -0.5630,                 loss: nan
Score delta: 2.0747719451289677, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/26929_0.
Episode: 27161/50100 (54.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2294s / 1853.6779 s
agent0:                 episode reward: -0.2079,                 loss: nan
agent1:                 episode reward: 0.2079,                 loss: 0.3793
Episode: 27181/50100 (54.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8837s / 1856.5616 s
agent0:                 episode reward: -0.8070,                 loss: 0.4336
agent1:                 episode reward: 0.8070,                 loss: 0.3776
Score delta: 2.1434446069718396, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/26955_1.
Episode: 27201/50100 (54.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0169s / 1858.5785 s
agent0:                 episode reward: -0.4855,                 loss: 0.4246
agent1:                 episode reward: 0.4855,                 loss: nan
Episode: 27221/50100 (54.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0011s / 1860.5796 s
agent0:                 episode reward: -0.0436,                 loss: 0.4167
agent1:                 episode reward: 0.0436,                 loss: nan
Episode: 27241/50100 (54.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0167s / 1862.5963 s
agent0:                 episode reward: 0.1413,                 loss: 0.4174
agent1:                 episode reward: -0.1413,                 loss: nan
Episode: 27261/50100 (54.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0416s / 1864.6379 s
agent0:                 episode reward: -0.4032,                 loss: 0.4151
agent1:                 episode reward: 0.4032,                 loss: nan
Episode: 27281/50100 (54.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9991s / 1866.6371 s
agent0:                 episode reward: 0.1803,                 loss: 0.4142
agent1:                 episode reward: -0.1803,                 loss: nan
Episode: 27301/50100 (54.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0160s / 1868.6531 s
agent0:                 episode reward: -0.0418,                 loss: 0.4120
agent1:                 episode reward: 0.0418,                 loss: nan
Episode: 27321/50100 (54.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9999s / 1870.6529 s
agent0:                 episode reward: -0.2356,                 loss: 0.4128
agent1:                 episode reward: 0.2356,                 loss: nan
Episode: 27341/50100 (54.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0071s / 1872.6601 s
agent0:                 episode reward: -0.4422,                 loss: 0.4126
agent1:                 episode reward: 0.4422,                 loss: nan
Episode: 27361/50100 (54.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0336s / 1874.6936 s
agent0:                 episode reward: 0.0862,                 loss: 0.4184
agent1:                 episode reward: -0.0862,                 loss: nan
Episode: 27381/50100 (54.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0362s / 1876.7299 s
agent0:                 episode reward: -0.1054,                 loss: 0.4251
agent1:                 episode reward: 0.1054,                 loss: nan
Episode: 27401/50100 (54.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0296s / 1878.7594 s
agent0:                 episode reward: -0.4346,                 loss: 0.4252
agent1:                 episode reward: 0.4346,                 loss: nan
Episode: 27421/50100 (54.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9911s / 1880.7505 s
agent0:                 episode reward: -0.2674,                 loss: 0.4245
agent1:                 episode reward: 0.2674,                 loss: nan
Episode: 27441/50100 (54.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9880s / 1882.7385 s
agent0:                 episode reward: -0.2334,                 loss: 0.4267
agent1:                 episode reward: 0.2334,                 loss: nan
Episode: 27461/50100 (54.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9969s / 1884.7354 s
agent0:                 episode reward: -0.0513,                 loss: 0.4258
agent1:                 episode reward: 0.0513,                 loss: nan
Episode: 27481/50100 (54.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0179s / 1886.7533 s
agent0:                 episode reward: -0.0628,                 loss: 0.4241
agent1:                 episode reward: 0.0628,                 loss: nan
Episode: 27501/50100 (54.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0116s / 1888.7648 s
agent0:                 episode reward: 0.0816,                 loss: 0.4237
agent1:                 episode reward: -0.0816,                 loss: nan
Episode: 27521/50100 (54.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0071s / 1890.7720 s
agent0:                 episode reward: 0.4174,                 loss: 0.4244
agent1:                 episode reward: -0.4174,                 loss: nan
Episode: 27541/50100 (54.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0181s / 1892.7901 s
agent0:                 episode reward: 0.2110,                 loss: 0.4431
agent1:                 episode reward: -0.2110,                 loss: nan
Episode: 27561/50100 (55.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0260s / 1894.8160 s
agent0:                 episode reward: -0.0739,                 loss: 0.4433
agent1:                 episode reward: 0.0739,                 loss: nan
Episode: 27581/50100 (55.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0030s / 1896.8190 s
agent0:                 episode reward: 0.2433,                 loss: 0.4420
agent1:                 episode reward: -0.2433,                 loss: nan
Episode: 27601/50100 (55.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0095s / 1898.8285 s
agent0:                 episode reward: 0.2283,                 loss: 0.4424
agent1:                 episode reward: -0.2283,                 loss: nan
Episode: 27621/50100 (55.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9992s / 1900.8277 s
agent0:                 episode reward: 0.0068,                 loss: 0.4418
agent1:                 episode reward: -0.0068,                 loss: nan
Episode: 27641/50100 (55.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0073s / 1902.8350 s
agent0:                 episode reward: 0.1368,                 loss: 0.4426
agent1:                 episode reward: -0.1368,                 loss: nan
Episode: 27661/50100 (55.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9925s / 1904.8274 s
agent0:                 episode reward: 0.4351,                 loss: 0.4419
agent1:                 episode reward: -0.4351,                 loss: nan
Episode: 27681/50100 (55.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0193s / 1906.8468 s
agent0:                 episode reward: -0.0917,                 loss: 0.4416
agent1:                 episode reward: 0.0917,                 loss: nan
Episode: 27701/50100 (55.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0109s / 1908.8577 s
agent0:                 episode reward: 0.3240,                 loss: 0.4511
agent1:                 episode reward: -0.3240,                 loss: nan
Episode: 27721/50100 (55.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0117s / 1910.8694 s
agent0:                 episode reward: -0.4531,                 loss: 0.4537
agent1:                 episode reward: 0.4531,                 loss: nan
Episode: 27741/50100 (55.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0064s / 1912.8757 s
agent0:                 episode reward: 0.3158,                 loss: 0.4525
agent1:                 episode reward: -0.3158,                 loss: nan
Episode: 27761/50100 (55.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0081s / 1914.8838 s
agent0:                 episode reward: 0.0080,                 loss: 0.4519
agent1:                 episode reward: -0.0080,                 loss: nan
Episode: 27781/50100 (55.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9912s / 1916.8751 s
agent0:                 episode reward: 0.2693,                 loss: 0.4524
agent1:                 episode reward: -0.2693,                 loss: nan
Episode: 27801/50100 (55.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 2.4300s / 1919.3051 s
agent0:                 episode reward: 0.0665,                 loss: 0.4503
agent1:                 episode reward: -0.0665,                 loss: 0.3600
Score delta: 2.0540609745314273, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/27571_0.
Episode: 27821/50100 (55.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2344s / 1920.5395 s
agent0:                 episode reward: 0.4996,                 loss: nan
agent1:                 episode reward: -0.4996,                 loss: 0.3416
Episode: 27841/50100 (55.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2332s / 1921.7727 s
agent0:                 episode reward: -0.4281,                 loss: nan
agent1:                 episode reward: 0.4281,                 loss: 0.3148
Episode: 27861/50100 (55.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2406s / 1923.0133 s
agent0:                 episode reward: 0.0521,                 loss: nan
agent1:                 episode reward: -0.0521,                 loss: 0.3126
Episode: 27881/50100 (55.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8434s / 1925.8567 s
agent0:                 episode reward: -0.4250,                 loss: 0.4313
agent1:                 episode reward: 0.4250,                 loss: 0.3140
Score delta: 2.3353701709542563, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/27657_1.
Episode: 27901/50100 (55.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0166s / 1927.8733 s
agent0:                 episode reward: -0.0364,                 loss: 0.4303
agent1:                 episode reward: 0.0364,                 loss: nan
Episode: 27921/50100 (55.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0184s / 1929.8917 s
agent0:                 episode reward: 0.2045,                 loss: 0.4294
agent1:                 episode reward: -0.2045,                 loss: nan
Episode: 27941/50100 (55.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0054s / 1931.8971 s
agent0:                 episode reward: -0.0638,                 loss: 0.4309
agent1:                 episode reward: 0.0638,                 loss: nan
Episode: 27961/50100 (55.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0169s / 1933.9140 s
agent0:                 episode reward: 0.0453,                 loss: 0.4385
agent1:                 episode reward: -0.0453,                 loss: nan
Episode: 27981/50100 (55.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0303s / 1935.9444 s
agent0:                 episode reward: 0.7034,                 loss: 0.4390
agent1:                 episode reward: -0.7034,                 loss: nan
Episode: 28001/50100 (55.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0217s / 1937.9661 s
agent0:                 episode reward: -0.3612,                 loss: 0.4397
agent1:                 episode reward: 0.3612,                 loss: nan
Episode: 28021/50100 (55.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0095s / 1939.9756 s
agent0:                 episode reward: 0.1731,                 loss: 0.4398
agent1:                 episode reward: -0.1731,                 loss: nan
Episode: 28041/50100 (55.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0287s / 1942.0043 s
agent0:                 episode reward: -0.4175,                 loss: 0.4395
agent1:                 episode reward: 0.4175,                 loss: nan
Episode: 28061/50100 (56.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9891s / 1943.9934 s
agent0:                 episode reward: 0.1864,                 loss: 0.4394
agent1:                 episode reward: -0.1864,                 loss: nan
Episode: 28081/50100 (56.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0045s / 1945.9979 s
agent0:                 episode reward: -0.0678,                 loss: 0.4395
agent1:                 episode reward: 0.0678,                 loss: nan
Episode: 28101/50100 (56.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 2.6932s / 1948.6910 s
agent0:                 episode reward: 0.2062,                 loss: 0.4393
agent1:                 episode reward: -0.2062,                 loss: 0.3240
Score delta: 2.3416264364471617, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/27876_0.
Episode: 28121/50100 (56.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2436s / 1949.9347 s
agent0:                 episode reward: -0.3305,                 loss: nan
agent1:                 episode reward: 0.3305,                 loss: 0.3254
Episode: 28141/50100 (56.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8998s / 1952.8344 s
agent0:                 episode reward: -0.6420,                 loss: 0.4359
agent1:                 episode reward: 0.6420,                 loss: 0.3260
Score delta: 2.0502930103818207, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/27916_1.
Episode: 28161/50100 (56.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0111s / 1954.8455 s
agent0:                 episode reward: 0.0411,                 loss: 0.4169
agent1:                 episode reward: -0.0411,                 loss: nan
Episode: 28181/50100 (56.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0086s / 1956.8540 s
agent0:                 episode reward: -0.0055,                 loss: 0.4097
agent1:                 episode reward: 0.0055,                 loss: nan
Episode: 28201/50100 (56.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0025s / 1958.8566 s
agent0:                 episode reward: 0.0780,                 loss: 0.4074
agent1:                 episode reward: -0.0780,                 loss: nan
Episode: 28221/50100 (56.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9964s / 1960.8529 s
agent0:                 episode reward: -0.1847,                 loss: 0.4099
agent1:                 episode reward: 0.1847,                 loss: nan
Episode: 28241/50100 (56.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0300s / 1962.8830 s
agent0:                 episode reward: 0.0039,                 loss: 0.4084
agent1:                 episode reward: -0.0039,                 loss: nan
Episode: 28261/50100 (56.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0182s / 1964.9012 s
agent0:                 episode reward: -0.5187,                 loss: 0.4078
agent1:                 episode reward: 0.5187,                 loss: nan
Episode: 28281/50100 (56.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0186s / 1966.9197 s
agent0:                 episode reward: 0.2160,                 loss: 0.4058
agent1:                 episode reward: -0.2160,                 loss: nan
Episode: 28301/50100 (56.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0073s / 1968.9270 s
agent0:                 episode reward: 0.4977,                 loss: 0.4066
agent1:                 episode reward: -0.4977,                 loss: nan
Episode: 28321/50100 (56.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0061s / 1970.9331 s
agent0:                 episode reward: -0.1952,                 loss: 0.4093
agent1:                 episode reward: 0.1952,                 loss: nan
Episode: 28341/50100 (56.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0243s / 1972.9574 s
agent0:                 episode reward: -0.1374,                 loss: 0.4135
agent1:                 episode reward: 0.1374,                 loss: nan
Episode: 28361/50100 (56.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0092s / 1974.9667 s
agent0:                 episode reward: 0.3991,                 loss: 0.4126
agent1:                 episode reward: -0.3991,                 loss: nan
Episode: 28381/50100 (56.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 2.5713s / 1977.5380 s
agent0:                 episode reward: 0.2630,                 loss: 0.4122
agent1:                 episode reward: -0.2630,                 loss: 0.3572
Score delta: 2.2846853659397324, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/28153_0.
Episode: 28401/50100 (56.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 2.6003s / 1980.1382 s
agent0:                 episode reward: -0.7298,                 loss: 0.4517
agent1:                 episode reward: 0.7298,                 loss: 0.3579
Score delta: 2.015734218548584, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/28184_1.
Episode: 28421/50100 (56.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0132s / 1982.1514 s
agent0:                 episode reward: -0.1497,                 loss: 0.4444
agent1:                 episode reward: 0.1497,                 loss: nan
Episode: 28441/50100 (56.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0110s / 1984.1624 s
agent0:                 episode reward: -0.5833,                 loss: 0.4430
agent1:                 episode reward: 0.5833,                 loss: nan
Episode: 28461/50100 (56.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0372s / 1986.1996 s
agent0:                 episode reward: 0.0354,                 loss: 0.4426
agent1:                 episode reward: -0.0354,                 loss: nan
Episode: 28481/50100 (56.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0107s / 1988.2102 s
agent0:                 episode reward: 0.0633,                 loss: 0.4428
agent1:                 episode reward: -0.0633,                 loss: nan
Episode: 28501/50100 (56.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0176s / 1990.2278 s
agent0:                 episode reward: -0.2627,                 loss: 0.4420
agent1:                 episode reward: 0.2627,                 loss: nan
Episode: 28521/50100 (56.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0334s / 1992.2612 s
agent0:                 episode reward: -0.1869,                 loss: 0.4511
agent1:                 episode reward: 0.1869,                 loss: nan
Episode: 28541/50100 (56.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0333s / 1994.2944 s
agent0:                 episode reward: -0.4474,                 loss: 0.4546
agent1:                 episode reward: 0.4474,                 loss: nan
Episode: 28561/50100 (57.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0320s / 1996.3264 s
agent0:                 episode reward: 0.0783,                 loss: 0.4542
agent1:                 episode reward: -0.0783,                 loss: nan
Episode: 28581/50100 (57.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0142s / 1998.3406 s
agent0:                 episode reward: 0.4693,                 loss: 0.4530
agent1:                 episode reward: -0.4693,                 loss: nan
Episode: 28601/50100 (57.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0258s / 2000.3663 s
agent0:                 episode reward: 0.1827,                 loss: 0.4534
agent1:                 episode reward: -0.1827,                 loss: nan
Episode: 28621/50100 (57.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0165s / 2002.3828 s
agent0:                 episode reward: 0.3011,                 loss: 0.4527
agent1:                 episode reward: -0.3011,                 loss: nan
Episode: 28641/50100 (57.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0107s / 2004.3935 s
agent0:                 episode reward: -0.1217,                 loss: 0.4528
agent1:                 episode reward: 0.1217,                 loss: nan
Episode: 28661/50100 (57.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0105s / 2006.4040 s
agent0:                 episode reward: 0.5591,                 loss: 0.4528
agent1:                 episode reward: -0.5591,                 loss: nan
Episode: 28681/50100 (57.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0271s / 2008.4311 s
agent0:                 episode reward: -0.0018,                 loss: 0.4525
agent1:                 episode reward: 0.0018,                 loss: nan
Episode: 28701/50100 (57.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9615s / 2011.3926 s
agent0:                 episode reward: 0.2301,                 loss: 0.4526
agent1:                 episode reward: -0.2301,                 loss: 0.3855
Score delta: 2.0046762787346166, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/28481_0.
Episode: 28721/50100 (57.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2350s / 2012.6276 s
agent0:                 episode reward: -0.0011,                 loss: nan
agent1:                 episode reward: 0.0011,                 loss: 0.3797
Episode: 28741/50100 (57.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2589s / 2013.8865 s
agent0:                 episode reward: -0.3426,                 loss: nan
agent1:                 episode reward: 0.3426,                 loss: 0.3841
Episode: 28761/50100 (57.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2667s / 2015.1532 s
agent0:                 episode reward: 0.5195,                 loss: nan
agent1:                 episode reward: -0.5195,                 loss: 0.3664
Episode: 28781/50100 (57.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2768s / 2016.4300 s
agent0:                 episode reward: -0.1139,                 loss: nan
agent1:                 episode reward: 0.1139,                 loss: 0.3647
Episode: 28801/50100 (57.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2792s / 2017.7092 s
agent0:                 episode reward: 0.5400,                 loss: nan
agent1:                 episode reward: -0.5400,                 loss: 0.3653
Episode: 28821/50100 (57.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2669s / 2018.9761 s
agent0:                 episode reward: 0.1585,                 loss: nan
agent1:                 episode reward: -0.1585,                 loss: 0.3637
Episode: 28841/50100 (57.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2682s / 2020.2444 s
agent0:                 episode reward: -0.2578,                 loss: nan
agent1:                 episode reward: 0.2578,                 loss: 0.3650
Episode: 28861/50100 (57.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2591s / 2021.5035 s
agent0:                 episode reward: -0.1393,                 loss: nan
agent1:                 episode reward: 0.1393,                 loss: 0.3648
Episode: 28881/50100 (57.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2685s / 2022.7720 s
agent0:                 episode reward: -0.0506,                 loss: nan
agent1:                 episode reward: 0.0506,                 loss: 0.3635
Episode: 28901/50100 (57.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2637s / 2024.0357 s
agent0:                 episode reward: -0.2679,                 loss: nan
agent1:                 episode reward: 0.2679,                 loss: 0.3603
Episode: 28921/50100 (57.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2942s / 2025.3299 s
agent0:                 episode reward: -0.0701,                 loss: nan
agent1:                 episode reward: 0.0701,                 loss: 0.3450
Episode: 28941/50100 (57.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3136s / 2026.6436 s
agent0:                 episode reward: -0.0197,                 loss: nan
agent1:                 episode reward: 0.0197,                 loss: 0.3450
Episode: 28961/50100 (57.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2988s / 2027.9424 s
agent0:                 episode reward: -0.0220,                 loss: nan
agent1:                 episode reward: 0.0220,                 loss: 0.3431
Episode: 28981/50100 (57.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3169s / 2029.2593 s
agent0:                 episode reward: -0.5529,                 loss: nan
agent1:                 episode reward: 0.5529,                 loss: 0.3419
Episode: 29001/50100 (57.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2926s / 2030.5518 s
agent0:                 episode reward: 0.0614,                 loss: nan
agent1:                 episode reward: -0.0614,                 loss: 0.3427
Episode: 29021/50100 (57.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2987s / 2031.8505 s
agent0:                 episode reward: -0.6307,                 loss: nan
agent1:                 episode reward: 0.6307,                 loss: 0.3415
Episode: 29041/50100 (57.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2751s / 2033.1256 s
agent0:                 episode reward: -0.0797,                 loss: nan
agent1:                 episode reward: 0.0797,                 loss: 0.3399
Episode: 29061/50100 (58.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0868s / 2036.2124 s
agent0:                 episode reward: -0.1252,                 loss: 0.4540
agent1:                 episode reward: 0.1252,                 loss: 0.3429
Score delta: 2.0792837703932725, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/28834_1.
Episode: 29081/50100 (58.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0355s / 2038.2478 s
agent0:                 episode reward: -0.3494,                 loss: 0.4529
agent1:                 episode reward: 0.3494,                 loss: nan
Episode: 29101/50100 (58.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0324s / 2040.2802 s
agent0:                 episode reward: -0.1529,                 loss: 0.4523
agent1:                 episode reward: 0.1529,                 loss: nan
Episode: 29121/50100 (58.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0320s / 2042.3121 s
agent0:                 episode reward: 0.1225,                 loss: 0.4529
agent1:                 episode reward: -0.1225,                 loss: nan
Episode: 29141/50100 (58.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0276s / 2044.3397 s
agent0:                 episode reward: 0.1031,                 loss: 0.4524
agent1:                 episode reward: -0.1031,                 loss: nan
Episode: 29161/50100 (58.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0236s / 2046.3634 s
agent0:                 episode reward: 0.1000,                 loss: 0.4513
agent1:                 episode reward: -0.1000,                 loss: nan
Episode: 29181/50100 (58.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0322s / 2048.3956 s
agent0:                 episode reward: 0.0141,                 loss: 0.4514
agent1:                 episode reward: -0.0141,                 loss: nan
Episode: 29201/50100 (58.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0203s / 2050.4159 s
agent0:                 episode reward: -0.2751,                 loss: 0.4500
agent1:                 episode reward: 0.2751,                 loss: nan
Episode: 29221/50100 (58.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0087s / 2052.4246 s
agent0:                 episode reward: 0.2111,                 loss: 0.4448
agent1:                 episode reward: -0.2111,                 loss: nan
Episode: 29241/50100 (58.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9952s / 2054.4198 s
agent0:                 episode reward: 0.1929,                 loss: 0.4421
agent1:                 episode reward: -0.1929,                 loss: nan
Episode: 29261/50100 (58.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0196s / 2056.4394 s
agent0:                 episode reward: 0.0861,                 loss: 0.4426
agent1:                 episode reward: -0.0861,                 loss: nan
Episode: 29281/50100 (58.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9982s / 2058.4375 s
agent0:                 episode reward: -0.0441,                 loss: 0.4430
agent1:                 episode reward: 0.0441,                 loss: nan
Episode: 29301/50100 (58.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 2.7715s / 2061.2090 s
agent0:                 episode reward: -0.0019,                 loss: 0.4413
agent1:                 episode reward: 0.0019,                 loss: 0.3804
Score delta: 2.0970623696808137, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/29077_0.
Episode: 29321/50100 (58.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2833s / 2062.4923 s
agent0:                 episode reward: -0.1060,                 loss: nan
agent1:                 episode reward: 0.1060,                 loss: 0.3929
Episode: 29341/50100 (58.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2723s / 2063.7646 s
agent0:                 episode reward: 0.1106,                 loss: nan
agent1:                 episode reward: -0.1106,                 loss: 0.3861
Episode: 29361/50100 (58.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2760s / 2065.0407 s
agent0:                 episode reward: 0.0698,                 loss: nan
agent1:                 episode reward: -0.0698,                 loss: 0.3828
Episode: 29381/50100 (58.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3041s / 2066.3448 s
agent0:                 episode reward: 0.5162,                 loss: nan
agent1:                 episode reward: -0.5162,                 loss: 0.3842
Episode: 29401/50100 (58.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2924s / 2067.6371 s
agent0:                 episode reward: -0.4153,                 loss: nan
agent1:                 episode reward: 0.4153,                 loss: 0.3838
Episode: 29421/50100 (58.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9415s / 2070.5786 s
agent0:                 episode reward: -0.0201,                 loss: 0.4094
agent1:                 episode reward: 0.0201,                 loss: 0.3829
Score delta: 2.30066442481865, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/29197_1.
Episode: 29441/50100 (58.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0113s / 2072.5899 s
agent0:                 episode reward: -0.0925,                 loss: 0.4084
agent1:                 episode reward: 0.0925,                 loss: nan
Episode: 29461/50100 (58.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0364s / 2074.6263 s
agent0:                 episode reward: -0.1147,                 loss: 0.4074
agent1:                 episode reward: 0.1147,                 loss: nan
Episode: 29481/50100 (58.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0323s / 2076.6585 s
agent0:                 episode reward: -0.1086,                 loss: 0.4061
agent1:                 episode reward: 0.1086,                 loss: nan
Episode: 29501/50100 (58.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0236s / 2078.6821 s
agent0:                 episode reward: 0.1833,                 loss: 0.4169
agent1:                 episode reward: -0.1833,                 loss: nan
Episode: 29521/50100 (58.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0290s / 2080.7111 s
agent0:                 episode reward: 0.0464,                 loss: 0.4155
agent1:                 episode reward: -0.0464,                 loss: nan
Episode: 29541/50100 (58.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0073s / 2082.7184 s
agent0:                 episode reward: 0.3029,                 loss: 0.4160
agent1:                 episode reward: -0.3029,                 loss: nan
Episode: 29561/50100 (59.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0268s / 2084.7452 s
agent0:                 episode reward: -0.0476,                 loss: 0.4156
agent1:                 episode reward: 0.0476,                 loss: nan
Episode: 29581/50100 (59.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0156s / 2086.7608 s
agent0:                 episode reward: 0.0199,                 loss: 0.4157
agent1:                 episode reward: -0.0199,                 loss: nan
Episode: 29601/50100 (59.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0446s / 2088.8054 s
agent0:                 episode reward: 0.2137,                 loss: 0.4153
agent1:                 episode reward: -0.2137,                 loss: nan
Episode: 29621/50100 (59.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0504s / 2090.8559 s
agent0:                 episode reward: 0.2905,                 loss: 0.4153
agent1:                 episode reward: -0.2905,                 loss: nan
Episode: 29641/50100 (59.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0238s / 2092.8796 s
agent0:                 episode reward: -0.5571,                 loss: 0.4169
agent1:                 episode reward: 0.5571,                 loss: nan
Episode: 29661/50100 (59.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0200s / 2094.8997 s
agent0:                 episode reward: 0.2080,                 loss: 0.4282
agent1:                 episode reward: -0.2080,                 loss: nan
Episode: 29681/50100 (59.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0187s / 2096.9184 s
agent0:                 episode reward: 0.1402,                 loss: 0.4373
agent1:                 episode reward: -0.1402,                 loss: nan
Episode: 29701/50100 (59.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 2.5652s / 2099.4836 s
agent0:                 episode reward: 0.2667,                 loss: 0.4425
agent1:                 episode reward: -0.2667,                 loss: 0.3680
Score delta: 2.069891776777517, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/29471_0.
Episode: 29721/50100 (59.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2827s / 2100.7663 s
agent0:                 episode reward: -0.1926,                 loss: nan
agent1:                 episode reward: 0.1926,                 loss: 0.3662
Episode: 29741/50100 (59.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2869s / 2102.0531 s
agent0:                 episode reward: -0.3854,                 loss: nan
agent1:                 episode reward: 0.3854,                 loss: 0.3621
Episode: 29761/50100 (59.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2799s / 2103.3330 s
agent0:                 episode reward: -0.5126,                 loss: nan
agent1:                 episode reward: 0.5126,                 loss: 0.3565
Episode: 29781/50100 (59.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2926s / 2104.6257 s
agent0:                 episode reward: -0.4143,                 loss: nan
agent1:                 episode reward: 0.4143,                 loss: 0.3387
Episode: 29801/50100 (59.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2910s / 2105.9166 s
agent0:                 episode reward: 0.0496,                 loss: nan
agent1:                 episode reward: -0.0496,                 loss: 0.3338
Episode: 29821/50100 (59.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 2.7094s / 2108.6261 s
agent0:                 episode reward: -0.5455,                 loss: 0.4081
agent1:                 episode reward: 0.5455,                 loss: 0.3338
Score delta: 2.4438671888861165, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/29605_1.
Episode: 29841/50100 (59.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 3.3016s / 2111.9277 s
agent0:                 episode reward: 0.5643,                 loss: 0.4095
agent1:                 episode reward: -0.5643,                 loss: 0.3701
Score delta: 2.2974354820219185, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/29626_0.
Episode: 29861/50100 (59.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3142s / 2113.2418 s
agent0:                 episode reward: 0.1492,                 loss: nan
agent1:                 episode reward: -0.1492,                 loss: 0.3671
Episode: 29881/50100 (59.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3100s / 2114.5519 s
agent0:                 episode reward: -0.0575,                 loss: nan
agent1:                 episode reward: 0.0575,                 loss: 0.3646
Episode: 29901/50100 (59.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3091s / 2115.8609 s
agent0:                 episode reward: -0.2716,                 loss: nan
agent1:                 episode reward: 0.2716,                 loss: 0.3630
Episode: 29921/50100 (59.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3060s / 2117.1670 s
agent0:                 episode reward: -0.0557,                 loss: nan
agent1:                 episode reward: 0.0557,                 loss: 0.3616
Episode: 29941/50100 (59.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3081s / 2118.4750 s
agent0:                 episode reward: -0.1474,                 loss: nan
agent1:                 episode reward: 0.1474,                 loss: 0.3714
Episode: 29961/50100 (59.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8723s / 2121.3473 s
agent0:                 episode reward: -0.3076,                 loss: 0.4225
agent1:                 episode reward: 0.3076,                 loss: 0.3751
Score delta: 2.1451239713849946, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/29741_1.
Episode: 29981/50100 (59.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0170s / 2123.3643 s
agent0:                 episode reward: 0.0422,                 loss: 0.4201
agent1:                 episode reward: -0.0422,                 loss: nan
Episode: 30001/50100 (59.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0254s / 2125.3897 s
agent0:                 episode reward: 0.1569,                 loss: 0.4184
agent1:                 episode reward: -0.1569,                 loss: nan
Episode: 30021/50100 (59.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0001s / 2127.3898 s
agent0:                 episode reward: 0.1141,                 loss: 0.4190
agent1:                 episode reward: -0.1141,                 loss: nan
Episode: 30041/50100 (59.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0037s / 2129.3935 s
agent0:                 episode reward: -0.0488,                 loss: 0.4186
agent1:                 episode reward: 0.0488,                 loss: nan
Episode: 30061/50100 (60.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0122s / 2131.4057 s
agent0:                 episode reward: -0.3258,                 loss: 0.4186
agent1:                 episode reward: 0.3258,                 loss: nan
Episode: 30081/50100 (60.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0331s / 2133.4388 s
agent0:                 episode reward: 0.1808,                 loss: 0.4364
agent1:                 episode reward: -0.1808,                 loss: nan
Episode: 30101/50100 (60.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0351s / 2135.4739 s
agent0:                 episode reward: 0.4932,                 loss: 0.4394
agent1:                 episode reward: -0.4932,                 loss: nan
Episode: 30121/50100 (60.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0343s / 2137.5082 s
agent0:                 episode reward: 0.0206,                 loss: 0.4402
agent1:                 episode reward: -0.0206,                 loss: nan
Episode: 30141/50100 (60.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2057s / 2140.7139 s
agent0:                 episode reward: 0.4042,                 loss: 0.4389
agent1:                 episode reward: -0.4042,                 loss: 0.3363
Score delta: 2.1180578323393404, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/29927_0.
Episode: 30161/50100 (60.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3197s / 2142.0336 s
agent0:                 episode reward: -0.5066,                 loss: nan
agent1:                 episode reward: 0.5066,                 loss: 0.3420
Episode: 30181/50100 (60.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3221s / 2143.3557 s
agent0:                 episode reward: 0.0737,                 loss: nan
agent1:                 episode reward: -0.0737,                 loss: 0.3405
Episode: 30201/50100 (60.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1928s / 2146.5485 s
agent0:                 episode reward: -0.4615,                 loss: 0.4315
agent1:                 episode reward: 0.4615,                 loss: 0.3425
Score delta: 2.1089031446688566, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/29972_1.
Episode: 30221/50100 (60.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0654s / 2148.6139 s
agent0:                 episode reward: 0.1136,                 loss: 0.4264
agent1:                 episode reward: -0.1136,                 loss: nan
Episode: 30241/50100 (60.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0316s / 2150.6454 s
agent0:                 episode reward: -0.2440,                 loss: 0.4262
agent1:                 episode reward: 0.2440,                 loss: nan
Episode: 30261/50100 (60.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0255s / 2152.6709 s
agent0:                 episode reward: -0.0404,                 loss: 0.4250
agent1:                 episode reward: 0.0404,                 loss: nan
Episode: 30281/50100 (60.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0473s / 2154.7182 s
agent0:                 episode reward: 0.1169,                 loss: 0.4301
agent1:                 episode reward: -0.1169,                 loss: nan
Episode: 30301/50100 (60.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0592s / 2156.7774 s
agent0:                 episode reward: -0.2047,                 loss: 0.4534
agent1:                 episode reward: 0.2047,                 loss: nan
Episode: 30321/50100 (60.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0597s / 2158.8371 s
agent0:                 episode reward: -0.2133,                 loss: 0.4532
agent1:                 episode reward: 0.2133,                 loss: nan
Episode: 30341/50100 (60.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0446s / 2160.8817 s
agent0:                 episode reward: -0.1671,                 loss: 0.4514
agent1:                 episode reward: 0.1671,                 loss: nan
Episode: 30361/50100 (60.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0314s / 2162.9131 s
agent0:                 episode reward: -0.0994,                 loss: 0.4524
agent1:                 episode reward: 0.0994,                 loss: nan
Episode: 30381/50100 (60.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0279s / 2164.9410 s
agent0:                 episode reward: -0.4632,                 loss: 0.4520
agent1:                 episode reward: 0.4632,                 loss: nan
Episode: 30401/50100 (60.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0374s / 2166.9785 s
agent0:                 episode reward: 0.3116,                 loss: 0.4521
agent1:                 episode reward: -0.3116,                 loss: nan
Episode: 30421/50100 (60.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0267s / 2169.0051 s
agent0:                 episode reward: 0.3252,                 loss: 0.4513
agent1:                 episode reward: -0.3252,                 loss: nan
Episode: 30441/50100 (60.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0393s / 2171.0444 s
agent0:                 episode reward: 0.1943,                 loss: 0.4506
agent1:                 episode reward: -0.1943,                 loss: nan
Episode: 30461/50100 (60.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0588s / 2173.1032 s
agent0:                 episode reward: 0.3858,                 loss: 0.4459
agent1:                 episode reward: -0.3858,                 loss: nan
Episode: 30481/50100 (60.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0513s / 2175.1545 s
agent0:                 episode reward: -0.1203,                 loss: 0.4461
agent1:                 episode reward: 0.1203,                 loss: nan
Episode: 30501/50100 (60.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0191s / 2177.1736 s
agent0:                 episode reward: -0.0427,                 loss: 0.4452
agent1:                 episode reward: 0.0427,                 loss: nan
Episode: 30521/50100 (60.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0096s / 2179.1832 s
agent0:                 episode reward: -0.0112,                 loss: 0.4457
agent1:                 episode reward: 0.0112,                 loss: nan
Episode: 30541/50100 (60.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0246s / 2181.2078 s
agent0:                 episode reward: -0.0555,                 loss: 0.4441
agent1:                 episode reward: 0.0555,                 loss: nan
Episode: 30561/50100 (61.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0240s / 2183.2318 s
agent0:                 episode reward: 0.3738,                 loss: 0.4453
agent1:                 episode reward: -0.3738,                 loss: nan
Episode: 30581/50100 (61.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0472s / 2185.2790 s
agent0:                 episode reward: 0.3291,                 loss: 0.4451
agent1:                 episode reward: -0.3291,                 loss: nan
Episode: 30601/50100 (61.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0319s / 2187.3109 s
agent0:                 episode reward: -0.1320,                 loss: 0.4445
agent1:                 episode reward: 0.1320,                 loss: nan
Episode: 30621/50100 (61.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0339s / 2189.3448 s
agent0:                 episode reward: -0.0871,                 loss: 0.4436
agent1:                 episode reward: 0.0871,                 loss: nan
Episode: 30641/50100 (61.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0396s / 2191.3843 s
agent0:                 episode reward: -0.0311,                 loss: 0.4402
agent1:                 episode reward: 0.0311,                 loss: nan
Episode: 30661/50100 (61.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0319s / 2193.4162 s
agent0:                 episode reward: -0.0490,                 loss: 0.4421
agent1:                 episode reward: 0.0490,                 loss: nan
Episode: 30681/50100 (61.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0405s / 2195.4568 s
agent0:                 episode reward: 0.3547,                 loss: 0.4427
agent1:                 episode reward: -0.3547,                 loss: nan
Episode: 30701/50100 (61.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0255s / 2197.4822 s
agent0:                 episode reward: -0.3494,                 loss: 0.4422
agent1:                 episode reward: 0.3494,                 loss: nan
Episode: 30721/50100 (61.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0459s / 2199.5281 s
agent0:                 episode reward: 0.1100,                 loss: 0.4425
agent1:                 episode reward: -0.1100,                 loss: nan
Episode: 30741/50100 (61.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0590s / 2201.5871 s
agent0:                 episode reward: 0.2921,                 loss: 0.4420
agent1:                 episode reward: -0.2921,                 loss: nan
Episode: 30761/50100 (61.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0443s / 2203.6314 s
agent0:                 episode reward: 0.1727,                 loss: 0.4417
agent1:                 episode reward: -0.1727,                 loss: nan
Episode: 30781/50100 (61.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9782s / 2206.6096 s
agent0:                 episode reward: 0.3879,                 loss: 0.4406
agent1:                 episode reward: -0.3879,                 loss: 0.3730
Score delta: 2.1132217789814787, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/30560_0.
Episode: 30801/50100 (61.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3099s / 2207.9195 s
agent0:                 episode reward: 0.1132,                 loss: nan
agent1:                 episode reward: -0.1132,                 loss: 0.3695
Episode: 30821/50100 (61.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3174s / 2209.2369 s
agent0:                 episode reward: 0.3289,                 loss: nan
agent1:                 episode reward: -0.3289,                 loss: 0.3653
Episode: 30841/50100 (61.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 2.7504s / 2211.9873 s
agent0:                 episode reward: -0.2973,                 loss: 0.4501
agent1:                 episode reward: 0.2973,                 loss: 0.3676
Score delta: 2.169716487424847, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/30624_1.
Episode: 30861/50100 (61.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0072s / 2213.9945 s
agent0:                 episode reward: 0.7102,                 loss: 0.4520
agent1:                 episode reward: -0.7102,                 loss: nan
Episode: 30881/50100 (61.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0478s / 2216.0423 s
agent0:                 episode reward: 0.3760,                 loss: 0.4517
agent1:                 episode reward: -0.3760,                 loss: nan
Episode: 30901/50100 (61.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0465s / 2218.0888 s
agent0:                 episode reward: 0.0170,                 loss: 0.4513
agent1:                 episode reward: -0.0170,                 loss: nan
Episode: 30921/50100 (61.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0403s / 2220.1291 s
agent0:                 episode reward: 0.3504,                 loss: 0.4508
agent1:                 episode reward: -0.3504,                 loss: nan
Episode: 30941/50100 (61.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0428s / 2222.1719 s
agent0:                 episode reward: -0.2257,                 loss: 0.4511
agent1:                 episode reward: 0.2257,                 loss: nan
Episode: 30961/50100 (61.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0289s / 2224.2008 s
agent0:                 episode reward: 0.1959,                 loss: 0.4499
agent1:                 episode reward: -0.1959,                 loss: nan
Episode: 30981/50100 (61.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0387s / 2226.2395 s
agent0:                 episode reward: 0.0678,                 loss: 0.4512
agent1:                 episode reward: -0.0678,                 loss: nan
Episode: 31001/50100 (61.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0173s / 2228.2567 s
agent0:                 episode reward: 0.0065,                 loss: 0.4507
agent1:                 episode reward: -0.0065,                 loss: nan
Episode: 31021/50100 (61.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0422s / 2230.2989 s
agent0:                 episode reward: 0.5490,                 loss: 0.4477
agent1:                 episode reward: -0.5490,                 loss: nan
Episode: 31041/50100 (61.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0170s / 2232.3159 s
agent0:                 episode reward: 0.1880,                 loss: 0.4487
agent1:                 episode reward: -0.1880,                 loss: nan
Episode: 31061/50100 (61.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0261s / 2234.3420 s
agent0:                 episode reward: 0.3608,                 loss: 0.4475
agent1:                 episode reward: -0.3608,                 loss: nan
Episode: 31081/50100 (62.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 3.3205s / 2237.6626 s
agent0:                 episode reward: 0.8735,                 loss: 0.4476
agent1:                 episode reward: -0.8735,                 loss: nan
Score delta: 2.333499507335898, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/30869_0.
Episode: 31101/50100 (62.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3257s / 2238.9882 s
agent0:                 episode reward: -0.5421,                 loss: nan
agent1:                 episode reward: 0.5421,                 loss: 0.3078
Episode: 31121/50100 (62.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3293s / 2240.3175 s
agent0:                 episode reward: -0.1076,                 loss: nan
agent1:                 episode reward: 0.1076,                 loss: 0.3129
Episode: 31141/50100 (62.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3172s / 2241.6347 s
agent0:                 episode reward: -0.2877,                 loss: nan
agent1:                 episode reward: 0.2877,                 loss: 0.3689
Episode: 31161/50100 (62.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3396s / 2242.9743 s
agent0:                 episode reward: -0.2310,                 loss: nan
agent1:                 episode reward: 0.2310,                 loss: 0.3663
Episode: 31181/50100 (62.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3320s / 2244.3063 s
agent0:                 episode reward: -0.0321,                 loss: nan
agent1:                 episode reward: 0.0321,                 loss: 0.3641
Episode: 31201/50100 (62.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0882s / 2247.3945 s
agent0:                 episode reward: -0.5838,                 loss: 0.3887
agent1:                 episode reward: 0.5838,                 loss: 0.3615
Score delta: 2.0252715518597095, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/30975_1.
Episode: 31221/50100 (62.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0416s / 2249.4361 s
agent0:                 episode reward: -0.3617,                 loss: 0.3847
agent1:                 episode reward: 0.3617,                 loss: nan
Episode: 31241/50100 (62.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1149s / 2251.5510 s
agent0:                 episode reward: -0.1716,                 loss: 0.3825
agent1:                 episode reward: 0.1716,                 loss: nan
Episode: 31261/50100 (62.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0399s / 2253.5909 s
agent0:                 episode reward: -0.0322,                 loss: 0.3807
agent1:                 episode reward: 0.0322,                 loss: nan
Episode: 31281/50100 (62.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8707s / 2256.4616 s
agent0:                 episode reward: -0.1671,                 loss: 0.3761
agent1:                 episode reward: 0.1671,                 loss: 0.3394
Score delta: 2.0183766385409334, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31056_0.
Episode: 31301/50100 (62.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3427s / 2257.8043 s
agent0:                 episode reward: 0.0446,                 loss: nan
agent1:                 episode reward: -0.0446,                 loss: 0.3425
Episode: 31321/50100 (62.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9716s / 2260.7760 s
agent0:                 episode reward: -1.0868,                 loss: 0.4450
agent1:                 episode reward: 1.0868,                 loss: 0.3407
Score delta: 2.067339496139577, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31099_1.
Episode: 31341/50100 (62.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0300s / 2262.8060 s
agent0:                 episode reward: 0.1803,                 loss: 0.4567
agent1:                 episode reward: -0.1803,                 loss: nan
Episode: 31361/50100 (62.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0938s / 2265.8998 s
agent0:                 episode reward: 0.3908,                 loss: 0.4560
agent1:                 episode reward: -0.3908,                 loss: 0.3095
Score delta: 2.0766956783856383, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31142_0.
Episode: 31381/50100 (62.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3418s / 2267.2416 s
agent0:                 episode reward: -0.1426,                 loss: nan
agent1:                 episode reward: 0.1426,                 loss: 0.3076
Episode: 31401/50100 (62.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3359s / 2268.5775 s
agent0:                 episode reward: 0.0287,                 loss: nan
agent1:                 episode reward: -0.0287,                 loss: 0.3090
Episode: 31421/50100 (62.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3235s / 2269.9010 s
agent0:                 episode reward: -0.1101,                 loss: nan
agent1:                 episode reward: 0.1101,                 loss: 0.3530
Episode: 31441/50100 (62.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1190s / 2273.0199 s
agent0:                 episode reward: -0.1665,                 loss: 0.4101
agent1:                 episode reward: 0.1665,                 loss: 0.3818
Score delta: 2.32171297995898, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31214_1.
Episode: 31461/50100 (62.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0443s / 2275.0643 s
agent0:                 episode reward: 0.4101,                 loss: 0.4071
agent1:                 episode reward: -0.4101,                 loss: nan
Episode: 31481/50100 (62.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0456s / 2277.1098 s
agent0:                 episode reward: 0.3538,                 loss: 0.4073
agent1:                 episode reward: -0.3538,                 loss: nan
Episode: 31501/50100 (62.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0584s / 2279.1682 s
agent0:                 episode reward: 0.1023,                 loss: 0.4067
agent1:                 episode reward: -0.1023,                 loss: nan
Episode: 31521/50100 (62.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0542s / 2281.2224 s
agent0:                 episode reward: -0.6659,                 loss: 0.4072
agent1:                 episode reward: 0.6659,                 loss: nan
Episode: 31541/50100 (62.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2703s / 2284.4927 s
agent0:                 episode reward: 0.6360,                 loss: 0.4034
agent1:                 episode reward: -0.6360,                 loss: 0.3588
Score delta: 2.3809109723815816, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31326_0.
Episode: 31561/50100 (62.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3408s / 2285.8335 s
agent0:                 episode reward: -0.0112,                 loss: nan
agent1:                 episode reward: 0.0112,                 loss: 0.3541
Episode: 31581/50100 (63.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3601s / 2287.1936 s
agent0:                 episode reward: -0.1208,                 loss: nan
agent1:                 episode reward: 0.1208,                 loss: 0.3518
Episode: 31601/50100 (63.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3518s / 2288.5454 s
agent0:                 episode reward: -0.4057,                 loss: nan
agent1:                 episode reward: 0.4057,                 loss: 0.3483
Episode: 31621/50100 (63.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3604s / 2289.9058 s
agent0:                 episode reward: -0.1866,                 loss: nan
agent1:                 episode reward: 0.1866,                 loss: 0.3472
Episode: 31641/50100 (63.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0988s / 2293.0046 s
agent0:                 episode reward: -0.3515,                 loss: 0.4506
agent1:                 episode reward: 0.3515,                 loss: 0.3470
Score delta: 2.2065270524261384, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31416_1.
Episode: 31661/50100 (63.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0585s / 2295.0631 s
agent0:                 episode reward: -0.3530,                 loss: 0.4511
agent1:                 episode reward: 0.3530,                 loss: nan
Episode: 31681/50100 (63.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0728s / 2298.1359 s
agent0:                 episode reward: 0.1676,                 loss: 0.4534
agent1:                 episode reward: -0.1676,                 loss: 0.3730
Score delta: 2.032040019963825, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31460_0.
Episode: 31701/50100 (63.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 2.6574s / 2300.7933 s
agent0:                 episode reward: -0.5519,                 loss: nan
agent1:                 episode reward: 0.5519,                 loss: 0.3676
Score delta: 2.102344997643567, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31489_1.
Episode: 31721/50100 (63.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0368s / 2302.8301 s
agent0:                 episode reward: 0.2352,                 loss: 0.3884
agent1:                 episode reward: -0.2352,                 loss: nan
Episode: 31741/50100 (63.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0264s / 2304.8564 s
agent0:                 episode reward: -0.5970,                 loss: 0.3833
agent1:                 episode reward: 0.5970,                 loss: nan
Episode: 31761/50100 (63.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0129s / 2306.8693 s
agent0:                 episode reward: -0.2518,                 loss: 0.3827
agent1:                 episode reward: 0.2518,                 loss: nan
Episode: 31781/50100 (63.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0460s / 2308.9153 s
agent0:                 episode reward: -0.0364,                 loss: 0.3815
agent1:                 episode reward: 0.0364,                 loss: nan
Episode: 31801/50100 (63.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0179s / 2310.9332 s
agent0:                 episode reward: 0.5553,                 loss: 0.3803
agent1:                 episode reward: -0.5553,                 loss: nan
Episode: 31821/50100 (63.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0280s / 2312.9612 s
agent0:                 episode reward: 0.0889,                 loss: 0.3802
agent1:                 episode reward: -0.0889,                 loss: nan
Episode: 31841/50100 (63.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0281s / 2314.9893 s
agent0:                 episode reward: -0.0965,                 loss: 0.3817
agent1:                 episode reward: 0.0965,                 loss: nan
Episode: 31861/50100 (63.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0306s / 2317.0199 s
agent0:                 episode reward: 0.2099,                 loss: 0.3980
agent1:                 episode reward: -0.2099,                 loss: nan
Episode: 31881/50100 (63.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0277s / 2319.0476 s
agent0:                 episode reward: 0.3714,                 loss: 0.4058
agent1:                 episode reward: -0.3714,                 loss: nan
Episode: 31901/50100 (63.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0307s / 2321.0783 s
agent0:                 episode reward: 0.2433,                 loss: 0.4050
agent1:                 episode reward: -0.2433,                 loss: nan
Episode: 31921/50100 (63.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0387s / 2323.1170 s
agent0:                 episode reward: -0.2343,                 loss: 0.4043
agent1:                 episode reward: 0.2343,                 loss: nan
Episode: 31941/50100 (63.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0600s / 2325.1769 s
agent0:                 episode reward: 0.1057,                 loss: 0.4046
agent1:                 episode reward: -0.1057,                 loss: nan
Episode: 31961/50100 (63.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0822s / 2327.2591 s
agent0:                 episode reward: -0.3536,                 loss: 0.4024
agent1:                 episode reward: 0.3536,                 loss: nan
Episode: 31981/50100 (63.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9524s / 2330.2116 s
agent0:                 episode reward: 0.0028,                 loss: 0.4016
agent1:                 episode reward: -0.0028,                 loss: 0.3082
Score delta: 2.1996695228196734, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31754_0.
Episode: 32001/50100 (63.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3477s / 2331.5593 s
agent0:                 episode reward: -0.3995,                 loss: nan
agent1:                 episode reward: 0.3995,                 loss: 0.3231
Episode: 32021/50100 (63.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3578s / 2332.9171 s
agent0:                 episode reward: -0.5259,                 loss: nan
agent1:                 episode reward: 0.5259,                 loss: 0.3823
Episode: 32041/50100 (63.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2334s / 2336.1505 s
agent0:                 episode reward: -0.1114,                 loss: 0.4406
agent1:                 episode reward: 0.1114,                 loss: 0.3799
Score delta: 2.3088012032971457, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31814_1.
Episode: 32061/50100 (63.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2447s / 2339.3951 s
agent0:                 episode reward: 0.4620,                 loss: 0.4359
agent1:                 episode reward: -0.4620,                 loss: 0.3677
Score delta: 2.24915182125518, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31843_0.
Episode: 32081/50100 (64.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3710s / 2340.7661 s
agent0:                 episode reward: 0.0606,                 loss: nan
agent1:                 episode reward: -0.0606,                 loss: 0.3638
Episode: 32101/50100 (64.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 3.3942s / 2344.1603 s
agent0:                 episode reward: -0.0660,                 loss: 0.4545
agent1:                 episode reward: 0.0660,                 loss: 0.3670
Score delta: 2.044318979569153, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/31870_1.
Episode: 32121/50100 (64.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0435s / 2346.2038 s
agent0:                 episode reward: 0.3322,                 loss: 0.4543
agent1:                 episode reward: -0.3322,                 loss: nan
Episode: 32141/50100 (64.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0428s / 2348.2467 s
agent0:                 episode reward: 0.0762,                 loss: 0.4533
agent1:                 episode reward: -0.0762,                 loss: nan
Episode: 32161/50100 (64.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0457s / 2350.2924 s
agent0:                 episode reward: 0.1333,                 loss: 0.4531
agent1:                 episode reward: -0.1333,                 loss: nan
Episode: 32181/50100 (64.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0343s / 2352.3267 s
agent0:                 episode reward: 0.0974,                 loss: 0.4527
agent1:                 episode reward: -0.0974,                 loss: nan
Episode: 32201/50100 (64.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0293s / 2354.3560 s
agent0:                 episode reward: 0.2269,                 loss: 0.4531
agent1:                 episode reward: -0.2269,                 loss: nan
Episode: 32221/50100 (64.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0368s / 2356.3928 s
agent0:                 episode reward: -0.2070,                 loss: 0.4520
agent1:                 episode reward: 0.2070,                 loss: nan
Episode: 32241/50100 (64.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0404s / 2358.4332 s
agent0:                 episode reward: -0.4836,                 loss: 0.4516
agent1:                 episode reward: 0.4836,                 loss: nan
Episode: 32261/50100 (64.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0400s / 2360.4732 s
agent0:                 episode reward: -0.2689,                 loss: 0.4522
agent1:                 episode reward: 0.2689,                 loss: nan
Episode: 32281/50100 (64.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0256s / 2362.4988 s
agent0:                 episode reward: 0.3852,                 loss: 0.4520
agent1:                 episode reward: -0.3852,                 loss: nan
Episode: 32301/50100 (64.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0669s / 2364.5656 s
agent0:                 episode reward: -0.5810,                 loss: 0.4499
agent1:                 episode reward: 0.5810,                 loss: nan
Episode: 32321/50100 (64.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0611s / 2366.6268 s
agent0:                 episode reward: -0.0918,                 loss: 0.4502
agent1:                 episode reward: 0.0918,                 loss: nan
Episode: 32341/50100 (64.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0436s / 2368.6704 s
agent0:                 episode reward: 0.0730,                 loss: 0.4494
agent1:                 episode reward: -0.0730,                 loss: nan
Episode: 32361/50100 (64.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0397s / 2370.7101 s
agent0:                 episode reward: -0.2543,                 loss: 0.4487
agent1:                 episode reward: 0.2543,                 loss: nan
Episode: 32381/50100 (64.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0390s / 2372.7491 s
agent0:                 episode reward: -0.0339,                 loss: 0.4508
agent1:                 episode reward: 0.0339,                 loss: nan
Episode: 32401/50100 (64.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0502s / 2374.7993 s
agent0:                 episode reward: -0.1838,                 loss: 0.4497
agent1:                 episode reward: 0.1838,                 loss: nan
Episode: 32421/50100 (64.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0393s / 2376.8386 s
agent0:                 episode reward: -0.1985,                 loss: 0.4490
agent1:                 episode reward: 0.1985,                 loss: nan
Episode: 32441/50100 (64.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0551s / 2378.8937 s
agent0:                 episode reward: -0.0042,                 loss: 0.4443
agent1:                 episode reward: 0.0042,                 loss: nan
Episode: 32461/50100 (64.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0614s / 2380.9552 s
agent0:                 episode reward: 0.2770,                 loss: 0.4311
agent1:                 episode reward: -0.2770,                 loss: nan
Episode: 32481/50100 (64.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8777s / 2383.8328 s
agent0:                 episode reward: -0.1600,                 loss: 0.4317
agent1:                 episode reward: 0.1600,                 loss: 0.3543
Score delta: 2.2108808779640117, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/32253_0.
Episode: 32501/50100 (64.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3617s / 2385.1945 s
agent0:                 episode reward: -0.0640,                 loss: nan
agent1:                 episode reward: 0.0640,                 loss: 0.3538
Episode: 32521/50100 (64.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3629s / 2386.5574 s
agent0:                 episode reward: -0.4143,                 loss: nan
agent1:                 episode reward: 0.4143,                 loss: 0.3507
Episode: 32541/50100 (64.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 3.3418s / 2389.8992 s
agent0:                 episode reward: -0.2685,                 loss: 0.4253
agent1:                 episode reward: 0.2685,                 loss: 0.3526
Score delta: 2.4314141117340826, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/32312_1.
Episode: 32561/50100 (64.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0617s / 2391.9609 s
agent0:                 episode reward: 0.1687,                 loss: 0.4206
agent1:                 episode reward: -0.1687,                 loss: nan
Episode: 32581/50100 (65.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0777s / 2394.0386 s
agent0:                 episode reward: 0.2591,                 loss: 0.4207
agent1:                 episode reward: -0.2591,                 loss: nan
Episode: 32601/50100 (65.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0569s / 2396.0955 s
agent0:                 episode reward: 0.4636,                 loss: 0.4213
agent1:                 episode reward: -0.4636,                 loss: nan
Episode: 32621/50100 (65.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0587s / 2398.1542 s
agent0:                 episode reward: -0.2628,                 loss: 0.4202
agent1:                 episode reward: 0.2628,                 loss: nan
Episode: 32641/50100 (65.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0481s / 2400.2022 s
agent0:                 episode reward: 0.2293,                 loss: 0.4204
agent1:                 episode reward: -0.2293,                 loss: nan
Episode: 32661/50100 (65.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0449s / 2402.2471 s
agent0:                 episode reward: -0.1443,                 loss: 0.4211
agent1:                 episode reward: 0.1443,                 loss: nan
Episode: 32681/50100 (65.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0597s / 2404.3069 s
agent0:                 episode reward: -0.3107,                 loss: 0.4361
agent1:                 episode reward: 0.3107,                 loss: nan
Episode: 32701/50100 (65.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0666s / 2406.3735 s
agent0:                 episode reward: -0.1920,                 loss: 0.4337
agent1:                 episode reward: 0.1920,                 loss: nan
Episode: 32721/50100 (65.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0683s / 2408.4418 s
agent0:                 episode reward: 0.2293,                 loss: 0.4331
agent1:                 episode reward: -0.2293,                 loss: nan
Episode: 32741/50100 (65.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0731s / 2410.5149 s
agent0:                 episode reward: -0.3939,                 loss: 0.4335
agent1:                 episode reward: 0.3939,                 loss: nan
Episode: 32761/50100 (65.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0555s / 2412.5704 s
agent0:                 episode reward: 0.0384,                 loss: 0.4331
agent1:                 episode reward: -0.0384,                 loss: nan
Episode: 32781/50100 (65.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0296s / 2414.6000 s
agent0:                 episode reward: -0.3602,                 loss: 0.4338
agent1:                 episode reward: 0.3602,                 loss: nan
Episode: 32801/50100 (65.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0425s / 2416.6425 s
agent0:                 episode reward: 0.1831,                 loss: 0.4327
agent1:                 episode reward: -0.1831,                 loss: nan
Episode: 32821/50100 (65.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2825s / 2419.9250 s
agent0:                 episode reward: 0.5073,                 loss: 0.4324
agent1:                 episode reward: -0.5073,                 loss: 0.3643
Score delta: 2.1016018958745795, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/32605_0.
Episode: 32841/50100 (65.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3833s / 2421.3082 s
agent0:                 episode reward: 0.1392,                 loss: nan
agent1:                 episode reward: -0.1392,                 loss: 0.3616
Episode: 32861/50100 (65.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3870s / 2422.6952 s
agent0:                 episode reward: -0.0925,                 loss: nan
agent1:                 episode reward: 0.0925,                 loss: 0.3585
Episode: 32881/50100 (65.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3852s / 2424.0804 s
agent0:                 episode reward: 0.1662,                 loss: nan
agent1:                 episode reward: -0.1662,                 loss: 0.3590
Episode: 32901/50100 (65.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3541s / 2425.4345 s
agent0:                 episode reward: 0.3376,                 loss: nan
agent1:                 episode reward: -0.3376,                 loss: 0.3438
Episode: 32921/50100 (65.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3762s / 2426.8107 s
agent0:                 episode reward: -0.5000,                 loss: nan
agent1:                 episode reward: 0.5000,                 loss: 0.3372
Episode: 32941/50100 (65.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1233s / 2429.9340 s
agent0:                 episode reward: -0.3663,                 loss: 0.4317
agent1:                 episode reward: 0.3663,                 loss: 0.3367
Score delta: 2.315976997835079, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/32719_1.
Episode: 32961/50100 (65.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0622s / 2431.9963 s
agent0:                 episode reward: -0.5759,                 loss: 0.4538
agent1:                 episode reward: 0.5759,                 loss: nan
Episode: 32981/50100 (65.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0650s / 2434.0613 s
agent0:                 episode reward: 0.4005,                 loss: 0.4524
agent1:                 episode reward: -0.4005,                 loss: nan
Episode: 33001/50100 (65.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 2.9059s / 2436.9672 s
agent0:                 episode reward: -0.5022,                 loss: 0.4514
agent1:                 episode reward: 0.5022,                 loss: 0.3312
Score delta: 2.4426659685842886, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/32772_0.
Episode: 33021/50100 (65.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 3.3755s / 2440.3427 s
agent0:                 episode reward: -0.5641,                 loss: 0.4450
agent1:                 episode reward: 0.5641,                 loss: 0.3345
Score delta: 2.964595095375575, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/32793_1.
Episode: 33041/50100 (65.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0476s / 2442.3902 s
agent0:                 episode reward: 0.3233,                 loss: 0.4430
agent1:                 episode reward: -0.3233,                 loss: nan
Episode: 33061/50100 (65.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0515s / 2444.4417 s
agent0:                 episode reward: 0.3454,                 loss: 0.4416
agent1:                 episode reward: -0.3454,                 loss: nan
Episode: 33081/50100 (66.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0621s / 2446.5038 s
agent0:                 episode reward: 0.1318,                 loss: 0.4419
agent1:                 episode reward: -0.1318,                 loss: nan
Episode: 33101/50100 (66.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0529s / 2448.5566 s
agent0:                 episode reward: 0.0038,                 loss: 0.4403
agent1:                 episode reward: -0.0038,                 loss: nan
Episode: 33121/50100 (66.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0609s / 2450.6176 s
agent0:                 episode reward: -0.4097,                 loss: 0.4419
agent1:                 episode reward: 0.4097,                 loss: nan
Episode: 33141/50100 (66.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0508s / 2452.6684 s
agent0:                 episode reward: 0.3810,                 loss: 0.4288
agent1:                 episode reward: -0.3810,                 loss: nan
Episode: 33161/50100 (66.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0608s / 2454.7292 s
agent0:                 episode reward: 0.0848,                 loss: 0.4180
agent1:                 episode reward: -0.0848,                 loss: nan
Episode: 33181/50100 (66.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0541s / 2456.7833 s
agent0:                 episode reward: -0.3428,                 loss: 0.4170
agent1:                 episode reward: 0.3428,                 loss: nan
Episode: 33201/50100 (66.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0467s / 2458.8300 s
agent0:                 episode reward: -0.1160,                 loss: 0.4161
agent1:                 episode reward: 0.1160,                 loss: nan
Episode: 33221/50100 (66.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0523s / 2460.8823 s
agent0:                 episode reward: -0.1894,                 loss: 0.4162
agent1:                 episode reward: 0.1894,                 loss: nan
Episode: 33241/50100 (66.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0492s / 2462.9315 s
agent0:                 episode reward: -0.0272,                 loss: 0.4181
agent1:                 episode reward: 0.0272,                 loss: nan
Episode: 33261/50100 (66.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0506s / 2464.9821 s
agent0:                 episode reward: -0.2414,                 loss: 0.4172
agent1:                 episode reward: 0.2414,                 loss: nan
Episode: 33281/50100 (66.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0496s / 2467.0318 s
agent0:                 episode reward: 0.1981,                 loss: 0.4158
agent1:                 episode reward: -0.1981,                 loss: nan
Episode: 33301/50100 (66.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2589s / 2470.2906 s
agent0:                 episode reward: 0.5009,                 loss: 0.4173
agent1:                 episode reward: -0.5009,                 loss: 0.3697
Score delta: 2.232145432431946, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/33082_0.
Episode: 33321/50100 (66.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3700s / 2471.6606 s
agent0:                 episode reward: 0.0104,                 loss: nan
agent1:                 episode reward: -0.0104,                 loss: 0.3665
Episode: 33341/50100 (66.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3716s / 2473.0323 s
agent0:                 episode reward: -0.0856,                 loss: nan
agent1:                 episode reward: 0.0856,                 loss: 0.3648
Episode: 33361/50100 (66.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2218s / 2476.2541 s
agent0:                 episode reward: -0.0236,                 loss: 0.4183
agent1:                 episode reward: 0.0236,                 loss: 0.3632
Score delta: 2.219183571770093, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/33137_1.
Episode: 33381/50100 (66.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0996s / 2478.3536 s
agent0:                 episode reward: -0.3340,                 loss: 0.4155
agent1:                 episode reward: 0.3340,                 loss: nan
Episode: 33401/50100 (66.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0670s / 2480.4207 s
agent0:                 episode reward: 0.2505,                 loss: 0.4152
agent1:                 episode reward: -0.2505,                 loss: nan
Episode: 33421/50100 (66.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0445s / 2482.4652 s
agent0:                 episode reward: 0.3183,                 loss: 0.4140
agent1:                 episode reward: -0.3183,                 loss: nan
Episode: 33441/50100 (66.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0315s / 2484.4967 s
agent0:                 episode reward: -0.3500,                 loss: 0.4135
agent1:                 episode reward: 0.3500,                 loss: nan
Episode: 33461/50100 (66.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0226s / 2486.5192 s
agent0:                 episode reward: -0.0498,                 loss: 0.4131
agent1:                 episode reward: 0.0498,                 loss: nan
Episode: 33481/50100 (66.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0364s / 2488.5557 s
agent0:                 episode reward: 0.1594,                 loss: 0.4112
agent1:                 episode reward: -0.1594,                 loss: nan
Episode: 33501/50100 (66.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0274s / 2490.5830 s
agent0:                 episode reward: -0.2182,                 loss: 0.4123
agent1:                 episode reward: 0.2182,                 loss: nan
Episode: 33521/50100 (66.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0453s / 2492.6283 s
agent0:                 episode reward: 0.2125,                 loss: 0.4167
agent1:                 episode reward: -0.2125,                 loss: nan
Episode: 33541/50100 (66.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0462s / 2494.6746 s
agent0:                 episode reward: 0.0340,                 loss: 0.4336
agent1:                 episode reward: -0.0340,                 loss: nan
Episode: 33561/50100 (66.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0510s / 2496.7256 s
agent0:                 episode reward: 0.2151,                 loss: 0.4337
agent1:                 episode reward: -0.2151,                 loss: nan
Episode: 33581/50100 (67.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0260s / 2498.7516 s
agent0:                 episode reward: 0.4384,                 loss: 0.4335
agent1:                 episode reward: -0.4384,                 loss: nan
Episode: 33601/50100 (67.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0067s / 2500.7583 s
agent0:                 episode reward: -0.4800,                 loss: 0.4331
agent1:                 episode reward: 0.4800,                 loss: nan
Episode: 33621/50100 (67.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0398s / 2502.7981 s
agent0:                 episode reward: -0.1992,                 loss: 0.4311
agent1:                 episode reward: 0.1992,                 loss: nan
Episode: 33641/50100 (67.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0221s / 2504.8201 s
agent0:                 episode reward: -0.0686,                 loss: 0.4329
agent1:                 episode reward: 0.0686,                 loss: nan
Episode: 33661/50100 (67.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0382s / 2506.8584 s
agent0:                 episode reward: 0.3433,                 loss: 0.4323
agent1:                 episode reward: -0.3433,                 loss: nan
Episode: 33681/50100 (67.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0390s / 2508.8973 s
agent0:                 episode reward: -0.1560,                 loss: 0.4309
agent1:                 episode reward: 0.1560,                 loss: nan
Episode: 33701/50100 (67.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0324s / 2510.9297 s
agent0:                 episode reward: 0.0371,                 loss: 0.4461
agent1:                 episode reward: -0.0371,                 loss: nan
Episode: 33721/50100 (67.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0379s / 2512.9676 s
agent0:                 episode reward: -0.1096,                 loss: 0.4463
agent1:                 episode reward: 0.1096,                 loss: nan
Episode: 33741/50100 (67.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0521s / 2515.0198 s
agent0:                 episode reward: -0.0989,                 loss: 0.4461
agent1:                 episode reward: 0.0989,                 loss: nan
Episode: 33761/50100 (67.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0262s / 2517.0460 s
agent0:                 episode reward: 0.0057,                 loss: 0.4464
agent1:                 episode reward: -0.0057,                 loss: nan
Episode: 33781/50100 (67.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0292s / 2519.0752 s
agent0:                 episode reward: 0.0166,                 loss: 0.4465
agent1:                 episode reward: -0.0166,                 loss: nan
Episode: 33801/50100 (67.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0718s / 2521.1470 s
agent0:                 episode reward: 0.1428,                 loss: 0.4453
agent1:                 episode reward: -0.1428,                 loss: nan
Episode: 33821/50100 (67.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1839s / 2524.3309 s
agent0:                 episode reward: 0.3714,                 loss: 0.4470
agent1:                 episode reward: -0.3714,                 loss: 0.3619
Score delta: 2.1371422258051376, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/33594_0.
Episode: 33841/50100 (67.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3919s / 2525.7228 s
agent0:                 episode reward: 0.2538,                 loss: nan
agent1:                 episode reward: -0.2538,                 loss: 0.3653
Episode: 33861/50100 (67.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3940s / 2527.1167 s
agent0:                 episode reward: -0.8184,                 loss: nan
agent1:                 episode reward: 0.8184,                 loss: 0.3546
Episode: 33881/50100 (67.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5165s / 2530.6332 s
agent0:                 episode reward: -0.0292,                 loss: 0.4327
agent1:                 episode reward: 0.0292,                 loss: 0.3553
Score delta: 2.1305009389830745, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/33650_1.
Episode: 33901/50100 (67.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0363s / 2532.6695 s
agent0:                 episode reward: 0.3205,                 loss: 0.4305
agent1:                 episode reward: -0.3205,                 loss: nan
Episode: 33921/50100 (67.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0502s / 2534.7197 s
agent0:                 episode reward: -0.5235,                 loss: 0.4445
agent1:                 episode reward: 0.5235,                 loss: nan
Episode: 33941/50100 (67.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0528s / 2536.7725 s
agent0:                 episode reward: 0.3441,                 loss: 0.4484
agent1:                 episode reward: -0.3441,                 loss: nan
Episode: 33961/50100 (67.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0604s / 2538.8329 s
agent0:                 episode reward: 0.1926,                 loss: 0.4484
agent1:                 episode reward: -0.1926,                 loss: nan
Episode: 33981/50100 (67.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0593s / 2540.8922 s
agent0:                 episode reward: 0.1110,                 loss: 0.4473
agent1:                 episode reward: -0.1110,                 loss: nan
Episode: 34001/50100 (67.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0321s / 2542.9244 s
agent0:                 episode reward: -0.0333,                 loss: 0.4468
agent1:                 episode reward: 0.0333,                 loss: nan
Episode: 34021/50100 (67.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0298s / 2544.9542 s
agent0:                 episode reward: -0.2952,                 loss: 0.4458
agent1:                 episode reward: 0.2952,                 loss: nan
Episode: 34041/50100 (67.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0296s / 2546.9837 s
agent0:                 episode reward: -0.1513,                 loss: 0.4468
agent1:                 episode reward: 0.1513,                 loss: nan
Episode: 34061/50100 (67.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0322s / 2549.0159 s
agent0:                 episode reward: 0.2200,                 loss: 0.4467
agent1:                 episode reward: -0.2200,                 loss: nan
Episode: 34081/50100 (68.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0246s / 2551.0405 s
agent0:                 episode reward: -0.0391,                 loss: 0.4488
agent1:                 episode reward: 0.0391,                 loss: nan
Episode: 34101/50100 (68.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0440s / 2553.0845 s
agent0:                 episode reward: -0.0584,                 loss: 0.4509
agent1:                 episode reward: 0.0584,                 loss: nan
Episode: 34121/50100 (68.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2104s / 2556.2948 s
agent0:                 episode reward: 0.5830,                 loss: 0.4496
agent1:                 episode reward: -0.5830,                 loss: 0.3611
Score delta: 2.1031281313129773, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/33901_0.
Episode: 34141/50100 (68.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3742s / 2557.6690 s
agent0:                 episode reward: 0.2112,                 loss: nan
agent1:                 episode reward: -0.2112,                 loss: 0.3603
Episode: 34161/50100 (68.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3646s / 2559.0337 s
agent0:                 episode reward: -0.4639,                 loss: nan
agent1:                 episode reward: 0.4639,                 loss: 0.3589
Episode: 34181/50100 (68.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3876s / 2560.4212 s
agent0:                 episode reward: 0.0438,                 loss: nan
agent1:                 episode reward: -0.0438,                 loss: 0.3549
Episode: 34201/50100 (68.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 3.3415s / 2563.7627 s
agent0:                 episode reward: -0.0511,                 loss: 0.4324
agent1:                 episode reward: 0.0511,                 loss: 0.3575
Score delta: 2.2486018335983426, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/33978_1.
Episode: 34221/50100 (68.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0621s / 2565.8248 s
agent0:                 episode reward: 0.1518,                 loss: 0.4312
agent1:                 episode reward: -0.1518,                 loss: nan
Episode: 34241/50100 (68.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6126s / 2569.4374 s
agent0:                 episode reward: 0.3123,                 loss: 0.4331
agent1:                 episode reward: -0.3123,                 loss: 0.3145
Score delta: 2.0078077781007604, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/34028_0.
Episode: 34261/50100 (68.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3878s / 2570.8252 s
agent0:                 episode reward: 0.2089,                 loss: nan
agent1:                 episode reward: -0.2089,                 loss: 0.3188
Episode: 34281/50100 (68.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0180s / 2573.8432 s
agent0:                 episode reward: -0.5709,                 loss: 0.4615
agent1:                 episode reward: 0.5709,                 loss: 0.3146
Score delta: 2.0983931034181373, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/34064_1.
Episode: 34301/50100 (68.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0451s / 2575.8883 s
agent0:                 episode reward: 0.1688,                 loss: 0.4551
agent1:                 episode reward: -0.1688,                 loss: nan
Episode: 34321/50100 (68.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0520s / 2577.9402 s
agent0:                 episode reward: -0.4710,                 loss: 0.4536
agent1:                 episode reward: 0.4710,                 loss: nan
Episode: 34341/50100 (68.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0525s / 2579.9927 s
agent0:                 episode reward: -0.1253,                 loss: 0.4538
agent1:                 episode reward: 0.1253,                 loss: nan
Episode: 34361/50100 (68.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0583s / 2582.0510 s
agent0:                 episode reward: 0.2418,                 loss: 0.4510
agent1:                 episode reward: -0.2418,                 loss: nan
Episode: 34381/50100 (68.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0432s / 2584.0943 s
agent0:                 episode reward: -0.2810,                 loss: 0.4462
agent1:                 episode reward: 0.2810,                 loss: nan
Episode: 34401/50100 (68.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0558s / 2586.1501 s
agent0:                 episode reward: 0.2480,                 loss: 0.4464
agent1:                 episode reward: -0.2480,                 loss: nan
Episode: 34421/50100 (68.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0314s / 2588.1814 s
agent0:                 episode reward: -0.4994,                 loss: 0.4454
agent1:                 episode reward: 0.4994,                 loss: nan
Episode: 34441/50100 (68.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0638s / 2590.2452 s
agent0:                 episode reward: -0.3769,                 loss: 0.4442
agent1:                 episode reward: 0.3769,                 loss: nan
Episode: 34461/50100 (68.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0553s / 2592.3005 s
agent0:                 episode reward: 0.0131,                 loss: 0.4456
agent1:                 episode reward: -0.0131,                 loss: nan
Episode: 34481/50100 (68.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 2.8525s / 2595.1530 s
agent0:                 episode reward: -0.5248,                 loss: 0.4469
agent1:                 episode reward: 0.5248,                 loss: 0.3710
Score delta: 2.2217994612289984, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/34252_0.
Episode: 34501/50100 (68.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3804s / 2596.5334 s
agent0:                 episode reward: -0.4807,                 loss: nan
agent1:                 episode reward: 0.4807,                 loss: 0.3731
Episode: 34521/50100 (68.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3899s / 2597.9233 s
agent0:                 episode reward: 0.3039,                 loss: nan
agent1:                 episode reward: -0.3039,                 loss: 0.3623
Episode: 34541/50100 (68.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3800s / 2599.3033 s
agent0:                 episode reward: -0.1274,                 loss: nan
agent1:                 episode reward: 0.1274,                 loss: 0.3568
Episode: 34561/50100 (68.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3998s / 2600.7031 s
agent0:                 episode reward: -0.3313,                 loss: nan
agent1:                 episode reward: 0.3313,                 loss: 0.3582
Episode: 34581/50100 (69.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4034s / 2602.1065 s
agent0:                 episode reward: 0.1642,                 loss: nan
agent1:                 episode reward: -0.1642,                 loss: 0.3573
Episode: 34601/50100 (69.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4048s / 2603.5113 s
agent0:                 episode reward: 0.3242,                 loss: nan
agent1:                 episode reward: -0.3242,                 loss: 0.3563
Episode: 34621/50100 (69.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2135s / 2606.7248 s
agent0:                 episode reward: -0.8518,                 loss: 0.4404
agent1:                 episode reward: 0.8518,                 loss: 0.3535
Score delta: 2.2390537869308673, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/34402_1.
Episode: 34641/50100 (69.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0794s / 2608.8043 s
agent0:                 episode reward: 0.0227,                 loss: 0.4399
agent1:                 episode reward: -0.0227,                 loss: nan
Episode: 34661/50100 (69.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0810s / 2610.8852 s
agent0:                 episode reward: 0.0321,                 loss: 0.4398
agent1:                 episode reward: -0.0321,                 loss: nan
Episode: 34681/50100 (69.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0665s / 2612.9518 s
agent0:                 episode reward: 0.2152,                 loss: 0.4446
agent1:                 episode reward: -0.2152,                 loss: nan
Episode: 34701/50100 (69.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0580s / 2615.0098 s
agent0:                 episode reward: -0.2422,                 loss: 0.4468
agent1:                 episode reward: 0.2422,                 loss: nan
Episode: 34721/50100 (69.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0565s / 2617.0662 s
agent0:                 episode reward: 0.0456,                 loss: 0.4489
agent1:                 episode reward: -0.0456,                 loss: nan
Episode: 34741/50100 (69.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0704s / 2619.1366 s
agent0:                 episode reward: 0.3326,                 loss: 0.4474
agent1:                 episode reward: -0.3326,                 loss: nan
Episode: 34761/50100 (69.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0649s / 2621.2015 s
agent0:                 episode reward: 0.2206,                 loss: 0.4474
agent1:                 episode reward: -0.2206,                 loss: nan
Episode: 34781/50100 (69.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0415s / 2623.2431 s
agent0:                 episode reward: -0.0314,                 loss: 0.4475
agent1:                 episode reward: 0.0314,                 loss: nan
Episode: 34801/50100 (69.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0643s / 2625.3073 s
agent0:                 episode reward: -0.2199,                 loss: 0.4471
agent1:                 episode reward: 0.2199,                 loss: nan
Episode: 34821/50100 (69.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0690s / 2627.3763 s
agent0:                 episode reward: -0.3172,                 loss: 0.4467
agent1:                 episode reward: 0.3172,                 loss: nan
Episode: 34841/50100 (69.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0644s / 2629.4408 s
agent0:                 episode reward: 0.2827,                 loss: 0.4465
agent1:                 episode reward: -0.2827,                 loss: nan
Episode: 34861/50100 (69.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0694s / 2631.5102 s
agent0:                 episode reward: 0.3005,                 loss: 0.4474
agent1:                 episode reward: -0.3005,                 loss: nan
Episode: 34881/50100 (69.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0836s / 2633.5937 s
agent0:                 episode reward: 0.2443,                 loss: 0.4464
agent1:                 episode reward: -0.2443,                 loss: nan
Episode: 34901/50100 (69.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0831s / 2635.6769 s
agent0:                 episode reward: 0.2678,                 loss: 0.4475
agent1:                 episode reward: -0.2678,                 loss: nan
Episode: 34921/50100 (69.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0600s / 2637.7369 s
agent0:                 episode reward: -0.4451,                 loss: 0.4465
agent1:                 episode reward: 0.4451,                 loss: nan
Episode: 34941/50100 (69.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0866s / 2639.8235 s
agent0:                 episode reward: -0.0566,                 loss: 0.4481
agent1:                 episode reward: 0.0566,                 loss: nan
Episode: 34961/50100 (69.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0729s / 2641.8964 s
agent0:                 episode reward: -0.0397,                 loss: 0.4472
agent1:                 episode reward: 0.0397,                 loss: nan
Episode: 34981/50100 (69.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0664s / 2643.9628 s
agent0:                 episode reward: -0.1198,                 loss: 0.4469
agent1:                 episode reward: 0.1198,                 loss: nan
Episode: 35001/50100 (69.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0464s / 2646.0091 s
agent0:                 episode reward: -0.1567,                 loss: 0.4470
agent1:                 episode reward: 0.1567,                 loss: nan
Episode: 35021/50100 (69.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0623s / 2648.0715 s
agent0:                 episode reward: -0.2315,                 loss: 0.4465
agent1:                 episode reward: 0.2315,                 loss: nan
Episode: 35041/50100 (69.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0699s / 2650.1414 s
agent0:                 episode reward: -0.2877,                 loss: 0.4463
agent1:                 episode reward: 0.2877,                 loss: nan
Episode: 35061/50100 (69.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0893s / 2652.2306 s
agent0:                 episode reward: -0.0409,                 loss: 0.4465
agent1:                 episode reward: 0.0409,                 loss: nan
Episode: 35081/50100 (70.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0768s / 2654.3075 s
agent0:                 episode reward: 0.3641,                 loss: 0.4459
agent1:                 episode reward: -0.3641,                 loss: nan
Episode: 35101/50100 (70.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0786s / 2656.3860 s
agent0:                 episode reward: -0.1567,                 loss: 0.4456
agent1:                 episode reward: 0.1567,                 loss: nan
Episode: 35121/50100 (70.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0790s / 2658.4650 s
agent0:                 episode reward: 0.0973,                 loss: 0.4454
agent1:                 episode reward: -0.0973,                 loss: nan
Episode: 35141/50100 (70.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0588s / 2660.5239 s
agent0:                 episode reward: -0.0642,                 loss: 0.4463
agent1:                 episode reward: 0.0642,                 loss: nan
Episode: 35161/50100 (70.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0560s / 2662.5799 s
agent0:                 episode reward: 0.0920,                 loss: 0.4467
agent1:                 episode reward: -0.0920,                 loss: nan
Episode: 35181/50100 (70.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0650s / 2664.6449 s
agent0:                 episode reward: -0.4331,                 loss: 0.4438
agent1:                 episode reward: 0.4331,                 loss: nan
Episode: 35201/50100 (70.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0353s / 2666.6802 s
agent0:                 episode reward: -0.2825,                 loss: 0.4416
agent1:                 episode reward: 0.2825,                 loss: nan
Episode: 35221/50100 (70.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0554s / 2668.7356 s
agent0:                 episode reward: -0.4518,                 loss: 0.4418
agent1:                 episode reward: 0.4518,                 loss: nan
Episode: 35241/50100 (70.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0723s / 2670.8079 s
agent0:                 episode reward: 0.1101,                 loss: 0.4417
agent1:                 episode reward: -0.1101,                 loss: nan
Episode: 35261/50100 (70.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0698s / 2672.8777 s
agent0:                 episode reward: -0.0135,                 loss: 0.4419
agent1:                 episode reward: 0.0135,                 loss: nan
Episode: 35281/50100 (70.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0613s / 2674.9391 s
agent0:                 episode reward: 0.0658,                 loss: 0.4396
agent1:                 episode reward: -0.0658,                 loss: nan
Episode: 35301/50100 (70.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0859s / 2677.0249 s
agent0:                 episode reward: -0.3321,                 loss: 0.4415
agent1:                 episode reward: 0.3321,                 loss: nan
Episode: 35321/50100 (70.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0604s / 2679.0853 s
agent0:                 episode reward: -0.3485,                 loss: 0.4399
agent1:                 episode reward: 0.3485,                 loss: nan
Episode: 35341/50100 (70.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0427s / 2681.1281 s
agent0:                 episode reward: -0.6834,                 loss: 0.4406
agent1:                 episode reward: 0.6834,                 loss: nan
Episode: 35361/50100 (70.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0397s / 2683.1678 s
agent0:                 episode reward: -0.0145,                 loss: 0.4388
agent1:                 episode reward: 0.0145,                 loss: nan
Episode: 35381/50100 (70.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0873s / 2685.2551 s
agent0:                 episode reward: -0.1585,                 loss: 0.4394
agent1:                 episode reward: 0.1585,                 loss: nan
Episode: 35401/50100 (70.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0653s / 2687.3204 s
agent0:                 episode reward: 0.0842,                 loss: 0.4398
agent1:                 episode reward: -0.0842,                 loss: nan
Episode: 35421/50100 (70.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0792s / 2689.3997 s
agent0:                 episode reward: 0.1669,                 loss: 0.4405
agent1:                 episode reward: -0.1669,                 loss: nan
Episode: 35441/50100 (70.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0595s / 2691.4591 s
agent0:                 episode reward: 0.1112,                 loss: 0.4395
agent1:                 episode reward: -0.1112,                 loss: nan
Episode: 35461/50100 (70.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0569s / 2693.5161 s
agent0:                 episode reward: 0.2165,                 loss: 0.4391
agent1:                 episode reward: -0.2165,                 loss: nan
Episode: 35481/50100 (70.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0636s / 2695.5796 s
agent0:                 episode reward: -0.0526,                 loss: 0.4385
agent1:                 episode reward: 0.0526,                 loss: nan
Episode: 35501/50100 (70.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0802s / 2697.6599 s
agent0:                 episode reward: 0.4814,                 loss: 0.4394
agent1:                 episode reward: -0.4814,                 loss: nan
Episode: 35521/50100 (70.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0645s / 2699.7244 s
agent0:                 episode reward: 0.1357,                 loss: 0.4366
agent1:                 episode reward: -0.1357,                 loss: nan
Episode: 35541/50100 (70.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0656s / 2701.7900 s
agent0:                 episode reward: 0.2211,                 loss: 0.4366
agent1:                 episode reward: -0.2211,                 loss: nan
Episode: 35561/50100 (70.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0769s / 2703.8669 s
agent0:                 episode reward: -0.2234,                 loss: 0.4348
agent1:                 episode reward: 0.2234,                 loss: nan
Episode: 35581/50100 (71.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0695s / 2705.9363 s
agent0:                 episode reward: 0.3479,                 loss: 0.4358
agent1:                 episode reward: -0.3479,                 loss: nan
Episode: 35601/50100 (71.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0749s / 2708.0113 s
agent0:                 episode reward: -0.0250,                 loss: 0.4347
agent1:                 episode reward: 0.0250,                 loss: nan
Episode: 35621/50100 (71.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5428s / 2711.5540 s
agent0:                 episode reward: 0.3139,                 loss: 0.4358
agent1:                 episode reward: -0.3139,                 loss: 0.3857
Score delta: 2.0165390657870064, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/35408_0.
Episode: 35641/50100 (71.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4090s / 2712.9630 s
agent0:                 episode reward: -0.0629,                 loss: nan
agent1:                 episode reward: 0.0629,                 loss: 0.3869
Episode: 35661/50100 (71.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4176s / 2714.3806 s
agent0:                 episode reward: -0.3963,                 loss: nan
agent1:                 episode reward: 0.3963,                 loss: 0.3837
Episode: 35681/50100 (71.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4091s / 2715.7897 s
agent0:                 episode reward: 0.0716,                 loss: nan
agent1:                 episode reward: -0.0716,                 loss: 0.3562
Episode: 35701/50100 (71.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 3.3463s / 2719.1360 s
agent0:                 episode reward: -0.0761,                 loss: 0.4103
agent1:                 episode reward: 0.0761,                 loss: 0.3424
Score delta: 2.0955519886765437, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/35476_1.
Episode: 35721/50100 (71.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5198s / 2722.6557 s
agent0:                 episode reward: 0.4864,                 loss: 0.4084
agent1:                 episode reward: -0.4864,                 loss: 0.3668
Score delta: 2.0845625009149122, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/35504_0.
Episode: 35741/50100 (71.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4411s / 2724.0968 s
agent0:                 episode reward: 0.0590,                 loss: nan
agent1:                 episode reward: -0.0590,                 loss: 0.3678
Episode: 35761/50100 (71.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4319s / 2725.5288 s
agent0:                 episode reward: -0.2870,                 loss: nan
agent1:                 episode reward: 0.2870,                 loss: 0.3652
Episode: 35781/50100 (71.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0192s / 2728.5480 s
agent0:                 episode reward: -0.2117,                 loss: 0.4548
agent1:                 episode reward: 0.2117,                 loss: 0.3622
Score delta: 2.0708790029986845, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/35567_1.
Episode: 35801/50100 (71.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0549s / 2730.6029 s
agent0:                 episode reward: -0.0931,                 loss: 0.4405
agent1:                 episode reward: 0.0931,                 loss: nan
Episode: 35821/50100 (71.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0580s / 2732.6609 s
agent0:                 episode reward: -0.5660,                 loss: 0.4564
agent1:                 episode reward: 0.5660,                 loss: nan
Episode: 35841/50100 (71.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0607s / 2734.7215 s
agent0:                 episode reward: -0.5450,                 loss: 0.4544
agent1:                 episode reward: 0.5450,                 loss: nan
Episode: 35861/50100 (71.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0627s / 2736.7842 s
agent0:                 episode reward: 0.3987,                 loss: 0.4542
agent1:                 episode reward: -0.3987,                 loss: nan
Episode: 35881/50100 (71.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5580s / 2740.3422 s
agent0:                 episode reward: 0.4959,                 loss: 0.4528
agent1:                 episode reward: -0.4959,                 loss: 0.3760
Score delta: 2.142523229807745, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/35668_0.
Episode: 35901/50100 (71.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4238s / 2741.7659 s
agent0:                 episode reward: -0.6083,                 loss: nan
agent1:                 episode reward: 0.6083,                 loss: 0.3645
Episode: 35921/50100 (71.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4169s / 2743.1829 s
agent0:                 episode reward: 0.1476,                 loss: nan
agent1:                 episode reward: -0.1476,                 loss: 0.3643
Episode: 35941/50100 (71.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4278s / 2744.6107 s
agent0:                 episode reward: 0.4141,                 loss: nan
agent1:                 episode reward: -0.4141,                 loss: 0.3629
Episode: 35961/50100 (71.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4257s / 2746.0364 s
agent0:                 episode reward: -0.0432,                 loss: nan
agent1:                 episode reward: 0.0432,                 loss: 0.3652
Episode: 35981/50100 (71.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4125s / 2747.4490 s
agent0:                 episode reward: -0.5077,                 loss: nan
agent1:                 episode reward: 0.5077,                 loss: 0.3876
Episode: 36001/50100 (71.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4473s / 2748.8962 s
agent0:                 episode reward: -0.3494,                 loss: nan
agent1:                 episode reward: 0.3494,                 loss: 0.3859
Episode: 36021/50100 (71.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4447s / 2750.3409 s
agent0:                 episode reward: 0.0271,                 loss: nan
agent1:                 episode reward: -0.0271,                 loss: 0.3855
Episode: 36041/50100 (71.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4282s / 2751.7692 s
agent0:                 episode reward: -0.0991,                 loss: nan
agent1:                 episode reward: 0.0991,                 loss: 0.3846
Episode: 36061/50100 (71.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0940s / 2754.8632 s
agent0:                 episode reward: -0.3279,                 loss: 0.4325
agent1:                 episode reward: 0.3279,                 loss: 0.3843
Score delta: 2.043921457329563, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/35846_1.
Episode: 36081/50100 (72.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0867s / 2756.9500 s
agent0:                 episode reward: -0.4440,                 loss: 0.4317
agent1:                 episode reward: 0.4440,                 loss: nan
Episode: 36101/50100 (72.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0459s / 2758.9959 s
agent0:                 episode reward: 0.2515,                 loss: 0.4317
agent1:                 episode reward: -0.2515,                 loss: nan
Episode: 36121/50100 (72.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1128s / 2762.1087 s
agent0:                 episode reward: 0.3600,                 loss: 0.4325
agent1:                 episode reward: -0.3600,                 loss: 0.3568
Score delta: 2.44854262893895, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/35893_0.
Episode: 36141/50100 (72.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4255s / 2763.5342 s
agent0:                 episode reward: -0.1486,                 loss: nan
agent1:                 episode reward: 0.1486,                 loss: 0.3587
Episode: 36161/50100 (72.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4258s / 2764.9600 s
agent0:                 episode reward: 0.2050,                 loss: nan
agent1:                 episode reward: -0.2050,                 loss: 0.3546
Episode: 36181/50100 (72.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4432s / 2766.4032 s
agent0:                 episode reward: -0.4618,                 loss: nan
agent1:                 episode reward: 0.4618,                 loss: 0.3809
Episode: 36201/50100 (72.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5043s / 2769.9075 s
agent0:                 episode reward: -0.3115,                 loss: 0.4216
agent1:                 episode reward: 0.3115,                 loss: 0.3967
Score delta: 2.4244877642222114, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/35972_1.
Episode: 36221/50100 (72.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0514s / 2771.9590 s
agent0:                 episode reward: 0.1291,                 loss: 0.4150
agent1:                 episode reward: -0.1291,                 loss: nan
Episode: 36241/50100 (72.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1980s / 2775.1570 s
agent0:                 episode reward: -0.3156,                 loss: 0.4191
agent1:                 episode reward: 0.3156,                 loss: 0.3566
Score delta: 2.392104715865335, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/36011_0.
Episode: 36261/50100 (72.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4383s / 2776.5953 s
agent0:                 episode reward: -0.4093,                 loss: nan
agent1:                 episode reward: 0.4093,                 loss: 0.3552
Episode: 36281/50100 (72.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4351s / 2778.0304 s
agent0:                 episode reward: -0.5163,                 loss: nan
agent1:                 episode reward: 0.5163,                 loss: 0.3536
Episode: 36301/50100 (72.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4632s / 2779.4936 s
agent0:                 episode reward: -0.1673,                 loss: nan
agent1:                 episode reward: 0.1673,                 loss: 0.3511
Episode: 36321/50100 (72.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4611s / 2780.9548 s
agent0:                 episode reward: -0.1905,                 loss: nan
agent1:                 episode reward: 0.1905,                 loss: 0.3507
Episode: 36341/50100 (72.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4473s / 2782.4021 s
agent0:                 episode reward: -0.1024,                 loss: nan
agent1:                 episode reward: 0.1024,                 loss: 0.3485
Episode: 36361/50100 (72.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4405s / 2783.8425 s
agent0:                 episode reward: -0.2916,                 loss: nan
agent1:                 episode reward: 0.2916,                 loss: 0.3486
Episode: 36381/50100 (72.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 3.0336s / 2786.8761 s
agent0:                 episode reward: -0.4906,                 loss: nan
agent1:                 episode reward: 0.4906,                 loss: 0.3517
Score delta: 2.091173980935733, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/36169_1.
Episode: 36401/50100 (72.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0483s / 2788.9244 s
agent0:                 episode reward: -0.0743,                 loss: 0.4590
agent1:                 episode reward: 0.0743,                 loss: nan
Episode: 36421/50100 (72.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0351s / 2790.9595 s
agent0:                 episode reward: -0.0684,                 loss: 0.4516
agent1:                 episode reward: 0.0684,                 loss: nan
Episode: 36441/50100 (72.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0523s / 2793.0118 s
agent0:                 episode reward: -0.3339,                 loss: 0.4524
agent1:                 episode reward: 0.3339,                 loss: nan
Episode: 36461/50100 (72.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0720s / 2795.0837 s
agent0:                 episode reward: -0.7525,                 loss: 0.4515
agent1:                 episode reward: 0.7525,                 loss: nan
Episode: 36481/50100 (72.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0530s / 2797.1367 s
agent0:                 episode reward: -0.0407,                 loss: 0.4512
agent1:                 episode reward: 0.0407,                 loss: nan
Episode: 36501/50100 (72.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0283s / 2799.1650 s
agent0:                 episode reward: -0.0519,                 loss: 0.4509
agent1:                 episode reward: 0.0519,                 loss: nan
Episode: 36521/50100 (72.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0669s / 2801.2319 s
agent0:                 episode reward: 0.2222,                 loss: 0.4511
agent1:                 episode reward: -0.2222,                 loss: nan
Episode: 36541/50100 (72.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0503s / 2803.2822 s
agent0:                 episode reward: -0.2428,                 loss: 0.4512
agent1:                 episode reward: 0.2428,                 loss: nan
Episode: 36561/50100 (72.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0572s / 2805.3395 s
agent0:                 episode reward: -0.5859,                 loss: 0.4536
agent1:                 episode reward: 0.5859,                 loss: nan
Episode: 36581/50100 (73.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0569s / 2807.3964 s
agent0:                 episode reward: -0.1173,                 loss: 0.4513
agent1:                 episode reward: 0.1173,                 loss: nan
Episode: 36601/50100 (73.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0539s / 2809.4503 s
agent0:                 episode reward: -0.2952,                 loss: 0.4508
agent1:                 episode reward: 0.2952,                 loss: nan
Episode: 36621/50100 (73.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0754s / 2811.5257 s
agent0:                 episode reward: 0.1087,                 loss: 0.4500
agent1:                 episode reward: -0.1087,                 loss: nan
Episode: 36641/50100 (73.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0664s / 2813.5921 s
agent0:                 episode reward: 0.0381,                 loss: 0.4497
agent1:                 episode reward: -0.0381,                 loss: nan
Episode: 36661/50100 (73.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6285s / 2817.2206 s
agent0:                 episode reward: 0.2639,                 loss: 0.4485
agent1:                 episode reward: -0.2639,                 loss: 0.3292
Score delta: 2.146336130514785, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/36448_0.
Episode: 36681/50100 (73.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4710s / 2818.6916 s
agent0:                 episode reward: 0.3943,                 loss: nan
agent1:                 episode reward: -0.3943,                 loss: 0.3254
Episode: 36701/50100 (73.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4558s / 2820.1474 s
agent0:                 episode reward: -0.5304,                 loss: nan
agent1:                 episode reward: 0.5304,                 loss: 0.3262
Episode: 36721/50100 (73.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1484s / 2823.2958 s
agent0:                 episode reward: -0.4148,                 loss: 0.4382
agent1:                 episode reward: 0.4148,                 loss: 0.3243
Score delta: 2.209825071713575, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/36506_1.
Episode: 36741/50100 (73.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0670s / 2825.3628 s
agent0:                 episode reward: -0.4874,                 loss: 0.4405
agent1:                 episode reward: 0.4874,                 loss: nan
Episode: 36761/50100 (73.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0601s / 2827.4230 s
agent0:                 episode reward: 0.0306,                 loss: 0.4402
agent1:                 episode reward: -0.0306,                 loss: nan
Episode: 36781/50100 (73.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0723s / 2829.4953 s
agent0:                 episode reward: 0.1574,                 loss: 0.4447
agent1:                 episode reward: -0.1574,                 loss: nan
Episode: 36801/50100 (73.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0706s / 2831.5659 s
agent0:                 episode reward: 0.1725,                 loss: 0.4509
agent1:                 episode reward: -0.1725,                 loss: nan
Episode: 36821/50100 (73.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0900s / 2833.6559 s
agent0:                 episode reward: -0.1627,                 loss: 0.4498
agent1:                 episode reward: 0.1627,                 loss: nan
Episode: 36841/50100 (73.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0861s / 2835.7420 s
agent0:                 episode reward: 0.4636,                 loss: 0.4502
agent1:                 episode reward: -0.4636,                 loss: nan
Episode: 36861/50100 (73.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5608s / 2839.3028 s
agent0:                 episode reward: 0.6488,                 loss: 0.4509
agent1:                 episode reward: -0.6488,                 loss: 0.3585
Score delta: 2.238582078044586, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/36642_0.
Episode: 36881/50100 (73.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4857s / 2840.7885 s
agent0:                 episode reward: 0.0430,                 loss: nan
agent1:                 episode reward: -0.0430,                 loss: 0.3522
Episode: 36901/50100 (73.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4787s / 2842.2672 s
agent0:                 episode reward: 0.0804,                 loss: nan
agent1:                 episode reward: -0.0804,                 loss: 0.3482
Episode: 36921/50100 (73.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4602s / 2843.7274 s
agent0:                 episode reward: 0.1818,                 loss: nan
agent1:                 episode reward: -0.1818,                 loss: 0.3467
Episode: 36941/50100 (73.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4663s / 2845.1937 s
agent0:                 episode reward: -0.0450,                 loss: nan
agent1:                 episode reward: 0.0450,                 loss: 0.3452
Episode: 36961/50100 (73.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4864s / 2846.6801 s
agent0:                 episode reward: -0.7977,                 loss: nan
agent1:                 episode reward: 0.7977,                 loss: 0.3435
Episode: 36981/50100 (73.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5359s / 2850.2160 s
agent0:                 episode reward: -0.2272,                 loss: 0.4341
agent1:                 episode reward: 0.2272,                 loss: 0.3517
Score delta: 2.370404878462195, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/36755_1.
Episode: 37001/50100 (73.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0492s / 2852.2652 s
agent0:                 episode reward: -0.0378,                 loss: 0.4265
agent1:                 episode reward: 0.0378,                 loss: nan
Episode: 37021/50100 (73.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0446s / 2854.3098 s
agent0:                 episode reward: 0.0322,                 loss: 0.4246
agent1:                 episode reward: -0.0322,                 loss: nan
Episode: 37041/50100 (73.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0892s / 2856.3990 s
agent0:                 episode reward: -0.8105,                 loss: 0.4241
agent1:                 episode reward: 0.8105,                 loss: nan
Episode: 37061/50100 (73.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0576s / 2858.4566 s
agent0:                 episode reward: 0.1476,                 loss: 0.4137
agent1:                 episode reward: -0.1476,                 loss: nan
Episode: 37081/50100 (74.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0603s / 2860.5169 s
agent0:                 episode reward: -0.2114,                 loss: 0.3858
agent1:                 episode reward: 0.2114,                 loss: nan
Episode: 37101/50100 (74.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0305s / 2862.5474 s
agent0:                 episode reward: -0.1299,                 loss: 0.3857
agent1:                 episode reward: 0.1299,                 loss: nan
Episode: 37121/50100 (74.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0353s / 2864.5827 s
agent0:                 episode reward: -0.5602,                 loss: 0.3837
agent1:                 episode reward: 0.5602,                 loss: nan
Episode: 37141/50100 (74.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0578s / 2866.6404 s
agent0:                 episode reward: -0.0704,                 loss: 0.3826
agent1:                 episode reward: 0.0704,                 loss: nan
Episode: 37161/50100 (74.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0445s / 2868.6849 s
agent0:                 episode reward: -0.4793,                 loss: 0.3846
agent1:                 episode reward: 0.4793,                 loss: nan
Episode: 37181/50100 (74.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0371s / 2870.7221 s
agent0:                 episode reward: -0.2838,                 loss: 0.3830
agent1:                 episode reward: 0.2838,                 loss: nan
Episode: 37201/50100 (74.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0684s / 2872.7904 s
agent0:                 episode reward: -0.1747,                 loss: 0.3844
agent1:                 episode reward: 0.1747,                 loss: nan
Episode: 37221/50100 (74.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0572s / 2874.8476 s
agent0:                 episode reward: 0.2653,                 loss: 0.3810
agent1:                 episode reward: -0.2653,                 loss: nan
Episode: 37241/50100 (74.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0511s / 2876.8987 s
agent0:                 episode reward: -0.1572,                 loss: 0.3924
agent1:                 episode reward: 0.1572,                 loss: nan
Episode: 37261/50100 (74.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0505s / 2878.9492 s
agent0:                 episode reward: 0.0576,                 loss: 0.3917
agent1:                 episode reward: -0.0576,                 loss: nan
Episode: 37281/50100 (74.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0649s / 2881.0141 s
agent0:                 episode reward: -0.3498,                 loss: 0.3901
agent1:                 episode reward: 0.3498,                 loss: nan
Episode: 37301/50100 (74.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0423s / 2883.0564 s
agent0:                 episode reward: -0.3570,                 loss: 0.3913
agent1:                 episode reward: 0.3570,                 loss: nan
Episode: 37321/50100 (74.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0745s / 2885.1309 s
agent0:                 episode reward: -0.2548,                 loss: 0.3907
agent1:                 episode reward: 0.2548,                 loss: nan
Episode: 37341/50100 (74.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0702s / 2887.2011 s
agent0:                 episode reward: 0.0866,                 loss: 0.3910
agent1:                 episode reward: -0.0866,                 loss: nan
Episode: 37361/50100 (74.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0761s / 2889.2772 s
agent0:                 episode reward: -0.0632,                 loss: 0.3898
agent1:                 episode reward: 0.0632,                 loss: nan
Episode: 37381/50100 (74.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0667s / 2891.3439 s
agent0:                 episode reward: 0.0920,                 loss: 0.3893
agent1:                 episode reward: -0.0920,                 loss: nan
Episode: 37401/50100 (74.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0733s / 2893.4172 s
agent0:                 episode reward: 0.1229,                 loss: 0.4240
agent1:                 episode reward: -0.1229,                 loss: nan
Episode: 37421/50100 (74.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0795s / 2895.4967 s
agent0:                 episode reward: 0.4222,                 loss: 0.4361
agent1:                 episode reward: -0.4222,                 loss: nan
Episode: 37441/50100 (74.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0729s / 2897.5696 s
agent0:                 episode reward: -0.7069,                 loss: 0.4360
agent1:                 episode reward: 0.7069,                 loss: nan
Episode: 37461/50100 (74.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0671s / 2899.6367 s
agent0:                 episode reward: -0.4754,                 loss: 0.4364
agent1:                 episode reward: 0.4754,                 loss: nan
Episode: 37481/50100 (74.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0692s / 2901.7059 s
agent0:                 episode reward: 0.1164,                 loss: 0.4368
agent1:                 episode reward: -0.1164,                 loss: nan
Episode: 37501/50100 (74.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0496s / 2903.7555 s
agent0:                 episode reward: -0.1354,                 loss: 0.4359
agent1:                 episode reward: 0.1354,                 loss: nan
Episode: 37521/50100 (74.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0849s / 2905.8404 s
agent0:                 episode reward: -0.3563,                 loss: 0.4363
agent1:                 episode reward: 0.3563,                 loss: nan
Episode: 37541/50100 (74.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0515s / 2907.8919 s
agent0:                 episode reward: 0.1938,                 loss: 0.4358
agent1:                 episode reward: -0.1938,                 loss: nan
Episode: 37561/50100 (74.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0735s / 2909.9653 s
agent0:                 episode reward: 0.0427,                 loss: 0.4414
agent1:                 episode reward: -0.0427,                 loss: nan
Episode: 37581/50100 (75.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1000s / 2912.0653 s
agent0:                 episode reward: -0.1341,                 loss: 0.4497
agent1:                 episode reward: 0.1341,                 loss: nan
Episode: 37601/50100 (75.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0748s / 2914.1402 s
agent0:                 episode reward: 0.2645,                 loss: 0.4489
agent1:                 episode reward: -0.2645,                 loss: nan
Episode: 37621/50100 (75.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0674s / 2916.2076 s
agent0:                 episode reward: 0.1244,                 loss: 0.4489
agent1:                 episode reward: -0.1244,                 loss: nan
Episode: 37641/50100 (75.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1657s / 2919.3732 s
agent0:                 episode reward: -0.5941,                 loss: 0.4472
agent1:                 episode reward: 0.5941,                 loss: 0.3805
Score delta: 2.0491968899714044, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/37411_0.
Episode: 37661/50100 (75.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4843s / 2920.8575 s
agent0:                 episode reward: -0.0822,                 loss: nan
agent1:                 episode reward: 0.0822,                 loss: 0.3770
Episode: 37681/50100 (75.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4907s / 2922.3483 s
agent0:                 episode reward: -0.1455,                 loss: nan
agent1:                 episode reward: 0.1455,                 loss: 0.3793
Episode: 37701/50100 (75.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4903s / 2923.8386 s
agent0:                 episode reward: -0.1329,                 loss: nan
agent1:                 episode reward: 0.1329,                 loss: 0.3779
Episode: 37721/50100 (75.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 3.1307s / 2926.9693 s
agent0:                 episode reward: -0.4349,                 loss: nan
agent1:                 episode reward: 0.4349,                 loss: 0.3769
Score delta: 2.087044254203147, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/37509_1.
Episode: 37741/50100 (75.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0699s / 2929.0392 s
agent0:                 episode reward: -0.1205,                 loss: 0.4522
agent1:                 episode reward: 0.1205,                 loss: nan
Episode: 37761/50100 (75.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0614s / 2931.1006 s
agent0:                 episode reward: -0.8530,                 loss: 0.4496
agent1:                 episode reward: 0.8530,                 loss: nan
Episode: 37781/50100 (75.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0825s / 2933.1831 s
agent0:                 episode reward: 0.1501,                 loss: 0.4481
agent1:                 episode reward: -0.1501,                 loss: nan
Episode: 37801/50100 (75.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0634s / 2935.2465 s
agent0:                 episode reward: -0.7365,                 loss: 0.4496
agent1:                 episode reward: 0.7365,                 loss: nan
Episode: 37821/50100 (75.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0644s / 2937.3109 s
agent0:                 episode reward: 0.0377,                 loss: 0.4497
agent1:                 episode reward: -0.0377,                 loss: nan
Episode: 37841/50100 (75.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0665s / 2939.3774 s
agent0:                 episode reward: 0.1500,                 loss: 0.4481
agent1:                 episode reward: -0.1500,                 loss: nan
Episode: 37861/50100 (75.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0490s / 2941.4264 s
agent0:                 episode reward: 0.0739,                 loss: 0.4487
agent1:                 episode reward: -0.0739,                 loss: nan
Episode: 37881/50100 (75.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 3.4172s / 2944.8436 s
agent0:                 episode reward: 0.1318,                 loss: 0.4493
agent1:                 episode reward: -0.1318,                 loss: 0.3836
Score delta: 2.382392103532603, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/37657_0.
Episode: 37901/50100 (75.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5048s / 2946.3484 s
agent0:                 episode reward: -0.0076,                 loss: nan
agent1:                 episode reward: 0.0076,                 loss: 0.3829
Episode: 37921/50100 (75.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4843s / 2947.8327 s
agent0:                 episode reward: 0.0121,                 loss: nan
agent1:                 episode reward: -0.0121,                 loss: 0.3787
Episode: 37941/50100 (75.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4814s / 2949.3142 s
agent0:                 episode reward: -0.0418,                 loss: nan
agent1:                 episode reward: 0.0418,                 loss: 0.3846
Episode: 37961/50100 (75.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5159s / 2950.8301 s
agent0:                 episode reward: -0.0940,                 loss: nan
agent1:                 episode reward: 0.0940,                 loss: 0.3691
Episode: 37981/50100 (75.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5249s / 2952.3550 s
agent0:                 episode reward: -0.1418,                 loss: nan
agent1:                 episode reward: 0.1418,                 loss: 0.3707
Episode: 38001/50100 (75.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5158s / 2953.8708 s
agent0:                 episode reward: 0.0664,                 loss: nan
agent1:                 episode reward: -0.0664,                 loss: 0.3673
Episode: 38021/50100 (75.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2444s / 2957.1152 s
agent0:                 episode reward: -0.6120,                 loss: 0.4534
agent1:                 episode reward: 0.6120,                 loss: 0.3676
Score delta: 2.34919904881764, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/37807_1.
Episode: 38041/50100 (75.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0762s / 2959.1913 s
agent0:                 episode reward: 0.1171,                 loss: 0.4502
agent1:                 episode reward: -0.1171,                 loss: nan
Episode: 38061/50100 (75.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0563s / 2961.2477 s
agent0:                 episode reward: 0.2599,                 loss: 0.4495
agent1:                 episode reward: -0.2599,                 loss: nan
Episode: 38081/50100 (76.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0772s / 2963.3249 s
agent0:                 episode reward: -0.2706,                 loss: 0.4491
agent1:                 episode reward: 0.2706,                 loss: nan
Episode: 38101/50100 (76.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0753s / 2965.4001 s
agent0:                 episode reward: -0.6256,                 loss: 0.4482
agent1:                 episode reward: 0.6256,                 loss: nan
Episode: 38121/50100 (76.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0633s / 2967.4634 s
agent0:                 episode reward: 0.0057,                 loss: 0.4488
agent1:                 episode reward: -0.0057,                 loss: nan
Episode: 38141/50100 (76.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0390s / 2969.5024 s
agent0:                 episode reward: -0.0647,                 loss: 0.4461
agent1:                 episode reward: 0.0647,                 loss: nan
Episode: 38161/50100 (76.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0751s / 2971.5775 s
agent0:                 episode reward: -0.0859,                 loss: 0.4416
agent1:                 episode reward: 0.0859,                 loss: nan
Episode: 38181/50100 (76.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0747s / 2973.6522 s
agent0:                 episode reward: -0.0230,                 loss: 0.4405
agent1:                 episode reward: 0.0230,                 loss: nan
Episode: 38201/50100 (76.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1328s / 2975.7850 s
agent0:                 episode reward: 0.2994,                 loss: 0.4416
agent1:                 episode reward: -0.2994,                 loss: nan
Episode: 38221/50100 (76.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1176s / 2977.9025 s
agent0:                 episode reward: -0.6106,                 loss: 0.4414
agent1:                 episode reward: 0.6106,                 loss: nan
Episode: 38241/50100 (76.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1242s / 2980.0267 s
agent0:                 episode reward: 0.2448,                 loss: 0.4413
agent1:                 episode reward: -0.2448,                 loss: nan
Episode: 38261/50100 (76.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1302s / 2982.1569 s
agent0:                 episode reward: 0.3823,                 loss: 0.4418
agent1:                 episode reward: -0.3823,                 loss: nan
Episode: 38281/50100 (76.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0739s / 2984.2309 s
agent0:                 episode reward: 0.0634,                 loss: 0.4416
agent1:                 episode reward: -0.0634,                 loss: nan
Episode: 38301/50100 (76.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0615s / 2986.2923 s
agent0:                 episode reward: -0.2701,                 loss: 0.4421
agent1:                 episode reward: 0.2701,                 loss: nan
Episode: 38321/50100 (76.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0315s / 2988.3238 s
agent0:                 episode reward: -0.1442,                 loss: 0.4332
agent1:                 episode reward: 0.1442,                 loss: nan
Episode: 38341/50100 (76.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0518s / 2990.3756 s
agent0:                 episode reward: -0.1074,                 loss: 0.4332
agent1:                 episode reward: 0.1074,                 loss: nan
Episode: 38361/50100 (76.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0683s / 2992.4438 s
agent0:                 episode reward: -0.4110,                 loss: 0.4322
agent1:                 episode reward: 0.4110,                 loss: nan
Episode: 38381/50100 (76.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0531s / 2994.4969 s
agent0:                 episode reward: 0.3098,                 loss: 0.4327
agent1:                 episode reward: -0.3098,                 loss: nan
Episode: 38401/50100 (76.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0631s / 2996.5600 s
agent0:                 episode reward: -0.3003,                 loss: 0.4328
agent1:                 episode reward: 0.3003,                 loss: nan
Episode: 38421/50100 (76.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0875s / 2998.6475 s
agent0:                 episode reward: 0.0846,                 loss: 0.4326
agent1:                 episode reward: -0.0846,                 loss: nan
Episode: 38441/50100 (76.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0703s / 3000.7178 s
agent0:                 episode reward: 0.2026,                 loss: 0.4327
agent1:                 episode reward: -0.2026,                 loss: nan
Episode: 38461/50100 (76.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0587s / 3002.7765 s
agent0:                 episode reward: 0.1214,                 loss: 0.4318
agent1:                 episode reward: -0.1214,                 loss: nan
Episode: 38481/50100 (76.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0585s / 3004.8349 s
agent0:                 episode reward: 0.4431,                 loss: 0.4336
agent1:                 episode reward: -0.4431,                 loss: nan
Episode: 38501/50100 (76.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0469s / 3006.8818 s
agent0:                 episode reward: -0.0832,                 loss: 0.4336
agent1:                 episode reward: 0.0832,                 loss: nan
Episode: 38521/50100 (76.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0592s / 3008.9411 s
agent0:                 episode reward: 0.0843,                 loss: 0.4341
agent1:                 episode reward: -0.0843,                 loss: nan
Episode: 38541/50100 (76.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0683s / 3011.0094 s
agent0:                 episode reward: -0.4663,                 loss: 0.4329
agent1:                 episode reward: 0.4663,                 loss: nan
Episode: 38561/50100 (76.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0658s / 3013.0752 s
agent0:                 episode reward: 0.3219,                 loss: 0.4340
agent1:                 episode reward: -0.3219,                 loss: nan
Episode: 38581/50100 (77.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0569s / 3015.1321 s
agent0:                 episode reward: 0.1522,                 loss: 0.4335
agent1:                 episode reward: -0.1522,                 loss: nan
Episode: 38601/50100 (77.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0637s / 3017.1958 s
agent0:                 episode reward: 0.2084,                 loss: 0.4341
agent1:                 episode reward: -0.2084,                 loss: nan
Episode: 38621/50100 (77.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0541s / 3019.2499 s
agent0:                 episode reward: -0.1976,                 loss: 0.4329
agent1:                 episode reward: 0.1976,                 loss: nan
Episode: 38641/50100 (77.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0530s / 3021.3029 s
agent0:                 episode reward: 0.2785,                 loss: 0.4345
agent1:                 episode reward: -0.2785,                 loss: nan
Episode: 38661/50100 (77.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0546s / 3023.3575 s
agent0:                 episode reward: -0.1312,                 loss: 0.4384
agent1:                 episode reward: 0.1312,                 loss: nan
Episode: 38681/50100 (77.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0617s / 3025.4193 s
agent0:                 episode reward: 0.0097,                 loss: 0.4374
agent1:                 episode reward: -0.0097,                 loss: nan
Episode: 38701/50100 (77.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5629s / 3028.9822 s
agent0:                 episode reward: 0.5809,                 loss: 0.4381
agent1:                 episode reward: -0.5809,                 loss: 0.3595
Score delta: 2.197525012480535, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/38483_0.
Episode: 38721/50100 (77.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5120s / 3030.4942 s
agent0:                 episode reward: -0.0559,                 loss: nan
agent1:                 episode reward: 0.0559,                 loss: 0.3563
Episode: 38741/50100 (77.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5165s / 3032.0107 s
agent0:                 episode reward: -0.0874,                 loss: nan
agent1:                 episode reward: 0.0874,                 loss: 0.3528
Episode: 38761/50100 (77.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5086s / 3033.5193 s
agent0:                 episode reward: -0.2943,                 loss: nan
agent1:                 episode reward: 0.2943,                 loss: 0.3505
Episode: 38781/50100 (77.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5365s / 3035.0558 s
agent0:                 episode reward: -0.2033,                 loss: nan
agent1:                 episode reward: 0.2033,                 loss: 0.3556
Episode: 38801/50100 (77.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5232s / 3036.5790 s
agent0:                 episode reward: 0.2366,                 loss: nan
agent1:                 episode reward: -0.2366,                 loss: 0.3446
Episode: 38821/50100 (77.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5309s / 3038.1099 s
agent0:                 episode reward: 0.2999,                 loss: nan
agent1:                 episode reward: -0.2999,                 loss: 0.3414
Episode: 38841/50100 (77.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 3.2252s / 3041.3351 s
agent0:                 episode reward: -0.4530,                 loss: 0.4737
agent1:                 episode reward: 0.4530,                 loss: 0.3419
Score delta: 2.0737747953992947, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/38627_1.
Episode: 38861/50100 (77.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0519s / 3043.3870 s
agent0:                 episode reward: -0.2503,                 loss: 0.4550
agent1:                 episode reward: 0.2503,                 loss: nan
Episode: 38881/50100 (77.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0441s / 3045.4311 s
agent0:                 episode reward: -0.2663,                 loss: 0.4536
agent1:                 episode reward: 0.2663,                 loss: nan
Episode: 38901/50100 (77.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0789s / 3047.5100 s
agent0:                 episode reward: -0.0094,                 loss: 0.4522
agent1:                 episode reward: 0.0094,                 loss: nan
Episode: 38921/50100 (77.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0659s / 3049.5760 s
agent0:                 episode reward: 0.4978,                 loss: 0.4524
agent1:                 episode reward: -0.4978,                 loss: nan
Episode: 38941/50100 (77.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7913s / 3053.3673 s
agent0:                 episode reward: 0.8113,                 loss: 0.4526
agent1:                 episode reward: -0.8113,                 loss: nan
Score delta: 2.265484333266876, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/38729_0.
Episode: 38961/50100 (77.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5451s / 3054.9124 s
agent0:                 episode reward: -0.5702,                 loss: nan
agent1:                 episode reward: 0.5702,                 loss: 0.3466
Episode: 38981/50100 (77.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7280s / 3058.6404 s
agent0:                 episode reward: -0.1858,                 loss: 0.3986
agent1:                 episode reward: 0.1858,                 loss: 0.3442
Score delta: 2.0225625446071547, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/38750_1.
Episode: 39001/50100 (77.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0933s / 3060.7337 s
agent0:                 episode reward: -0.0145,                 loss: 0.3972
agent1:                 episode reward: 0.0145,                 loss: nan
Episode: 39021/50100 (77.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0262s / 3062.7599 s
agent0:                 episode reward: -0.1347,                 loss: 0.3964
agent1:                 episode reward: 0.1347,                 loss: nan
Episode: 39041/50100 (77.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0421s / 3064.8020 s
agent0:                 episode reward: -0.1511,                 loss: 0.3965
agent1:                 episode reward: 0.1511,                 loss: nan
Episode: 39061/50100 (77.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0684s / 3066.8703 s
agent0:                 episode reward: -0.2728,                 loss: 0.3944
agent1:                 episode reward: 0.2728,                 loss: nan
Episode: 39081/50100 (78.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0676s / 3068.9379 s
agent0:                 episode reward: -0.3401,                 loss: 0.3952
agent1:                 episode reward: 0.3401,                 loss: nan
Episode: 39101/50100 (78.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0794s / 3071.0173 s
agent0:                 episode reward: -0.3467,                 loss: 0.3966
agent1:                 episode reward: 0.3467,                 loss: nan
Episode: 39121/50100 (78.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0576s / 3073.0749 s
agent0:                 episode reward: -0.1540,                 loss: 0.3942
agent1:                 episode reward: 0.1540,                 loss: nan
Episode: 39141/50100 (78.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0770s / 3075.1519 s
agent0:                 episode reward: -0.2143,                 loss: 0.4064
agent1:                 episode reward: 0.2143,                 loss: nan
Episode: 39161/50100 (78.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0569s / 3077.2088 s
agent0:                 episode reward: 0.2342,                 loss: 0.4256
agent1:                 episode reward: -0.2342,                 loss: nan
Episode: 39181/50100 (78.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0592s / 3079.2680 s
agent0:                 episode reward: 0.1155,                 loss: 0.4231
agent1:                 episode reward: -0.1155,                 loss: nan
Episode: 39201/50100 (78.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0769s / 3081.3450 s
agent0:                 episode reward: -0.6492,                 loss: 0.4247
agent1:                 episode reward: 0.6492,                 loss: nan
Episode: 39221/50100 (78.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0723s / 3083.4172 s
agent0:                 episode reward: 0.0680,                 loss: 0.4234
agent1:                 episode reward: -0.0680,                 loss: nan
Episode: 39241/50100 (78.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0724s / 3085.4896 s
agent0:                 episode reward: 0.1608,                 loss: 0.4235
agent1:                 episode reward: -0.1608,                 loss: nan
Episode: 39261/50100 (78.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0553s / 3087.5449 s
agent0:                 episode reward: -0.0452,                 loss: 0.4223
agent1:                 episode reward: 0.0452,                 loss: nan
Episode: 39281/50100 (78.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0651s / 3089.6100 s
agent0:                 episode reward: 0.2381,                 loss: 0.4229
agent1:                 episode reward: -0.2381,                 loss: nan
Episode: 39301/50100 (78.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0745s / 3091.6845 s
agent0:                 episode reward: 0.6168,                 loss: 0.4251
agent1:                 episode reward: -0.6168,                 loss: nan
Episode: 39321/50100 (78.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0676s / 3093.7521 s
agent0:                 episode reward: 0.0247,                 loss: 0.4414
agent1:                 episode reward: -0.0247,                 loss: nan
Episode: 39341/50100 (78.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0677s / 3095.8198 s
agent0:                 episode reward: 0.2073,                 loss: 0.4430
agent1:                 episode reward: -0.2073,                 loss: nan
Episode: 39361/50100 (78.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0882s / 3097.9080 s
agent0:                 episode reward: -0.4110,                 loss: 0.4424
agent1:                 episode reward: 0.4110,                 loss: nan
Episode: 39381/50100 (78.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0636s / 3099.9716 s
agent0:                 episode reward: -0.1620,                 loss: 0.4414
agent1:                 episode reward: 0.1620,                 loss: nan
Episode: 39401/50100 (78.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0647s / 3102.0363 s
agent0:                 episode reward: 0.4479,                 loss: 0.4422
agent1:                 episode reward: -0.4479,                 loss: nan
Episode: 39421/50100 (78.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 3.3627s / 3105.3989 s
agent0:                 episode reward: -0.1468,                 loss: 0.4441
agent1:                 episode reward: 0.1468,                 loss: 0.3606
Score delta: 2.017880612098576, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/39193_0.
Episode: 39441/50100 (78.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5298s / 3106.9287 s
agent0:                 episode reward: 0.3001,                 loss: nan
agent1:                 episode reward: -0.3001,                 loss: 0.3559
Episode: 39461/50100 (78.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5293s / 3108.4580 s
agent0:                 episode reward: 0.3517,                 loss: nan
agent1:                 episode reward: -0.3517,                 loss: 0.3549
Episode: 39481/50100 (78.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5396s / 3109.9976 s
agent0:                 episode reward: 0.1895,                 loss: nan
agent1:                 episode reward: -0.1895,                 loss: 0.3565
Episode: 39501/50100 (78.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5197s / 3111.5173 s
agent0:                 episode reward: -0.7494,                 loss: nan
agent1:                 episode reward: 0.7494,                 loss: 0.3832
Episode: 39521/50100 (78.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5231s / 3113.0404 s
agent0:                 episode reward: 0.0328,                 loss: nan
agent1:                 episode reward: -0.0328,                 loss: 0.3735
Episode: 39541/50100 (78.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6192s / 3114.6596 s
agent0:                 episode reward: 0.3745,                 loss: nan
agent1:                 episode reward: -0.3745,                 loss: 0.3741
Episode: 39561/50100 (78.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5651s / 3116.2247 s
agent0:                 episode reward: -0.2514,                 loss: nan
agent1:                 episode reward: 0.2514,                 loss: 0.3731
Episode: 39581/50100 (79.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5539s / 3117.7785 s
agent0:                 episode reward: -0.5489,                 loss: nan
agent1:                 episode reward: 0.5489,                 loss: 0.3723
Episode: 39601/50100 (79.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5455s / 3119.3241 s
agent0:                 episode reward: -0.5479,                 loss: nan
agent1:                 episode reward: 0.5479,                 loss: 0.3728
Episode: 39621/50100 (79.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5693s / 3120.8934 s
agent0:                 episode reward: -0.2304,                 loss: nan
agent1:                 episode reward: 0.2304,                 loss: 0.3712
Episode: 39641/50100 (79.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5584s / 3122.4517 s
agent0:                 episode reward: -0.2628,                 loss: nan
agent1:                 episode reward: 0.2628,                 loss: 0.3727
Episode: 39661/50100 (79.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6698s / 3126.1216 s
agent0:                 episode reward: -0.1823,                 loss: 0.4521
agent1:                 episode reward: 0.1823,                 loss: 0.3708
Score delta: 2.067122780077389, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/39433_1.
Episode: 39681/50100 (79.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0493s / 3128.1708 s
agent0:                 episode reward: -0.0605,                 loss: 0.4506
agent1:                 episode reward: 0.0605,                 loss: nan
Episode: 39701/50100 (79.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0764s / 3130.2472 s
agent0:                 episode reward: -0.1197,                 loss: 0.4513
agent1:                 episode reward: 0.1197,                 loss: nan
Episode: 39721/50100 (79.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0673s / 3132.3145 s
agent0:                 episode reward: 0.3550,                 loss: 0.4421
agent1:                 episode reward: -0.3550,                 loss: nan
Episode: 39741/50100 (79.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0712s / 3134.3857 s
agent0:                 episode reward: 0.2461,                 loss: 0.4417
agent1:                 episode reward: -0.2461,                 loss: nan
Episode: 39761/50100 (79.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0729s / 3136.4585 s
agent0:                 episode reward: 0.4289,                 loss: 0.4408
agent1:                 episode reward: -0.4289,                 loss: nan
Episode: 39781/50100 (79.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0697s / 3138.5283 s
agent0:                 episode reward: 0.5661,                 loss: 0.4397
agent1:                 episode reward: -0.5661,                 loss: nan
Episode: 39801/50100 (79.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0804s / 3140.6086 s
agent0:                 episode reward: 0.1742,                 loss: 0.4398
agent1:                 episode reward: -0.1742,                 loss: nan
Episode: 39821/50100 (79.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0147s / 3142.6233 s
agent0:                 episode reward: -0.0429,                 loss: 0.4397
agent1:                 episode reward: 0.0429,                 loss: nan
Episode: 39841/50100 (79.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0525s / 3144.6758 s
agent0:                 episode reward: 0.0825,                 loss: 0.4384
agent1:                 episode reward: -0.0825,                 loss: nan
Episode: 39861/50100 (79.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0584s / 3146.7342 s
agent0:                 episode reward: -0.0273,                 loss: 0.4392
agent1:                 episode reward: 0.0273,                 loss: nan
Episode: 39881/50100 (79.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 3.4494s / 3150.1836 s
agent0:                 episode reward: 0.0629,                 loss: 0.4383
agent1:                 episode reward: -0.0629,                 loss: 0.3700
Score delta: 2.2669624110875772, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/39655_0.
Episode: 39901/50100 (79.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5626s / 3151.7462 s
agent0:                 episode reward: -0.2981,                 loss: nan
agent1:                 episode reward: 0.2981,                 loss: 0.3494
Episode: 39921/50100 (79.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5507s / 3153.2968 s
agent0:                 episode reward: 0.0427,                 loss: nan
agent1:                 episode reward: -0.0427,                 loss: 0.3465
Episode: 39941/50100 (79.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5569s / 3154.8537 s
agent0:                 episode reward: -0.2630,                 loss: nan
agent1:                 episode reward: 0.2630,                 loss: 0.3478
Episode: 39961/50100 (79.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5631s / 3156.4168 s
agent0:                 episode reward: 0.1672,                 loss: nan
agent1:                 episode reward: -0.1672,                 loss: 0.3467
Episode: 39981/50100 (79.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5738s / 3157.9907 s
agent0:                 episode reward: -0.0318,                 loss: nan
agent1:                 episode reward: 0.0318,                 loss: 0.3457
Episode: 40001/50100 (79.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5628s / 3159.5535 s
agent0:                 episode reward: 0.1257,                 loss: nan
agent1:                 episode reward: -0.1257,                 loss: 0.3448
Episode: 40021/50100 (79.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5801s / 3161.1335 s
agent0:                 episode reward: 0.0893,                 loss: nan
agent1:                 episode reward: -0.0893,                 loss: 0.3449
Episode: 40041/50100 (79.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5589s / 3162.6924 s
agent0:                 episode reward: 0.0443,                 loss: nan
agent1:                 episode reward: -0.0443,                 loss: 0.3451
Episode: 40061/50100 (79.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5633s / 3164.2558 s
agent0:                 episode reward: -0.1754,                 loss: nan
agent1:                 episode reward: 0.1754,                 loss: 0.3201
Episode: 40081/50100 (80.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5179s / 3167.7736 s
agent0:                 episode reward: -0.7245,                 loss: 0.4436
agent1:                 episode reward: 0.7245,                 loss: 0.3149
Score delta: 2.395129004132332, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/39860_1.
Episode: 40101/50100 (80.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0508s / 3169.8244 s
agent0:                 episode reward: -0.1890,                 loss: 0.4560
agent1:                 episode reward: 0.1890,                 loss: nan
Episode: 40121/50100 (80.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0601s / 3171.8845 s
agent0:                 episode reward: -0.4504,                 loss: 0.4547
agent1:                 episode reward: 0.4504,                 loss: nan
Episode: 40141/50100 (80.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0162s / 3173.9007 s
agent0:                 episode reward: 0.0004,                 loss: 0.4541
agent1:                 episode reward: -0.0004,                 loss: nan
Episode: 40161/50100 (80.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0696s / 3175.9703 s
agent0:                 episode reward: 0.4304,                 loss: 0.4531
agent1:                 episode reward: -0.4304,                 loss: nan
Episode: 40181/50100 (80.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6730s / 3179.6433 s
agent0:                 episode reward: 0.2921,                 loss: 0.4535
agent1:                 episode reward: -0.2921,                 loss: 0.3463
Score delta: 2.1157886150100476, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/39965_0.
Episode: 40201/50100 (80.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5645s / 3181.2077 s
agent0:                 episode reward: -0.4790,                 loss: nan
agent1:                 episode reward: 0.4790,                 loss: 0.3484
Episode: 40221/50100 (80.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6792s / 3184.8870 s
agent0:                 episode reward: -0.3535,                 loss: 0.4230
agent1:                 episode reward: 0.3535,                 loss: 0.3482
Score delta: 2.348143236395439, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/39994_1.
Episode: 40241/50100 (80.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0681s / 3186.9550 s
agent0:                 episode reward: -0.2650,                 loss: 0.4185
agent1:                 episode reward: 0.2650,                 loss: nan
Episode: 40261/50100 (80.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0436s / 3188.9986 s
agent0:                 episode reward: -0.1678,                 loss: 0.4174
agent1:                 episode reward: 0.1678,                 loss: nan
Episode: 40281/50100 (80.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0612s / 3191.0599 s
agent0:                 episode reward: 0.3006,                 loss: 0.4337
agent1:                 episode reward: -0.3006,                 loss: nan
Episode: 40301/50100 (80.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0934s / 3193.1533 s
agent0:                 episode reward: -0.2684,                 loss: 0.4530
agent1:                 episode reward: 0.2684,                 loss: nan
Episode: 40321/50100 (80.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0896s / 3195.2429 s
agent0:                 episode reward: -0.5848,                 loss: 0.4518
agent1:                 episode reward: 0.5848,                 loss: nan
Episode: 40341/50100 (80.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0754s / 3197.3183 s
agent0:                 episode reward: 0.5291,                 loss: 0.4516
agent1:                 episode reward: -0.5291,                 loss: nan
Episode: 40361/50100 (80.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5273s / 3200.8457 s
agent0:                 episode reward: 0.1751,                 loss: 0.4525
agent1:                 episode reward: -0.1751,                 loss: 0.3763
Score delta: 2.00546927347486, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/40133_0.
Episode: 40381/50100 (80.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5956s / 3202.4413 s
agent0:                 episode reward: 0.4468,                 loss: nan
agent1:                 episode reward: -0.4468,                 loss: 0.3778
Episode: 40401/50100 (80.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6022s / 3204.0435 s
agent0:                 episode reward: 0.0254,                 loss: nan
agent1:                 episode reward: -0.0254,                 loss: 0.3775
Episode: 40421/50100 (80.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5896s / 3205.6331 s
agent0:                 episode reward: 0.5161,                 loss: nan
agent1:                 episode reward: -0.5161,                 loss: 0.3757
Episode: 40441/50100 (80.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5611s / 3209.1942 s
agent0:                 episode reward: -0.6871,                 loss: 0.4497
agent1:                 episode reward: 0.6871,                 loss: 0.3762
Score delta: 2.15540601941328, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/40219_1.
Episode: 40461/50100 (80.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0613s / 3211.2555 s
agent0:                 episode reward: -0.3168,                 loss: 0.4496
agent1:                 episode reward: 0.3168,                 loss: nan
Episode: 40481/50100 (80.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0806s / 3213.3361 s
agent0:                 episode reward: -0.7079,                 loss: 0.4498
agent1:                 episode reward: 0.7079,                 loss: nan
Episode: 40501/50100 (80.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0717s / 3215.4079 s
agent0:                 episode reward: 0.3140,                 loss: 0.4491
agent1:                 episode reward: -0.3140,                 loss: nan
Episode: 40521/50100 (80.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0772s / 3217.4851 s
agent0:                 episode reward: 0.2382,                 loss: 0.4493
agent1:                 episode reward: -0.2382,                 loss: nan
Episode: 40541/50100 (80.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0839s / 3219.5690 s
agent0:                 episode reward: 0.4875,                 loss: 0.4446
agent1:                 episode reward: -0.4875,                 loss: nan
Episode: 40561/50100 (80.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0722s / 3221.6412 s
agent0:                 episode reward: -0.0856,                 loss: 0.4425
agent1:                 episode reward: 0.0856,                 loss: nan
Episode: 40581/50100 (81.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0681s / 3223.7093 s
agent0:                 episode reward: -0.3366,                 loss: 0.4432
agent1:                 episode reward: 0.3366,                 loss: nan
Episode: 40601/50100 (81.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0671s / 3225.7764 s
agent0:                 episode reward: -0.6392,                 loss: 0.4421
agent1:                 episode reward: 0.6392,                 loss: nan
Episode: 40621/50100 (81.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0830s / 3227.8594 s
agent0:                 episode reward: -0.4683,                 loss: 0.4422
agent1:                 episode reward: 0.4683,                 loss: nan
Episode: 40641/50100 (81.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0721s / 3229.9315 s
agent0:                 episode reward: 0.0233,                 loss: 0.4423
agent1:                 episode reward: -0.0233,                 loss: nan
Episode: 40661/50100 (81.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0743s / 3232.0058 s
agent0:                 episode reward: -0.1033,                 loss: 0.4416
agent1:                 episode reward: 0.1033,                 loss: nan
Episode: 40681/50100 (81.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7992s / 3235.8050 s
agent0:                 episode reward: 0.5719,                 loss: 0.4416
agent1:                 episode reward: -0.5719,                 loss: 0.3376
Score delta: 2.1166714297522438, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/40467_0.
Episode: 40701/50100 (81.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5828s / 3237.3878 s
agent0:                 episode reward: 0.1698,                 loss: nan
agent1:                 episode reward: -0.1698,                 loss: 0.3475
Episode: 40721/50100 (81.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5744s / 3238.9622 s
agent0:                 episode reward: -0.1241,                 loss: nan
agent1:                 episode reward: 0.1241,                 loss: 0.3164
Episode: 40741/50100 (81.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5691s / 3240.5313 s
agent0:                 episode reward: 0.1977,                 loss: nan
agent1:                 episode reward: -0.1977,                 loss: 0.3098
Episode: 40761/50100 (81.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6319s / 3244.1633 s
agent0:                 episode reward: -0.7608,                 loss: 0.4518
agent1:                 episode reward: 0.7608,                 loss: 0.3098
Score delta: 2.331435232616743, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/40537_1.
Episode: 40781/50100 (81.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0500s / 3246.2133 s
agent0:                 episode reward: 0.1446,                 loss: 0.4418
agent1:                 episode reward: -0.1446,                 loss: nan
Episode: 40801/50100 (81.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6735s / 3249.8867 s
agent0:                 episode reward: 0.4583,                 loss: 0.4410
agent1:                 episode reward: -0.4583,                 loss: 0.3800
Score delta: 2.044276501997744, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/40581_0.
Episode: 40821/50100 (81.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5986s / 3251.4854 s
agent0:                 episode reward: -0.0704,                 loss: nan
agent1:                 episode reward: 0.0704,                 loss: 0.3798
Episode: 40841/50100 (81.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5922s / 3253.0776 s
agent0:                 episode reward: 0.1566,                 loss: nan
agent1:                 episode reward: -0.1566,                 loss: 0.3787
Episode: 40861/50100 (81.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6038s / 3254.6813 s
agent0:                 episode reward: -0.1707,                 loss: nan
agent1:                 episode reward: 0.1707,                 loss: 0.3780
Episode: 40881/50100 (81.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6042s / 3256.2855 s
agent0:                 episode reward: -0.3324,                 loss: nan
agent1:                 episode reward: 0.3324,                 loss: 0.3765
Episode: 40901/50100 (81.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6544s / 3259.9400 s
agent0:                 episode reward: -0.4329,                 loss: 0.4302
agent1:                 episode reward: 0.4329,                 loss: 0.3734
Score delta: 2.4351506272925763, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/40678_1.
Episode: 40921/50100 (81.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0742s / 3262.0141 s
agent0:                 episode reward: -0.3010,                 loss: 0.4311
agent1:                 episode reward: 0.3010,                 loss: nan
Episode: 40941/50100 (81.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0358s / 3264.0499 s
agent0:                 episode reward: 0.0467,                 loss: 0.4283
agent1:                 episode reward: -0.0467,                 loss: nan
Episode: 40961/50100 (81.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0362s / 3266.0860 s
agent0:                 episode reward: 0.2433,                 loss: 0.4301
agent1:                 episode reward: -0.2433,                 loss: nan
Episode: 40981/50100 (81.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0562s / 3268.1422 s
agent0:                 episode reward: -0.2708,                 loss: 0.4289
agent1:                 episode reward: 0.2708,                 loss: nan
Episode: 41001/50100 (81.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0318s / 3270.1741 s
agent0:                 episode reward: -0.0342,                 loss: 0.4283
agent1:                 episode reward: 0.0342,                 loss: nan
Episode: 41021/50100 (81.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0468s / 3272.2209 s
agent0:                 episode reward: 0.3565,                 loss: 0.4289
agent1:                 episode reward: -0.3565,                 loss: nan
Episode: 41041/50100 (81.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0463s / 3274.2672 s
agent0:                 episode reward: -0.1561,                 loss: 0.4200
agent1:                 episode reward: 0.1561,                 loss: nan
Episode: 41061/50100 (81.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0267s / 3276.2939 s
agent0:                 episode reward: -0.4627,                 loss: 0.4160
agent1:                 episode reward: 0.4627,                 loss: nan
Episode: 41081/50100 (81.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0400s / 3278.3339 s
agent0:                 episode reward: 0.2127,                 loss: 0.4140
agent1:                 episode reward: -0.2127,                 loss: nan
Episode: 41101/50100 (82.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0674s / 3280.4013 s
agent0:                 episode reward: 0.1788,                 loss: 0.4133
agent1:                 episode reward: -0.1788,                 loss: nan
Episode: 41121/50100 (82.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5249s / 3283.9263 s
agent0:                 episode reward: 0.1626,                 loss: 0.4119
agent1:                 episode reward: -0.1626,                 loss: 0.3447
Score delta: 2.4009434312388303, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/40895_0.
Episode: 41141/50100 (82.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6123s / 3285.5386 s
agent0:                 episode reward: 0.1701,                 loss: nan
agent1:                 episode reward: -0.1701,                 loss: 0.3324
Episode: 41161/50100 (82.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6307s / 3287.1692 s
agent0:                 episode reward: -0.3832,                 loss: nan
agent1:                 episode reward: 0.3832,                 loss: 0.3096
Episode: 41181/50100 (82.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6981s / 3290.8673 s
agent0:                 episode reward: -0.2757,                 loss: 0.4267
agent1:                 episode reward: 0.2757,                 loss: 0.3031
Score delta: 2.1089793337746308, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/40958_1.
Episode: 41201/50100 (82.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0493s / 3292.9166 s
agent0:                 episode reward: 0.0560,                 loss: 0.4181
agent1:                 episode reward: -0.0560,                 loss: nan
Episode: 41221/50100 (82.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0551s / 3294.9717 s
agent0:                 episode reward: 0.5366,                 loss: 0.4171
agent1:                 episode reward: -0.5366,                 loss: nan
Episode: 41241/50100 (82.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8564s / 3298.8281 s
agent0:                 episode reward: 0.6506,                 loss: 0.4178
agent1:                 episode reward: -0.6506,                 loss: nan
Score delta: 2.263618558313164, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/41029_0.
Episode: 41261/50100 (82.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6148s / 3300.4430 s
agent0:                 episode reward: 0.1149,                 loss: nan
agent1:                 episode reward: -0.1149,                 loss: 0.3441
Episode: 41281/50100 (82.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6116s / 3302.0546 s
agent0:                 episode reward: -0.3828,                 loss: nan
agent1:                 episode reward: 0.3828,                 loss: 0.3404
Episode: 41301/50100 (82.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5982s / 3303.6528 s
agent0:                 episode reward: -0.2237,                 loss: nan
agent1:                 episode reward: 0.2237,                 loss: 0.3438
Episode: 41321/50100 (82.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6224s / 3305.2752 s
agent0:                 episode reward: -0.2266,                 loss: nan
agent1:                 episode reward: 0.2266,                 loss: 0.3409
Episode: 41341/50100 (82.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6262s / 3306.9014 s
agent0:                 episode reward: 0.0868,                 loss: nan
agent1:                 episode reward: -0.0868,                 loss: 0.3402
Episode: 41361/50100 (82.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5089s / 3310.4103 s
agent0:                 episode reward: -0.4458,                 loss: 0.4518
agent1:                 episode reward: 0.4458,                 loss: 0.3386
Score delta: 2.287512955181101, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/41146_1.
Episode: 41381/50100 (82.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0702s / 3312.4805 s
agent0:                 episode reward: -0.0153,                 loss: 0.4446
agent1:                 episode reward: 0.0153,                 loss: nan
Episode: 41401/50100 (82.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0707s / 3314.5511 s
agent0:                 episode reward: -0.1729,                 loss: 0.4368
agent1:                 episode reward: 0.1729,                 loss: nan
Episode: 41421/50100 (82.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0773s / 3316.6284 s
agent0:                 episode reward: -0.3519,                 loss: 0.4362
agent1:                 episode reward: 0.3519,                 loss: nan
Episode: 41441/50100 (82.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0457s / 3318.6741 s
agent0:                 episode reward: -0.5643,                 loss: 0.4349
agent1:                 episode reward: 0.5643,                 loss: nan
Episode: 41461/50100 (82.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0835s / 3320.7576 s
agent0:                 episode reward: 0.5414,                 loss: 0.4356
agent1:                 episode reward: -0.5414,                 loss: nan
Episode: 41481/50100 (82.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0731s / 3322.8307 s
agent0:                 episode reward: -0.0207,                 loss: 0.4349
agent1:                 episode reward: 0.0207,                 loss: nan
Episode: 41501/50100 (82.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0733s / 3324.9039 s
agent0:                 episode reward: -0.3863,                 loss: 0.4328
agent1:                 episode reward: 0.3863,                 loss: nan
Episode: 41521/50100 (82.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0647s / 3326.9687 s
agent0:                 episode reward: -0.2108,                 loss: 0.4340
agent1:                 episode reward: 0.2108,                 loss: nan
Episode: 41541/50100 (82.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0852s / 3329.0539 s
agent0:                 episode reward: -0.0546,                 loss: 0.4344
agent1:                 episode reward: 0.0546,                 loss: nan
Episode: 41561/50100 (82.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0659s / 3331.1198 s
agent0:                 episode reward: -0.0832,                 loss: 0.4344
agent1:                 episode reward: 0.0832,                 loss: nan
Episode: 41581/50100 (82.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0773s / 3333.1971 s
agent0:                 episode reward: 0.2085,                 loss: 0.4335
agent1:                 episode reward: -0.2085,                 loss: nan
Episode: 41601/50100 (83.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0813s / 3335.2784 s
agent0:                 episode reward: -0.0525,                 loss: 0.4330
agent1:                 episode reward: 0.0525,                 loss: nan
Episode: 41621/50100 (83.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0688s / 3337.3471 s
agent0:                 episode reward: 0.0060,                 loss: 0.4327
agent1:                 episode reward: -0.0060,                 loss: nan
Episode: 41641/50100 (83.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0869s / 3339.4341 s
agent0:                 episode reward: -0.2056,                 loss: 0.4337
agent1:                 episode reward: 0.2056,                 loss: nan
Episode: 41661/50100 (83.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0768s / 3341.5108 s
agent0:                 episode reward: -0.0365,                 loss: 0.4333
agent1:                 episode reward: 0.0365,                 loss: nan
Episode: 41681/50100 (83.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8099s / 3345.3207 s
agent0:                 episode reward: 0.5830,                 loss: 0.4323
agent1:                 episode reward: -0.5830,                 loss: 0.2962
Score delta: 2.623425336163128, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/41463_0.
Episode: 41701/50100 (83.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6183s / 3346.9390 s
agent0:                 episode reward: -0.1843,                 loss: nan
agent1:                 episode reward: 0.1843,                 loss: 0.3735
Episode: 41721/50100 (83.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6178s / 3348.5568 s
agent0:                 episode reward: -0.1710,                 loss: nan
agent1:                 episode reward: 0.1710,                 loss: 0.3695
Episode: 41741/50100 (83.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6285s / 3350.1852 s
agent0:                 episode reward: -0.5038,                 loss: nan
agent1:                 episode reward: 0.5038,                 loss: 0.3673
Episode: 41761/50100 (83.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6437s / 3351.8289 s
agent0:                 episode reward: -0.2618,                 loss: nan
agent1:                 episode reward: 0.2618,                 loss: 0.3646
Episode: 41781/50100 (83.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 4.0040s / 3355.8329 s
agent0:                 episode reward: 0.4351,                 loss: 0.4406
agent1:                 episode reward: -0.4351,                 loss: 0.3595
Score delta: 2.0222726274122316, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/41550_1.
Episode: 41801/50100 (83.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1360s / 3357.9690 s
agent0:                 episode reward: -0.6405,                 loss: 0.4449
agent1:                 episode reward: 0.6405,                 loss: nan
Episode: 41821/50100 (83.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0972s / 3360.0662 s
agent0:                 episode reward: -0.1076,                 loss: 0.4528
agent1:                 episode reward: 0.1076,                 loss: nan
Episode: 41841/50100 (83.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1087s / 3362.1748 s
agent0:                 episode reward: -0.1553,                 loss: 0.4518
agent1:                 episode reward: 0.1553,                 loss: nan
Episode: 41861/50100 (83.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1120s / 3364.2868 s
agent0:                 episode reward: 0.2091,                 loss: 0.4512
agent1:                 episode reward: -0.2091,                 loss: nan
Episode: 41881/50100 (83.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1076s / 3366.3944 s
agent0:                 episode reward: -0.0898,                 loss: 0.4512
agent1:                 episode reward: 0.0898,                 loss: nan
Episode: 41901/50100 (83.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1170s / 3368.5114 s
agent0:                 episode reward: 0.2538,                 loss: 0.4511
agent1:                 episode reward: -0.2538,                 loss: nan
Episode: 41921/50100 (83.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1208s / 3370.6322 s
agent0:                 episode reward: -0.3688,                 loss: 0.4506
agent1:                 episode reward: 0.3688,                 loss: nan
Episode: 41941/50100 (83.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1289s / 3372.7611 s
agent0:                 episode reward: -0.3394,                 loss: 0.4516
agent1:                 episode reward: 0.3394,                 loss: nan
Episode: 41961/50100 (83.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1130s / 3374.8741 s
agent0:                 episode reward: 0.3329,                 loss: 0.4509
agent1:                 episode reward: -0.3329,                 loss: nan
Episode: 41981/50100 (83.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1113s / 3376.9854 s
agent0:                 episode reward: 0.0532,                 loss: 0.4481
agent1:                 episode reward: -0.0532,                 loss: nan
Episode: 42001/50100 (83.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0886s / 3379.0740 s
agent0:                 episode reward: 0.0807,                 loss: 0.4479
agent1:                 episode reward: -0.0807,                 loss: nan
Episode: 42021/50100 (83.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7170s / 3382.7910 s
agent0:                 episode reward: 0.5492,                 loss: 0.4475
agent1:                 episode reward: -0.5492,                 loss: 0.3528
Score delta: 2.17672115894466, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/41798_0.
Episode: 42041/50100 (83.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6629s / 3384.4539 s
agent0:                 episode reward: 0.5221,                 loss: nan
agent1:                 episode reward: -0.5221,                 loss: 0.3478
Episode: 42061/50100 (83.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6492s / 3386.1031 s
agent0:                 episode reward: -0.0105,                 loss: nan
agent1:                 episode reward: 0.0105,                 loss: 0.3461
Episode: 42081/50100 (83.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6493s / 3387.7523 s
agent0:                 episode reward: -0.1988,                 loss: nan
agent1:                 episode reward: 0.1988,                 loss: 0.3429
Episode: 42101/50100 (84.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6787s / 3389.4310 s
agent0:                 episode reward: 0.1189,                 loss: nan
agent1:                 episode reward: -0.1189,                 loss: 0.3404
Episode: 42121/50100 (84.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5718s / 3393.0028 s
agent0:                 episode reward: -0.2208,                 loss: 0.4483
agent1:                 episode reward: 0.2208,                 loss: 0.3021
Score delta: 2.1478652769886226, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/41906_1.
Episode: 42141/50100 (84.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1197s / 3395.1225 s
agent0:                 episode reward: 0.3807,                 loss: 0.4477
agent1:                 episode reward: -0.3807,                 loss: nan
Episode: 42161/50100 (84.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1133s / 3397.2358 s
agent0:                 episode reward: -0.2577,                 loss: 0.4475
agent1:                 episode reward: 0.2577,                 loss: nan
Episode: 42181/50100 (84.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0931s / 3399.3289 s
agent0:                 episode reward: -0.0688,                 loss: 0.4471
agent1:                 episode reward: 0.0688,                 loss: nan
Episode: 42201/50100 (84.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0963s / 3401.4252 s
agent0:                 episode reward: 0.3331,                 loss: 0.4478
agent1:                 episode reward: -0.3331,                 loss: nan
Episode: 42221/50100 (84.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1068s / 3403.5321 s
agent0:                 episode reward: 0.2403,                 loss: 0.4476
agent1:                 episode reward: -0.2403,                 loss: nan
Episode: 42241/50100 (84.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6131s / 3407.1452 s
agent0:                 episode reward: 0.5966,                 loss: 0.4443
agent1:                 episode reward: -0.5966,                 loss: 0.3805
Score delta: 2.2928028069436293, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/42010_0.
Episode: 42261/50100 (84.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6368s / 3408.7819 s
agent0:                 episode reward: -0.1376,                 loss: nan
agent1:                 episode reward: 0.1376,                 loss: 0.3781
Episode: 42281/50100 (84.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6441s / 3410.4260 s
agent0:                 episode reward: -0.1257,                 loss: nan
agent1:                 episode reward: 0.1257,                 loss: 0.3782
Episode: 42301/50100 (84.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6468s / 3412.0728 s
agent0:                 episode reward: 0.0307,                 loss: nan
agent1:                 episode reward: -0.0307,                 loss: 0.3778
Episode: 42321/50100 (84.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6505s / 3413.7233 s
agent0:                 episode reward: 0.0528,                 loss: nan
agent1:                 episode reward: -0.0528,                 loss: 0.3770
Episode: 42341/50100 (84.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8542s / 3417.5775 s
agent0:                 episode reward: -0.4754,                 loss: 0.4403
agent1:                 episode reward: 0.4754,                 loss: 0.3736
Score delta: 2.0486831253793714, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/42115_1.
Episode: 42361/50100 (84.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0933s / 3419.6708 s
agent0:                 episode reward: 0.1926,                 loss: 0.4252
agent1:                 episode reward: -0.1926,                 loss: nan
Episode: 42381/50100 (84.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0964s / 3421.7672 s
agent0:                 episode reward: -0.1515,                 loss: 0.4224
agent1:                 episode reward: 0.1515,                 loss: nan
Episode: 42401/50100 (84.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0956s / 3423.8629 s
agent0:                 episode reward: -0.0965,                 loss: 0.4232
agent1:                 episode reward: 0.0965,                 loss: nan
Episode: 42421/50100 (84.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0985s / 3425.9614 s
agent0:                 episode reward: -0.0723,                 loss: 0.4232
agent1:                 episode reward: 0.0723,                 loss: nan
Episode: 42441/50100 (84.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1025s / 3428.0639 s
agent0:                 episode reward: -0.1148,                 loss: 0.4222
agent1:                 episode reward: 0.1148,                 loss: nan
Episode: 42461/50100 (84.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0785s / 3430.1424 s
agent0:                 episode reward: 0.2688,                 loss: 0.4232
agent1:                 episode reward: -0.2688,                 loss: nan
Episode: 42481/50100 (84.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0931s / 3432.2355 s
agent0:                 episode reward: 0.3080,                 loss: 0.4224
agent1:                 episode reward: -0.3080,                 loss: nan
Episode: 42501/50100 (84.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0909s / 3434.3264 s
agent0:                 episode reward: -0.7728,                 loss: 0.4205
agent1:                 episode reward: 0.7728,                 loss: nan
Episode: 42521/50100 (84.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8054s / 3438.1318 s
agent0:                 episode reward: 0.2659,                 loss: 0.4161
agent1:                 episode reward: -0.2659,                 loss: 0.3578
Score delta: 2.195923701319052, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/42302_0.
Episode: 42541/50100 (84.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6496s / 3439.7814 s
agent0:                 episode reward: -0.3904,                 loss: nan
agent1:                 episode reward: 0.3904,                 loss: 0.3583
Episode: 42561/50100 (84.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6637s / 3441.4451 s
agent0:                 episode reward: 0.3033,                 loss: nan
agent1:                 episode reward: -0.3033,                 loss: 0.3734
Episode: 42581/50100 (84.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6579s / 3443.1030 s
agent0:                 episode reward: -0.3200,                 loss: nan
agent1:                 episode reward: 0.3200,                 loss: 0.3851
Episode: 42601/50100 (85.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6820s / 3444.7850 s
agent0:                 episode reward: -0.1645,                 loss: nan
agent1:                 episode reward: 0.1645,                 loss: 0.3827
Episode: 42621/50100 (85.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6549s / 3446.4399 s
agent0:                 episode reward: -0.2577,                 loss: nan
agent1:                 episode reward: 0.2577,                 loss: 0.3811
Episode: 42641/50100 (85.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6548s / 3448.0947 s
agent0:                 episode reward: -0.4087,                 loss: nan
agent1:                 episode reward: 0.4087,                 loss: 0.3828
Episode: 42661/50100 (85.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6872s / 3449.7819 s
agent0:                 episode reward: 0.0944,                 loss: nan
agent1:                 episode reward: -0.0944,                 loss: 0.3813
Episode: 42681/50100 (85.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6590s / 3451.4409 s
agent0:                 episode reward: 0.5036,                 loss: nan
agent1:                 episode reward: -0.5036,                 loss: 0.3830
Episode: 42701/50100 (85.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6775s / 3453.1184 s
agent0:                 episode reward: -0.3070,                 loss: nan
agent1:                 episode reward: 0.3070,                 loss: 0.3806
Episode: 42721/50100 (85.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6778s / 3454.7962 s
agent0:                 episode reward: -0.1834,                 loss: nan
agent1:                 episode reward: 0.1834,                 loss: 0.3812
Episode: 42741/50100 (85.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6559s / 3456.4521 s
agent0:                 episode reward: 0.5406,                 loss: nan
agent1:                 episode reward: -0.5406,                 loss: 0.3531
Episode: 42761/50100 (85.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6639s / 3458.1160 s
agent0:                 episode reward: -0.0852,                 loss: nan
agent1:                 episode reward: 0.0852,                 loss: 0.3462
Episode: 42781/50100 (85.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6827s / 3459.7987 s
agent0:                 episode reward: -0.6217,                 loss: nan
agent1:                 episode reward: 0.6217,                 loss: 0.3473
Episode: 42801/50100 (85.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6725s / 3461.4712 s
agent0:                 episode reward: 0.2965,                 loss: nan
agent1:                 episode reward: -0.2965,                 loss: 0.3483
Episode: 42821/50100 (85.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6749s / 3463.1461 s
agent0:                 episode reward: 0.3017,                 loss: nan
agent1:                 episode reward: -0.3017,                 loss: 0.3462
Episode: 42841/50100 (85.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6677s / 3464.8138 s
agent0:                 episode reward: -0.0389,                 loss: nan
agent1:                 episode reward: 0.0389,                 loss: 0.3473
Episode: 42861/50100 (85.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8644s / 3468.6782 s
agent0:                 episode reward: -0.2550,                 loss: 0.4599
agent1:                 episode reward: 0.2550,                 loss: 0.3467
Score delta: 2.1960648223581556, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/42636_1.
Episode: 42881/50100 (85.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1007s / 3470.7789 s
agent0:                 episode reward: -0.5055,                 loss: 0.4539
agent1:                 episode reward: 0.5055,                 loss: nan
Episode: 42901/50100 (85.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1036s / 3472.8826 s
agent0:                 episode reward: -0.3791,                 loss: 0.4527
agent1:                 episode reward: 0.3791,                 loss: nan
Episode: 42921/50100 (85.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0811s / 3474.9637 s
agent0:                 episode reward: -0.1798,                 loss: 0.4528
agent1:                 episode reward: 0.1798,                 loss: nan
Episode: 42941/50100 (85.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0974s / 3477.0611 s
agent0:                 episode reward: -0.7395,                 loss: 0.4528
agent1:                 episode reward: 0.7395,                 loss: nan
Episode: 42961/50100 (85.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1099s / 3479.1710 s
agent0:                 episode reward: -0.1570,                 loss: 0.4524
agent1:                 episode reward: 0.1570,                 loss: nan
Episode: 42981/50100 (85.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1041s / 3481.2752 s
agent0:                 episode reward: 0.0335,                 loss: 0.4525
agent1:                 episode reward: -0.0335,                 loss: nan
Episode: 43001/50100 (85.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0871s / 3483.3622 s
agent0:                 episode reward: -0.1402,                 loss: 0.4514
agent1:                 episode reward: 0.1402,                 loss: nan
Episode: 43021/50100 (85.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0973s / 3485.4596 s
agent0:                 episode reward: -0.0903,                 loss: 0.4511
agent1:                 episode reward: 0.0903,                 loss: nan
Episode: 43041/50100 (85.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1082s / 3487.5677 s
agent0:                 episode reward: -0.4072,                 loss: 0.4487
agent1:                 episode reward: 0.4072,                 loss: nan
Episode: 43061/50100 (85.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0979s / 3489.6656 s
agent0:                 episode reward: 0.1809,                 loss: 0.4478
agent1:                 episode reward: -0.1809,                 loss: nan
Episode: 43081/50100 (85.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1079s / 3491.7735 s
agent0:                 episode reward: -0.3359,                 loss: 0.4470
agent1:                 episode reward: 0.3359,                 loss: nan
Episode: 43101/50100 (86.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7785s / 3495.5519 s
agent0:                 episode reward: 0.2151,                 loss: 0.4478
agent1:                 episode reward: -0.2151,                 loss: 0.3418
Score delta: 2.308826668614743, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/42879_0.
Episode: 43121/50100 (86.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6916s / 3497.2436 s
agent0:                 episode reward: 0.0805,                 loss: nan
agent1:                 episode reward: -0.0805,                 loss: 0.3437
Episode: 43141/50100 (86.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8191s / 3501.0627 s
agent0:                 episode reward: -0.6485,                 loss: 0.4528
agent1:                 episode reward: 0.6485,                 loss: 0.3446
Score delta: 2.0661443778027815, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/42921_1.
Episode: 43161/50100 (86.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0985s / 3503.1611 s
agent0:                 episode reward: -0.1074,                 loss: 0.4503
agent1:                 episode reward: 0.1074,                 loss: nan
Episode: 43181/50100 (86.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1099s / 3505.2710 s
agent0:                 episode reward: 0.1784,                 loss: 0.4489
agent1:                 episode reward: -0.1784,                 loss: nan
Episode: 43201/50100 (86.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1063s / 3507.3773 s
agent0:                 episode reward: -0.0454,                 loss: 0.4480
agent1:                 episode reward: 0.0454,                 loss: nan
Episode: 43221/50100 (86.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1023s / 3509.4796 s
agent0:                 episode reward: 0.1980,                 loss: 0.4453
agent1:                 episode reward: -0.1980,                 loss: nan
Episode: 43241/50100 (86.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0792s / 3511.5588 s
agent0:                 episode reward: -0.5758,                 loss: 0.4338
agent1:                 episode reward: 0.5758,                 loss: nan
Episode: 43261/50100 (86.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0825s / 3513.6413 s
agent0:                 episode reward: -0.4036,                 loss: 0.4333
agent1:                 episode reward: 0.4036,                 loss: nan
Episode: 43281/50100 (86.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 3.9160s / 3517.5573 s
agent0:                 episode reward: 0.4740,                 loss: 0.4339
agent1:                 episode reward: -0.4740,                 loss: 0.3603
Score delta: 2.2208850726371887, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/43065_0.
Episode: 43301/50100 (86.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6476s / 3521.2049 s
agent0:                 episode reward: -0.5498,                 loss: 0.4415
agent1:                 episode reward: 0.5498,                 loss: 0.3574
Score delta: 2.04848365163141, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/43087_1.
Episode: 43321/50100 (86.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0911s / 3523.2960 s
agent0:                 episode reward: 0.1335,                 loss: 0.4277
agent1:                 episode reward: -0.1335,                 loss: nan
Episode: 43341/50100 (86.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0871s / 3525.3830 s
agent0:                 episode reward: 0.0057,                 loss: 0.4247
agent1:                 episode reward: -0.0057,                 loss: nan
Episode: 43361/50100 (86.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0843s / 3527.4673 s
agent0:                 episode reward: -0.1916,                 loss: 0.4238
agent1:                 episode reward: 0.1916,                 loss: nan
Episode: 43381/50100 (86.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0835s / 3529.5508 s
agent0:                 episode reward: -0.3794,                 loss: 0.4216
agent1:                 episode reward: 0.3794,                 loss: nan
Episode: 43401/50100 (86.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8721s / 3533.4229 s
agent0:                 episode reward: 0.3661,                 loss: 0.4208
agent1:                 episode reward: -0.3661,                 loss: 0.3667
Score delta: 2.0668956510154315, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/43182_0.
Episode: 43421/50100 (86.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6941s / 3535.1171 s
agent0:                 episode reward: 0.0853,                 loss: nan
agent1:                 episode reward: -0.0853,                 loss: 0.3656
Episode: 43441/50100 (86.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6914s / 3536.8084 s
agent0:                 episode reward: -0.2804,                 loss: nan
agent1:                 episode reward: 0.2804,                 loss: 0.3641
Episode: 43461/50100 (86.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7398s / 3540.5482 s
agent0:                 episode reward: -0.4806,                 loss: 0.4456
agent1:                 episode reward: 0.4806,                 loss: 0.3614
Score delta: 2.104187427180671, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/43247_1.
Episode: 43481/50100 (86.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0982s / 3542.6464 s
agent0:                 episode reward: -0.4992,                 loss: 0.4163
agent1:                 episode reward: 0.4992,                 loss: nan
Episode: 43501/50100 (86.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0903s / 3544.7367 s
agent0:                 episode reward: 0.1520,                 loss: 0.3999
agent1:                 episode reward: -0.1520,                 loss: nan
Episode: 43521/50100 (86.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0928s / 3546.8295 s
agent0:                 episode reward: 0.1293,                 loss: 0.3967
agent1:                 episode reward: -0.1293,                 loss: nan
Episode: 43541/50100 (86.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1055s / 3548.9350 s
agent0:                 episode reward: 0.1992,                 loss: 0.3959
agent1:                 episode reward: -0.1992,                 loss: nan
Episode: 43561/50100 (86.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 4.0167s / 3552.9517 s
agent0:                 episode reward: 0.5046,                 loss: 0.3955
agent1:                 episode reward: -0.5046,                 loss: nan
Score delta: 2.348934943274073, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/43349_0.
Episode: 43581/50100 (86.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6931s / 3554.6449 s
agent0:                 episode reward: 0.0597,                 loss: nan
agent1:                 episode reward: -0.0597,                 loss: 0.3476
Episode: 43601/50100 (87.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6527s / 3556.2976 s
agent0:                 episode reward: -0.2249,                 loss: nan
agent1:                 episode reward: 0.2249,                 loss: 0.3464
Episode: 43621/50100 (87.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6638s / 3557.9613 s
agent0:                 episode reward: -0.2083,                 loss: nan
agent1:                 episode reward: 0.2083,                 loss: 0.3434
Episode: 43641/50100 (87.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6722s / 3559.6335 s
agent0:                 episode reward: 0.1370,                 loss: nan
agent1:                 episode reward: -0.1370,                 loss: 0.3474
Episode: 43661/50100 (87.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6925s / 3561.3261 s
agent0:                 episode reward: 0.5638,                 loss: nan
agent1:                 episode reward: -0.5638,                 loss: 0.3230
Episode: 43681/50100 (87.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6831s / 3563.0092 s
agent0:                 episode reward: -0.0130,                 loss: nan
agent1:                 episode reward: 0.0130,                 loss: 0.3175
Episode: 43701/50100 (87.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6878s / 3564.6970 s
agent0:                 episode reward: 0.0934,                 loss: nan
agent1:                 episode reward: -0.0934,                 loss: 0.3156
Episode: 43721/50100 (87.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6883s / 3566.3853 s
agent0:                 episode reward: -0.4439,                 loss: nan
agent1:                 episode reward: 0.4439,                 loss: 0.3154
Episode: 43741/50100 (87.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7153s / 3568.1006 s
agent0:                 episode reward: 0.0306,                 loss: nan
agent1:                 episode reward: -0.0306,                 loss: 0.3180
Episode: 43761/50100 (87.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7318s / 3569.8325 s
agent0:                 episode reward: -0.0247,                 loss: nan
agent1:                 episode reward: 0.0247,                 loss: 0.3143
Episode: 43781/50100 (87.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7192s / 3571.5517 s
agent0:                 episode reward: 0.0974,                 loss: nan
agent1:                 episode reward: -0.0974,                 loss: 0.3151
Episode: 43801/50100 (87.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7255s / 3573.2772 s
agent0:                 episode reward: 0.5644,                 loss: nan
agent1:                 episode reward: -0.5644,                 loss: 0.3145
Episode: 43821/50100 (87.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7125s / 3574.9897 s
agent0:                 episode reward: 0.3225,                 loss: nan
agent1:                 episode reward: -0.3225,                 loss: 0.3651
Episode: 43841/50100 (87.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7230s / 3576.7127 s
agent0:                 episode reward: -0.0309,                 loss: nan
agent1:                 episode reward: 0.0309,                 loss: 0.3701
Episode: 43861/50100 (87.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7568s / 3578.4695 s
agent0:                 episode reward: 0.0214,                 loss: nan
agent1:                 episode reward: -0.0214,                 loss: 0.3686
Episode: 43881/50100 (87.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7200s / 3580.1895 s
agent0:                 episode reward: 0.3895,                 loss: nan
agent1:                 episode reward: -0.3895,                 loss: 0.3691
Episode: 43901/50100 (87.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7151s / 3581.9046 s
agent0:                 episode reward: -0.0492,                 loss: nan
agent1:                 episode reward: 0.0492,                 loss: 0.3669
Episode: 43921/50100 (87.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7190s / 3583.6237 s
agent0:                 episode reward: 0.0165,                 loss: nan
agent1:                 episode reward: -0.0165,                 loss: 0.3660
Episode: 43941/50100 (87.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7308s / 3585.3545 s
agent0:                 episode reward: -0.0980,                 loss: nan
agent1:                 episode reward: 0.0980,                 loss: 0.3681
Episode: 43961/50100 (87.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7196s / 3587.0741 s
agent0:                 episode reward: 0.0088,                 loss: nan
agent1:                 episode reward: -0.0088,                 loss: 0.3663
Episode: 43981/50100 (87.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8687s / 3590.9428 s
agent0:                 episode reward: -0.1747,                 loss: 0.4515
agent1:                 episode reward: 0.1747,                 loss: 0.3655
Score delta: 2.3595977423054406, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/43758_1.
Episode: 44001/50100 (87.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0930s / 3593.0358 s
agent0:                 episode reward: -0.0549,                 loss: 0.4486
agent1:                 episode reward: 0.0549,                 loss: nan
Episode: 44021/50100 (87.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1196s / 3595.1554 s
agent0:                 episode reward: -0.1303,                 loss: 0.4479
agent1:                 episode reward: 0.1303,                 loss: nan
Episode: 44041/50100 (87.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8462s / 3599.0016 s
agent0:                 episode reward: 0.4569,                 loss: 0.4469
agent1:                 episode reward: -0.4569,                 loss: 0.3260
Score delta: 2.366190949451023, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/43820_0.
Episode: 44061/50100 (87.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7177s / 3600.7193 s
agent0:                 episode reward: 0.2830,                 loss: nan
agent1:                 episode reward: -0.2830,                 loss: 0.2999
Episode: 44081/50100 (87.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8108s / 3604.5301 s
agent0:                 episode reward: -0.5385,                 loss: 0.4535
agent1:                 episode reward: 0.5385,                 loss: 0.2978
Score delta: 2.0146991441808413, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/43862_1.
Episode: 44101/50100 (88.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0992s / 3606.6292 s
agent0:                 episode reward: -0.2307,                 loss: 0.4462
agent1:                 episode reward: 0.2307,                 loss: nan
Episode: 44121/50100 (88.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0836s / 3608.7128 s
agent0:                 episode reward: 0.0171,                 loss: 0.4431
agent1:                 episode reward: -0.0171,                 loss: nan
Episode: 44141/50100 (88.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0982s / 3610.8110 s
agent0:                 episode reward: 0.2257,                 loss: 0.4421
agent1:                 episode reward: -0.2257,                 loss: nan
Episode: 44161/50100 (88.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0963s / 3612.9073 s
agent0:                 episode reward: -0.4805,                 loss: 0.4420
agent1:                 episode reward: 0.4805,                 loss: nan
Episode: 44181/50100 (88.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0577s / 3614.9650 s
agent0:                 episode reward: 0.2047,                 loss: 0.4425
agent1:                 episode reward: -0.2047,                 loss: nan
Episode: 44201/50100 (88.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0636s / 3617.0286 s
agent0:                 episode reward: -0.3599,                 loss: 0.4420
agent1:                 episode reward: 0.3599,                 loss: nan
Episode: 44221/50100 (88.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0649s / 3619.0935 s
agent0:                 episode reward: -0.0954,                 loss: 0.4419
agent1:                 episode reward: 0.0954,                 loss: nan
Episode: 44241/50100 (88.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0834s / 3621.1769 s
agent0:                 episode reward: 0.6088,                 loss: 0.4414
agent1:                 episode reward: -0.6088,                 loss: nan
Episode: 44261/50100 (88.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0769s / 3623.2538 s
agent0:                 episode reward: 0.2361,                 loss: 0.4319
agent1:                 episode reward: -0.2361,                 loss: nan
Episode: 44281/50100 (88.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0517s / 3625.3055 s
agent0:                 episode reward: -0.2328,                 loss: 0.4129
agent1:                 episode reward: 0.2328,                 loss: nan
Episode: 44301/50100 (88.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0704s / 3627.3759 s
agent0:                 episode reward: 0.2509,                 loss: 0.4128
agent1:                 episode reward: -0.2509,                 loss: nan
Episode: 44321/50100 (88.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8288s / 3631.2047 s
agent0:                 episode reward: 0.4217,                 loss: 0.4112
agent1:                 episode reward: -0.4217,                 loss: 0.3560
Score delta: 2.1126917076922274, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/44107_0.
Episode: 44341/50100 (88.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7010s / 3632.9056 s
agent0:                 episode reward: 0.4753,                 loss: nan
agent1:                 episode reward: -0.4753,                 loss: 0.3490
Episode: 44361/50100 (88.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7016s / 3634.6072 s
agent0:                 episode reward: -0.5491,                 loss: nan
agent1:                 episode reward: 0.5491,                 loss: 0.3476
Episode: 44381/50100 (88.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7494s / 3638.3566 s
agent0:                 episode reward: -0.3972,                 loss: 0.4573
agent1:                 episode reward: 0.3972,                 loss: 0.3420
Score delta: 2.5400616452660025, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/44155_1.
Episode: 44401/50100 (88.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0721s / 3640.4287 s
agent0:                 episode reward: 0.0236,                 loss: 0.4530
agent1:                 episode reward: -0.0236,                 loss: nan
Episode: 44421/50100 (88.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0631s / 3642.4918 s
agent0:                 episode reward: -0.6965,                 loss: 0.4516
agent1:                 episode reward: 0.6965,                 loss: nan
Episode: 44441/50100 (88.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0824s / 3644.5742 s
agent0:                 episode reward: -0.2309,                 loss: 0.4527
agent1:                 episode reward: 0.2309,                 loss: nan
Episode: 44461/50100 (88.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0782s / 3646.6524 s
agent0:                 episode reward: -0.0448,                 loss: 0.4511
agent1:                 episode reward: 0.0448,                 loss: nan
Episode: 44481/50100 (88.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0652s / 3648.7176 s
agent0:                 episode reward: -0.7640,                 loss: 0.4487
agent1:                 episode reward: 0.7640,                 loss: nan
Episode: 44501/50100 (88.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0863s / 3650.8039 s
agent0:                 episode reward: 0.4057,                 loss: 0.4448
agent1:                 episode reward: -0.4057,                 loss: nan
Episode: 44521/50100 (88.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0664s / 3652.8703 s
agent0:                 episode reward: -0.4486,                 loss: 0.4462
agent1:                 episode reward: 0.4486,                 loss: nan
Episode: 44541/50100 (88.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0628s / 3654.9331 s
agent0:                 episode reward: 0.0378,                 loss: 0.4456
agent1:                 episode reward: -0.0378,                 loss: nan
Episode: 44561/50100 (88.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0726s / 3657.0057 s
agent0:                 episode reward: 0.4173,                 loss: 0.4453
agent1:                 episode reward: -0.4173,                 loss: nan
Episode: 44581/50100 (88.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0679s / 3659.0736 s
agent0:                 episode reward: -0.5243,                 loss: 0.4459
agent1:                 episode reward: 0.5243,                 loss: nan
Episode: 44601/50100 (89.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0786s / 3661.1522 s
agent0:                 episode reward: 0.1136,                 loss: 0.4455
agent1:                 episode reward: -0.1136,                 loss: nan
Episode: 44621/50100 (89.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0700s / 3663.2222 s
agent0:                 episode reward: -0.1364,                 loss: 0.4458
agent1:                 episode reward: 0.1364,                 loss: nan
Episode: 44641/50100 (89.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0834s / 3665.3056 s
agent0:                 episode reward: -0.2347,                 loss: 0.4408
agent1:                 episode reward: 0.2347,                 loss: nan
Episode: 44661/50100 (89.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0744s / 3667.3799 s
agent0:                 episode reward: 0.2835,                 loss: 0.4317
agent1:                 episode reward: -0.2835,                 loss: nan
Episode: 44681/50100 (89.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0573s / 3669.4373 s
agent0:                 episode reward: 0.3333,                 loss: 0.4311
agent1:                 episode reward: -0.3333,                 loss: nan
Episode: 44701/50100 (89.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0426s / 3671.4799 s
agent0:                 episode reward: -0.0298,                 loss: 0.4320
agent1:                 episode reward: 0.0298,                 loss: nan
Episode: 44721/50100 (89.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0424s / 3673.5223 s
agent0:                 episode reward: -0.1942,                 loss: 0.4311
agent1:                 episode reward: 0.1942,                 loss: nan
Episode: 44741/50100 (89.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0503s / 3675.5726 s
agent0:                 episode reward: -0.0835,                 loss: 0.4313
agent1:                 episode reward: 0.0835,                 loss: nan
Episode: 44761/50100 (89.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 3.5681s / 3679.1407 s
agent0:                 episode reward: -0.2037,                 loss: 0.4308
agent1:                 episode reward: 0.2037,                 loss: 0.3548
Score delta: 2.0729864247109697, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/44535_0.
Episode: 44781/50100 (89.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7136s / 3680.8543 s
agent0:                 episode reward: 0.2945,                 loss: nan
agent1:                 episode reward: -0.2945,                 loss: 0.3492
Episode: 44801/50100 (89.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7147s / 3682.5691 s
agent0:                 episode reward: -0.2953,                 loss: nan
agent1:                 episode reward: 0.2953,                 loss: 0.3486
Episode: 44821/50100 (89.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8611s / 3686.4302 s
agent0:                 episode reward: -0.1894,                 loss: 0.4112
agent1:                 episode reward: 0.1894,                 loss: 0.3476
Score delta: 2.1890850080549336, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/44590_1.
Episode: 44841/50100 (89.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0780s / 3688.5082 s
agent0:                 episode reward: 0.1380,                 loss: 0.4113
agent1:                 episode reward: -0.1380,                 loss: nan
Episode: 44861/50100 (89.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0641s / 3690.5723 s
agent0:                 episode reward: 0.0160,                 loss: 0.4075
agent1:                 episode reward: -0.0160,                 loss: nan
Episode: 44881/50100 (89.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0550s / 3692.6274 s
agent0:                 episode reward: -0.2313,                 loss: 0.3989
agent1:                 episode reward: 0.2313,                 loss: nan
Episode: 44901/50100 (89.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0838s / 3694.7112 s
agent0:                 episode reward: -0.2186,                 loss: 0.3979
agent1:                 episode reward: 0.2186,                 loss: nan
Episode: 44921/50100 (89.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0543s / 3696.7654 s
agent0:                 episode reward: -0.0258,                 loss: 0.3985
agent1:                 episode reward: 0.0258,                 loss: nan
Episode: 44941/50100 (89.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0612s / 3698.8266 s
agent0:                 episode reward: 0.0094,                 loss: 0.3954
agent1:                 episode reward: -0.0094,                 loss: nan
Episode: 44961/50100 (89.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6606s / 3702.4872 s
agent0:                 episode reward: 0.1563,                 loss: 0.4001
agent1:                 episode reward: -0.1563,                 loss: 0.3455
Score delta: 2.0514059915327816, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/44737_0.
Episode: 44981/50100 (89.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7202s / 3704.2075 s
agent0:                 episode reward: 0.0414,                 loss: nan
agent1:                 episode reward: -0.0414,                 loss: 0.3352
Episode: 45001/50100 (89.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7242s / 3705.9316 s
agent0:                 episode reward: 0.2525,                 loss: nan
agent1:                 episode reward: -0.2525,                 loss: 0.3017
Episode: 45021/50100 (89.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7054s / 3707.6370 s
agent0:                 episode reward: 0.1865,                 loss: nan
agent1:                 episode reward: -0.1865,                 loss: 0.3002
Episode: 45041/50100 (89.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7335s / 3709.3705 s
agent0:                 episode reward: -0.1502,                 loss: nan
agent1:                 episode reward: 0.1502,                 loss: 0.3010
Episode: 45061/50100 (89.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7365s / 3711.1070 s
agent0:                 episode reward: -0.2009,                 loss: nan
agent1:                 episode reward: 0.2009,                 loss: 0.2984
Episode: 45081/50100 (89.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7229s / 3712.8299 s
agent0:                 episode reward: -0.2696,                 loss: nan
agent1:                 episode reward: 0.2696,                 loss: 0.2967
Episode: 45101/50100 (90.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7220s / 3714.5518 s
agent0:                 episode reward: -0.2050,                 loss: nan
agent1:                 episode reward: 0.2050,                 loss: 0.2970
Episode: 45121/50100 (90.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7291s / 3716.2809 s
agent0:                 episode reward: -0.0698,                 loss: nan
agent1:                 episode reward: 0.0698,                 loss: 0.2976
Episode: 45141/50100 (90.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7322s / 3718.0131 s
agent0:                 episode reward: 0.0174,                 loss: nan
agent1:                 episode reward: -0.0174,                 loss: 0.3068
Episode: 45161/50100 (90.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8584s / 3721.8716 s
agent0:                 episode reward: -0.0220,                 loss: 0.4396
agent1:                 episode reward: 0.0220,                 loss: 0.3547
Score delta: 2.1135073255285057, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/44934_1.
Episode: 45181/50100 (90.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0697s / 3723.9413 s
agent0:                 episode reward: 0.1721,                 loss: 0.4394
agent1:                 episode reward: -0.1721,                 loss: nan
Episode: 45201/50100 (90.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0804s / 3726.0217 s
agent0:                 episode reward: -0.1368,                 loss: 0.4376
agent1:                 episode reward: 0.1368,                 loss: nan
Episode: 45221/50100 (90.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0592s / 3728.0808 s
agent0:                 episode reward: 0.1263,                 loss: 0.4388
agent1:                 episode reward: -0.1263,                 loss: nan
Episode: 45241/50100 (90.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0762s / 3730.1570 s
agent0:                 episode reward: -0.3357,                 loss: 0.4487
agent1:                 episode reward: 0.3357,                 loss: nan
Episode: 45261/50100 (90.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0674s / 3732.2245 s
agent0:                 episode reward: -0.1110,                 loss: 0.4480
agent1:                 episode reward: 0.1110,                 loss: nan
Episode: 45281/50100 (90.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0628s / 3734.2873 s
agent0:                 episode reward: -0.8850,                 loss: 0.4470
agent1:                 episode reward: 0.8850,                 loss: nan
Episode: 45301/50100 (90.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0619s / 3736.3492 s
agent0:                 episode reward: 0.2398,                 loss: 0.4472
agent1:                 episode reward: -0.2398,                 loss: nan
Episode: 45321/50100 (90.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0922s / 3738.4413 s
agent0:                 episode reward: 0.2638,                 loss: 0.4466
agent1:                 episode reward: -0.2638,                 loss: nan
Episode: 45341/50100 (90.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8907s / 3742.3321 s
agent0:                 episode reward: 0.1706,                 loss: 0.4472
agent1:                 episode reward: -0.1706,                 loss: 0.3750
Score delta: 2.0408075282797338, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/45127_0.
Episode: 45361/50100 (90.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7490s / 3744.0811 s
agent0:                 episode reward: -0.1891,                 loss: nan
agent1:                 episode reward: 0.1891,                 loss: 0.3719
Episode: 45381/50100 (90.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7377s / 3745.8189 s
agent0:                 episode reward: -0.6064,                 loss: nan
agent1:                 episode reward: 0.6064,                 loss: 0.3662
Episode: 45401/50100 (90.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7268s / 3749.5457 s
agent0:                 episode reward: -0.6166,                 loss: 0.4631
agent1:                 episode reward: 0.6166,                 loss: 0.3648
Score delta: 2.0297864946030932, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/45184_1.
Episode: 45421/50100 (90.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0787s / 3751.6244 s
agent0:                 episode reward: 0.0178,                 loss: 0.4529
agent1:                 episode reward: -0.0178,                 loss: nan
Episode: 45441/50100 (90.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0846s / 3753.7091 s
agent0:                 episode reward: 0.5630,                 loss: 0.4525
agent1:                 episode reward: -0.5630,                 loss: nan
Episode: 45461/50100 (90.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0865s / 3755.7956 s
agent0:                 episode reward: 0.1959,                 loss: 0.4467
agent1:                 episode reward: -0.1959,                 loss: nan
Episode: 45481/50100 (90.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0829s / 3757.8785 s
agent0:                 episode reward: 0.2027,                 loss: 0.4439
agent1:                 episode reward: -0.2027,                 loss: nan
Episode: 45501/50100 (90.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0929s / 3759.9714 s
agent0:                 episode reward: -0.0963,                 loss: 0.4431
agent1:                 episode reward: 0.0963,                 loss: nan
Episode: 45521/50100 (90.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0821s / 3762.0535 s
agent0:                 episode reward: 0.2673,                 loss: 0.4441
agent1:                 episode reward: -0.2673,                 loss: nan
Episode: 45541/50100 (90.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0629s / 3764.1164 s
agent0:                 episode reward: -0.0929,                 loss: 0.4439
agent1:                 episode reward: 0.0929,                 loss: nan
Episode: 45561/50100 (90.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0658s / 3766.1823 s
agent0:                 episode reward: 0.6312,                 loss: 0.4430
agent1:                 episode reward: -0.6312,                 loss: nan
Episode: 45581/50100 (90.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0874s / 3768.2696 s
agent0:                 episode reward: 0.1501,                 loss: 0.4432
agent1:                 episode reward: -0.1501,                 loss: nan
Episode: 45601/50100 (91.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1075s / 3770.3772 s
agent0:                 episode reward: -0.2460,                 loss: 0.4427
agent1:                 episode reward: 0.2460,                 loss: nan
Episode: 45621/50100 (91.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7164s / 3774.0935 s
agent0:                 episode reward: 0.8356,                 loss: 0.4419
agent1:                 episode reward: -0.8356,                 loss: 0.3587
Score delta: 2.0842588673004534, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/45397_0.
Episode: 45641/50100 (91.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7552s / 3775.8488 s
agent0:                 episode reward: 0.3244,                 loss: nan
agent1:                 episode reward: -0.3244,                 loss: 0.3557
Episode: 45661/50100 (91.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7559s / 3777.6047 s
agent0:                 episode reward: 0.1703,                 loss: nan
agent1:                 episode reward: -0.1703,                 loss: 0.3539
Episode: 45681/50100 (91.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7593s / 3779.3640 s
agent0:                 episode reward: -0.6189,                 loss: nan
agent1:                 episode reward: 0.6189,                 loss: 0.3541
Episode: 45701/50100 (91.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 3.9951s / 3783.3591 s
agent0:                 episode reward: -0.1196,                 loss: 0.4438
agent1:                 episode reward: 0.1196,                 loss: 0.3548
Score delta: 2.3031961431289507, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/45471_1.
Episode: 45721/50100 (91.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1017s / 3785.4608 s
agent0:                 episode reward: -0.1229,                 loss: 0.4439
agent1:                 episode reward: 0.1229,                 loss: nan
Episode: 45741/50100 (91.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7232s / 3789.1840 s
agent0:                 episode reward: 0.2426,                 loss: 0.4436
agent1:                 episode reward: -0.2426,                 loss: 0.3547
Score delta: 2.1281762073510815, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/45515_0.
Episode: 45761/50100 (91.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7498s / 3790.9338 s
agent0:                 episode reward: -0.2458,                 loss: nan
agent1:                 episode reward: 0.2458,                 loss: 0.3682
Episode: 45781/50100 (91.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7608s / 3792.6945 s
agent0:                 episode reward: -0.1494,                 loss: nan
agent1:                 episode reward: 0.1494,                 loss: 0.3732
Episode: 45801/50100 (91.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7722s / 3794.4667 s
agent0:                 episode reward: -0.3645,                 loss: nan
agent1:                 episode reward: 0.3645,                 loss: 0.3727
Episode: 45821/50100 (91.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7677s / 3796.2344 s
agent0:                 episode reward: -0.0203,                 loss: nan
agent1:                 episode reward: 0.0203,                 loss: 0.3728
Episode: 45841/50100 (91.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7548s / 3797.9892 s
agent0:                 episode reward: -0.4586,                 loss: nan
agent1:                 episode reward: 0.4586,                 loss: 0.3714
Episode: 45861/50100 (91.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7472s / 3799.7364 s
agent0:                 episode reward: -0.0681,                 loss: nan
agent1:                 episode reward: 0.0681,                 loss: 0.3722
Episode: 45881/50100 (91.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7548s / 3801.4912 s
agent0:                 episode reward: -0.2315,                 loss: nan
agent1:                 episode reward: 0.2315,                 loss: 0.3693
Episode: 45901/50100 (91.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7478s / 3803.2390 s
agent0:                 episode reward: 0.0235,                 loss: nan
agent1:                 episode reward: -0.0235,                 loss: 0.3706
Episode: 45921/50100 (91.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8044s / 3807.0434 s
agent0:                 episode reward: -0.9534,                 loss: 0.4482
agent1:                 episode reward: 0.9534,                 loss: 0.3712
Score delta: 2.8541689948642843, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/45705_1.
Episode: 45941/50100 (91.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0790s / 3809.1224 s
agent0:                 episode reward: -0.7458,                 loss: 0.4399
agent1:                 episode reward: 0.7458,                 loss: nan
Episode: 45961/50100 (91.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0794s / 3811.2018 s
agent0:                 episode reward: 0.3348,                 loss: 0.4380
agent1:                 episode reward: -0.3348,                 loss: nan
Episode: 45981/50100 (91.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0923s / 3813.2941 s
agent0:                 episode reward: -0.1634,                 loss: 0.4379
agent1:                 episode reward: 0.1634,                 loss: nan
Episode: 46001/50100 (91.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0828s / 3815.3769 s
agent0:                 episode reward: 0.4707,                 loss: 0.4378
agent1:                 episode reward: -0.4707,                 loss: nan
Episode: 46021/50100 (91.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0946s / 3817.4715 s
agent0:                 episode reward: 0.3882,                 loss: 0.4381
agent1:                 episode reward: -0.3882,                 loss: nan
Episode: 46041/50100 (91.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0834s / 3819.5549 s
agent0:                 episode reward: 0.3664,                 loss: 0.4382
agent1:                 episode reward: -0.3664,                 loss: nan
Episode: 46061/50100 (91.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0783s / 3821.6332 s
agent0:                 episode reward: -0.0761,                 loss: 0.4445
agent1:                 episode reward: 0.0761,                 loss: nan
Episode: 46081/50100 (91.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0898s / 3823.7230 s
agent0:                 episode reward: 0.3443,                 loss: 0.4448
agent1:                 episode reward: -0.3443,                 loss: nan
Episode: 46101/50100 (92.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0729s / 3825.7959 s
agent0:                 episode reward: -0.1414,                 loss: 0.4449
agent1:                 episode reward: 0.1414,                 loss: nan
Episode: 46121/50100 (92.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0701s / 3827.8661 s
agent0:                 episode reward: -0.0435,                 loss: 0.4452
agent1:                 episode reward: 0.0435,                 loss: nan
Episode: 46141/50100 (92.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8560s / 3831.7221 s
agent0:                 episode reward: 0.1435,                 loss: 0.4447
agent1:                 episode reward: -0.1435,                 loss: 0.3747
Score delta: 2.1085153196305253, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/45923_0.
Episode: 46161/50100 (92.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 3.6504s / 3835.3724 s
agent0:                 episode reward: -0.5718,                 loss: nan
agent1:                 episode reward: 0.5718,                 loss: 0.3999
Score delta: 2.019416544357738, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/45949_1.
Episode: 46181/50100 (92.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1020s / 3837.4745 s
agent0:                 episode reward: -0.4376,                 loss: 0.4309
agent1:                 episode reward: 0.4376,                 loss: nan
Episode: 46201/50100 (92.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0968s / 3839.5713 s
agent0:                 episode reward: 0.1269,                 loss: 0.4304
agent1:                 episode reward: -0.1269,                 loss: nan
Episode: 46221/50100 (92.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0801s / 3841.6514 s
agent0:                 episode reward: -0.2551,                 loss: 0.4278
agent1:                 episode reward: 0.2551,                 loss: nan
Episode: 46241/50100 (92.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1092s / 3843.7606 s
agent0:                 episode reward: 0.0889,                 loss: 0.4289
agent1:                 episode reward: -0.0889,                 loss: nan
Episode: 46261/50100 (92.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0963s / 3845.8569 s
agent0:                 episode reward: 0.2937,                 loss: 0.4313
agent1:                 episode reward: -0.2937,                 loss: nan
Episode: 46281/50100 (92.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7844s / 3849.6413 s
agent0:                 episode reward: 0.2019,                 loss: 0.4297
agent1:                 episode reward: -0.2019,                 loss: 0.3480
Score delta: 2.3644191066496987, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/46055_0.
Episode: 46301/50100 (92.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7714s / 3851.4127 s
agent0:                 episode reward: -0.1072,                 loss: nan
agent1:                 episode reward: 0.1072,                 loss: 0.3480
Episode: 46321/50100 (92.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7884s / 3853.2011 s
agent0:                 episode reward: -0.3804,                 loss: nan
agent1:                 episode reward: 0.3804,                 loss: 0.3484
Episode: 46341/50100 (92.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7878s / 3854.9890 s
agent0:                 episode reward: -0.1566,                 loss: nan
agent1:                 episode reward: 0.1566,                 loss: 0.3480
Episode: 46361/50100 (92.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7636s / 3858.7525 s
agent0:                 episode reward: -0.0414,                 loss: 0.4358
agent1:                 episode reward: 0.0414,                 loss: 0.3441
Score delta: 2.1185529576939928, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/46144_1.
Episode: 46381/50100 (92.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1043s / 3860.8568 s
agent0:                 episode reward: 0.0301,                 loss: 0.4352
agent1:                 episode reward: -0.0301,                 loss: nan
Episode: 46401/50100 (92.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0973s / 3862.9541 s
agent0:                 episode reward: 0.3809,                 loss: 0.4337
agent1:                 episode reward: -0.3809,                 loss: nan
Episode: 46421/50100 (92.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1051s / 3865.0592 s
agent0:                 episode reward: -0.0349,                 loss: 0.4334
agent1:                 episode reward: 0.0349,                 loss: nan
Episode: 46441/50100 (92.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1169s / 3867.1761 s
agent0:                 episode reward: 0.4563,                 loss: 0.4328
agent1:                 episode reward: -0.4563,                 loss: nan
Episode: 46461/50100 (92.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1216s / 3869.2977 s
agent0:                 episode reward: 0.0760,                 loss: 0.4325
agent1:                 episode reward: -0.0760,                 loss: nan
Episode: 46481/50100 (92.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1297s / 3871.4274 s
agent0:                 episode reward: -0.1396,                 loss: 0.4339
agent1:                 episode reward: 0.1396,                 loss: nan
Episode: 46501/50100 (92.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8716s / 3875.2990 s
agent0:                 episode reward: 0.6168,                 loss: 0.4331
agent1:                 episode reward: -0.6168,                 loss: 0.3540
Score delta: 2.2758774917524267, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/46278_0.
Episode: 46521/50100 (92.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8049s / 3877.1039 s
agent0:                 episode reward: 0.1431,                 loss: nan
agent1:                 episode reward: -0.1431,                 loss: 0.3515
Episode: 46541/50100 (92.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8135s / 3878.9174 s
agent0:                 episode reward: 0.0163,                 loss: nan
agent1:                 episode reward: -0.0163,                 loss: 0.3480
Episode: 46561/50100 (92.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7970s / 3880.7144 s
agent0:                 episode reward: -0.2824,                 loss: nan
agent1:                 episode reward: 0.2824,                 loss: 0.3336
Episode: 46581/50100 (92.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8035s / 3882.5179 s
agent0:                 episode reward: 0.0531,                 loss: nan
agent1:                 episode reward: -0.0531,                 loss: 0.3148
Episode: 46601/50100 (93.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8156s / 3884.3335 s
agent0:                 episode reward: -0.3375,                 loss: nan
agent1:                 episode reward: 0.3375,                 loss: 0.3134
Episode: 46621/50100 (93.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8143s / 3886.1479 s
agent0:                 episode reward: 0.0713,                 loss: nan
agent1:                 episode reward: -0.0713,                 loss: 0.3107
Episode: 46641/50100 (93.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 3.7457s / 3889.8936 s
agent0:                 episode reward: -0.5729,                 loss: 0.4347
agent1:                 episode reward: 0.5729,                 loss: 0.3102
Score delta: 2.3259245018097916, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/46428_1.
Episode: 46661/50100 (93.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1013s / 3891.9948 s
agent0:                 episode reward: 0.2127,                 loss: 0.4218
agent1:                 episode reward: -0.2127,                 loss: nan
Episode: 46681/50100 (93.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8502s / 3895.8450 s
agent0:                 episode reward: 0.4869,                 loss: 0.4230
agent1:                 episode reward: -0.4869,                 loss: 0.3686
Score delta: 2.1154084462108997, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/46457_0.
Episode: 46701/50100 (93.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8176s / 3897.6626 s
agent0:                 episode reward: -0.5040,                 loss: nan
agent1:                 episode reward: 0.5040,                 loss: 0.3634
Episode: 46721/50100 (93.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8163s / 3899.4789 s
agent0:                 episode reward: -0.0626,                 loss: nan
agent1:                 episode reward: 0.0626,                 loss: 0.3619
Episode: 46741/50100 (93.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8114s / 3901.2903 s
agent0:                 episode reward: 0.1832,                 loss: nan
agent1:                 episode reward: -0.1832,                 loss: 0.3601
Episode: 46761/50100 (93.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 3.9296s / 3905.2199 s
agent0:                 episode reward: -0.8440,                 loss: 0.4524
agent1:                 episode reward: 0.8440,                 loss: 0.3798
Score delta: 2.0134622898799965, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/46536_1.
Episode: 46781/50100 (93.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1079s / 3907.3278 s
agent0:                 episode reward: 0.0093,                 loss: 0.4508
agent1:                 episode reward: -0.0093,                 loss: nan
Episode: 46801/50100 (93.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1096s / 3909.4374 s
agent0:                 episode reward: 0.1553,                 loss: 0.4505
agent1:                 episode reward: -0.1553,                 loss: nan
Episode: 46821/50100 (93.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0859s / 3911.5233 s
agent0:                 episode reward: -0.3691,                 loss: 0.4508
agent1:                 episode reward: 0.3691,                 loss: nan
Episode: 46841/50100 (93.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1010s / 3913.6243 s
agent0:                 episode reward: 0.1843,                 loss: 0.4501
agent1:                 episode reward: -0.1843,                 loss: nan
Episode: 46861/50100 (93.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1011s / 3915.7254 s
agent0:                 episode reward: -0.4564,                 loss: 0.4494
agent1:                 episode reward: 0.4564,                 loss: nan
Episode: 46881/50100 (93.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0895s / 3917.8149 s
agent0:                 episode reward: 0.2207,                 loss: 0.4500
agent1:                 episode reward: -0.2207,                 loss: nan
Episode: 46901/50100 (93.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0890s / 3919.9038 s
agent0:                 episode reward: 0.3325,                 loss: 0.4407
agent1:                 episode reward: -0.3325,                 loss: nan
Episode: 46921/50100 (93.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0891s / 3921.9929 s
agent0:                 episode reward: 0.1548,                 loss: 0.4373
agent1:                 episode reward: -0.1548,                 loss: nan
Episode: 46941/50100 (93.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0779s / 3924.0708 s
agent0:                 episode reward: -0.0362,                 loss: 0.4362
agent1:                 episode reward: 0.0362,                 loss: nan
Episode: 46961/50100 (93.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0988s / 3926.1696 s
agent0:                 episode reward: -0.0968,                 loss: 0.4366
agent1:                 episode reward: 0.0968,                 loss: nan
Episode: 46981/50100 (93.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0701s / 3928.2397 s
agent0:                 episode reward: -0.3393,                 loss: 0.4354
agent1:                 episode reward: 0.3393,                 loss: nan
Episode: 47001/50100 (93.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1043s / 3930.3440 s
agent0:                 episode reward: 0.0019,                 loss: 0.4365
agent1:                 episode reward: -0.0019,                 loss: nan
Episode: 47021/50100 (93.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0862s / 3932.4302 s
agent0:                 episode reward: 0.5021,                 loss: 0.4346
agent1:                 episode reward: -0.5021,                 loss: nan
Episode: 47041/50100 (93.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0844s / 3934.5145 s
agent0:                 episode reward: 0.3963,                 loss: 0.4340
agent1:                 episode reward: -0.3963,                 loss: nan
Episode: 47061/50100 (93.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0919s / 3936.6064 s
agent0:                 episode reward: 0.4161,                 loss: 0.4349
agent1:                 episode reward: -0.4161,                 loss: nan
Episode: 47081/50100 (93.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0754s / 3938.6818 s
agent0:                 episode reward: 0.3318,                 loss: 0.4298
agent1:                 episode reward: -0.3318,                 loss: nan
Episode: 47101/50100 (94.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0990s / 3940.7808 s
agent0:                 episode reward: 0.4206,                 loss: 0.4290
agent1:                 episode reward: -0.4206,                 loss: nan
Episode: 47121/50100 (94.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8759s / 3944.6567 s
agent0:                 episode reward: 0.4458,                 loss: 0.4275
agent1:                 episode reward: -0.4458,                 loss: 0.3568
Score delta: 2.171569727276007, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/46895_0.
Episode: 47141/50100 (94.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8231s / 3946.4798 s
agent0:                 episode reward: 0.0416,                 loss: nan
agent1:                 episode reward: -0.0416,                 loss: 0.3543
Episode: 47161/50100 (94.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8063s / 3948.2861 s
agent0:                 episode reward: 0.0923,                 loss: nan
agent1:                 episode reward: -0.0923,                 loss: 0.3551
Episode: 47181/50100 (94.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8127s / 3950.0988 s
agent0:                 episode reward: 0.3202,                 loss: nan
agent1:                 episode reward: -0.3202,                 loss: 0.3518
Episode: 47201/50100 (94.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7791s / 3951.8780 s
agent0:                 episode reward: -0.0716,                 loss: nan
agent1:                 episode reward: 0.0716,                 loss: 0.3525
Episode: 47221/50100 (94.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7806s / 3953.6585 s
agent0:                 episode reward: -0.3945,                 loss: nan
agent1:                 episode reward: 0.3945,                 loss: 0.3530
Episode: 47241/50100 (94.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 3.9104s / 3957.5689 s
agent0:                 episode reward: -0.4573,                 loss: 0.4552
agent1:                 episode reward: 0.4573,                 loss: 0.3557
Score delta: 2.1243226884790163, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/47014_1.
Episode: 47261/50100 (94.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0626s / 3959.6315 s
agent0:                 episode reward: 0.0696,                 loss: 0.4508
agent1:                 episode reward: -0.0696,                 loss: nan
Episode: 47281/50100 (94.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0712s / 3961.7027 s
agent0:                 episode reward: -0.2885,                 loss: 0.4492
agent1:                 episode reward: 0.2885,                 loss: nan
Episode: 47301/50100 (94.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0705s / 3963.7732 s
agent0:                 episode reward: 0.4455,                 loss: 0.4507
agent1:                 episode reward: -0.4455,                 loss: nan
Episode: 47321/50100 (94.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0676s / 3965.8408 s
agent0:                 episode reward: -0.1512,                 loss: 0.4491
agent1:                 episode reward: 0.1512,                 loss: nan
Episode: 47341/50100 (94.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0888s / 3967.9296 s
agent0:                 episode reward: 0.4294,                 loss: 0.4490
agent1:                 episode reward: -0.4294,                 loss: nan
Episode: 47361/50100 (94.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0929s / 3970.0226 s
agent0:                 episode reward: 0.0253,                 loss: 0.4425
agent1:                 episode reward: -0.0253,                 loss: nan
Episode: 47381/50100 (94.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0789s / 3972.1015 s
agent0:                 episode reward: -0.0333,                 loss: 0.4418
agent1:                 episode reward: 0.0333,                 loss: nan
Episode: 47401/50100 (94.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0698s / 3974.1712 s
agent0:                 episode reward: -0.2248,                 loss: 0.4424
agent1:                 episode reward: 0.2248,                 loss: nan
Episode: 47421/50100 (94.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0769s / 3976.2481 s
agent0:                 episode reward: -0.0708,                 loss: 0.4419
agent1:                 episode reward: 0.0708,                 loss: nan
Episode: 47441/50100 (94.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0890s / 3978.3371 s
agent0:                 episode reward: 0.1263,                 loss: 0.4422
agent1:                 episode reward: -0.1263,                 loss: nan
Episode: 47461/50100 (94.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0739s / 3980.4111 s
agent0:                 episode reward: -0.4987,                 loss: 0.4416
agent1:                 episode reward: 0.4987,                 loss: nan
Episode: 47481/50100 (94.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0931s / 3982.5042 s
agent0:                 episode reward: -0.1510,                 loss: 0.4415
agent1:                 episode reward: 0.1510,                 loss: nan
Episode: 47501/50100 (94.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0759s / 3984.5801 s
agent0:                 episode reward: 0.1828,                 loss: 0.4411
agent1:                 episode reward: -0.1828,                 loss: nan
Episode: 47521/50100 (94.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0800s / 3986.6601 s
agent0:                 episode reward: 0.1302,                 loss: 0.4348
agent1:                 episode reward: -0.1302,                 loss: nan
Episode: 47541/50100 (94.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0856s / 3988.7457 s
agent0:                 episode reward: 0.1266,                 loss: 0.4327
agent1:                 episode reward: -0.1266,                 loss: nan
Episode: 47561/50100 (94.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0598s / 3990.8056 s
agent0:                 episode reward: 0.2806,                 loss: 0.4316
agent1:                 episode reward: -0.2806,                 loss: nan
Episode: 47581/50100 (94.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8040s / 3994.6096 s
agent0:                 episode reward: 0.0710,                 loss: 0.4324
agent1:                 episode reward: -0.0710,                 loss: 0.3446
Score delta: 2.5806544169554266, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/47353_0.
Episode: 47601/50100 (95.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8333s / 3996.4429 s
agent0:                 episode reward: -0.0155,                 loss: nan
agent1:                 episode reward: 0.0155,                 loss: 0.3429
Episode: 47621/50100 (95.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8695s / 4000.3124 s
agent0:                 episode reward: -0.3542,                 loss: 0.3877
agent1:                 episode reward: 0.3542,                 loss: 0.3333
Score delta: 2.3036148465090744, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/47403_1.
Episode: 47641/50100 (95.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0658s / 4002.3783 s
agent0:                 episode reward: -0.1172,                 loss: 0.3803
agent1:                 episode reward: 0.1172,                 loss: nan
Episode: 47661/50100 (95.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0956s / 4004.4738 s
agent0:                 episode reward: -0.5699,                 loss: 0.3785
agent1:                 episode reward: 0.5699,                 loss: nan
Episode: 47681/50100 (95.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0862s / 4006.5600 s
agent0:                 episode reward: -0.8835,                 loss: 0.3799
agent1:                 episode reward: 0.8835,                 loss: nan
Episode: 47701/50100 (95.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0706s / 4008.6306 s
agent0:                 episode reward: 0.4897,                 loss: 0.3772
agent1:                 episode reward: -0.4897,                 loss: nan
Episode: 47721/50100 (95.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0788s / 4010.7094 s
agent0:                 episode reward: 0.0101,                 loss: 0.3781
agent1:                 episode reward: -0.0101,                 loss: nan
Episode: 47741/50100 (95.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0900s / 4012.7994 s
agent0:                 episode reward: 0.0330,                 loss: 0.3944
agent1:                 episode reward: -0.0330,                 loss: nan
Episode: 47761/50100 (95.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8667s / 4016.6661 s
agent0:                 episode reward: 0.3957,                 loss: 0.3962
agent1:                 episode reward: -0.3957,                 loss: 0.3504
Score delta: 2.0096190057455545, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/47536_0.
Episode: 47781/50100 (95.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8360s / 4018.5021 s
agent0:                 episode reward: -0.5156,                 loss: nan
agent1:                 episode reward: 0.5156,                 loss: 0.3488
Episode: 47801/50100 (95.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8275s / 4020.3296 s
agent0:                 episode reward: -0.0109,                 loss: nan
agent1:                 episode reward: 0.0109,                 loss: 0.3470
Episode: 47821/50100 (95.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8296s / 4022.1592 s
agent0:                 episode reward: 0.4366,                 loss: nan
agent1:                 episode reward: -0.4366,                 loss: 0.3456
Episode: 47841/50100 (95.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8371s / 4023.9963 s
agent0:                 episode reward: -0.4853,                 loss: nan
agent1:                 episode reward: 0.4853,                 loss: 0.3437
Episode: 47861/50100 (95.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8398s / 4025.8361 s
agent0:                 episode reward: -0.0679,                 loss: nan
agent1:                 episode reward: 0.0679,                 loss: 0.3437
Episode: 47881/50100 (95.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8486s / 4027.6846 s
agent0:                 episode reward: -0.1940,                 loss: nan
agent1:                 episode reward: 0.1940,                 loss: 0.3411
Episode: 47901/50100 (95.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9588s / 4029.6434 s
agent0:                 episode reward: 0.2059,                 loss: nan
agent1:                 episode reward: -0.2059,                 loss: 0.3408
Episode: 47921/50100 (95.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8321s / 4031.4755 s
agent0:                 episode reward: 0.5375,                 loss: nan
agent1:                 episode reward: -0.5375,                 loss: 0.3295
Episode: 47941/50100 (95.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 3.9733s / 4035.4488 s
agent0:                 episode reward: -0.2874,                 loss: 0.4322
agent1:                 episode reward: 0.2874,                 loss: 0.3123
Score delta: 2.30491309439397, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/47717_1.
Episode: 47961/50100 (95.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0867s / 4037.5355 s
agent0:                 episode reward: 0.2196,                 loss: 0.4300
agent1:                 episode reward: -0.2196,                 loss: nan
Episode: 47981/50100 (95.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0591s / 4039.5946 s
agent0:                 episode reward: -0.1343,                 loss: 0.4302
agent1:                 episode reward: 0.1343,                 loss: nan
Episode: 48001/50100 (95.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0823s / 4041.6768 s
agent0:                 episode reward: -0.3025,                 loss: 0.4299
agent1:                 episode reward: 0.3025,                 loss: nan
Episode: 48021/50100 (95.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1200s / 4043.7968 s
agent0:                 episode reward: 0.0219,                 loss: 0.4281
agent1:                 episode reward: -0.0219,                 loss: nan
Episode: 48041/50100 (95.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1329s / 4045.9298 s
agent0:                 episode reward: -0.2505,                 loss: 0.4307
agent1:                 episode reward: 0.2505,                 loss: nan
Episode: 48061/50100 (95.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1327s / 4048.0625 s
agent0:                 episode reward: -0.0019,                 loss: 0.4287
agent1:                 episode reward: 0.0019,                 loss: nan
Episode: 48081/50100 (95.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1363s / 4050.1987 s
agent0:                 episode reward: 0.2501,                 loss: 0.4383
agent1:                 episode reward: -0.2501,                 loss: nan
Episode: 48101/50100 (96.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 4.0079s / 4054.2066 s
agent0:                 episode reward: 0.1037,                 loss: 0.4430
agent1:                 episode reward: -0.1037,                 loss: 0.3485
Score delta: 2.14413660741535, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/47875_0.
Episode: 48121/50100 (96.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 4.0663s / 4058.2729 s
agent0:                 episode reward: -0.6716,                 loss: 0.3977
agent1:                 episode reward: 0.6716,                 loss: 0.3492
Score delta: 2.0554301411674505, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/47906_1.
Episode: 48141/50100 (96.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1058s / 4060.3787 s
agent0:                 episode reward: 0.0780,                 loss: 0.3822
agent1:                 episode reward: -0.0780,                 loss: nan
Episode: 48161/50100 (96.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1336s / 4062.5123 s
agent0:                 episode reward: 0.1610,                 loss: 0.3782
agent1:                 episode reward: -0.1610,                 loss: nan
Episode: 48181/50100 (96.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1203s / 4064.6325 s
agent0:                 episode reward: -0.0335,                 loss: 0.3781
agent1:                 episode reward: 0.0335,                 loss: nan
Episode: 48201/50100 (96.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1076s / 4066.7401 s
agent0:                 episode reward: -0.1664,                 loss: 0.3783
agent1:                 episode reward: 0.1664,                 loss: nan
Episode: 48221/50100 (96.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1093s / 4068.8495 s
agent0:                 episode reward: -0.5214,                 loss: 0.3780
agent1:                 episode reward: 0.5214,                 loss: nan
Episode: 48241/50100 (96.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1321s / 4070.9816 s
agent0:                 episode reward: -0.2831,                 loss: 0.3762
agent1:                 episode reward: 0.2831,                 loss: nan
Episode: 48261/50100 (96.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0871s / 4073.0687 s
agent0:                 episode reward: 0.2616,                 loss: 0.3759
agent1:                 episode reward: -0.2616,                 loss: nan
Episode: 48281/50100 (96.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0959s / 4075.1646 s
agent0:                 episode reward: -0.2004,                 loss: 0.3893
agent1:                 episode reward: 0.2004,                 loss: nan
Episode: 48301/50100 (96.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0734s / 4077.2380 s
agent0:                 episode reward: -0.5964,                 loss: 0.3923
agent1:                 episode reward: 0.5964,                 loss: nan
Episode: 48321/50100 (96.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0964s / 4079.3344 s
agent0:                 episode reward: 0.0371,                 loss: 0.3931
agent1:                 episode reward: -0.0371,                 loss: nan
Episode: 48341/50100 (96.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0632s / 4081.3976 s
agent0:                 episode reward: -0.3150,                 loss: 0.3922
agent1:                 episode reward: 0.3150,                 loss: nan
Episode: 48361/50100 (96.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0620s / 4083.4596 s
agent0:                 episode reward: -0.1532,                 loss: 0.3915
agent1:                 episode reward: 0.1532,                 loss: nan
Episode: 48381/50100 (96.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0786s / 4085.5381 s
agent0:                 episode reward: -0.4518,                 loss: 0.3899
agent1:                 episode reward: 0.4518,                 loss: nan
Episode: 48401/50100 (96.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0658s / 4087.6039 s
agent0:                 episode reward: -0.0631,                 loss: 0.3910
agent1:                 episode reward: 0.0631,                 loss: nan
Episode: 48421/50100 (96.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0787s / 4089.6826 s
agent0:                 episode reward: 0.0118,                 loss: 0.3901
agent1:                 episode reward: -0.0118,                 loss: nan
Episode: 48441/50100 (96.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0661s / 4091.7487 s
agent0:                 episode reward: 0.2493,                 loss: 0.4087
agent1:                 episode reward: -0.2493,                 loss: nan
Episode: 48461/50100 (96.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0701s / 4093.8188 s
agent0:                 episode reward: -0.1506,                 loss: 0.4359
agent1:                 episode reward: 0.1506,                 loss: nan
Episode: 48481/50100 (96.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 3.9885s / 4097.8074 s
agent0:                 episode reward: 0.5196,                 loss: 0.4363
agent1:                 episode reward: -0.5196,                 loss: 0.3535
Score delta: 2.1639450582271094, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/48260_0.
Episode: 48501/50100 (96.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8406s / 4099.6480 s
agent0:                 episode reward: -0.0337,                 loss: nan
agent1:                 episode reward: 0.0337,                 loss: 0.3498
Episode: 48521/50100 (96.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8222s / 4101.4702 s
agent0:                 episode reward: 0.1200,                 loss: nan
agent1:                 episode reward: -0.1200,                 loss: 0.3482
Episode: 48541/50100 (96.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8318s / 4103.3020 s
agent0:                 episode reward: -0.1586,                 loss: nan
agent1:                 episode reward: 0.1586,                 loss: 0.3483
Episode: 48561/50100 (96.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8645s / 4107.1664 s
agent0:                 episode reward: -0.3014,                 loss: 0.4526
agent1:                 episode reward: 0.3014,                 loss: 0.3454
Score delta: 2.1063771044370023, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/48348_1.
Episode: 48581/50100 (96.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0679s / 4109.2343 s
agent0:                 episode reward: 0.5378,                 loss: 0.4404
agent1:                 episode reward: -0.5378,                 loss: nan
Episode: 48601/50100 (97.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0772s / 4111.3115 s
agent0:                 episode reward: -0.1091,                 loss: 0.4379
agent1:                 episode reward: 0.1091,                 loss: nan
Episode: 48621/50100 (97.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0864s / 4113.3979 s
agent0:                 episode reward: 0.2181,                 loss: 0.4368
agent1:                 episode reward: -0.2181,                 loss: nan
Episode: 48641/50100 (97.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0549s / 4115.4528 s
agent0:                 episode reward: 0.1307,                 loss: 0.4358
agent1:                 episode reward: -0.1307,                 loss: nan
Episode: 48661/50100 (97.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0946s / 4117.5474 s
agent0:                 episode reward: 0.5550,                 loss: 0.4355
agent1:                 episode reward: -0.5550,                 loss: nan
Episode: 48681/50100 (97.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0797s / 4119.6271 s
agent0:                 episode reward: 0.0878,                 loss: 0.4355
agent1:                 episode reward: -0.0878,                 loss: nan
Episode: 48701/50100 (97.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0856s / 4121.7126 s
agent0:                 episode reward: -0.1431,                 loss: 0.4280
agent1:                 episode reward: 0.1431,                 loss: nan
Episode: 48721/50100 (97.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0863s / 4123.7990 s
agent0:                 episode reward: 0.0485,                 loss: 0.4216
agent1:                 episode reward: -0.0485,                 loss: nan
Episode: 48741/50100 (97.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8656s / 4127.6645 s
agent0:                 episode reward: -0.0763,                 loss: 0.4199
agent1:                 episode reward: 0.0763,                 loss: 0.3675
Score delta: 2.009970878047585, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/48511_0.
Episode: 48761/50100 (97.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8391s / 4129.5037 s
agent0:                 episode reward: -0.0392,                 loss: nan
agent1:                 episode reward: 0.0392,                 loss: 0.3703
Episode: 48781/50100 (97.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8636s / 4131.3673 s
agent0:                 episode reward: -0.1837,                 loss: nan
agent1:                 episode reward: 0.1837,                 loss: 0.3658
Episode: 48801/50100 (97.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8388s / 4133.2061 s
agent0:                 episode reward: -0.4697,                 loss: nan
agent1:                 episode reward: 0.4697,                 loss: 0.3674
Episode: 48821/50100 (97.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8555s / 4135.0616 s
agent0:                 episode reward: -0.4744,                 loss: nan
agent1:                 episode reward: 0.4744,                 loss: 0.3651
Episode: 48841/50100 (97.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8579s / 4136.9195 s
agent0:                 episode reward: -0.1222,                 loss: nan
agent1:                 episode reward: 0.1222,                 loss: 0.3648
Episode: 48861/50100 (97.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8449s / 4138.7644 s
agent0:                 episode reward: -0.6306,                 loss: nan
agent1:                 episode reward: 0.6306,                 loss: 0.3644
Episode: 48881/50100 (97.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8340s / 4140.5984 s
agent0:                 episode reward: 0.2035,                 loss: nan
agent1:                 episode reward: -0.2035,                 loss: 0.3639
Episode: 48901/50100 (97.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8531s / 4142.4514 s
agent0:                 episode reward: -0.1542,                 loss: nan
agent1:                 episode reward: 0.1542,                 loss: 0.3652
Episode: 48921/50100 (97.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8509s / 4144.3023 s
agent0:                 episode reward: -0.2139,                 loss: nan
agent1:                 episode reward: 0.2139,                 loss: 0.3842
Episode: 48941/50100 (97.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8444s / 4146.1467 s
agent0:                 episode reward: -0.4305,                 loss: nan
agent1:                 episode reward: 0.4305,                 loss: 0.4199
Episode: 48961/50100 (97.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8704s / 4148.0170 s
agent0:                 episode reward: -0.0539,                 loss: nan
agent1:                 episode reward: 0.0539,                 loss: 0.4185
Episode: 48981/50100 (97.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8485s / 4149.8656 s
agent0:                 episode reward: 0.2264,                 loss: nan
agent1:                 episode reward: -0.2264,                 loss: 0.4184
Episode: 49001/50100 (97.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8860s / 4151.7516 s
agent0:                 episode reward: 0.0790,                 loss: nan
agent1:                 episode reward: -0.0790,                 loss: 0.4180
Episode: 49021/50100 (97.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8636s / 4153.6152 s
agent0:                 episode reward: -0.2166,                 loss: nan
agent1:                 episode reward: 0.2166,                 loss: 0.4171
Episode: 49041/50100 (97.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8711s / 4155.4863 s
agent0:                 episode reward: -0.3572,                 loss: nan
agent1:                 episode reward: 0.3572,                 loss: 0.4167
Episode: 49061/50100 (97.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 3.8618s / 4159.3481 s
agent0:                 episode reward: -0.6570,                 loss: nan
agent1:                 episode reward: 0.6570,                 loss: 0.4186
Score delta: 2.5627151045565117, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/48849_1.
Episode: 49081/50100 (97.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0843s / 4161.4325 s
agent0:                 episode reward: 0.1384,                 loss: 0.4192
agent1:                 episode reward: -0.1384,                 loss: nan
Episode: 49101/50100 (98.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0575s / 4163.4899 s
agent0:                 episode reward: 0.1809,                 loss: 0.4181
agent1:                 episode reward: -0.1809,                 loss: nan
Episode: 49121/50100 (98.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1190s / 4165.6090 s
agent0:                 episode reward: 0.1499,                 loss: 0.4171
agent1:                 episode reward: -0.1499,                 loss: nan
Episode: 49141/50100 (98.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0841s / 4167.6931 s
agent0:                 episode reward: 0.4054,                 loss: 0.4142
agent1:                 episode reward: -0.4054,                 loss: nan
Episode: 49161/50100 (98.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0952s / 4169.7882 s
agent0:                 episode reward: 0.0532,                 loss: 0.4171
agent1:                 episode reward: -0.0532,                 loss: nan
Episode: 49181/50100 (98.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1089s / 4171.8972 s
agent0:                 episode reward: -0.0884,                 loss: 0.4145
agent1:                 episode reward: 0.0884,                 loss: nan
Episode: 49201/50100 (98.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0923s / 4173.9895 s
agent0:                 episode reward: 0.1425,                 loss: 0.4180
agent1:                 episode reward: -0.1425,                 loss: nan
Episode: 49221/50100 (98.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0923s / 4176.0818 s
agent0:                 episode reward: -0.2022,                 loss: 0.4218
agent1:                 episode reward: 0.2022,                 loss: nan
Episode: 49241/50100 (98.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0960s / 4178.1778 s
agent0:                 episode reward: 0.2496,                 loss: 0.4201
agent1:                 episode reward: -0.2496,                 loss: nan
Episode: 49261/50100 (98.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0998s / 4180.2776 s
agent0:                 episode reward: -0.0003,                 loss: 0.4203
agent1:                 episode reward: 0.0003,                 loss: nan
Episode: 49281/50100 (98.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1039s / 4182.3815 s
agent0:                 episode reward: -0.0759,                 loss: 0.4209
agent1:                 episode reward: 0.0759,                 loss: nan
Episode: 49301/50100 (98.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0936s / 4184.4751 s
agent0:                 episode reward: 0.3737,                 loss: 0.4190
agent1:                 episode reward: -0.3737,                 loss: nan
Episode: 49321/50100 (98.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 3.9332s / 4188.4083 s
agent0:                 episode reward: 0.1169,                 loss: 0.4206
agent1:                 episode reward: -0.1169,                 loss: 0.3776
Score delta: 2.556815325827068, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/49093_0.
Episode: 49341/50100 (98.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8914s / 4190.2996 s
agent0:                 episode reward: -0.3456,                 loss: nan
agent1:                 episode reward: 0.3456,                 loss: 0.3914
Episode: 49361/50100 (98.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8538s / 4192.1535 s
agent0:                 episode reward: 0.0475,                 loss: nan
agent1:                 episode reward: -0.0475,                 loss: 0.3817
Episode: 49381/50100 (98.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 3.9847s / 4196.1382 s
agent0:                 episode reward: -0.5405,                 loss: 0.4521
agent1:                 episode reward: 0.5405,                 loss: 0.3826
Score delta: 2.029364159015233, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/49164_1.
Episode: 49401/50100 (98.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0989s / 4198.2370 s
agent0:                 episode reward: -0.1167,                 loss: 0.4508
agent1:                 episode reward: 0.1167,                 loss: nan
Episode: 49421/50100 (98.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0792s / 4200.3162 s
agent0:                 episode reward: -0.4126,                 loss: 0.4490
agent1:                 episode reward: 0.4126,                 loss: nan
Episode: 49441/50100 (98.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0946s / 4202.4108 s
agent0:                 episode reward: -0.0264,                 loss: 0.4452
agent1:                 episode reward: 0.0264,                 loss: nan
Episode: 49461/50100 (98.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1100s / 4204.5208 s
agent0:                 episode reward: -0.1405,                 loss: 0.4411
agent1:                 episode reward: 0.1405,                 loss: nan
Episode: 49481/50100 (98.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0983s / 4206.6191 s
agent0:                 episode reward: -0.7272,                 loss: 0.4412
agent1:                 episode reward: 0.7272,                 loss: nan
Episode: 49501/50100 (98.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0941s / 4208.7132 s
agent0:                 episode reward: -0.1286,                 loss: 0.4398
agent1:                 episode reward: 0.1286,                 loss: nan
Episode: 49521/50100 (98.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0959s / 4210.8090 s
agent0:                 episode reward: -0.6398,                 loss: 0.4409
agent1:                 episode reward: 0.6398,                 loss: nan
Episode: 49541/50100 (98.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1000s / 4212.9090 s
agent0:                 episode reward: 0.3764,                 loss: 0.4406
agent1:                 episode reward: -0.3764,                 loss: nan
Episode: 49561/50100 (98.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0863s / 4214.9953 s
agent0:                 episode reward: -0.1369,                 loss: 0.4395
agent1:                 episode reward: 0.1369,                 loss: nan
Episode: 49581/50100 (98.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1137s / 4217.1090 s
agent0:                 episode reward: 0.0285,                 loss: 0.4407
agent1:                 episode reward: -0.0285,                 loss: nan
Episode: 49601/50100 (99.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0859s / 4219.1949 s
agent0:                 episode reward: -0.1721,                 loss: 0.4371
agent1:                 episode reward: 0.1721,                 loss: nan
Episode: 49621/50100 (99.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1035s / 4221.2985 s
agent0:                 episode reward: -0.3783,                 loss: 0.4230
agent1:                 episode reward: 0.3783,                 loss: nan
Episode: 49641/50100 (99.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1013s / 4223.3998 s
agent0:                 episode reward: 0.1901,                 loss: 0.4224
agent1:                 episode reward: -0.1901,                 loss: nan
Episode: 49661/50100 (99.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0875s / 4225.4872 s
agent0:                 episode reward: -0.1963,                 loss: 0.4218
agent1:                 episode reward: 0.1963,                 loss: nan
Episode: 49681/50100 (99.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0814s / 4227.5687 s
agent0:                 episode reward: 0.0642,                 loss: 0.4209
agent1:                 episode reward: -0.0642,                 loss: nan
Episode: 49701/50100 (99.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0858s / 4229.6544 s
agent0:                 episode reward: 0.3774,                 loss: 0.4219
agent1:                 episode reward: -0.3774,                 loss: nan
Episode: 49721/50100 (99.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0885s / 4231.7429 s
agent0:                 episode reward: -0.3279,                 loss: 0.4223
agent1:                 episode reward: 0.3279,                 loss: nan
Episode: 49741/50100 (99.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0889s / 4233.8318 s
agent0:                 episode reward: -0.3493,                 loss: 0.4217
agent1:                 episode reward: 0.3493,                 loss: nan
Episode: 49761/50100 (99.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0837s / 4235.9155 s
agent0:                 episode reward: -0.3868,                 loss: 0.4216
agent1:                 episode reward: 0.3868,                 loss: nan
Episode: 49781/50100 (99.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1024s / 4238.0180 s
agent0:                 episode reward: -0.0056,                 loss: 0.4314
agent1:                 episode reward: 0.0056,                 loss: nan
Episode: 49801/50100 (99.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1008s / 4240.1187 s
agent0:                 episode reward: 0.0143,                 loss: 0.4326
agent1:                 episode reward: -0.0143,                 loss: nan
Episode: 49821/50100 (99.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0891s / 4242.2078 s
agent0:                 episode reward: -0.4648,                 loss: 0.4321
agent1:                 episode reward: 0.4648,                 loss: nan
Episode: 49841/50100 (99.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1179s / 4244.3257 s
agent0:                 episode reward: -0.2180,                 loss: 0.4324
agent1:                 episode reward: 0.2180,                 loss: nan
Episode: 49861/50100 (99.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0877s / 4246.4134 s
agent0:                 episode reward: -0.1280,                 loss: 0.4316
agent1:                 episode reward: 0.1280,                 loss: nan
Episode: 49881/50100 (99.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1000s / 4248.5133 s
agent0:                 episode reward: -0.1186,                 loss: 0.4321
agent1:                 episode reward: 0.1186,                 loss: nan
Episode: 49901/50100 (99.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0647s / 4250.5781 s
agent0:                 episode reward: -0.1327,                 loss: 0.4322
agent1:                 episode reward: 0.1327,                 loss: nan
Episode: 49921/50100 (99.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0965s / 4252.6746 s
agent0:                 episode reward: 0.0633,                 loss: 0.4322
agent1:                 episode reward: -0.0633,                 loss: nan
Episode: 49941/50100 (99.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0702s / 4254.7448 s
agent0:                 episode reward: 0.0696,                 loss: 0.4340
agent1:                 episode reward: -0.0696,                 loss: nan
Episode: 49961/50100 (99.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1116s / 4256.8564 s
agent0:                 episode reward: -0.1603,                 loss: 0.4351
agent1:                 episode reward: 0.1603,                 loss: nan
Episode: 49981/50100 (99.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 4.1586s / 4261.0150 s
agent0:                 episode reward: 0.1593,                 loss: 0.4358
agent1:                 episode reward: -0.1593,                 loss: 0.3260
Score delta: 2.148164037846956, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/49767_0.
Episode: 50001/50100 (99.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9084s / 4262.9234 s
agent0:                 episode reward: -0.0188,                 loss: nan/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr = np.asanyarray(arr)

agent1:                 episode reward: 0.0188,                 loss: 0.3183
Episode: 50021/50100 (99.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9006s / 4264.8240 s
agent0:                 episode reward: -0.2893,                 loss: nan
agent1:                 episode reward: 0.2893,                 loss: 0.3165
Episode: 50041/50100 (99.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8899s / 4266.7138 s
agent0:                 episode reward: 0.0881,                 loss: nan
agent1:                 episode reward: -0.0881,                 loss: 0.3120
Episode: 50061/50100 (99.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9012s / 4268.6150 s
agent0:                 episode reward: 0.0025,                 loss: nan
agent1:                 episode reward: -0.0025,                 loss: 0.3123
Episode: 50081/50100 (99.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 4.0269s / 4272.6419 s
agent0:                 episode reward: -0.6707,                 loss: 0.4411
agent1:                 episode reward: 0.6707,                 loss: 0.3110
Score delta: 2.0145717435174415, save the model to .//data/model/20220516171217/mdp_arbitrary_mdp_nxdo2/49862_1.
