pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0526/pettingzoo_boxing_v1_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0526/pettingzoo_boxing_v1_nfsp.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 12.1571s / 12.1571 s
env0_first_0:                 episode reward: 3.0000,                 loss: nan
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1039.0733s / 1051.2303 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0087
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0085
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1334.6201s / 2385.8504 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0172
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0160
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1587.55,                last time consumption/overall running time: 1207.2231s / 3593.0735 s
env0_first_0:                 episode reward: -20.8500,                 loss: 0.0520
env0_second_0:                 episode reward: 20.8500,                 loss: 0.0490
env1_first_0:                 episode reward: -28.0500,                 loss: nan
env1_second_0:                 episode reward: 28.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1207.6,                last time consumption/overall running time: 920.6139s / 4513.6874 s
env0_first_0:                 episode reward: -41.8500,                 loss: 0.1024
env0_second_0:                 episode reward: 41.8500,                 loss: 0.0934
env1_first_0:                 episode reward: -38.3500,                 loss: nan
env1_second_0:                 episode reward: 38.3500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 782.9,                last time consumption/overall running time: 597.0408s / 5110.7282 s
env0_first_0:                 episode reward: -57.0500,                 loss: 0.1224
env0_second_0:                 episode reward: 57.0500,                 loss: 0.1390
env1_first_0:                 episode reward: -51.2000,                 loss: nan
env1_second_0:                 episode reward: 51.2000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 594.25,                last time consumption/overall running time: 454.8505s / 5565.5787 s
env0_first_0:                 episode reward: -72.3500,                 loss: 0.1589
env0_second_0:                 episode reward: 72.3500,                 loss: 0.2087
env1_first_0:                 episode reward: -27.2000,                 loss: nan
env1_second_0:                 episode reward: 27.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 327.8,                last time consumption/overall running time: 251.3882s / 5816.9669 s
env0_first_0:                 episode reward: -76.2500,                 loss: 0.1845
env0_second_0:                 episode reward: 76.2500,                 loss: 0.2715
env1_first_0:                 episode reward: -70.4500,                 loss: nan
env1_second_0:                 episode reward: 70.4500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 323.55,                last time consumption/overall running time: 249.6204s / 6066.5873 s
env0_first_0:                 episode reward: -77.0500,                 loss: 0.2082
env0_second_0:                 episode reward: 77.0500,                 loss: 0.2923
env1_first_0:                 episode reward: -67.0500,                 loss: nan
env1_second_0:                 episode reward: 67.0500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 315.65,                last time consumption/overall running time: 242.1226s / 6308.7099 s
env0_first_0:                 episode reward: -70.6000,                 loss: 0.2503
env0_second_0:                 episode reward: 70.6000,                 loss: 0.3348
env1_first_0:                 episode reward: -79.4000,                 loss: nan
env1_second_0:                 episode reward: 79.4000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 255.35,                last time consumption/overall running time: 196.5148s / 6505.2247 s
env0_first_0:                 episode reward: -83.0500,                 loss: 0.2930
env0_second_0:                 episode reward: 83.0500,                 loss: 0.3695
env1_first_0:                 episode reward: -85.3000,                 loss: nan
env1_second_0:                 episode reward: 85.3000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 277.8,                last time consumption/overall running time: 213.4208s / 6718.6455 s
env0_first_0:                 episode reward: -87.9000,                 loss: 0.3135
env0_second_0:                 episode reward: 87.9000,                 loss: 0.4160
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 375.75,                last time consumption/overall running time: 289.7779s / 7008.4234 s
env0_first_0:                 episode reward: -77.2500,                 loss: 0.3251
env0_second_0:                 episode reward: 77.2500,                 loss: 0.4032
env1_first_0:                 episode reward: -60.3000,                 loss: nan
env1_second_0:                 episode reward: 60.3000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 267.25,                last time consumption/overall running time: 205.0564s / 7213.4798 s
env0_first_0:                 episode reward: -82.3000,                 loss: 0.3251
env0_second_0:                 episode reward: 82.3000,                 loss: 0.3573
env1_first_0:                 episode reward: -87.5500,                 loss: nan
env1_second_0:                 episode reward: 87.5500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 261.6,                last time consumption/overall running time: 201.4400s / 7414.9198 s
env0_first_0:                 episode reward: -85.1500,                 loss: 0.3189
env0_second_0:                 episode reward: 85.1500,                 loss: 0.3690
env1_first_0:                 episode reward: -73.0000,                 loss: nan
env1_second_0:                 episode reward: 73.0000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 275.7,                last time consumption/overall running time: 211.4172s / 7626.3370 s
env0_first_0:                 episode reward: -80.0500,                 loss: 0.3352
env0_second_0:                 episode reward: 80.0500,                 loss: 0.4079
env1_first_0:                 episode reward: -73.9500,                 loss: nan
env1_second_0:                 episode reward: 73.9500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 256.4,                last time consumption/overall running time: 196.6327s / 7822.9697 s
env0_first_0:                 episode reward: -70.7000,                 loss: 0.3564
env0_second_0:                 episode reward: 70.7000,                 loss: 0.4031
env1_first_0:                 episode reward: -83.6000,                 loss: nan
env1_second_0:                 episode reward: 83.6000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 302.05,                last time consumption/overall running time: 232.3335s / 8055.3032 s
env0_first_0:                 episode reward: -76.6500,                 loss: 0.3883
env0_second_0:                 episode reward: 76.6500,                 loss: 0.4071
env1_first_0:                 episode reward: -70.2000,                 loss: nan
env1_second_0:                 episode reward: 70.2000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 275.4,                last time consumption/overall running time: 211.4917s / 8266.7949 s
env0_first_0:                 episode reward: -77.9000,                 loss: 0.4017
env0_second_0:                 episode reward: 77.9000,                 loss: 0.4238
env1_first_0:                 episode reward: -80.6500,                 loss: nan
env1_second_0:                 episode reward: 80.6500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 272.25,                last time consumption/overall running time: 209.4970s / 8476.2919 s
env0_first_0:                 episode reward: -89.3500,                 loss: 0.4254
env0_second_0:                 episode reward: 89.3500,                 loss: 0.4540
env1_first_0:                 episode reward: -75.4000,                 loss: nan
env1_second_0:                 episode reward: 75.4000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 271.35,                last time consumption/overall running time: 207.9807s / 8684.2726 s
env0_first_0:                 episode reward: -84.0500,                 loss: 0.4352
env0_second_0:                 episode reward: 84.0500,                 loss: 0.4600
env1_first_0:                 episode reward: -88.7500,                 loss: nan
env1_second_0:                 episode reward: 88.7500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 244.25,                last time consumption/overall running time: 187.9014s / 8872.1740 s
env0_first_0:                 episode reward: -79.8000,                 loss: 0.4390
env0_second_0:                 episode reward: 79.8000,                 loss: 0.4636
env1_first_0:                 episode reward: -81.1000,                 loss: nan
env1_second_0:                 episode reward: 81.1000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 278.9,                last time consumption/overall running time: 214.4688s / 9086.6428 s
env0_first_0:                 episode reward: -76.8000,                 loss: 0.4846
env0_second_0:                 episode reward: 76.8000,                 loss: 0.4739
env1_first_0:                 episode reward: -85.7500,                 loss: nan
env1_second_0:                 episode reward: 85.7500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 258.2,                last time consumption/overall running time: 198.2717s / 9284.9145 s
env0_first_0:                 episode reward: -72.3000,                 loss: 0.5213
env0_second_0:                 episode reward: 72.3000,                 loss: 0.4727
env1_first_0:                 episode reward: -92.8000,                 loss: nan
env1_second_0:                 episode reward: 92.8000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 262.9,                last time consumption/overall running time: 201.9305s / 9486.8450 s
env0_first_0:                 episode reward: -75.9500,                 loss: 0.5176
env0_second_0:                 episode reward: 75.9500,                 loss: 0.4800
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 313.25,                last time consumption/overall running time: 240.3204s / 9727.1653 s
env0_first_0:                 episode reward: -66.4000,                 loss: 0.5347
env0_second_0:                 episode reward: 66.4000,                 loss: 0.5043
env1_first_0:                 episode reward: -75.0000,                 loss: nan
env1_second_0:                 episode reward: 75.0000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 273.8,                last time consumption/overall running time: 210.6925s / 9937.8578 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.5831
env0_second_0:                 episode reward: 89.3000,                 loss: 0.5221
env1_first_0:                 episode reward: -75.5000,                 loss: nan
env1_second_0:                 episode reward: 75.5000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 309.95,                last time consumption/overall running time: 238.2866s / 10176.1444 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.5747
env0_second_0:                 episode reward: 86.6500,                 loss: 0.4958
env1_first_0:                 episode reward: -60.5500,                 loss: nan
env1_second_0:                 episode reward: 60.5500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 430.1,                last time consumption/overall running time: 329.7546s / 10505.8990 s
env0_first_0:                 episode reward: -62.8500,                 loss: 0.5662
env0_second_0:                 episode reward: 62.8500,                 loss: 0.5203
env1_first_0:                 episode reward: -69.6500,                 loss: nan
env1_second_0:                 episode reward: 69.6500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 397.45,                last time consumption/overall running time: 304.5633s / 10810.4623 s
env0_first_0:                 episode reward: -76.5500,                 loss: 0.6035
env0_second_0:                 episode reward: 76.5500,                 loss: 0.5282
env1_first_0:                 episode reward: -81.2500,                 loss: nan
env1_second_0:                 episode reward: 81.2500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 723.15,                last time consumption/overall running time: 553.9089s / 11364.3712 s
env0_first_0:                 episode reward: -58.2500,                 loss: 0.4686
env0_second_0:                 episode reward: 58.2500,                 loss: 0.4590
env1_first_0:                 episode reward: -44.0000,                 loss: nan
env1_second_0:                 episode reward: 44.0000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 333.85,                last time consumption/overall running time: 256.3133s / 11620.6845 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.4376
env0_second_0:                 episode reward: 86.4500,                 loss: 0.4324
env1_first_0:                 episode reward: -78.2500,                 loss: nan
env1_second_0:                 episode reward: 78.2500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 308.05,                last time consumption/overall running time: 236.9082s / 11857.5928 s
env0_first_0:                 episode reward: -78.3500,                 loss: 0.4386
env0_second_0:                 episode reward: 78.3500,                 loss: 0.4015
env1_first_0:                 episode reward: -60.6500,                 loss: nan
env1_second_0:                 episode reward: 60.6500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 322.65,                last time consumption/overall running time: 247.5155s / 12105.1083 s
env0_first_0:                 episode reward: -61.9000,                 loss: 0.4588
env0_second_0:                 episode reward: 61.9000,                 loss: 0.4054
env1_first_0:                 episode reward: -67.0500,                 loss: nan
env1_second_0:                 episode reward: 67.0500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 352.9,                last time consumption/overall running time: 271.0466s / 12376.1548 s
env0_first_0:                 episode reward: -70.4000,                 loss: 0.4560
env0_second_0:                 episode reward: 70.4000,                 loss: 0.4034
env1_first_0:                 episode reward: -69.8000,                 loss: nan
env1_second_0:                 episode reward: 69.8000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 275.15,                last time consumption/overall running time: 212.6648s / 12588.8196 s
env0_first_0:                 episode reward: -83.9500,                 loss: 0.4866
env0_second_0:                 episode reward: 83.9500,                 loss: 0.4181
env1_first_0:                 episode reward: -74.2000,                 loss: nan
env1_second_0:                 episode reward: 74.2000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 313.8,                last time consumption/overall running time: 241.8232s / 12830.6428 s
env0_first_0:                 episode reward: -69.1000,                 loss: 0.5402
env0_second_0:                 episode reward: 69.1000,                 loss: 0.4527
env1_first_0:                 episode reward: -74.9000,                 loss: nan
env1_second_0:                 episode reward: 74.9000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 279.1,                last time consumption/overall running time: 213.6856s / 13044.3284 s
env0_first_0:                 episode reward: -71.4000,                 loss: 0.5896
env0_second_0:                 episode reward: 71.4000,                 loss: 0.5485
env1_first_0:                 episode reward: -78.4000,                 loss: nan
env1_second_0:                 episode reward: 78.4000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 273.65,                last time consumption/overall running time: 211.0567s / 13255.3851 s
env0_first_0:                 episode reward: -84.3000,                 loss: 0.6598
env0_second_0:                 episode reward: 84.3000,                 loss: 0.5921
env1_first_0:                 episode reward: -74.4000,                 loss: nan
env1_second_0:                 episode reward: 74.4000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 257.95,                last time consumption/overall running time: 199.2863s / 13454.6714 s
env0_first_0:                 episode reward: -83.8500,                 loss: 0.6636
env0_second_0:                 episode reward: 83.8500,                 loss: 0.6028
env1_first_0:                 episode reward: -79.6000,                 loss: nan
env1_second_0:                 episode reward: 79.6000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 293.55,                last time consumption/overall running time: 225.1186s / 13679.7900 s
env0_first_0:                 episode reward: -81.3500,                 loss: 0.7141
env0_second_0:                 episode reward: 81.3500,                 loss: 0.6275
env1_first_0:                 episode reward: -82.3000,                 loss: nan
env1_second_0:                 episode reward: 82.3000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 260.25,                last time consumption/overall running time: 199.9055s / 13879.6954 s
env0_first_0:                 episode reward: -82.9500,                 loss: 0.7037
env0_second_0:                 episode reward: 82.9500,                 loss: 0.6286
env1_first_0:                 episode reward: -88.7500,                 loss: nan
env1_second_0:                 episode reward: 88.7500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 283.85,                last time consumption/overall running time: 217.7054s / 14097.4008 s
env0_first_0:                 episode reward: -82.4000,                 loss: 0.7756
env0_second_0:                 episode reward: 82.4000,                 loss: 0.6256
env1_first_0:                 episode reward: -76.8000,                 loss: nan
env1_second_0:                 episode reward: 76.8000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 287.6,                last time consumption/overall running time: 221.3782s / 14318.7790 s
env0_first_0:                 episode reward: -80.6500,                 loss: 0.7844
env0_second_0:                 episode reward: 80.6500,                 loss: 0.6680
env1_first_0:                 episode reward: -76.5500,                 loss: nan
env1_second_0:                 episode reward: 76.5500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 332.6,                last time consumption/overall running time: 255.0909s / 14573.8699 s
env0_first_0:                 episode reward: -58.8000,                 loss: 0.8272
env0_second_0:                 episode reward: 58.8000,                 loss: 0.7096
env1_first_0:                 episode reward: -73.5000,                 loss: nan
env1_second_0:                 episode reward: 73.5000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 268.4,                last time consumption/overall running time: 206.6762s / 14780.5461 s
env0_first_0:                 episode reward: -92.1000,                 loss: 0.8707
env0_second_0:                 episode reward: 92.1000,                 loss: 0.7021
env1_first_0:                 episode reward: -73.8000,                 loss: nan
env1_second_0:                 episode reward: 73.8000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 280.75,                last time consumption/overall running time: 215.6057s / 14996.1519 s
env0_first_0:                 episode reward: -64.5500,                 loss: 0.8934
env0_second_0:                 episode reward: 64.5500,                 loss: 0.7106
env1_first_0:                 episode reward: -84.1500,                 loss: nan
env1_second_0:                 episode reward: 84.1500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 391.55,                last time consumption/overall running time: 301.3274s / 15297.4793 s
env0_first_0:                 episode reward: -65.9000,                 loss: 0.8408
env0_second_0:                 episode reward: 65.9000,                 loss: 0.7247
env1_first_0:                 episode reward: -66.1500,                 loss: nan
env1_second_0:                 episode reward: 66.1500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 288.55,                last time consumption/overall running time: 221.5428s / 15519.0221 s
env0_first_0:                 episode reward: -71.5500,                 loss: 0.9126
env0_second_0:                 episode reward: 71.5500,                 loss: 0.7807
env1_first_0:                 episode reward: -83.4500,                 loss: nan
env1_second_0:                 episode reward: 83.4500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 315.35,                last time consumption/overall running time: 242.2052s / 15761.2272 s
env0_first_0:                 episode reward: -68.0000,                 loss: 0.9801
env0_second_0:                 episode reward: 68.0000,                 loss: 0.8005
env1_first_0:                 episode reward: -75.4000,                 loss: nan
env1_second_0:                 episode reward: 75.4000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 294.6,                last time consumption/overall running time: 226.3619s / 15987.5892 s
env0_first_0:                 episode reward: -80.0000,                 loss: 1.0031
env0_second_0:                 episode reward: 80.0000,                 loss: 0.8265
env1_first_0:                 episode reward: -74.7500,                 loss: nan
env1_second_0:                 episode reward: 74.7500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 293.35,                last time consumption/overall running time: 225.3001s / 16212.8893 s
env0_first_0:                 episode reward: -82.6000,                 loss: 0.9444
env0_second_0:                 episode reward: 82.6000,                 loss: 0.7658
env1_first_0:                 episode reward: -82.7500,                 loss: nan
env1_second_0:                 episode reward: 82.7500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 262.8,                last time consumption/overall running time: 201.6177s / 16414.5070 s
env0_first_0:                 episode reward: -84.9000,                 loss: 0.9357
env0_second_0:                 episode reward: 84.9000,                 loss: 0.8160
env1_first_0:                 episode reward: -79.0000,                 loss: nan
env1_second_0:                 episode reward: 79.0000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 270.45,                last time consumption/overall running time: 208.4474s / 16622.9544 s
env0_first_0:                 episode reward: -81.8500,                 loss: 0.9285
env0_second_0:                 episode reward: 81.8500,                 loss: 0.8171
env1_first_0:                 episode reward: -79.6000,                 loss: nan
env1_second_0:                 episode reward: 79.6000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 331.0,                last time consumption/overall running time: 254.1197s / 16877.0741 s
env0_first_0:                 episode reward: -75.4000,                 loss: 0.9733
env0_second_0:                 episode reward: 75.4000,                 loss: 0.8236
env1_first_0:                 episode reward: -78.6500,                 loss: nan
env1_second_0:                 episode reward: 78.6500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1319.85,                last time consumption/overall running time: 1009.0613s / 17886.1354 s
env0_first_0:                 episode reward: -18.0500,                 loss: 0.7757
env0_second_0:                 episode reward: 18.0500,                 loss: 0.6504
env1_first_0:                 episode reward: -27.3500,                 loss: nan
env1_second_0:                 episode reward: 27.3500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 337.8,                last time consumption/overall running time: 259.9171s / 18146.0525 s
env0_first_0:                 episode reward: -76.7500,                 loss: 0.4815
env0_second_0:                 episode reward: 76.7500,                 loss: 0.4199
env1_first_0:                 episode reward: -73.8500,                 loss: nan
env1_second_0:                 episode reward: 73.8500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 332.0,                last time consumption/overall running time: 255.1174s / 18401.1699 s
env0_first_0:                 episode reward: -72.4500,                 loss: 0.4407
env0_second_0:                 episode reward: 72.4500,                 loss: 0.3842
env1_first_0:                 episode reward: -78.5500,                 loss: nan
env1_second_0:                 episode reward: 78.5500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 398.8,                last time consumption/overall running time: 306.0034s / 18707.1733 s
env0_first_0:                 episode reward: -75.5000,                 loss: 0.4084
env0_second_0:                 episode reward: 75.5000,                 loss: 0.3844
env1_first_0:                 episode reward: -67.3000,                 loss: nan
env1_second_0:                 episode reward: 67.3000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 607.25,                last time consumption/overall running time: 464.6346s / 19171.8079 s
env0_first_0:                 episode reward: -61.7000,                 loss: 0.4448
env0_second_0:                 episode reward: 61.7000,                 loss: 0.3967
env1_first_0:                 episode reward: -64.7000,                 loss: nan
env1_second_0:                 episode reward: 64.7000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 309.15,                last time consumption/overall running time: 237.8510s / 19409.6590 s
env0_first_0:                 episode reward: -78.4500,                 loss: 0.5762
env0_second_0:                 episode reward: 78.4500,                 loss: 0.4635
env1_first_0:                 episode reward: -77.9500,                 loss: nan
env1_second_0:                 episode reward: 77.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 335.8,                last time consumption/overall running time: 258.6569s / 19668.3158 s
env0_first_0:                 episode reward: -65.6500,                 loss: 0.7406
env0_second_0:                 episode reward: 65.6500,                 loss: 0.5587
env1_first_0:                 episode reward: -75.1000,                 loss: nan
env1_second_0:                 episode reward: 75.1000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 347.0,                last time consumption/overall running time: 267.2387s / 19935.5545 s
env0_first_0:                 episode reward: -71.0000,                 loss: 0.9227
env0_second_0:                 episode reward: 71.0000,                 loss: 0.6706
env1_first_0:                 episode reward: -74.2000,                 loss: nan
env1_second_0:                 episode reward: 74.2000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 318.0,                last time consumption/overall running time: 244.3391s / 20179.8936 s
env0_first_0:                 episode reward: -59.0000,                 loss: 0.9633
env0_second_0:                 episode reward: 59.0000,                 loss: 0.7104
env1_first_0:                 episode reward: -74.8000,                 loss: nan
env1_second_0:                 episode reward: 74.8000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 306.8,                last time consumption/overall running time: 236.3244s / 20416.2180 s
env0_first_0:                 episode reward: -77.4500,                 loss: 1.0478
env0_second_0:                 episode reward: 77.4500,                 loss: 0.7561
env1_first_0:                 episode reward: -65.5500,                 loss: nan
env1_second_0:                 episode reward: 65.5500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 286.7,                last time consumption/overall running time: 220.5170s / 20636.7350 s
env0_first_0:                 episode reward: -79.6500,                 loss: 1.0210
env0_second_0:                 episode reward: 79.6500,                 loss: 0.7325
env1_first_0:                 episode reward: -63.2000,                 loss: nan
env1_second_0:                 episode reward: 63.2000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 298.75,                last time consumption/overall running time: 229.1624s / 20865.8974 s
env0_first_0:                 episode reward: -81.7000,                 loss: 1.1198
env0_second_0:                 episode reward: 81.7000,                 loss: 0.7604
env1_first_0:                 episode reward: -70.2000,                 loss: nan
env1_second_0:                 episode reward: 70.2000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 338.75,                last time consumption/overall running time: 259.9150s / 21125.8124 s
env0_first_0:                 episode reward: -72.3500,                 loss: 1.1723
env0_second_0:                 episode reward: 72.3500,                 loss: 0.7976
env1_first_0:                 episode reward: -78.3500,                 loss: nan
env1_second_0:                 episode reward: 78.3500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 283.0,                last time consumption/overall running time: 217.2410s / 21343.0534 s
env0_first_0:                 episode reward: -69.4000,                 loss: 1.1615
env0_second_0:                 episode reward: 69.4000,                 loss: 0.8256
env1_first_0:                 episode reward: -83.7000,                 loss: nan
env1_second_0:                 episode reward: 83.7000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 260.8,                last time consumption/overall running time: 200.4347s / 21543.4881 s
env0_first_0:                 episode reward: -77.2000,                 loss: 1.2091
env0_second_0:                 episode reward: 77.2000,                 loss: 0.8511
env1_first_0:                 episode reward: -75.7500,                 loss: nan
env1_second_0:                 episode reward: 75.7500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 301.25,                last time consumption/overall running time: 230.9286s / 21774.4167 s
env0_first_0:                 episode reward: -75.9000,                 loss: 1.1532
env0_second_0:                 episode reward: 75.9000,                 loss: 0.8526
env1_first_0:                 episode reward: -82.5500,                 loss: nan
env1_second_0:                 episode reward: 82.5500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 348.15,                last time consumption/overall running time: 267.2645s / 22041.6812 s
env0_first_0:                 episode reward: -81.1000,                 loss: 1.0446
env0_second_0:                 episode reward: 81.1000,                 loss: 0.8000
env1_first_0:                 episode reward: -68.9000,                 loss: nan
env1_second_0:                 episode reward: 68.9000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 237.85,                last time consumption/overall running time: 182.7398s / 22224.4210 s
env0_first_0:                 episode reward: -74.3000,                 loss: 1.0430
env0_second_0:                 episode reward: 74.3000,                 loss: 0.7768
env1_first_0:                 episode reward: -98.9000,                 loss: nan
env1_second_0:                 episode reward: 98.9000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 265.9,                last time consumption/overall running time: 203.9742s / 22428.3952 s
env0_first_0:                 episode reward: -82.9500,                 loss: 1.0659
env0_second_0:                 episode reward: 82.9500,                 loss: 0.7795
env1_first_0:                 episode reward: -81.8000,                 loss: nan
env1_second_0:                 episode reward: 81.8000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 243.1,                last time consumption/overall running time: 186.5888s / 22614.9839 s
env0_first_0:                 episode reward: -83.0500,                 loss: 0.9612
env0_second_0:                 episode reward: 83.0500,                 loss: 0.7168
env1_first_0:                 episode reward: -88.1000,                 loss: nan
env1_second_0:                 episode reward: 88.1000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 225.4,                last time consumption/overall running time: 173.2105s / 22788.1945 s
env0_first_0:                 episode reward: -88.2500,                 loss: 0.8952
env0_second_0:                 episode reward: 88.2500,                 loss: 0.6869
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 238.95,                last time consumption/overall running time: 183.8448s / 22972.0392 s
env0_first_0:                 episode reward: -84.7000,                 loss: 0.8464
env0_second_0:                 episode reward: 84.7000,                 loss: 0.6467
env1_first_0:                 episode reward: -81.9500,                 loss: nan
env1_second_0:                 episode reward: 81.9500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 241.85,                last time consumption/overall running time: 186.3726s / 23158.4118 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.8448
env0_second_0:                 episode reward: 86.4000,                 loss: 0.6535
env1_first_0:                 episode reward: -83.8000,                 loss: nan
env1_second_0:                 episode reward: 83.8000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 248.35,                last time consumption/overall running time: 191.4001s / 23349.8120 s
env0_first_0:                 episode reward: -87.5500,                 loss: 0.8048
env0_second_0:                 episode reward: 87.5500,                 loss: 0.6342
env1_first_0:                 episode reward: -89.1000,                 loss: nan
env1_second_0:                 episode reward: 89.1000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 253.6,                last time consumption/overall running time: 195.6004s / 23545.4123 s
env0_first_0:                 episode reward: -78.6000,                 loss: 0.8105
env0_second_0:                 episode reward: 78.6000,                 loss: 0.6282
env1_first_0:                 episode reward: -83.3000,                 loss: nan
env1_second_0:                 episode reward: 83.3000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 418.35,                last time consumption/overall running time: 320.7471s / 23866.1594 s
env0_first_0:                 episode reward: -79.2000,                 loss: 0.8153
env0_second_0:                 episode reward: 79.2000,                 loss: 0.6229
env1_first_0:                 episode reward: -79.2500,                 loss: nan
env1_second_0:                 episode reward: 79.2500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 434.95,                last time consumption/overall running time: 333.7553s / 24199.9146 s
env0_first_0:                 episode reward: -49.6500,                 loss: 0.7310
env0_second_0:                 episode reward: 49.6500,                 loss: 0.5746
env1_first_0:                 episode reward: -73.4500,                 loss: nan
env1_second_0:                 episode reward: 73.4500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 380.95,                last time consumption/overall running time: 291.3935s / 24491.3081 s
env0_first_0:                 episode reward: -66.0500,                 loss: 0.7966
env0_second_0:                 episode reward: 66.0500,                 loss: 0.6120
env1_first_0:                 episode reward: -66.9000,                 loss: nan
env1_second_0:                 episode reward: 66.9000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 333.3,                last time consumption/overall running time: 254.4801s / 24745.7882 s
env0_first_0:                 episode reward: -76.7500,                 loss: 0.8513
env0_second_0:                 episode reward: 76.7500,                 loss: 0.6627
env1_first_0:                 episode reward: -61.2000,                 loss: nan
env1_second_0:                 episode reward: 61.2000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 320.6,                last time consumption/overall running time: 246.3376s / 24992.1259 s
env0_first_0:                 episode reward: -66.1500,                 loss: 0.9616
env0_second_0:                 episode reward: 66.1500,                 loss: 0.7077
env1_first_0:                 episode reward: -73.7000,                 loss: nan
env1_second_0:                 episode reward: 73.7000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 290.05,                last time consumption/overall running time: 221.9054s / 25214.0313 s
env0_first_0:                 episode reward: -75.6000,                 loss: 1.0275
env0_second_0:                 episode reward: 75.6000,                 loss: 0.6971
env1_first_0:                 episode reward: -76.5500,                 loss: nan
env1_second_0:                 episode reward: 76.5500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 304.5,                last time consumption/overall running time: 233.1869s / 25447.2181 s
env0_first_0:                 episode reward: -77.5500,                 loss: 1.0215
env0_second_0:                 episode reward: 77.5500,                 loss: 0.6827
env1_first_0:                 episode reward: -81.0500,                 loss: nan
env1_second_0:                 episode reward: 81.0500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 290.4,                last time consumption/overall running time: 223.3234s / 25670.5416 s
env0_first_0:                 episode reward: -69.4500,                 loss: 1.0409
env0_second_0:                 episode reward: 69.4500,                 loss: 0.6947
env1_first_0:                 episode reward: -80.1000,                 loss: nan
env1_second_0:                 episode reward: 80.1000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 403.55,                last time consumption/overall running time: 308.1564s / 25978.6979 s
env0_first_0:                 episode reward: -70.5000,                 loss: 1.1013
env0_second_0:                 episode reward: 70.5000,                 loss: 0.7639
env1_first_0:                 episode reward: -69.9000,                 loss: nan
env1_second_0:                 episode reward: 69.9000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 751.0,                last time consumption/overall running time: 574.8269s / 26553.5249 s
env0_first_0:                 episode reward: -35.5500,                 loss: 0.9923
env0_second_0:                 episode reward: 35.5500,                 loss: 0.7767
env1_first_0:                 episode reward: -55.2000,                 loss: nan
env1_second_0:                 episode reward: 55.2000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 630.85,                last time consumption/overall running time: 483.1721s / 27036.6969 s
env0_first_0:                 episode reward: -48.1500,                 loss: 0.8464
env0_second_0:                 episode reward: 48.1500,                 loss: 0.6773
env1_first_0:                 episode reward: -55.3000,                 loss: nan
env1_second_0:                 episode reward: 55.3000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 254.35,                last time consumption/overall running time: 195.1156s / 27231.8126 s
env0_first_0:                 episode reward: -91.3000,                 loss: 0.6984
env0_second_0:                 episode reward: 91.3000,                 loss: 0.6041
env1_first_0:                 episode reward: -89.8000,                 loss: nan
env1_second_0:                 episode reward: 89.8000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 242.55,                last time consumption/overall running time: 186.8315s / 27418.6441 s
env0_first_0:                 episode reward: -84.0500,                 loss: 0.6839
env0_second_0:                 episode reward: 84.0500,                 loss: 0.6062
env1_first_0:                 episode reward: -92.5500,                 loss: nan
env1_second_0:                 episode reward: 92.5500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 362.5,                last time consumption/overall running time: 277.0920s / 27695.7360 s
env0_first_0:                 episode reward: -64.3000,                 loss: 0.6784
env0_second_0:                 episode reward: 64.3000,                 loss: 0.5774
env1_first_0:                 episode reward: -73.8500,                 loss: nan
env1_second_0:                 episode reward: 73.8500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 358.4,                last time consumption/overall running time: 275.1362s / 27970.8723 s
env0_first_0:                 episode reward: -80.6000,                 loss: 0.6551
env0_second_0:                 episode reward: 80.6000,                 loss: 0.5710
env1_first_0:                 episode reward: -81.8000,                 loss: nan
env1_second_0:                 episode reward: 81.8000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 332.65,                last time consumption/overall running time: 253.8315s / 28224.7038 s
env0_first_0:                 episode reward: -73.0500,                 loss: 0.7973
env0_second_0:                 episode reward: 73.0500,                 loss: 0.7017
env1_first_0:                 episode reward: -75.9500,                 loss: nan
env1_second_0:                 episode reward: 75.9500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 306.45,                last time consumption/overall running time: 235.9754s / 28460.6791 s
env0_first_0:                 episode reward: -74.1500,                 loss: 0.9505
env0_second_0:                 episode reward: 74.1500,                 loss: 0.7742
env1_first_0:                 episode reward: -71.5500,                 loss: nan
env1_second_0:                 episode reward: 71.5500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 316.4,                last time consumption/overall running time: 243.1551s / 28703.8342 s
env0_first_0:                 episode reward: -80.3000,                 loss: 1.0252
env0_second_0:                 episode reward: 80.3000,                 loss: 0.7930
env1_first_0:                 episode reward: -77.3500,                 loss: nan
env1_second_0:                 episode reward: 77.3500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 293.2,                last time consumption/overall running time: 225.2860s / 28929.1202 s
env0_first_0:                 episode reward: -78.6500,                 loss: 1.0555
env0_second_0:                 episode reward: 78.6500,                 loss: 0.8179
env1_first_0:                 episode reward: -80.8000,                 loss: nan
env1_second_0:                 episode reward: 80.8000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 284.95,                last time consumption/overall running time: 218.9025s / 29148.0227 s
env0_first_0:                 episode reward: -89.9000,                 loss: 1.0716
env0_second_0:                 episode reward: 89.9000,                 loss: 0.8313
env1_first_0:                 episode reward: -71.4000,                 loss: nan
env1_second_0:                 episode reward: 71.4000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 255.7,                last time consumption/overall running time: 197.0188s / 29345.0415 s
env0_first_0:                 episode reward: -71.6500,                 loss: 1.0016
env0_second_0:                 episode reward: 71.6500,                 loss: 0.8538
env1_first_0:                 episode reward: -91.6000,                 loss: nan
env1_second_0:                 episode reward: 91.6000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 299.4,                last time consumption/overall running time: 229.9279s / 29574.9694 s
env0_first_0:                 episode reward: -73.0500,                 loss: 1.0023
env0_second_0:                 episode reward: 73.0500,                 loss: 0.8406
env1_first_0:                 episode reward: -87.5500,                 loss: nan
env1_second_0:                 episode reward: 87.5500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 266.9,                last time consumption/overall running time: 204.8673s / 29779.8367 s
env0_first_0:                 episode reward: -85.0500,                 loss: 1.0274
env0_second_0:                 episode reward: 85.0500,                 loss: 0.8124
env1_first_0:                 episode reward: -85.6500,                 loss: nan
env1_second_0:                 episode reward: 85.6500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 262.55,                last time consumption/overall running time: 201.0452s / 29980.8819 s
env0_first_0:                 episode reward: -84.1500,                 loss: 0.9969
env0_second_0:                 episode reward: 84.1500,                 loss: 0.7467
env1_first_0:                 episode reward: -76.5500,                 loss: nan
env1_second_0:                 episode reward: 76.5500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 255.5,                last time consumption/overall running time: 197.4158s / 30178.2978 s
env0_first_0:                 episode reward: -87.4500,                 loss: 0.9378
env0_second_0:                 episode reward: 87.4500,                 loss: 0.7050
env1_first_0:                 episode reward: -79.0500,                 loss: nan
env1_second_0:                 episode reward: 79.0500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 263.3,                last time consumption/overall running time: 202.4469s / 30380.7447 s
env0_first_0:                 episode reward: -91.5500,                 loss: 0.9026
env0_second_0:                 episode reward: 91.5500,                 loss: 0.6694
env1_first_0:                 episode reward: -82.0000,                 loss: nan
env1_second_0:                 episode reward: 82.0000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 263.05,                last time consumption/overall running time: 201.7436s / 30582.4883 s
env0_first_0:                 episode reward: -78.8000,                 loss: 0.8760
env0_second_0:                 episode reward: 78.8000,                 loss: 0.6607
env1_first_0:                 episode reward: -86.5500,                 loss: nan
env1_second_0:                 episode reward: 86.5500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 264.8,                last time consumption/overall running time: 202.4905s / 30784.9788 s
env0_first_0:                 episode reward: -77.7500,                 loss: 0.8686
env0_second_0:                 episode reward: 77.7500,                 loss: 0.6164
env1_first_0:                 episode reward: -86.4500,                 loss: nan
env1_second_0:                 episode reward: 86.4500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 241.05,                last time consumption/overall running time: 184.7922s / 30969.7710 s
env0_first_0:                 episode reward: -83.0500,                 loss: 0.8843
env0_second_0:                 episode reward: 83.0500,                 loss: 0.6343
env1_first_0:                 episode reward: -81.6500,                 loss: nan
env1_second_0:                 episode reward: 81.6500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 333.25,                last time consumption/overall running time: 256.1725s / 31225.9434 s
env0_first_0:                 episode reward: -72.7000,                 loss: 0.8628
env0_second_0:                 episode reward: 72.7000,                 loss: 0.6419
env1_first_0:                 episode reward: -79.8500,                 loss: nan
env1_second_0:                 episode reward: 79.8500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 317.65,                last time consumption/overall running time: 243.8580s / 31469.8014 s
env0_first_0:                 episode reward: -82.6000,                 loss: 0.8731
env0_second_0:                 episode reward: 82.6000,                 loss: 0.6915
env1_first_0:                 episode reward: -82.2500,                 loss: nan
env1_second_0:                 episode reward: 82.2500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 288.25,                last time consumption/overall running time: 220.7012s / 31690.5026 s
env0_first_0:                 episode reward: -84.7500,                 loss: 0.9511
env0_second_0:                 episode reward: 84.7500,                 loss: 0.7082
env1_first_0:                 episode reward: -85.3000,                 loss: nan
env1_second_0:                 episode reward: 85.3000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 252.65,                last time consumption/overall running time: 193.7398s / 31884.2424 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.9793
env0_second_0:                 episode reward: 88.5500,                 loss: 0.7246
env1_first_0:                 episode reward: -92.0000,                 loss: nan
env1_second_0:                 episode reward: 92.0000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 239.6,                last time consumption/overall running time: 184.4325s / 32068.6749 s
env0_first_0:                 episode reward: -91.8000,                 loss: 0.9236
env0_second_0:                 episode reward: 91.8000,                 loss: 0.7260
env1_first_0:                 episode reward: -75.6500,                 loss: nan
env1_second_0:                 episode reward: 75.6500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 252.45,                last time consumption/overall running time: 194.6410s / 32263.3159 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.9337
env0_second_0:                 episode reward: 85.6500,                 loss: 0.7181
env1_first_0:                 episode reward: -75.4500,                 loss: nan
env1_second_0:                 episode reward: 75.4500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 275.75,                last time consumption/overall running time: 211.4626s / 32474.7785 s
env0_first_0:                 episode reward: -78.7500,                 loss: 0.9529
env0_second_0:                 episode reward: 78.7500,                 loss: 0.7245
env1_first_0:                 episode reward: -79.7000,                 loss: nan
env1_second_0:                 episode reward: 79.7000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 322.3,                last time consumption/overall running time: 248.8955s / 32723.6740 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.9434
env0_second_0:                 episode reward: 85.6500,                 loss: 0.7531
env1_first_0:                 episode reward: -83.2000,                 loss: nan
env1_second_0:                 episode reward: 83.2000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 418.3,                last time consumption/overall running time: 320.6225s / 33044.2965 s
env0_first_0:                 episode reward: -73.2500,                 loss: 0.9364
env0_second_0:                 episode reward: 73.2500,                 loss: 0.7119
env1_first_0:                 episode reward: -79.0000,                 loss: nan
env1_second_0:                 episode reward: 79.0000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 293.95,                last time consumption/overall running time: 226.6139s / 33270.9104 s
env0_first_0:                 episode reward: -71.5000,                 loss: 0.9093
env0_second_0:                 episode reward: 71.5000,                 loss: 0.6764
env1_first_0:                 episode reward: -62.2000,                 loss: nan
env1_second_0:                 episode reward: 62.2000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 362.9,                last time consumption/overall running time: 278.6213s / 33549.5317 s
env0_first_0:                 episode reward: -69.2000,                 loss: 0.8825
env0_second_0:                 episode reward: 69.2000,                 loss: 0.6395
env1_first_0:                 episode reward: -77.9000,                 loss: nan
env1_second_0:                 episode reward: 77.9000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 294.15,                last time consumption/overall running time: 226.7015s / 33776.2332 s
env0_first_0:                 episode reward: -79.6500,                 loss: 0.8630
env0_second_0:                 episode reward: 79.6500,                 loss: 0.6028
env1_first_0:                 episode reward: -75.8000,                 loss: nan
env1_second_0:                 episode reward: 75.8000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 283.9,                last time consumption/overall running time: 218.4483s / 33994.6815 s
env0_first_0:                 episode reward: -71.7500,                 loss: 0.8697
env0_second_0:                 episode reward: 71.7500,                 loss: 0.6335
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 287.8,                last time consumption/overall running time: 220.6609s / 34215.3424 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.8528
env0_second_0:                 episode reward: 85.4000,                 loss: 0.6445
env1_first_0:                 episode reward: -71.5000,                 loss: nan
env1_second_0:                 episode reward: 71.5000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 356.35,                last time consumption/overall running time: 274.4235s / 34489.7659 s
env0_first_0:                 episode reward: -83.7500,                 loss: 0.8039
env0_second_0:                 episode reward: 83.7500,                 loss: 0.6276
env1_first_0:                 episode reward: -69.0000,                 loss: nan
env1_second_0:                 episode reward: 69.0000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 448.15,                last time consumption/overall running time: 343.9275s / 34833.6935 s
env0_first_0:                 episode reward: -66.2000,                 loss: 0.8395
env0_second_0:                 episode reward: 66.2000,                 loss: 0.6547
env1_first_0:                 episode reward: -65.9500,                 loss: nan
env1_second_0:                 episode reward: 65.9500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 422.7,                last time consumption/overall running time: 322.9555s / 35156.6490 s
env0_first_0:                 episode reward: -82.8500,                 loss: 0.9080
env0_second_0:                 episode reward: 82.8500,                 loss: 0.6702
env1_first_0:                 episode reward: -65.0000,                 loss: nan
env1_second_0:                 episode reward: 65.0000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 336.55,                last time consumption/overall running time: 258.1214s / 35414.7704 s
env0_first_0:                 episode reward: -75.9000,                 loss: 0.8232
env0_second_0:                 episode reward: 75.9000,                 loss: 0.6285
env1_first_0:                 episode reward: -81.2500,                 loss: nan
env1_second_0:                 episode reward: 81.2500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 338.5,                last time consumption/overall running time: 260.5905s / 35675.3608 s
env0_first_0:                 episode reward: -81.3000,                 loss: 0.7807
env0_second_0:                 episode reward: 81.3000,                 loss: 0.6088
env1_first_0:                 episode reward: -83.5500,                 loss: nan
env1_second_0:                 episode reward: 83.5500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 243.95,                last time consumption/overall running time: 187.1085s / 35862.4694 s
env0_first_0:                 episode reward: -79.8000,                 loss: 0.7477
env0_second_0:                 episode reward: 79.8000,                 loss: 0.6098
env1_first_0:                 episode reward: -86.7500,                 loss: nan
env1_second_0:                 episode reward: 86.7500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 287.45,                last time consumption/overall running time: 220.5533s / 36083.0226 s
env0_first_0:                 episode reward: -86.7500,                 loss: 0.7271
env0_second_0:                 episode reward: 86.7500,                 loss: 0.6292
env1_first_0:                 episode reward: -80.3500,                 loss: nan
env1_second_0:                 episode reward: 80.3500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 263.3,                last time consumption/overall running time: 202.8306s / 36285.8532 s
env0_first_0:                 episode reward: -86.9000,                 loss: 0.7994
env0_second_0:                 episode reward: 86.9000,                 loss: 0.6642
env1_first_0:                 episode reward: -73.7500,                 loss: nan
env1_second_0:                 episode reward: 73.7500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 321.0,                last time consumption/overall running time: 247.1648s / 36533.0181 s
env0_first_0:                 episode reward: -74.8000,                 loss: 0.8896
env0_second_0:                 episode reward: 74.8000,                 loss: 0.7134
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 285.2,                last time consumption/overall running time: 220.4287s / 36753.4468 s
env0_first_0:                 episode reward: -80.4000,                 loss: 0.9526
env0_second_0:                 episode reward: 80.4000,                 loss: 0.7498
env1_first_0:                 episode reward: -77.1000,                 loss: nan
env1_second_0:                 episode reward: 77.1000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 269.2,                last time consumption/overall running time: 207.3303s / 36960.7771 s
env0_first_0:                 episode reward: -79.2000,                 loss: 1.0210
env0_second_0:                 episode reward: 79.2000,                 loss: 0.7819
env1_first_0:                 episode reward: -76.4500,                 loss: nan
env1_second_0:                 episode reward: 76.4500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 287.8,                last time consumption/overall running time: 232.4754s / 37193.2524 s
env0_first_0:                 episode reward: -81.8500,                 loss: 1.0693
env0_second_0:                 episode reward: 81.8500,                 loss: 0.9014
env1_first_0:                 episode reward: -74.5500,                 loss: nan
env1_second_0:                 episode reward: 74.5500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 312.6,                last time consumption/overall running time: 271.9089s / 37465.1613 s
env0_first_0:                 episode reward: -75.2000,                 loss: 1.1715
env0_second_0:                 episode reward: 75.2000,                 loss: 0.9938
env1_first_0:                 episode reward: -82.4000,                 loss: nan
env1_second_0:                 episode reward: 82.4000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 302.2,                last time consumption/overall running time: 276.2952s / 37741.4565 s
env0_first_0:                 episode reward: -71.3500,                 loss: 1.2667
env0_second_0:                 episode reward: 71.3500,                 loss: 1.0453
env1_first_0:                 episode reward: -79.0500,                 loss: nan
env1_second_0:                 episode reward: 79.0500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 368.1,                last time consumption/overall running time: 339.2998s / 38080.7563 s
env0_first_0:                 episode reward: -71.6500,                 loss: 1.2759
env0_second_0:                 episode reward: 71.6500,                 loss: 1.1185
env1_first_0:                 episode reward: -65.0500,                 loss: nan
env1_second_0:                 episode reward: 65.0500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 302.35,                last time consumption/overall running time: 278.0772s / 38358.8335 s
env0_first_0:                 episode reward: -85.2000,                 loss: 1.2345
env0_second_0:                 episode reward: 85.2000,                 loss: 1.0494
env1_first_0:                 episode reward: -70.3000,                 loss: nan
env1_second_0:                 episode reward: 70.3000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 503.75,                last time consumption/overall running time: 460.9495s / 38819.7830 s
env0_first_0:                 episode reward: -57.5500,                 loss: 1.1609
env0_second_0:                 episode reward: 57.5500,                 loss: 0.9473
env1_first_0:                 episode reward: -75.5500,                 loss: nan
env1_second_0:                 episode reward: 75.5500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 307.85,                last time consumption/overall running time: 281.1484s / 39100.9314 s
env0_first_0:                 episode reward: -76.5000,                 loss: 1.0988
env0_second_0:                 episode reward: 76.5000,                 loss: 0.8507
env1_first_0:                 episode reward: -85.2000,                 loss: nan
env1_second_0:                 episode reward: 85.2000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 298.7,                last time consumption/overall running time: 272.8529s / 39373.7844 s
env0_first_0:                 episode reward: -78.8000,                 loss: 1.0712
env0_second_0:                 episode reward: 78.8000,                 loss: 0.8323
env1_first_0:                 episode reward: -79.1500,                 loss: nan
env1_second_0:                 episode reward: 79.1500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 525.1,                last time consumption/overall running time: 478.3845s / 39852.1689 s
env0_first_0:                 episode reward: -72.8000,                 loss: 0.9750
env0_second_0:                 episode reward: 72.8000,                 loss: 0.8098
env1_first_0:                 episode reward: -72.9500,                 loss: nan
env1_second_0:                 episode reward: 72.9500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 455.4,                last time consumption/overall running time: 414.7640s / 40266.9329 s
env0_first_0:                 episode reward: -67.4000,                 loss: 0.9477
env0_second_0:                 episode reward: 67.4000,                 loss: 0.8141
env1_first_0:                 episode reward: -72.0000,                 loss: nan
env1_second_0:                 episode reward: 72.0000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 338.8,                last time consumption/overall running time: 309.7614s / 40576.6943 s
env0_first_0:                 episode reward: -71.1000,                 loss: 0.9382
env0_second_0:                 episode reward: 71.1000,                 loss: 0.8370
env1_first_0:                 episode reward: -77.8500,                 loss: nan
env1_second_0:                 episode reward: 77.8500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 324.25,                last time consumption/overall running time: 296.4888s / 40873.1831 s
env0_first_0:                 episode reward: -81.4500,                 loss: 0.9517
env0_second_0:                 episode reward: 81.4500,                 loss: 0.8584
env1_first_0:                 episode reward: -75.1000,                 loss: nan
env1_second_0:                 episode reward: 75.1000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 309.95,                last time consumption/overall running time: 284.1078s / 41157.2909 s
env0_first_0:                 episode reward: -72.1000,                 loss: 1.0095
env0_second_0:                 episode reward: 72.1000,                 loss: 0.8843
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 300.3,                last time consumption/overall running time: 274.3815s / 41431.6724 s
env0_first_0:                 episode reward: -74.7500,                 loss: 1.0818
env0_second_0:                 episode reward: 74.7500,                 loss: 0.9690
env1_first_0:                 episode reward: -67.0500,                 loss: nan
env1_second_0:                 episode reward: 67.0500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 274.7,                last time consumption/overall running time: 251.2282s / 41682.9005 s
env0_first_0:                 episode reward: -78.4500,                 loss: 1.1751
env0_second_0:                 episode reward: 78.4500,                 loss: 0.9813
env1_first_0:                 episode reward: -83.0000,                 loss: nan
env1_second_0:                 episode reward: 83.0000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 336.45,                last time consumption/overall running time: 306.9705s / 41989.8711 s
env0_first_0:                 episode reward: -78.9500,                 loss: 1.1104
env0_second_0:                 episode reward: 78.9500,                 loss: 1.0247
env1_first_0:                 episode reward: -71.1500,                 loss: nan
env1_second_0:                 episode reward: 71.1500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 257.35,                last time consumption/overall running time: 235.2528s / 42225.1239 s
env0_first_0:                 episode reward: -88.5500,                 loss: 1.0542
env0_second_0:                 episode reward: 88.5500,                 loss: 0.9807
env1_first_0:                 episode reward: -77.4500,                 loss: nan
env1_second_0:                 episode reward: 77.4500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 313.15,                last time consumption/overall running time: 285.9714s / 42511.0954 s
env0_first_0:                 episode reward: -85.3000,                 loss: 1.0686
env0_second_0:                 episode reward: 85.3000,                 loss: 0.9919
env1_first_0:                 episode reward: -84.9500,                 loss: nan
env1_second_0:                 episode reward: 84.9500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 311.0,                last time consumption/overall running time: 283.7650s / 42794.8604 s
env0_first_0:                 episode reward: -72.1000,                 loss: 1.0714
env0_second_0:                 episode reward: 72.1000,                 loss: 1.0014
env1_first_0:                 episode reward: -92.5500,                 loss: nan
env1_second_0:                 episode reward: 92.5500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 365.25,                last time consumption/overall running time: 334.1586s / 43129.0190 s
env0_first_0:                 episode reward: -71.4000,                 loss: 0.9959
env0_second_0:                 episode reward: 71.4000,                 loss: 0.9782
env1_first_0:                 episode reward: -67.0000,                 loss: nan
env1_second_0:                 episode reward: 67.0000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 265.85,                last time consumption/overall running time: 243.3447s / 43372.3637 s
env0_first_0:                 episode reward: -78.5500,                 loss: 0.9251
env0_second_0:                 episode reward: 78.5500,                 loss: 0.9343
env1_first_0:                 episode reward: -88.0000,                 loss: nan
env1_second_0:                 episode reward: 88.0000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 295.95,                last time consumption/overall running time: 270.7301s / 43643.0938 s
env0_first_0:                 episode reward: -78.0500,                 loss: 0.9356
env0_second_0:                 episode reward: 78.0500,                 loss: 0.9179
env1_first_0:                 episode reward: -76.7500,                 loss: nan
env1_second_0:                 episode reward: 76.7500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 312.55,                last time consumption/overall running time: 285.5316s / 43928.6254 s
env0_first_0:                 episode reward: -68.0500,                 loss: 0.9325
env0_second_0:                 episode reward: 68.0500,                 loss: 0.8573
env1_first_0:                 episode reward: -86.5500,                 loss: nan
env1_second_0:                 episode reward: 86.5500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 345.15,                last time consumption/overall running time: 315.2504s / 44243.8758 s
env0_first_0:                 episode reward: -77.8000,                 loss: 0.9788
env0_second_0:                 episode reward: 77.8000,                 loss: 0.9030
env1_first_0:                 episode reward: -67.7500,                 loss: nan
env1_second_0:                 episode reward: 67.7500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 316.85,                last time consumption/overall running time: 289.3728s / 44533.2486 s
env0_first_0:                 episode reward: -70.8000,                 loss: 1.0524
env0_second_0:                 episode reward: 70.8000,                 loss: 0.9554
env1_first_0:                 episode reward: -77.7000,                 loss: nan
env1_second_0:                 episode reward: 77.7000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 374.0,                last time consumption/overall running time: 341.2776s / 44874.5262 s
env0_first_0:                 episode reward: -56.8500,                 loss: 1.0668
env0_second_0:                 episode reward: 56.8500,                 loss: 0.9030
env1_first_0:                 episode reward: -76.5500,                 loss: nan
env1_second_0:                 episode reward: 76.5500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 328.6,                last time consumption/overall running time: 300.2504s / 45174.7766 s
env0_first_0:                 episode reward: -76.2000,                 loss: 1.0678
env0_second_0:                 episode reward: 76.2000,                 loss: 0.9318
env1_first_0:                 episode reward: -75.0000,                 loss: nan
env1_second_0:                 episode reward: 75.0000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 408.05,                last time consumption/overall running time: 370.7759s / 45545.5525 s
env0_first_0:                 episode reward: -73.2500,                 loss: 1.1303
env0_second_0:                 episode reward: 73.2500,                 loss: 0.9820
env1_first_0:                 episode reward: -77.6500,                 loss: nan
env1_second_0:                 episode reward: 77.6500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 323.05,                last time consumption/overall running time: 294.5375s / 45840.0900 s
env0_first_0:                 episode reward: -78.2000,                 loss: 1.1861
env0_second_0:                 episode reward: 78.2000,                 loss: 0.9881
env1_first_0:                 episode reward: -60.4000,                 loss: nan
env1_second_0:                 episode reward: 60.4000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 470.45,                last time consumption/overall running time: 429.0801s / 46269.1701 s
env0_first_0:                 episode reward: -71.6500,                 loss: 1.1983
env0_second_0:                 episode reward: 71.6500,                 loss: 0.9823
env1_first_0:                 episode reward: -57.0500,                 loss: nan
env1_second_0:                 episode reward: 57.0500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 377.8,                last time consumption/overall running time: 344.2595s / 46613.4296 s
env0_first_0:                 episode reward: -70.7500,                 loss: 1.1173
env0_second_0:                 episode reward: 70.7500,                 loss: 0.9332
env1_first_0:                 episode reward: -59.7000,                 loss: nan
env1_second_0:                 episode reward: 59.7000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 327.1,                last time consumption/overall running time: 298.0345s / 46911.4641 s
env0_first_0:                 episode reward: -65.8500,                 loss: 1.1230
env0_second_0:                 episode reward: 65.8500,                 loss: 0.9154
env1_first_0:                 episode reward: -67.0000,                 loss: nan
env1_second_0:                 episode reward: 67.0000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 388.75,                last time consumption/overall running time: 353.7782s / 47265.2423 s
env0_first_0:                 episode reward: -64.0500,                 loss: 1.1248
env0_second_0:                 episode reward: 64.0500,                 loss: 0.9203
env1_first_0:                 episode reward: -71.7500,                 loss: nan
env1_second_0:                 episode reward: 71.7500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 265.55,                last time consumption/overall running time: 242.1221s / 47507.3644 s
env0_first_0:                 episode reward: -75.8000,                 loss: 1.0359
env0_second_0:                 episode reward: 75.8000,                 loss: 0.8888
env1_first_0:                 episode reward: -83.4000,                 loss: nan
env1_second_0:                 episode reward: 83.4000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 282.9,                last time consumption/overall running time: 258.1570s / 47765.5214 s
env0_first_0:                 episode reward: -79.3500,                 loss: 1.0030
env0_second_0:                 episode reward: 79.3500,                 loss: 0.8795
env1_first_0:                 episode reward: -72.1500,                 loss: nan
env1_second_0:                 episode reward: 72.1500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 282.95,                last time consumption/overall running time: 257.8703s / 48023.3916 s
env0_first_0:                 episode reward: -92.4000,                 loss: 1.0119
env0_second_0:                 episode reward: 92.4000,                 loss: 0.8762
env1_first_0:                 episode reward: -75.8500,                 loss: nan
env1_second_0:                 episode reward: 75.8500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 251.75,                last time consumption/overall running time: 229.8983s / 48253.2899 s
env0_first_0:                 episode reward: -86.2000,                 loss: 0.9791
env0_second_0:                 episode reward: 86.2000,                 loss: 0.8398
env1_first_0:                 episode reward: -76.4500,                 loss: nan
env1_second_0:                 episode reward: 76.4500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 317.15,                last time consumption/overall running time: 288.9899s / 48542.2798 s
env0_first_0:                 episode reward: -75.9500,                 loss: 0.9719
env0_second_0:                 episode reward: 75.9500,                 loss: 0.8137
env1_first_0:                 episode reward: -68.2000,                 loss: nan
env1_second_0:                 episode reward: 68.2000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 302.6,                last time consumption/overall running time: 276.4321s / 48818.7119 s
env0_first_0:                 episode reward: -66.0500,                 loss: 1.0343
env0_second_0:                 episode reward: 66.0500,                 loss: 0.8445
env1_first_0:                 episode reward: -75.7000,                 loss: nan
env1_second_0:                 episode reward: 75.7000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 382.8,                last time consumption/overall running time: 349.5147s / 49168.2266 s
env0_first_0:                 episode reward: -65.3500,                 loss: 1.0729
env0_second_0:                 episode reward: 65.3500,                 loss: 0.8452
env1_first_0:                 episode reward: -47.5500,                 loss: nan
env1_second_0:                 episode reward: 47.5500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 478.25,                last time consumption/overall running time: 435.5091s / 49603.7357 s
env0_first_0:                 episode reward: -60.5500,                 loss: 1.0663
env0_second_0:                 episode reward: 60.5500,                 loss: 0.8635
env1_first_0:                 episode reward: -60.1000,                 loss: nan
env1_second_0:                 episode reward: 60.1000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 297.05,                last time consumption/overall running time: 271.3901s / 49875.1259 s
env0_first_0:                 episode reward: -79.3500,                 loss: 1.0791
env0_second_0:                 episode reward: 79.3500,                 loss: 0.9052
env1_first_0:                 episode reward: -76.7000,                 loss: nan
env1_second_0:                 episode reward: 76.7000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 274.45,                last time consumption/overall running time: 250.6154s / 50125.7413 s
env0_first_0:                 episode reward: -89.0500,                 loss: 1.0822
env0_second_0:                 episode reward: 89.0500,                 loss: 0.8953
env1_first_0:                 episode reward: -75.3000,                 loss: nan
env1_second_0:                 episode reward: 75.3000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 284.1,                last time consumption/overall running time: 259.9213s / 50385.6625 s
env0_first_0:                 episode reward: -87.3000,                 loss: 1.0630
env0_second_0:                 episode reward: 87.3000,                 loss: 0.9166
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 308.7,                last time consumption/overall running time: 280.9364s / 50666.5990 s
env0_first_0:                 episode reward: -77.1000,                 loss: 1.0369
env0_second_0:                 episode reward: 77.1000,                 loss: 0.8986
env1_first_0:                 episode reward: -78.7500,                 loss: nan
env1_second_0:                 episode reward: 78.7500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 328.4,                last time consumption/overall running time: 299.8628s / 50966.4618 s
env0_first_0:                 episode reward: -63.2000,                 loss: 0.9920
env0_second_0:                 episode reward: 63.2000,                 loss: 0.9302
env1_first_0:                 episode reward: -84.7500,                 loss: nan
env1_second_0:                 episode reward: 84.7500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 372.35,                last time consumption/overall running time: 338.8550s / 51305.3167 s
env0_first_0:                 episode reward: -71.9000,                 loss: 1.0251
env0_second_0:                 episode reward: 71.9000,                 loss: 0.8662
env1_first_0:                 episode reward: -64.8500,                 loss: nan
env1_second_0:                 episode reward: 64.8500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 448.9,                last time consumption/overall running time: 408.9324s / 51714.2491 s
env0_first_0:                 episode reward: -59.7500,                 loss: 0.9979
env0_second_0:                 episode reward: 59.7500,                 loss: 0.8645
env1_first_0:                 episode reward: -57.5500,                 loss: nan
env1_second_0:                 episode reward: 57.5500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 407.75,                last time consumption/overall running time: 371.4790s / 52085.7282 s
env0_first_0:                 episode reward: -82.5500,                 loss: 0.9063
env0_second_0:                 episode reward: 82.5500,                 loss: 0.7451
env1_first_0:                 episode reward: -72.5000,                 loss: nan
env1_second_0:                 episode reward: 72.5000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 604.4,                last time consumption/overall running time: 550.1068s / 52635.8350 s
env0_first_0:                 episode reward: -56.2500,                 loss: 0.7916
env0_second_0:                 episode reward: 56.2500,                 loss: 0.7072
env1_first_0:                 episode reward: -66.1000,                 loss: nan
env1_second_0:                 episode reward: 66.1000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1056.25,                last time consumption/overall running time: 962.4554s / 53598.2904 s
env0_first_0:                 episode reward: -24.9500,                 loss: 0.5811
env0_second_0:                 episode reward: 24.9500,                 loss: 0.5699
env1_first_0:                 episode reward: -46.7000,                 loss: nan
env1_second_0:                 episode reward: 46.7000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 838.35,                last time consumption/overall running time: 763.1907s / 54361.4811 s
env0_first_0:                 episode reward: -52.7000,                 loss: 0.4625
env0_second_0:                 episode reward: 52.7000,                 loss: 0.4988
env1_first_0:                 episode reward: -34.6500,                 loss: nan
env1_second_0:                 episode reward: 34.6500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 691.95,                last time consumption/overall running time: 631.2790s / 54992.7600 s
env0_first_0:                 episode reward: -58.5500,                 loss: 0.6388
env0_second_0:                 episode reward: 58.5500,                 loss: 0.6783
env1_first_0:                 episode reward: -48.0000,                 loss: nan
env1_second_0:                 episode reward: 48.0000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 576.15,                last time consumption/overall running time: 524.4084s / 55517.1685 s
env0_first_0:                 episode reward: -62.6500,                 loss: 0.8135
env0_second_0:                 episode reward: 62.6500,                 loss: 0.8238
env1_first_0:                 episode reward: -73.7500,                 loss: nan
env1_second_0:                 episode reward: 73.7500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 446.0,                last time consumption/overall running time: 406.4053s / 55923.5738 s
env0_first_0:                 episode reward: -71.0500,                 loss: 1.0202
env0_second_0:                 episode reward: 71.0500,                 loss: 0.9400
env1_first_0:                 episode reward: -58.1500,                 loss: nan
env1_second_0:                 episode reward: 58.1500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 342.4,                last time consumption/overall running time: 312.4753s / 56236.0491 s
env0_first_0:                 episode reward: -68.8000,                 loss: 1.1862
env0_second_0:                 episode reward: 68.8000,                 loss: 1.0107
env1_first_0:                 episode reward: -65.0000,                 loss: nan
env1_second_0:                 episode reward: 65.0000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 293.5,                last time consumption/overall running time: 267.5357s / 56503.5848 s
env0_first_0:                 episode reward: -84.4000,                 loss: 1.2701
env0_second_0:                 episode reward: 84.4000,                 loss: 1.0236
env1_first_0:                 episode reward: -69.1000,                 loss: nan
env1_second_0:                 episode reward: 69.1000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 281.8,                last time consumption/overall running time: 257.3912s / 56760.9760 s
env0_first_0:                 episode reward: -72.1000,                 loss: 1.1965
env0_second_0:                 episode reward: 72.1000,                 loss: 0.9668
env1_first_0:                 episode reward: -85.0000,                 loss: nan
env1_second_0:                 episode reward: 85.0000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 317.4,                last time consumption/overall running time: 289.6883s / 57050.6644 s
env0_first_0:                 episode reward: -68.6000,                 loss: 1.1907
env0_second_0:                 episode reward: 68.6000,                 loss: 0.9516
env1_first_0:                 episode reward: -73.0500,                 loss: nan
env1_second_0:                 episode reward: 73.0500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 315.2,                last time consumption/overall running time: 287.8367s / 57338.5011 s
env0_first_0:                 episode reward: -79.1500,                 loss: 1.2639
env0_second_0:                 episode reward: 79.1500,                 loss: 0.9857
env1_first_0:                 episode reward: -78.2500,                 loss: nan
env1_second_0:                 episode reward: 78.2500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 290.75,                last time consumption/overall running time: 265.1803s / 57603.6814 s
env0_first_0:                 episode reward: -82.8500,                 loss: 1.2870
env0_second_0:                 episode reward: 82.8500,                 loss: 0.9637
env1_first_0:                 episode reward: -78.4500,                 loss: nan
env1_second_0:                 episode reward: 78.4500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 264.75,                last time consumption/overall running time: 241.4109s / 57845.0923 s
env0_first_0:                 episode reward: -77.3000,                 loss: 1.2952
env0_second_0:                 episode reward: 77.3000,                 loss: 0.9498
env1_first_0:                 episode reward: -80.4500,                 loss: nan
env1_second_0:                 episode reward: 80.4500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 270.15,                last time consumption/overall running time: 246.1701s / 58091.2624 s
env0_first_0:                 episode reward: -70.1000,                 loss: 1.3346
env0_second_0:                 episode reward: 70.1000,                 loss: 0.9720
env1_first_0:                 episode reward: -86.3500,                 loss: nan
env1_second_0:                 episode reward: 86.3500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 326.0,                last time consumption/overall running time: 297.0552s / 58388.3176 s
env0_first_0:                 episode reward: -61.5500,                 loss: 1.3028
env0_second_0:                 episode reward: 61.5500,                 loss: 0.9945
env1_first_0:                 episode reward: -89.0000,                 loss: nan
env1_second_0:                 episode reward: 89.0000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 425.1,                last time consumption/overall running time: 387.4431s / 58775.7607 s
env0_first_0:                 episode reward: -78.8500,                 loss: 1.2760
env0_second_0:                 episode reward: 78.8500,                 loss: 0.9430
env1_first_0:                 episode reward: -79.7000,                 loss: nan
env1_second_0:                 episode reward: 79.7000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 272.85,                last time consumption/overall running time: 249.2553s / 59025.0159 s
env0_first_0:                 episode reward: -75.0500,                 loss: 1.1919
env0_second_0:                 episode reward: 75.0500,                 loss: 0.9207
env1_first_0:                 episode reward: -87.9500,                 loss: nan
env1_second_0:                 episode reward: 87.9500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 383.5,                last time consumption/overall running time: 350.8642s / 59375.8801 s
env0_first_0:                 episode reward: -86.6000,                 loss: 1.1108
env0_second_0:                 episode reward: 86.6000,                 loss: 0.8766
env1_first_0:                 episode reward: -55.9500,                 loss: nan
env1_second_0:                 episode reward: 55.9500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 291.9,                last time consumption/overall running time: 266.6969s / 59642.5770 s
env0_first_0:                 episode reward: -75.6000,                 loss: 1.0833
env0_second_0:                 episode reward: 75.6000,                 loss: 0.7961
env1_first_0:                 episode reward: -76.1000,                 loss: nan
env1_second_0:                 episode reward: 76.1000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 436.1,                last time consumption/overall running time: 397.2111s / 60039.7881 s
env0_first_0:                 episode reward: -73.3000,                 loss: 1.0055
env0_second_0:                 episode reward: 73.3000,                 loss: 0.7564
env1_first_0:                 episode reward: -74.8500,                 loss: nan
env1_second_0:                 episode reward: 74.8500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 331.9,                last time consumption/overall running time: 303.0910s / 60342.8791 s
env0_first_0:                 episode reward: -83.7500,                 loss: 0.9611
env0_second_0:                 episode reward: 83.7500,                 loss: 0.7149
env1_first_0:                 episode reward: -78.2000,                 loss: nan
env1_second_0:                 episode reward: 78.2000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 366.85,                last time consumption/overall running time: 334.6490s / 60677.5281 s
env0_first_0:                 episode reward: -77.1500,                 loss: 0.9221
env0_second_0:                 episode reward: 77.1500,                 loss: 0.6970
env1_first_0:                 episode reward: -85.4000,                 loss: nan
env1_second_0:                 episode reward: 85.4000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 259.15,                last time consumption/overall running time: 236.8494s / 60914.3776 s
env0_first_0:                 episode reward: -81.0500,                 loss: 0.9434
env0_second_0:                 episode reward: 81.0500,                 loss: 0.7112
env1_first_0:                 episode reward: -81.4000,                 loss: nan
env1_second_0:                 episode reward: 81.4000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 282.0,                last time consumption/overall running time: 257.2321s / 61171.6097 s
env0_first_0:                 episode reward: -77.2000,                 loss: 0.9683
env0_second_0:                 episode reward: 77.2000,                 loss: 0.7516
env1_first_0:                 episode reward: -83.8000,                 loss: nan
env1_second_0:                 episode reward: 83.8000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 261.35,                last time consumption/overall running time: 238.7198s / 61410.3294 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.9997
env0_second_0:                 episode reward: 85.4000,                 loss: 0.7422
env1_first_0:                 episode reward: -68.8000,                 loss: nan
env1_second_0:                 episode reward: 68.8000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 303.25,                last time consumption/overall running time: 276.7280s / 61687.0574 s
env0_first_0:                 episode reward: -79.3000,                 loss: 1.0426
env0_second_0:                 episode reward: 79.3000,                 loss: 0.7404
env1_first_0:                 episode reward: -74.5500,                 loss: nan
env1_second_0:                 episode reward: 74.5500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 310.65,                last time consumption/overall running time: 282.7127s / 61969.7701 s
env0_first_0:                 episode reward: -85.2500,                 loss: 1.0410
env0_second_0:                 episode reward: 85.2500,                 loss: 0.7208
env1_first_0:                 episode reward: -65.4500,                 loss: nan
env1_second_0:                 episode reward: 65.4500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 346.9,                last time consumption/overall running time: 316.4408s / 62286.2109 s
env0_first_0:                 episode reward: -80.5500,                 loss: 1.0552
env0_second_0:                 episode reward: 80.5500,                 loss: 0.7386
env1_first_0:                 episode reward: -63.3000,                 loss: nan
env1_second_0:                 episode reward: 63.3000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 310.55,                last time consumption/overall running time: 283.4658s / 62569.6767 s
env0_first_0:                 episode reward: -80.1000,                 loss: 1.0584
env0_second_0:                 episode reward: 80.1000,                 loss: 0.7878
env1_first_0:                 episode reward: -73.5500,                 loss: nan
env1_second_0:                 episode reward: 73.5500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 330.1,                last time consumption/overall running time: 300.5391s / 62870.2157 s
env0_first_0:                 episode reward: -79.4500,                 loss: 1.0937
env0_second_0:                 episode reward: 79.4500,                 loss: 0.8535
env1_first_0:                 episode reward: -81.3500,                 loss: nan
env1_second_0:                 episode reward: 81.3500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 354.7,                last time consumption/overall running time: 324.2913s / 63194.5071 s
env0_first_0:                 episode reward: -74.6000,                 loss: 1.0906
env0_second_0:                 episode reward: 74.6000,                 loss: 0.8816
env1_first_0:                 episode reward: -67.7000,                 loss: nan
env1_second_0:                 episode reward: 67.7000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 323.8,                last time consumption/overall running time: 295.5405s / 63490.0476 s
env0_first_0:                 episode reward: -76.1500,                 loss: 1.1582
env0_second_0:                 episode reward: 76.1500,                 loss: 0.9383
env1_first_0:                 episode reward: -65.5000,                 loss: nan
env1_second_0:                 episode reward: 65.5000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 308.65,                last time consumption/overall running time: 281.9601s / 63772.0077 s
env0_first_0:                 episode reward: -79.9500,                 loss: 1.2238
env0_second_0:                 episode reward: 79.9500,                 loss: 0.9988
env1_first_0:                 episode reward: -79.1500,                 loss: nan
env1_second_0:                 episode reward: 79.1500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 280.7,                last time consumption/overall running time: 256.5600s / 64028.5677 s
env0_first_0:                 episode reward: -67.8000,                 loss: 1.2941
env0_second_0:                 episode reward: 67.8000,                 loss: 1.0098
env1_first_0:                 episode reward: -78.9000,                 loss: nan
env1_second_0:                 episode reward: 78.9000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 305.3,                last time consumption/overall running time: 278.4811s / 64307.0488 s
env0_first_0:                 episode reward: -80.7000,                 loss: 1.3294
env0_second_0:                 episode reward: 80.7000,                 loss: 0.9806
env1_first_0:                 episode reward: -69.7500,                 loss: nan
env1_second_0:                 episode reward: 69.7500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 324.25,                last time consumption/overall running time: 296.1179s / 64603.1667 s
env0_first_0:                 episode reward: -63.7500,                 loss: 1.3419
env0_second_0:                 episode reward: 63.7500,                 loss: 1.0434
env1_first_0:                 episode reward: -70.8000,                 loss: nan
env1_second_0:                 episode reward: 70.8000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 353.75,                last time consumption/overall running time: 322.5106s / 64925.6773 s
env0_first_0:                 episode reward: -79.8000,                 loss: 1.3477
env0_second_0:                 episode reward: 79.8000,                 loss: 1.0197
env1_first_0:                 episode reward: -70.7500,                 loss: nan
env1_second_0:                 episode reward: 70.7500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 315.9,                last time consumption/overall running time: 288.6609s / 65214.3382 s
env0_first_0:                 episode reward: -58.4500,                 loss: 1.3353
env0_second_0:                 episode reward: 58.4500,                 loss: 0.9786
env1_first_0:                 episode reward: -71.9500,                 loss: nan
env1_second_0:                 episode reward: 71.9500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 287.6,                last time consumption/overall running time: 263.0337s / 65477.3719 s
env0_first_0:                 episode reward: -83.3500,                 loss: 1.3309
env0_second_0:                 episode reward: 83.3500,                 loss: 0.9726
env1_first_0:                 episode reward: -67.2000,                 loss: nan
env1_second_0:                 episode reward: 67.2000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 287.2,                last time consumption/overall running time: 262.2163s / 65739.5882 s
env0_first_0:                 episode reward: -71.5500,                 loss: 1.2835
env0_second_0:                 episode reward: 71.5500,                 loss: 0.9242
env1_first_0:                 episode reward: -83.9000,                 loss: nan
env1_second_0:                 episode reward: 83.9000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 287.0,                last time consumption/overall running time: 261.5888s / 66001.1770 s
env0_first_0:                 episode reward: -72.4500,                 loss: 1.2659
env0_second_0:                 episode reward: 72.4500,                 loss: 0.9425
env1_first_0:                 episode reward: -70.7500,                 loss: nan
env1_second_0:                 episode reward: 70.7500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 281.25,                last time consumption/overall running time: 256.7368s / 66257.9138 s
env0_first_0:                 episode reward: -78.3000,                 loss: 1.2534
env0_second_0:                 episode reward: 78.3000,                 loss: 0.9265
env1_first_0:                 episode reward: -76.6000,                 loss: nan
env1_second_0:                 episode reward: 76.6000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 294.65,                last time consumption/overall running time: 269.0461s / 66526.9599 s
env0_first_0:                 episode reward: -63.1500,                 loss: 1.3353
env0_second_0:                 episode reward: 63.1500,                 loss: 0.9205
env1_first_0:                 episode reward: -85.1500,                 loss: nan
env1_second_0:                 episode reward: 85.1500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 282.9,                last time consumption/overall running time: 258.2158s / 66785.1757 s
env0_first_0:                 episode reward: -68.4500,                 loss: 1.3896
env0_second_0:                 episode reward: 68.4500,                 loss: 0.9261
env1_first_0:                 episode reward: -79.2500,                 loss: nan
env1_second_0:                 episode reward: 79.2500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 267.8,                last time consumption/overall running time: 243.7714s / 67028.9471 s
env0_first_0:                 episode reward: -58.4500,                 loss: 1.4030
env0_second_0:                 episode reward: 58.4500,                 loss: 0.9113
env1_first_0:                 episode reward: -79.2500,                 loss: nan
env1_second_0:                 episode reward: 79.2500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 415.3,                last time consumption/overall running time: 377.5490s / 67406.4961 s
env0_first_0:                 episode reward: -73.0500,                 loss: 1.3560
env0_second_0:                 episode reward: 73.0500,                 loss: 0.9305
env1_first_0:                 episode reward: -76.6000,                 loss: nan
env1_second_0:                 episode reward: 76.6000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 314.9,                last time consumption/overall running time: 288.4600s / 67694.9560 s
env0_first_0:                 episode reward: -74.4500,                 loss: 1.2866
env0_second_0:                 episode reward: 74.4500,                 loss: 0.8621
env1_first_0:                 episode reward: -72.7000,                 loss: nan
env1_second_0:                 episode reward: 72.7000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 346.25,                last time consumption/overall running time: 316.7948s / 68011.7508 s
env0_first_0:                 episode reward: -82.2000,                 loss: 1.2743
env0_second_0:                 episode reward: 82.2000,                 loss: 0.8565
env1_first_0:                 episode reward: -77.5500,                 loss: nan
env1_second_0:                 episode reward: 77.5500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 300.15,                last time consumption/overall running time: 274.4944s / 68286.2452 s
env0_first_0:                 episode reward: -78.0500,                 loss: 1.2484
env0_second_0:                 episode reward: 78.0500,                 loss: 0.8415
env1_first_0:                 episode reward: -65.9500,                 loss: nan
env1_second_0:                 episode reward: 65.9500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 315.6,                last time consumption/overall running time: 288.7373s / 68574.9825 s
env0_first_0:                 episode reward: -69.1000,                 loss: 1.2202
env0_second_0:                 episode reward: 69.1000,                 loss: 0.8383
env1_first_0:                 episode reward: -67.7500,                 loss: nan
env1_second_0:                 episode reward: 67.7500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 266.35,                last time consumption/overall running time: 242.7351s / 68817.7176 s
env0_first_0:                 episode reward: -79.8000,                 loss: 1.1819
env0_second_0:                 episode reward: 79.8000,                 loss: 0.8216
env1_first_0:                 episode reward: -60.3000,                 loss: nan
env1_second_0:                 episode reward: 60.3000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 286.95,                last time consumption/overall running time: 261.2721s / 69078.9896 s
env0_first_0:                 episode reward: -65.1500,                 loss: 1.0647
env0_second_0:                 episode reward: 65.1500,                 loss: 0.7877
env1_first_0:                 episode reward: -80.4000,                 loss: nan
env1_second_0:                 episode reward: 80.4000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 317.6,                last time consumption/overall running time: 289.4385s / 69368.4281 s
env0_first_0:                 episode reward: -73.7000,                 loss: 1.0641
env0_second_0:                 episode reward: 73.7000,                 loss: 0.7978
env1_first_0:                 episode reward: -66.0000,                 loss: nan
env1_second_0:                 episode reward: 66.0000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 303.55,                last time consumption/overall running time: 277.0842s / 69645.5123 s
env0_first_0:                 episode reward: -65.9000,                 loss: 1.0816
env0_second_0:                 episode reward: 65.9000,                 loss: 0.8057
env1_first_0:                 episode reward: -63.0500,                 loss: nan
env1_second_0:                 episode reward: 63.0500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 292.9,                last time consumption/overall running time: 266.4691s / 69911.9814 s
env0_first_0:                 episode reward: -69.1000,                 loss: 1.1251
env0_second_0:                 episode reward: 69.1000,                 loss: 0.8456
env1_first_0:                 episode reward: -68.1000,                 loss: nan
env1_second_0:                 episode reward: 68.1000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 274.7,                last time consumption/overall running time: 250.3278s / 70162.3092 s
env0_first_0:                 episode reward: -84.4000,                 loss: 1.1072
env0_second_0:                 episode reward: 84.4000,                 loss: 0.8410
env1_first_0:                 episode reward: -65.5500,                 loss: nan
env1_second_0:                 episode reward: 65.5500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 250.2,                last time consumption/overall running time: 229.1306s / 70391.4398 s
env0_first_0:                 episode reward: -84.6000,                 loss: 1.1011
env0_second_0:                 episode reward: 84.6000,                 loss: 0.8411
env1_first_0:                 episode reward: -74.5000,                 loss: nan
env1_second_0:                 episode reward: 74.5000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 322.85,                last time consumption/overall running time: 294.7650s / 70686.2048 s
env0_first_0:                 episode reward: -53.9000,                 loss: 1.0802
env0_second_0:                 episode reward: 53.9000,                 loss: 0.8430
env1_first_0:                 episode reward: -80.0500,                 loss: nan
env1_second_0:                 episode reward: 80.0500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 351.35,                last time consumption/overall running time: 319.8687s / 71006.0735 s
env0_first_0:                 episode reward: -58.1500,                 loss: 1.1318
env0_second_0:                 episode reward: 58.1500,                 loss: 0.8805
env1_first_0:                 episode reward: -55.9000,                 loss: nan
env1_second_0:                 episode reward: 55.9000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 323.85,                last time consumption/overall running time: 295.2560s / 71301.3294 s
env0_first_0:                 episode reward: -69.0000,                 loss: 1.2100
env0_second_0:                 episode reward: 69.0000,                 loss: 0.9285
env1_first_0:                 episode reward: -56.8000,                 loss: nan
env1_second_0:                 episode reward: 56.8000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 306.85,                last time consumption/overall running time: 279.4092s / 71580.7387 s
env0_first_0:                 episode reward: -76.9500,                 loss: 1.2791
env0_second_0:                 episode reward: 76.9500,                 loss: 0.9835
env1_first_0:                 episode reward: -73.0000,                 loss: nan
env1_second_0:                 episode reward: 73.0000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 237.5,                last time consumption/overall running time: 216.7517s / 71797.4904 s
env0_first_0:                 episode reward: -84.2500,                 loss: 1.2601
env0_second_0:                 episode reward: 84.2500,                 loss: 0.9618
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 346.2,                last time consumption/overall running time: 315.4226s / 72112.9130 s
env0_first_0:                 episode reward: -80.7500,                 loss: 1.2622
env0_second_0:                 episode reward: 80.7500,                 loss: 0.9439
env1_first_0:                 episode reward: -70.3500,                 loss: nan
env1_second_0:                 episode reward: 70.3500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 260.65,                last time consumption/overall running time: 237.7079s / 72350.6209 s
env0_first_0:                 episode reward: -83.9000,                 loss: 1.1975
env0_second_0:                 episode reward: 83.9000,                 loss: 0.9321
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 257.4,                last time consumption/overall running time: 234.5819s / 72585.2027 s
env0_first_0:                 episode reward: -69.3500,                 loss: 1.2221
env0_second_0:                 episode reward: 69.3500,                 loss: 0.9245
env1_first_0:                 episode reward: -74.4500,                 loss: nan
env1_second_0:                 episode reward: 74.4500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 294.35,                last time consumption/overall running time: 267.7287s / 72852.9314 s
env0_first_0:                 episode reward: -45.9000,                 loss: 1.2099
env0_second_0:                 episode reward: 45.9000,                 loss: 0.9582
env1_first_0:                 episode reward: -58.1500,                 loss: nan
env1_second_0:                 episode reward: 58.1500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 499.7,                last time consumption/overall running time: 454.1951s / 73307.1265 s
env0_first_0:                 episode reward: -55.8000,                 loss: 1.1906
env0_second_0:                 episode reward: 55.8000,                 loss: 0.9479
env1_first_0:                 episode reward: -48.4000,                 loss: nan
env1_second_0:                 episode reward: 48.4000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 352.75,                last time consumption/overall running time: 320.5423s / 73627.6688 s
env0_first_0:                 episode reward: -55.0000,                 loss: 1.0764
env0_second_0:                 episode reward: 55.0000,                 loss: 0.9004
env1_first_0:                 episode reward: -43.4500,                 loss: nan
env1_second_0:                 episode reward: 43.4500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 478.1,                last time consumption/overall running time: 435.2257s / 74062.8945 s
env0_first_0:                 episode reward: -72.7000,                 loss: 1.0302
env0_second_0:                 episode reward: 72.7000,                 loss: 0.9114
env1_first_0:                 episode reward: -39.9500,                 loss: nan
env1_second_0:                 episode reward: 39.9500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1391.25,                last time consumption/overall running time: 1264.7617s / 75327.6562 s
env0_first_0:                 episode reward: -10.5500,                 loss: 0.7748
env0_second_0:                 episode reward: 10.5500,                 loss: 0.6480
env1_first_0:                 episode reward: -13.7000,                 loss: nan
env1_second_0:                 episode reward: 13.7000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 497.15,                last time consumption/overall running time: 452.6237s / 75780.2799 s
env0_first_0:                 episode reward: -41.1000,                 loss: 0.4931
env0_second_0:                 episode reward: 41.1000,                 loss: 0.4470
env1_first_0:                 episode reward: -63.3000,                 loss: nan
env1_second_0:                 episode reward: 63.3000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 320.35,                last time consumption/overall running time: 291.8720s / 76072.1519 s
env0_first_0:                 episode reward: -40.9500,                 loss: 0.5914
env0_second_0:                 episode reward: 40.9500,                 loss: 0.4737
env1_first_0:                 episode reward: -54.2000,                 loss: nan
env1_second_0:                 episode reward: 54.2000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 334.9,                last time consumption/overall running time: 305.2061s / 76377.3580 s
env0_first_0:                 episode reward: -40.6500,                 loss: 0.6808
env0_second_0:                 episode reward: 40.6500,                 loss: 0.5257
env1_first_0:                 episode reward: -54.6500,                 loss: nan
env1_second_0:                 episode reward: 54.6500,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 335.8,                last time consumption/overall running time: 306.5419s / 76683.8999 s
env0_first_0:                 episode reward: -41.8500,                 loss: 0.8536
env0_second_0:                 episode reward: 41.8500,                 loss: 0.6263
env1_first_0:                 episode reward: -44.6000,                 loss: nan
env1_second_0:                 episode reward: 44.6000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 305.1,                last time consumption/overall running time: 278.2416s / 76962.1415 s
env0_first_0:                 episode reward: -72.0500,                 loss: 1.0925
env0_second_0:                 episode reward: 72.0500,                 loss: 0.8181
env1_first_0:                 episode reward: -57.5500,                 loss: nan
env1_second_0:                 episode reward: 57.5500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 299.5,                last time consumption/overall running time: 273.0678s / 77235.2094 s
env0_first_0:                 episode reward: -48.3000,                 loss: 1.3364
env0_second_0:                 episode reward: 48.3000,                 loss: 1.0093
env1_first_0:                 episode reward: -51.9000,                 loss: nan
env1_second_0:                 episode reward: 51.9000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 289.6,                last time consumption/overall running time: 263.8063s / 77499.0156 s
env0_first_0:                 episode reward: -66.0500,                 loss: 1.4521
env0_second_0:                 episode reward: 66.0500,                 loss: 1.1001
env1_first_0:                 episode reward: -62.2000,                 loss: nan
env1_second_0:                 episode reward: 62.2000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 278.85,                last time consumption/overall running time: 254.1646s / 77753.1803 s
env0_first_0:                 episode reward: -79.6000,                 loss: 1.6047
env0_second_0:                 episode reward: 79.6000,                 loss: 1.1844
env1_first_0:                 episode reward: -72.6000,                 loss: nan
env1_second_0:                 episode reward: 72.6000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 309.55,                last time consumption/overall running time: 282.3021s / 78035.4824 s
env0_first_0:                 episode reward: -47.6500,                 loss: 1.6799
env0_second_0:                 episode reward: 47.6500,                 loss: 1.2239
env1_first_0:                 episode reward: -77.9000,                 loss: nan
env1_second_0:                 episode reward: 77.9000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 287.2,                last time consumption/overall running time: 261.5039s / 78296.9862 s
env0_first_0:                 episode reward: -66.6000,                 loss: 1.6811
env0_second_0:                 episode reward: 66.6000,                 loss: 1.2125
env1_first_0:                 episode reward: -74.3500,                 loss: nan
env1_second_0:                 episode reward: 74.3500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 290.95,                last time consumption/overall running time: 265.2648s / 78562.2510 s
env0_first_0:                 episode reward: -59.9500,                 loss: 1.6541
env0_second_0:                 episode reward: 59.9500,                 loss: 1.2289
env1_first_0:                 episode reward: -55.5500,                 loss: nan
env1_second_0:                 episode reward: 55.5500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 289.15,                last time consumption/overall running time: 263.2420s / 78825.4930 s
env0_first_0:                 episode reward: -51.1500,                 loss: 1.6318
env0_second_0:                 episode reward: 51.1500,                 loss: 1.2050
env1_first_0:                 episode reward: -51.9500,                 loss: nan
env1_second_0:                 episode reward: 51.9500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 348.55,                last time consumption/overall running time: 317.8811s / 79143.3741 s
env0_first_0:                 episode reward: -35.3500,                 loss: 1.5969
env0_second_0:                 episode reward: 35.3500,                 loss: 1.1859
env1_first_0:                 episode reward: -45.2000,                 loss: nan
env1_second_0:                 episode reward: 45.2000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 310.5,                last time consumption/overall running time: 282.3350s / 79425.7091 s
env0_first_0:                 episode reward: -42.0500,                 loss: 1.5574
env0_second_0:                 episode reward: 42.0500,                 loss: 1.1734
env1_first_0:                 episode reward: -8.2500,                 loss: nan
env1_second_0:                 episode reward: 8.2500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 300.9,                last time consumption/overall running time: 274.6336s / 79700.3427 s
env0_first_0:                 episode reward: -73.9500,                 loss: 1.5422
env0_second_0:                 episode reward: 73.9500,                 loss: 1.2176
env1_first_0:                 episode reward: -45.9000,                 loss: nan
env1_second_0:                 episode reward: 45.9000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 353.15,                last time consumption/overall running time: 322.2677s / 80022.6104 s
env0_first_0:                 episode reward: -49.8000,                 loss: 1.5119
env0_second_0:                 episode reward: 49.8000,                 loss: 1.1836
env1_first_0:                 episode reward: -35.9000,                 loss: nan
env1_second_0:                 episode reward: 35.9000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 371.5,                last time consumption/overall running time: 337.9908s / 80360.6012 s
env0_first_0:                 episode reward: -65.3000,                 loss: 1.4519
env0_second_0:                 episode reward: 65.3000,                 loss: 1.1346
env1_first_0:                 episode reward: -56.5500,                 loss: nan
env1_second_0:                 episode reward: 56.5500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 324.85,                last time consumption/overall running time: 296.1589s / 80656.7601 s
env0_first_0:                 episode reward: -40.7000,                 loss: 1.4194
env0_second_0:                 episode reward: 40.7000,                 loss: 1.0979
env1_first_0:                 episode reward: -21.1000,                 loss: nan
env1_second_0:                 episode reward: 21.1000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 418.6,                last time consumption/overall running time: 382.2049s / 81038.9649 s
env0_first_0:                 episode reward: -56.9000,                 loss: 1.3881
env0_second_0:                 episode reward: 56.9000,                 loss: 1.0544
env1_first_0:                 episode reward: -53.2000,                 loss: nan
env1_second_0:                 episode reward: 53.2000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 351.25,                last time consumption/overall running time: 320.9956s / 81359.9605 s
env0_first_0:                 episode reward: -60.7000,                 loss: 1.2873
env0_second_0:                 episode reward: 60.7000,                 loss: 1.0232
env1_first_0:                 episode reward: -67.6000,                 loss: nan
env1_second_0:                 episode reward: 67.6000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 278.0,                last time consumption/overall running time: 253.2141s / 81613.1747 s
env0_first_0:                 episode reward: -49.1000,                 loss: 1.2151
env0_second_0:                 episode reward: 49.1000,                 loss: 0.8942
env1_first_0:                 episode reward: -38.4500,                 loss: nan
env1_second_0:                 episode reward: 38.4500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 270.65,                last time consumption/overall running time: 246.2393s / 81859.4140 s
env0_first_0:                 episode reward: -68.2000,                 loss: 1.1479
env0_second_0:                 episode reward: 68.2000,                 loss: 0.8055
env1_first_0:                 episode reward: -37.3500,                 loss: nan
env1_second_0:                 episode reward: 37.3500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 257.85,                last time consumption/overall running time: 235.0598s / 82094.4738 s
env0_first_0:                 episode reward: -55.4000,                 loss: 1.1057
env0_second_0:                 episode reward: 55.4000,                 loss: 0.7776
env1_first_0:                 episode reward: -48.5500,                 loss: nan
env1_second_0:                 episode reward: 48.5500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 293.35,                last time consumption/overall running time: 267.3617s / 82361.8355 s
env0_first_0:                 episode reward: -62.1000,                 loss: 1.1074
env0_second_0:                 episode reward: 62.1000,                 loss: 0.7865
env1_first_0:                 episode reward: -68.7500,                 loss: nan
env1_second_0:                 episode reward: 68.7500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 260.7,                last time consumption/overall running time: 237.5131s / 82599.3486 s
env0_first_0:                 episode reward: -69.8500,                 loss: 1.1017
env0_second_0:                 episode reward: 69.8500,                 loss: 0.7609
env1_first_0:                 episode reward: -68.3500,                 loss: nan
env1_second_0:                 episode reward: 68.3500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 317.85,                last time consumption/overall running time: 289.5159s / 82888.8645 s
env0_first_0:                 episode reward: -61.3500,                 loss: 1.0989
env0_second_0:                 episode reward: 61.3500,                 loss: 0.7657
env1_first_0:                 episode reward: -33.1000,                 loss: nan
env1_second_0:                 episode reward: 33.1000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 296.1,                last time consumption/overall running time: 270.3597s / 83159.2243 s
env0_first_0:                 episode reward: -59.4500,                 loss: 1.1236
env0_second_0:                 episode reward: 59.4500,                 loss: 0.7589
env1_first_0:                 episode reward: -72.5500,                 loss: nan
env1_second_0:                 episode reward: 72.5500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 282.0,                last time consumption/overall running time: 256.8570s / 83416.0813 s
env0_first_0:                 episode reward: -49.1500,                 loss: 1.1946
env0_second_0:                 episode reward: 49.1500,                 loss: 0.8073
env1_first_0:                 episode reward: -69.5000,                 loss: nan
env1_second_0:                 episode reward: 69.5000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 307.4,                last time consumption/overall running time: 279.4613s / 83695.5426 s
env0_first_0:                 episode reward: -68.2500,                 loss: 1.2393
env0_second_0:                 episode reward: 68.2500,                 loss: 0.8695
env1_first_0:                 episode reward: -44.6000,                 loss: nan
env1_second_0:                 episode reward: 44.6000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 298.1,                last time consumption/overall running time: 271.4422s / 83966.9848 s
env0_first_0:                 episode reward: -48.2000,                 loss: 1.2662
env0_second_0:                 episode reward: 48.2000,                 loss: 0.9139
env1_first_0:                 episode reward: -71.5000,                 loss: nan
env1_second_0:                 episode reward: 71.5000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 306.0,                last time consumption/overall running time: 279.6695s / 84246.6543 s
env0_first_0:                 episode reward: -63.0000,                 loss: 1.2983
env0_second_0:                 episode reward: 63.0000,                 loss: 0.9386
env1_first_0:                 episode reward: -60.7000,                 loss: nan
env1_second_0:                 episode reward: 60.7000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 401.25,                last time consumption/overall running time: 364.8057s / 84611.4600 s
env0_first_0:                 episode reward: -22.4000,                 loss: 1.3275
env0_second_0:                 episode reward: 22.4000,                 loss: 0.9657
env1_first_0:                 episode reward: -58.1000,                 loss: nan
env1_second_0:                 episode reward: 58.1000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 357.55,                last time consumption/overall running time: 325.9498s / 84937.4098 s
env0_first_0:                 episode reward: -61.5500,                 loss: 1.3125
env0_second_0:                 episode reward: 61.5500,                 loss: 0.9545
env1_first_0:                 episode reward: -41.0500,                 loss: nan
env1_second_0:                 episode reward: 41.0500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 324.45,                last time consumption/overall running time: 295.1984s / 85232.6081 s
env0_first_0:                 episode reward: -71.0500,                 loss: 1.2736
env0_second_0:                 episode reward: 71.0500,                 loss: 0.9167
env1_first_0:                 episode reward: -67.6500,                 loss: nan
env1_second_0:                 episode reward: 67.6500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 372.7,                last time consumption/overall running time: 339.4059s / 85572.0140 s
env0_first_0:                 episode reward: -66.7500,                 loss: 1.2111
env0_second_0:                 episode reward: 66.7500,                 loss: 0.9139
env1_first_0:                 episode reward: -56.8500,                 loss: nan
env1_second_0:                 episode reward: 56.8500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 335.4,                last time consumption/overall running time: 306.0087s / 85878.0228 s
env0_first_0:                 episode reward: -63.1000,                 loss: 1.1839
env0_second_0:                 episode reward: 63.1000,                 loss: 0.8855
env1_first_0:                 episode reward: -58.8500,                 loss: nan
env1_second_0:                 episode reward: 58.8500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 509.6,                last time consumption/overall running time: 463.9187s / 86341.9415 s
env0_first_0:                 episode reward: -49.7000,                 loss: 1.1213
env0_second_0:                 episode reward: 49.7000,                 loss: 0.8284
env1_first_0:                 episode reward: -63.6500,                 loss: nan
env1_second_0:                 episode reward: 63.6500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 345.15,                last time consumption/overall running time: 314.1626s / 86656.1041 s
env0_first_0:                 episode reward: -53.4500,                 loss: 1.1013
env0_second_0:                 episode reward: 53.4500,                 loss: 0.8329
env1_first_0:                 episode reward: -38.4500,                 loss: nan
env1_second_0:                 episode reward: 38.4500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 263.8,                last time consumption/overall running time: 241.2712s / 86897.3753 s
env0_first_0:                 episode reward: -46.1500,                 loss: 1.1500
env0_second_0:                 episode reward: 46.1500,                 loss: 0.8484
env1_first_0:                 episode reward: -49.6500,                 loss: nan
env1_second_0:                 episode reward: 49.6500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 302.35,                last time consumption/overall running time: 276.0408s / 87173.4160 s
env0_first_0:                 episode reward: -25.5000,                 loss: 1.1664
env0_second_0:                 episode reward: 25.5000,                 loss: 0.9148
env1_first_0:                 episode reward: -42.8500,                 loss: nan
env1_second_0:                 episode reward: 42.8500,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 294.6,                last time consumption/overall running time: 268.3610s / 87441.7771 s
env0_first_0:                 episode reward: -18.7500,                 loss: 1.2042
env0_second_0:                 episode reward: 18.7500,                 loss: 0.9451
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 317.85,                last time consumption/overall running time: 290.2126s / 87731.9897 s
env0_first_0:                 episode reward: -32.9500,                 loss: 1.2759
env0_second_0:                 episode reward: 32.9500,                 loss: 1.0027
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 338.65,                last time consumption/overall running time: 309.0062s / 88040.9959 s
env0_first_0:                 episode reward: -25.6500,                 loss: 1.3041
env0_second_0:                 episode reward: 25.6500,                 loss: 1.0728
env1_first_0:                 episode reward: -69.9000,                 loss: nan
env1_second_0:                 episode reward: 69.9000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 294.5,                last time consumption/overall running time: 268.5649s / 88309.5608 s
env0_first_0:                 episode reward: -58.1500,                 loss: 1.3300
env0_second_0:                 episode reward: 58.1500,                 loss: 1.0734
env1_first_0:                 episode reward: -53.2000,                 loss: nan
env1_second_0:                 episode reward: 53.2000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 320.6,                last time consumption/overall running time: 292.0025s / 88601.5634 s
env0_first_0:                 episode reward: -36.6500,                 loss: 1.3915
env0_second_0:                 episode reward: 36.6500,                 loss: 1.1135
env1_first_0:                 episode reward: -42.6000,                 loss: nan
env1_second_0:                 episode reward: 42.6000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 363.5,                last time consumption/overall running time: 330.8286s / 88932.3919 s
env0_first_0:                 episode reward: -51.3500,                 loss: 1.3537
env0_second_0:                 episode reward: 51.3500,                 loss: 1.0894
env1_first_0:                 episode reward: -37.8000,                 loss: nan
env1_second_0:                 episode reward: 37.8000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 355.95,                last time consumption/overall running time: 323.6321s / 89256.0240 s
env0_first_0:                 episode reward: -44.1000,                 loss: 1.2767
env0_second_0:                 episode reward: 44.1000,                 loss: 1.0557
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 384.6,                last time consumption/overall running time: 349.1534s / 89605.1774 s
env0_first_0:                 episode reward: -82.8500,                 loss: 1.2461
env0_second_0:                 episode reward: 82.8500,                 loss: 1.0237
env1_first_0:                 episode reward: -55.9500,                 loss: nan
env1_second_0:                 episode reward: 55.9500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 353.0,                last time consumption/overall running time: 321.0866s / 89926.2640 s
env0_first_0:                 episode reward: -10.1500,                 loss: 1.2064
env0_second_0:                 episode reward: 10.1500,                 loss: 0.9505
env1_first_0:                 episode reward: -19.7000,                 loss: nan
env1_second_0:                 episode reward: 19.7000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 365.1,                last time consumption/overall running time: 331.6768s / 90257.9407 s
env0_first_0:                 episode reward: -68.8000,                 loss: 1.1967
env0_second_0:                 episode reward: 68.8000,                 loss: 0.9532
env1_first_0:                 episode reward: -63.8500,                 loss: nan
env1_second_0:                 episode reward: 63.8500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 241.15,                last time consumption/overall running time: 220.2715s / 90478.2123 s
env0_first_0:                 episode reward: -58.3000,                 loss: 1.1427
env0_second_0:                 episode reward: 58.3000,                 loss: 0.8996
env1_first_0:                 episode reward: -81.8500,                 loss: nan
env1_second_0:                 episode reward: 81.8500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 343.8,                last time consumption/overall running time: 312.6440s / 90790.8563 s
env0_first_0:                 episode reward: -88.2000,                 loss: 1.0697
env0_second_0:                 episode reward: 88.2000,                 loss: 0.8304
env1_first_0:                 episode reward: -44.4500,                 loss: nan
env1_second_0:                 episode reward: 44.4500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 299.5,                last time consumption/overall running time: 273.1589s / 91064.0151 s
env0_first_0:                 episode reward: -60.0000,                 loss: 1.0574
env0_second_0:                 episode reward: 60.0000,                 loss: 0.8622
env1_first_0:                 episode reward: -38.3500,                 loss: nan
env1_second_0:                 episode reward: 38.3500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 276.1,                last time consumption/overall running time: 251.2642s / 91315.2793 s
env0_first_0:                 episode reward: -21.6500,                 loss: 1.1023
env0_second_0:                 episode reward: 21.6500,                 loss: 0.8982
env1_first_0:                 episode reward: -24.3500,                 loss: nan
env1_second_0:                 episode reward: 24.3500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 290.45,                last time consumption/overall running time: 264.7838s / 91580.0631 s
env0_first_0:                 episode reward: -57.5500,                 loss: 1.1345
env0_second_0:                 episode reward: 57.5500,                 loss: 0.9041
env1_first_0:                 episode reward: -37.6000,                 loss: nan
env1_second_0:                 episode reward: 37.6000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 281.55,                last time consumption/overall running time: 256.6816s / 91836.7447 s
env0_first_0:                 episode reward: -54.0500,                 loss: 1.1167
env0_second_0:                 episode reward: 54.0500,                 loss: 0.9079
env1_first_0:                 episode reward: -63.9500,                 loss: nan
env1_second_0:                 episode reward: 63.9500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 321.6,                last time consumption/overall running time: 293.4186s / 92130.1634 s
env0_first_0:                 episode reward: -46.6000,                 loss: 1.1328
env0_second_0:                 episode reward: 46.6000,                 loss: 0.9133
env1_first_0:                 episode reward: -59.2500,                 loss: nan
env1_second_0:                 episode reward: 59.2500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 442.1,                last time consumption/overall running time: 401.6669s / 92531.8303 s
env0_first_0:                 episode reward: -60.2000,                 loss: 1.1297
env0_second_0:                 episode reward: 60.2000,                 loss: 0.9352
env1_first_0:                 episode reward: -60.0500,                 loss: nan
env1_second_0:                 episode reward: 60.0500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 391.05,                last time consumption/overall running time: 355.8671s / 92887.6975 s
env0_first_0:                 episode reward: -72.5500,                 loss: 1.2308
env0_second_0:                 episode reward: 72.5500,                 loss: 0.9573
env1_first_0:                 episode reward: -55.5500,                 loss: nan
env1_second_0:                 episode reward: 55.5500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 296.7,                last time consumption/overall running time: 270.0435s / 93157.7409 s
env0_first_0:                 episode reward: -69.9500,                 loss: 1.2869
env0_second_0:                 episode reward: 69.9500,                 loss: 0.9582
env1_first_0:                 episode reward: -55.4000,                 loss: nan
env1_second_0:                 episode reward: 55.4000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 316.2,                last time consumption/overall running time: 288.0668s / 93445.8077 s
env0_first_0:                 episode reward: -71.7500,                 loss: 1.2424
env0_second_0:                 episode reward: 71.7500,                 loss: 0.9334
env1_first_0:                 episode reward: -84.5000,                 loss: nan
env1_second_0:                 episode reward: 84.5000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 352.5,                last time consumption/overall running time: 320.4176s / 93766.2253 s
env0_first_0:                 episode reward: -49.9000,                 loss: 1.2151
env0_second_0:                 episode reward: 49.9000,                 loss: 0.9357
env1_first_0:                 episode reward: -58.7000,                 loss: nan
env1_second_0:                 episode reward: 58.7000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 350.35,                last time consumption/overall running time: 317.6660s / 94083.8913 s
env0_first_0:                 episode reward: -46.5500,                 loss: 1.2652
env0_second_0:                 episode reward: 46.5500,                 loss: 0.9281
env1_first_0:                 episode reward: -43.0500,                 loss: nan
env1_second_0:                 episode reward: 43.0500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 284.2,                last time consumption/overall running time: 258.6347s / 94342.5259 s
env0_first_0:                 episode reward: -37.6000,                 loss: 1.2758
env0_second_0:                 episode reward: 37.6000,                 loss: 0.9346
env1_first_0:                 episode reward: -53.0000,                 loss: nan
env1_second_0:                 episode reward: 53.0000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 344.25,                last time consumption/overall running time: 313.7033s / 94656.2292 s
env0_first_0:                 episode reward: -6.7000,                 loss: 1.2251
env0_second_0:                 episode reward: 6.7000,                 loss: 0.9404
env1_first_0:                 episode reward: -18.8500,                 loss: nan
env1_second_0:                 episode reward: 18.8500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 327.1,                last time consumption/overall running time: 297.6946s / 94953.9238 s
env0_first_0:                 episode reward: -36.9500,                 loss: 1.2134
env0_second_0:                 episode reward: 36.9500,                 loss: 0.9681
env1_first_0:                 episode reward: -45.3500,                 loss: nan
env1_second_0:                 episode reward: 45.3500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 292.2,                last time consumption/overall running time: 266.3997s / 95220.3235 s
env0_first_0:                 episode reward: -24.3500,                 loss: 1.2211
env0_second_0:                 episode reward: 24.3500,                 loss: 0.9978
env1_first_0:                 episode reward: -24.2500,                 loss: nan
env1_second_0:                 episode reward: 24.2500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 309.7,                last time consumption/overall running time: 281.5733s / 95501.8968 s
env0_first_0:                 episode reward: -81.9500,                 loss: 1.1938
env0_second_0:                 episode reward: 81.9500,                 loss: 1.0270
env1_first_0:                 episode reward: -63.3000,                 loss: nan
env1_second_0:                 episode reward: 63.3000,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 299.65,                last time consumption/overall running time: 272.3799s / 95774.2767 s
env0_first_0:                 episode reward: -21.7500,                 loss: 1.2434
env0_second_0:                 episode reward: 21.7500,                 loss: 1.0830
env1_first_0:                 episode reward: -31.0000,                 loss: nan
env1_second_0:                 episode reward: 31.0000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 348.15,                last time consumption/overall running time: 314.6816s / 96088.9583 s
env0_first_0:                 episode reward: -66.3000,                 loss: 1.2660
env0_second_0:                 episode reward: 66.3000,                 loss: 1.0945
env1_first_0:                 episode reward: -69.5000,                 loss: nan
env1_second_0:                 episode reward: 69.5000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 285.55,                last time consumption/overall running time: 259.6301s / 96348.5884 s
env0_first_0:                 episode reward: -70.6500,                 loss: 1.2464
env0_second_0:                 episode reward: 70.6500,                 loss: 1.0347
env1_first_0:                 episode reward: -81.2500,                 loss: nan
env1_second_0:                 episode reward: 81.2500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 355.2,                last time consumption/overall running time: 321.9777s / 96670.5661 s
env0_first_0:                 episode reward: -75.7000,                 loss: 1.1963
env0_second_0:                 episode reward: 75.7000,                 loss: 0.9958
env1_first_0:                 episode reward: -57.7500,                 loss: nan
env1_second_0:                 episode reward: 57.7500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 267.15,                last time consumption/overall running time: 242.2691s / 96912.8352 s
env0_first_0:                 episode reward: -63.2000,                 loss: 1.1825
env0_second_0:                 episode reward: 63.2000,                 loss: 0.9814
env1_first_0:                 episode reward: -59.3500,                 loss: nan
env1_second_0:                 episode reward: 59.3500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 308.45,                last time consumption/overall running time: 281.0088s / 97193.8440 s
env0_first_0:                 episode reward: -71.5500,                 loss: 1.2131
env0_second_0:                 episode reward: 71.5500,                 loss: 0.9883
env1_first_0:                 episode reward: -69.9500,                 loss: nan
env1_second_0:                 episode reward: 69.9500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 298.8,                last time consumption/overall running time: 270.1890s / 97464.0330 s
env0_first_0:                 episode reward: -27.9500,                 loss: 1.2034
env0_second_0:                 episode reward: 27.9500,                 loss: 0.9609
env1_first_0:                 episode reward: -18.3500,                 loss: nan
env1_second_0:                 episode reward: 18.3500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 275.75,                last time consumption/overall running time: 250.5221s / 97714.5551 s
env0_first_0:                 episode reward: -67.9000,                 loss: 1.1962
env0_second_0:                 episode reward: 67.9000,                 loss: 0.9578
env1_first_0:                 episode reward: -45.8500,                 loss: nan
env1_second_0:                 episode reward: 45.8500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 285.75,                last time consumption/overall running time: 258.8101s / 97973.3651 s
env0_first_0:                 episode reward: -53.0000,                 loss: 1.2308
env0_second_0:                 episode reward: 53.0000,                 loss: 0.9728
env1_first_0:                 episode reward: -49.2500,                 loss: nan
env1_second_0:                 episode reward: 49.2500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 300.85,                last time consumption/overall running time: 272.1730s / 98245.5381 s
env0_first_0:                 episode reward: -75.6500,                 loss: 1.2690
env0_second_0:                 episode reward: 75.6500,                 loss: 0.9422
env1_first_0:                 episode reward: -77.3500,                 loss: nan
env1_second_0:                 episode reward: 77.3500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 245.6,                last time consumption/overall running time: 222.3157s / 98467.8538 s
env0_first_0:                 episode reward: -76.7000,                 loss: 1.3527
env0_second_0:                 episode reward: 76.7000,                 loss: 0.9697
env1_first_0:                 episode reward: -73.5500,                 loss: nan
env1_second_0:                 episode reward: 73.5500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 302.05,                last time consumption/overall running time: 273.7941s / 98741.6479 s
env0_first_0:                 episode reward: -59.9000,                 loss: 1.4124
env0_second_0:                 episode reward: 59.9000,                 loss: 1.0085
env1_first_0:                 episode reward: -41.8000,                 loss: nan
env1_second_0:                 episode reward: 41.8000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 297.6,                last time consumption/overall running time: 269.9128s / 99011.5607 s
env0_first_0:                 episode reward: -42.3000,                 loss: 1.4811
env0_second_0:                 episode reward: 42.3000,                 loss: 1.0596
env1_first_0:                 episode reward: -50.4500,                 loss: nan
env1_second_0:                 episode reward: 50.4500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 287.4,                last time consumption/overall running time: 260.6977s / 99272.2584 s
env0_first_0:                 episode reward: -38.9000,                 loss: 1.4704
env0_second_0:                 episode reward: 38.9000,                 loss: 1.0399
env1_first_0:                 episode reward: -72.2500,                 loss: nan
env1_second_0:                 episode reward: 72.2500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 534.3,                last time consumption/overall running time: 482.6651s / 99754.9235 s
env0_first_0:                 episode reward: -52.8000,                 loss: 1.3880
env0_second_0:                 episode reward: 52.8000,                 loss: 1.0136
env1_first_0:                 episode reward: -64.8500,                 loss: nan
env1_second_0:                 episode reward: 64.8500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 377.75,                last time consumption/overall running time: 342.4954s / 100097.4189 s
env0_first_0:                 episode reward: -42.4000,                 loss: 1.2103
env0_second_0:                 episode reward: 42.4000,                 loss: 0.8865
env1_first_0:                 episode reward: -47.7500,                 loss: nan
env1_second_0:                 episode reward: 47.7500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 334.4,                last time consumption/overall running time: 303.2929s / 100400.7118 s
env0_first_0:                 episode reward: -29.5500,                 loss: 1.2034
env0_second_0:                 episode reward: 29.5500,                 loss: 0.8685
env1_first_0:                 episode reward: -54.0000,                 loss: nan
env1_second_0:                 episode reward: 54.0000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 291.45,                last time consumption/overall running time: 264.8660s / 100665.5778 s
env0_first_0:                 episode reward: -76.1500,                 loss: 1.2082
env0_second_0:                 episode reward: 76.1500,                 loss: 0.8787
env1_first_0:                 episode reward: -58.5000,                 loss: nan
env1_second_0:                 episode reward: 58.5000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 257.4,                last time consumption/overall running time: 234.0463s / 100899.6241 s
env0_first_0:                 episode reward: -78.7000,                 loss: 1.1840
env0_second_0:                 episode reward: 78.7000,                 loss: 0.8861
env1_first_0:                 episode reward: -79.6000,                 loss: nan
env1_second_0:                 episode reward: 79.6000,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 258.95,                last time consumption/overall running time: 235.6180s / 101135.2421 s
env0_first_0:                 episode reward: -47.1000,                 loss: 1.1591
env0_second_0:                 episode reward: 47.1000,                 loss: 0.8486
env1_first_0:                 episode reward: -43.3500,                 loss: nan
env1_second_0:                 episode reward: 43.3500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 248.55,                last time consumption/overall running time: 226.1409s / 101361.3830 s
env0_first_0:                 episode reward: -61.1000,                 loss: 1.1649
env0_second_0:                 episode reward: 61.1000,                 loss: 0.8391
env1_first_0:                 episode reward: -69.8000,                 loss: nan
env1_second_0:                 episode reward: 69.8000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 266.1,                last time consumption/overall running time: 241.5509s / 101602.9338 s
env0_first_0:                 episode reward: -48.2500,                 loss: 1.1698
env0_second_0:                 episode reward: 48.2500,                 loss: 0.8895
env1_first_0:                 episode reward: -69.8000,                 loss: nan
env1_second_0:                 episode reward: 69.8000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 261.75,                last time consumption/overall running time: 238.1561s / 101841.0900 s
env0_first_0:                 episode reward: -59.6500,                 loss: 1.2060
env0_second_0:                 episode reward: 59.6500,                 loss: 0.9207
env1_first_0:                 episode reward: -75.0000,                 loss: nan
env1_second_0:                 episode reward: 75.0000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 295.9,                last time consumption/overall running time: 269.3680s / 102110.4579 s
env0_first_0:                 episode reward: -42.0000,                 loss: 1.2193
env0_second_0:                 episode reward: 42.0000,                 loss: 0.9577
env1_first_0:                 episode reward: -28.0000,                 loss: nan
env1_second_0:                 episode reward: 28.0000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 326.4,                last time consumption/overall running time: 296.5424s / 102407.0004 s
env0_first_0:                 episode reward: -59.9500,                 loss: 1.2767
env0_second_0:                 episode reward: 59.9500,                 loss: 0.9550
env1_first_0:                 episode reward: -71.2000,                 loss: nan
env1_second_0:                 episode reward: 71.2000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 305.05,                last time consumption/overall running time: 276.6907s / 102683.6910 s
env0_first_0:                 episode reward: -52.2000,                 loss: 1.2354
env0_second_0:                 episode reward: 52.2000,                 loss: 0.8875
env1_first_0:                 episode reward: -50.1500,                 loss: nan
env1_second_0:                 episode reward: 50.1500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 290.05,                last time consumption/overall running time: 262.5802s / 102946.2713 s
env0_first_0:                 episode reward: -51.8500,                 loss: 1.1892
env0_second_0:                 episode reward: 51.8500,                 loss: 0.8920
env1_first_0:                 episode reward: -60.0500,                 loss: nan
env1_second_0:                 episode reward: 60.0500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 269.8,                last time consumption/overall running time: 245.0205s / 103191.2918 s
env0_first_0:                 episode reward: -64.9500,                 loss: 1.2056
env0_second_0:                 episode reward: 64.9500,                 loss: 0.8854
env1_first_0:                 episode reward: -67.3500,                 loss: nan
env1_second_0:                 episode reward: 67.3500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 276.1,                last time consumption/overall running time: 250.9934s / 103442.2852 s
env0_first_0:                 episode reward: -79.5000,                 loss: 1.1857
env0_second_0:                 episode reward: 79.5000,                 loss: 0.9223
env1_first_0:                 episode reward: -79.1000,                 loss: nan
env1_second_0:                 episode reward: 79.1000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 365.7,                last time consumption/overall running time: 331.3511s / 103773.6364 s
env0_first_0:                 episode reward: -57.4000,                 loss: 1.1250
env0_second_0:                 episode reward: 57.4000,                 loss: 0.8252
env1_first_0:                 episode reward: -49.9000,                 loss: nan
env1_second_0:                 episode reward: 49.9000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 289.35,                last time consumption/overall running time: 262.4848s / 104036.1211 s
env0_first_0:                 episode reward: -84.2500,                 loss: 1.0771
env0_second_0:                 episode reward: 84.2500,                 loss: 0.8056
env1_first_0:                 episode reward: -61.5500,                 loss: nan
env1_second_0:                 episode reward: 61.5500,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 342.3,                last time consumption/overall running time: 310.0548s / 104346.1759 s
env0_first_0:                 episode reward: -55.5000,                 loss: 1.1519
env0_second_0:                 episode reward: 55.5000,                 loss: 0.8434
env1_first_0:                 episode reward: -81.5000,                 loss: nan
env1_second_0:                 episode reward: 81.5000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 252.95,                last time consumption/overall running time: 228.9997s / 104575.1757 s
env0_first_0:                 episode reward: -65.1000,                 loss: 1.1794
env0_second_0:                 episode reward: 65.1000,                 loss: 0.8400
env1_first_0:                 episode reward: -69.0000,                 loss: nan
env1_second_0:                 episode reward: 69.0000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 257.5,                last time consumption/overall running time: 233.6011s / 104808.7767 s
env0_first_0:                 episode reward: -71.9000,                 loss: 1.1448
env0_second_0:                 episode reward: 71.9000,                 loss: 0.8577
env1_first_0:                 episode reward: -70.6500,                 loss: nan
env1_second_0:                 episode reward: 70.6500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 263.35,                last time consumption/overall running time: 238.9022s / 105047.6789 s
env0_first_0:                 episode reward: -66.8000,                 loss: 1.0858
env0_second_0:                 episode reward: 66.8000,                 loss: 0.8707
env1_first_0:                 episode reward: -28.8000,                 loss: nan
env1_second_0:                 episode reward: 28.8000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 299.55,                last time consumption/overall running time: 272.3224s / 105320.0013 s
env0_first_0:                 episode reward: -45.3500,                 loss: 1.0810
env0_second_0:                 episode reward: 45.3500,                 loss: 0.8878
env1_first_0:                 episode reward: -38.6500,                 loss: nan
env1_second_0:                 episode reward: 38.6500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 261.65,                last time consumption/overall running time: 237.6194s / 105557.6207 s
env0_first_0:                 episode reward: -61.0000,                 loss: 1.0628
env0_second_0:                 episode reward: 61.0000,                 loss: 0.9142
env1_first_0:                 episode reward: -81.4000,                 loss: nan
env1_second_0:                 episode reward: 81.4000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 303.7,                last time consumption/overall running time: 274.8165s / 105832.4372 s
env0_first_0:                 episode reward: -65.3500,                 loss: 1.1494
env0_second_0:                 episode reward: 65.3500,                 loss: 0.9294
env1_first_0:                 episode reward: -70.1500,                 loss: nan
env1_second_0:                 episode reward: 70.1500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 295.2,                last time consumption/overall running time: 267.7442s / 106100.1814 s
env0_first_0:                 episode reward: -56.7000,                 loss: 1.2076
env0_second_0:                 episode reward: 56.7000,                 loss: 0.9548
env1_first_0:                 episode reward: -65.4500,                 loss: nan
env1_second_0:                 episode reward: 65.4500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 280.9,                last time consumption/overall running time: 255.1450s / 106355.3264 s
env0_first_0:                 episode reward: -67.7000,                 loss: 1.2156
env0_second_0:                 episode reward: 67.7000,                 loss: 0.9299
env1_first_0:                 episode reward: -58.3000,                 loss: nan
env1_second_0:                 episode reward: 58.3000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 288.5,                last time consumption/overall running time: 262.3438s / 106617.6701 s
env0_first_0:                 episode reward: -69.9000,                 loss: 1.2687
env0_second_0:                 episode reward: 69.9000,                 loss: 0.9318
env1_first_0:                 episode reward: -61.1000,                 loss: nan
env1_second_0:                 episode reward: 61.1000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 281.05,                last time consumption/overall running time: 255.8873s / 106873.5574 s
env0_first_0:                 episode reward: -76.7000,                 loss: 1.2934
env0_second_0:                 episode reward: 76.7000,                 loss: 0.9934
env1_first_0:                 episode reward: -59.5500,                 loss: nan
env1_second_0:                 episode reward: 59.5500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 247.5,                last time consumption/overall running time: 224.9031s / 107098.4605 s
env0_first_0:                 episode reward: -80.5500,                 loss: 1.3076
env0_second_0:                 episode reward: 80.5500,                 loss: 1.0008
env1_first_0:                 episode reward: -62.3000,                 loss: nan
env1_second_0:                 episode reward: 62.3000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 261.85,                last time consumption/overall running time: 237.5987s / 107336.0593 s
env0_first_0:                 episode reward: -55.8500,                 loss: 1.3425
env0_second_0:                 episode reward: 55.8500,                 loss: 0.9725
env1_first_0:                 episode reward: -66.6000,                 loss: nan
env1_second_0:                 episode reward: 66.6000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 275.25,                last time consumption/overall running time: 249.9682s / 107586.0275 s
env0_first_0:                 episode reward: -62.2000,                 loss: 1.3335
env0_second_0:                 episode reward: 62.2000,                 loss: 0.9949
env1_first_0:                 episode reward: -38.4500,                 loss: nan
env1_second_0:                 episode reward: 38.4500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 272.85,                last time consumption/overall running time: 247.4153s / 107833.4427 s
env0_first_0:                 episode reward: -48.7000,                 loss: 1.3289
env0_second_0:                 episode reward: 48.7000,                 loss: 0.9515
env1_first_0:                 episode reward: -37.9500,                 loss: nan
env1_second_0:                 episode reward: 37.9500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 261.8,                last time consumption/overall running time: 237.9099s / 108071.3526 s
env0_first_0:                 episode reward: -27.1500,                 loss: 1.3444
env0_second_0:                 episode reward: 27.1500,                 loss: 0.9525
env1_first_0:                 episode reward: -52.3000,                 loss: nan
env1_second_0:                 episode reward: 52.3000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 270.7,                last time consumption/overall running time: 246.0370s / 108317.3896 s
env0_first_0:                 episode reward: -61.5500,                 loss: 1.3188
env0_second_0:                 episode reward: 61.5500,                 loss: 0.9129
env1_first_0:                 episode reward: -44.2500,                 loss: nan
env1_second_0:                 episode reward: 44.2500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 421.4,                last time consumption/overall running time: 382.2139s / 108699.6035 s
env0_first_0:                 episode reward: -46.4000,                 loss: 1.3765
env0_second_0:                 episode reward: 46.4000,                 loss: 0.9332
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 358.15,                last time consumption/overall running time: 324.5534s / 109024.1569 s
env0_first_0:                 episode reward: -40.7000,                 loss: 1.2660
env0_second_0:                 episode reward: 40.7000,                 loss: 0.8560
env1_first_0:                 episode reward: -61.1500,                 loss: nan
env1_second_0:                 episode reward: 61.1500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 281.65,                last time consumption/overall running time: 255.4190s / 109279.5759 s
env0_first_0:                 episode reward: -78.0000,                 loss: 1.2327
env0_second_0:                 episode reward: 78.0000,                 loss: 0.7934
env1_first_0:                 episode reward: -76.7000,                 loss: nan
env1_second_0:                 episode reward: 76.7000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 356.55,                last time consumption/overall running time: 324.0399s / 109603.6157 s
env0_first_0:                 episode reward: -67.9000,                 loss: 1.2603
env0_second_0:                 episode reward: 67.9000,                 loss: 0.8097
env1_first_0:                 episode reward: -71.4500,                 loss: nan
env1_second_0:                 episode reward: 71.4500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 290.65,                last time consumption/overall running time: 263.3856s / 109867.0013 s
env0_first_0:                 episode reward: -26.1000,                 loss: 1.2733
env0_second_0:                 episode reward: 26.1000,                 loss: 0.8252
env1_first_0:                 episode reward: -66.5500,                 loss: nan
env1_second_0:                 episode reward: 66.5500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 296.05,                last time consumption/overall running time: 268.2858s / 110135.2871 s
env0_first_0:                 episode reward: -54.6000,                 loss: 1.2954
env0_second_0:                 episode reward: 54.6000,                 loss: 0.8596
env1_first_0:                 episode reward: -45.0000,                 loss: nan
env1_second_0:                 episode reward: 45.0000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 304.8,                last time consumption/overall running time: 276.9641s / 110412.2512 s
env0_first_0:                 episode reward: -76.7000,                 loss: 1.2886
env0_second_0:                 episode reward: 76.7000,                 loss: 0.8635
env1_first_0:                 episode reward: -68.0500,                 loss: nan
env1_second_0:                 episode reward: 68.0500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 289.95,                last time consumption/overall running time: 263.4154s / 110675.6666 s
env0_first_0:                 episode reward: -67.1500,                 loss: 1.3317
env0_second_0:                 episode reward: 67.1500,                 loss: 0.8592
env1_first_0:                 episode reward: -71.4500,                 loss: nan
env1_second_0:                 episode reward: 71.4500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 304.1,                last time consumption/overall running time: 275.7353s / 110951.4019 s
env0_first_0:                 episode reward: -63.7000,                 loss: 1.4216
env0_second_0:                 episode reward: 63.7000,                 loss: 0.8994
env1_first_0:                 episode reward: -42.3000,                 loss: nan
env1_second_0:                 episode reward: 42.3000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 282.45,                last time consumption/overall running time: 255.9945s / 111207.3965 s
env0_first_0:                 episode reward: -31.7500,                 loss: 1.4089
env0_second_0:                 episode reward: 31.7500,                 loss: 0.9515
env1_first_0:                 episode reward: -44.0000,                 loss: nan
env1_second_0:                 episode reward: 44.0000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 259.35,                last time consumption/overall running time: 235.9454s / 111443.3419 s
env0_first_0:                 episode reward: -93.6000,                 loss: 1.4369
env0_second_0:                 episode reward: 93.6000,                 loss: 0.9678
env1_first_0:                 episode reward: -66.6000,                 loss: nan
env1_second_0:                 episode reward: 66.6000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 249.1,                last time consumption/overall running time: 226.0372s / 111669.3791 s
env0_first_0:                 episode reward: -73.8000,                 loss: 1.3934
env0_second_0:                 episode reward: 73.8000,                 loss: 0.9861
env1_first_0:                 episode reward: -73.2000,                 loss: nan
env1_second_0:                 episode reward: 73.2000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 273.7,                last time consumption/overall running time: 249.0972s / 111918.4763 s
env0_first_0:                 episode reward: -50.2000,                 loss: 1.4047
env0_second_0:                 episode reward: 50.2000,                 loss: 0.9729
env1_first_0:                 episode reward: -57.9500,                 loss: nan
env1_second_0:                 episode reward: 57.9500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 238.0,                last time consumption/overall running time: 217.0941s / 112135.5705 s
env0_first_0:                 episode reward: -68.8500,                 loss: 1.3873
env0_second_0:                 episode reward: 68.8500,                 loss: 0.9507
env1_first_0:                 episode reward: -68.2500,                 loss: nan
env1_second_0:                 episode reward: 68.2500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 263.0,                last time consumption/overall running time: 239.5212s / 112375.0917 s
env0_first_0:                 episode reward: -58.0000,                 loss: 1.3341
env0_second_0:                 episode reward: 58.0000,                 loss: 0.8914
env1_first_0:                 episode reward: -52.0500,                 loss: nan
env1_second_0:                 episode reward: 52.0500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 256.55,                last time consumption/overall running time: 233.2122s / 112608.3039 s
env0_first_0:                 episode reward: -65.6500,                 loss: 1.3298
env0_second_0:                 episode reward: 65.6500,                 loss: 0.9031
env1_first_0:                 episode reward: -70.5000,                 loss: nan
env1_second_0:                 episode reward: 70.5000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 250.65,                last time consumption/overall running time: 227.8730s / 112836.1769 s
env0_first_0:                 episode reward: -18.5500,                 loss: 1.3341
env0_second_0:                 episode reward: 18.5500,                 loss: 0.9174
env1_first_0:                 episode reward: -48.9000,                 loss: nan
env1_second_0:                 episode reward: 48.9000,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 251.2,                last time consumption/overall running time: 228.0955s / 113064.2724 s
env0_first_0:                 episode reward: -58.0000,                 loss: 1.3031
env0_second_0:                 episode reward: 58.0000,                 loss: 0.8806
env1_first_0:                 episode reward: -75.9000,                 loss: nan
env1_second_0:                 episode reward: 75.9000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 293.4,                last time consumption/overall running time: 266.2613s / 113330.5337 s
env0_first_0:                 episode reward: -42.5500,                 loss: 1.2197
env0_second_0:                 episode reward: 42.5500,                 loss: 0.8738
env1_first_0:                 episode reward: -46.8000,                 loss: nan
env1_second_0:                 episode reward: 46.8000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 325.25,                last time consumption/overall running time: 295.6139s / 113626.1476 s
env0_first_0:                 episode reward: -44.5000,                 loss: 1.2348
env0_second_0:                 episode reward: 44.5000,                 loss: 0.8557
env1_first_0:                 episode reward: -66.9500,                 loss: nan
env1_second_0:                 episode reward: 66.9500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 349.6,                last time consumption/overall running time: 317.6444s / 113943.7920 s
env0_first_0:                 episode reward: -66.6500,                 loss: 1.2469
env0_second_0:                 episode reward: 66.6500,                 loss: 0.8717
env1_first_0:                 episode reward: -73.5000,                 loss: nan
env1_second_0:                 episode reward: 73.5000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 271.1,                last time consumption/overall running time: 246.5310s / 114190.3231 s
env0_first_0:                 episode reward: -58.3000,                 loss: 1.3255
env0_second_0:                 episode reward: 58.3000,                 loss: 0.8938
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 253.65,                last time consumption/overall running time: 230.5740s / 114420.8970 s
env0_first_0:                 episode reward: -57.2000,                 loss: 1.3582
env0_second_0:                 episode reward: 57.2000,                 loss: 0.8995
env1_first_0:                 episode reward: -55.1000,                 loss: nan
env1_second_0:                 episode reward: 55.1000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 279.75,                last time consumption/overall running time: 254.0133s / 114674.9103 s
env0_first_0:                 episode reward: -84.3000,                 loss: 1.3738
env0_second_0:                 episode reward: 84.3000,                 loss: 0.9504
env1_first_0:                 episode reward: -71.9000,                 loss: nan
env1_second_0:                 episode reward: 71.9000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 258.55,                last time consumption/overall running time: 235.5206s / 114910.4309 s
env0_first_0:                 episode reward: -61.6000,                 loss: 1.3712
env0_second_0:                 episode reward: 61.6000,                 loss: 0.9339
env1_first_0:                 episode reward: -63.9500,                 loss: nan
env1_second_0:                 episode reward: 63.9500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 327.6,                last time consumption/overall running time: 298.6216s / 115209.0525 s
env0_first_0:                 episode reward: -60.7000,                 loss: 1.3454
env0_second_0:                 episode reward: 60.7000,                 loss: 0.8928
env1_first_0:                 episode reward: -77.9000,                 loss: nan
env1_second_0:                 episode reward: 77.9000,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 270.25,                last time consumption/overall running time: 245.9954s / 115455.0479 s
env0_first_0:                 episode reward: -68.2000,                 loss: 1.3206
env0_second_0:                 episode reward: 68.2000,                 loss: 0.8914
env1_first_0:                 episode reward: -71.2500,                 loss: nan
env1_second_0:                 episode reward: 71.2500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 275.65,                last time consumption/overall running time: 250.0473s / 115705.0953 s
env0_first_0:                 episode reward: -82.7500,                 loss: 1.3328
env0_second_0:                 episode reward: 82.7500,                 loss: 0.8886
env1_first_0:                 episode reward: -66.0000,                 loss: nan
env1_second_0:                 episode reward: 66.0000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 267.8,                last time consumption/overall running time: 243.6292s / 115948.7245 s
env0_first_0:                 episode reward: -69.4000,                 loss: 1.3230
env0_second_0:                 episode reward: 69.4000,                 loss: 0.9035
env1_first_0:                 episode reward: -79.1000,                 loss: nan
env1_second_0:                 episode reward: 79.1000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 247.65,                last time consumption/overall running time: 225.8897s / 116174.6142 s
env0_first_0:                 episode reward: -64.2500,                 loss: 1.2993
env0_second_0:                 episode reward: 64.2500,                 loss: 0.8921
env1_first_0:                 episode reward: -60.0500,                 loss: nan
env1_second_0:                 episode reward: 60.0500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 252.4,                last time consumption/overall running time: 230.0148s / 116404.6290 s
env0_first_0:                 episode reward: -90.4500,                 loss: 1.3628
env0_second_0:                 episode reward: 90.4500,                 loss: 0.8921
env1_first_0:                 episode reward: -47.0500,                 loss: nan
env1_second_0:                 episode reward: 47.0500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 260.45,                last time consumption/overall running time: 236.4868s / 116641.1158 s
env0_first_0:                 episode reward: -62.6000,                 loss: 1.2894
env0_second_0:                 episode reward: 62.6000,                 loss: 0.8816
env1_first_0:                 episode reward: -84.0500,                 loss: nan
env1_second_0:                 episode reward: 84.0500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 294.5,                last time consumption/overall running time: 266.7178s / 116907.8336 s
env0_first_0:                 episode reward: -24.7500,                 loss: 1.3130
env0_second_0:                 episode reward: 24.7500,                 loss: 0.9143
env1_first_0:                 episode reward: -61.5500,                 loss: nan
env1_second_0:                 episode reward: 61.5500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 285.75,                last time consumption/overall running time: 259.8839s / 117167.7174 s
env0_first_0:                 episode reward: -59.9500,                 loss: 1.3776
env0_second_0:                 episode reward: 59.9500,                 loss: 0.9605
env1_first_0:                 episode reward: -64.5000,                 loss: nan
env1_second_0:                 episode reward: 64.5000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 276.9,                last time consumption/overall running time: 251.2134s / 117418.9308 s
env0_first_0:                 episode reward: -78.1500,                 loss: 1.5048
env0_second_0:                 episode reward: 78.1500,                 loss: 1.0087
env1_first_0:                 episode reward: -60.2500,                 loss: nan
env1_second_0:                 episode reward: 60.2500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 279.8,                last time consumption/overall running time: 253.5068s / 117672.4376 s
env0_first_0:                 episode reward: -69.2500,                 loss: 1.5082
env0_second_0:                 episode reward: 69.2500,                 loss: 1.0426
env1_first_0:                 episode reward: -74.0500,                 loss: nan
env1_second_0:                 episode reward: 74.0500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 266.3,                last time consumption/overall running time: 241.5095s / 117913.9471 s
env0_first_0:                 episode reward: -69.9000,                 loss: 1.5125
env0_second_0:                 episode reward: 69.9000,                 loss: 1.0899
env1_first_0:                 episode reward: -78.0000,                 loss: nan
env1_second_0:                 episode reward: 78.0000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 262.0,                last time consumption/overall running time: 237.9453s / 118151.8924 s
env0_first_0:                 episode reward: -86.4500,                 loss: 1.4978
env0_second_0:                 episode reward: 86.4500,                 loss: 1.0719
env1_first_0:                 episode reward: -74.6000,                 loss: nan
env1_second_0:                 episode reward: 74.6000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 305.85,                last time consumption/overall running time: 277.4890s / 118429.3814 s
env0_first_0:                 episode reward: -70.3000,                 loss: 1.5455
env0_second_0:                 episode reward: 70.3000,                 loss: 1.0979
env1_first_0:                 episode reward: -57.1000,                 loss: nan
env1_second_0:                 episode reward: 57.1000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 314.85,                last time consumption/overall running time: 286.3855s / 118715.7669 s
env0_first_0:                 episode reward: -48.3500,                 loss: 1.6312
env0_second_0:                 episode reward: 48.3500,                 loss: 1.1145
env1_first_0:                 episode reward: -68.1500,                 loss: nan
env1_second_0:                 episode reward: 68.1500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 388.1,                last time consumption/overall running time: 352.3863s / 119068.1532 s
env0_first_0:                 episode reward: -62.2500,                 loss: 1.6003
env0_second_0:                 episode reward: 62.2500,                 loss: 1.1145
env1_first_0:                 episode reward: -51.9500,                 loss: nan
env1_second_0:                 episode reward: 51.9500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 299.25,                last time consumption/overall running time: 272.0147s / 119340.1679 s
env0_first_0:                 episode reward: -70.7000,                 loss: 1.5083
env0_second_0:                 episode reward: 70.7000,                 loss: 1.0639
env1_first_0:                 episode reward: -72.2500,                 loss: nan
env1_second_0:                 episode reward: 72.2500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 331.6,                last time consumption/overall running time: 301.9765s / 119642.1443 s
env0_first_0:                 episode reward: -75.2500,                 loss: 1.4369
env0_second_0:                 episode reward: 75.2500,                 loss: 1.0224
env1_first_0:                 episode reward: -67.6500,                 loss: nan
env1_second_0:                 episode reward: 67.6500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 376.3,                last time consumption/overall running time: 342.2276s / 119984.3719 s
env0_first_0:                 episode reward: -64.5000,                 loss: 1.4245
env0_second_0:                 episode reward: 64.5000,                 loss: 0.9749
env1_first_0:                 episode reward: -47.9500,                 loss: nan
env1_second_0:                 episode reward: 47.9500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 319.8,                last time consumption/overall running time: 290.0871s / 120274.4590 s
env0_first_0:                 episode reward: -49.4500,                 loss: 1.4239
env0_second_0:                 episode reward: 49.4500,                 loss: 0.9411
env1_first_0:                 episode reward: -49.6500,                 loss: nan
env1_second_0:                 episode reward: 49.6500,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 266.3,                last time consumption/overall running time: 242.5170s / 120516.9761 s
env0_first_0:                 episode reward: -60.6500,                 loss: 1.4151
env0_second_0:                 episode reward: 60.6500,                 loss: 0.9663
env1_first_0:                 episode reward: -50.1000,                 loss: nan
env1_second_0:                 episode reward: 50.1000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 294.3,                last time consumption/overall running time: 267.9070s / 120784.8831 s
env0_first_0:                 episode reward: -46.3500,                 loss: 1.4264
env0_second_0:                 episode reward: 46.3500,                 loss: 0.9594
env1_first_0:                 episode reward: -71.4500,                 loss: nan
env1_second_0:                 episode reward: 71.4500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 253.05,                last time consumption/overall running time: 231.3027s / 121016.1858 s
env0_first_0:                 episode reward: -70.0000,                 loss: 1.5111
env0_second_0:                 episode reward: 70.0000,                 loss: 0.9636
env1_first_0:                 episode reward: -71.7500,                 loss: nan
env1_second_0:                 episode reward: 71.7500,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 272.2,                last time consumption/overall running time: 247.7033s / 121263.8891 s
env0_first_0:                 episode reward: -63.4000,                 loss: 1.4994
env0_second_0:                 episode reward: 63.4000,                 loss: 0.9662
env1_first_0:                 episode reward: -55.8000,                 loss: nan
env1_second_0:                 episode reward: 55.8000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 256.2,                last time consumption/overall running time: 234.0750s / 121497.9640 s
env0_first_0:                 episode reward: -83.0000,                 loss: 1.4525
env0_second_0:                 episode reward: 83.0000,                 loss: 0.9558
env1_first_0:                 episode reward: -75.3500,                 loss: nan
env1_second_0:                 episode reward: 75.3500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 258.55,                last time consumption/overall running time: 235.6088s / 121733.5729 s
env0_first_0:                 episode reward: -67.5500,                 loss: 1.4937
env0_second_0:                 episode reward: 67.5500,                 loss: 0.9089
env1_first_0:                 episode reward: -69.3000,                 loss: nan
env1_second_0:                 episode reward: 69.3000,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 261.15,                last time consumption/overall running time: 237.2169s / 121970.7898 s
env0_first_0:                 episode reward: -83.2500,                 loss: 1.4673
env0_second_0:                 episode reward: 83.2500,                 loss: 0.9059
env1_first_0:                 episode reward: -63.2000,                 loss: nan
env1_second_0:                 episode reward: 63.2000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 281.35,                last time consumption/overall running time: 255.2995s / 122226.0893 s
env0_first_0:                 episode reward: -77.6000,                 loss: 1.4750
env0_second_0:                 episode reward: 77.6000,                 loss: 0.9412
env1_first_0:                 episode reward: -61.0500,                 loss: nan
env1_second_0:                 episode reward: 61.0500,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 273.7,                last time consumption/overall running time: 249.5166s / 122475.6058 s
env0_first_0:                 episode reward: -70.4500,                 loss: 1.5090
env0_second_0:                 episode reward: 70.4500,                 loss: 0.9518
env1_first_0:                 episode reward: -66.9500,                 loss: nan
env1_second_0:                 episode reward: 66.9500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 289.75,                last time consumption/overall running time: 263.4355s / 122739.0414 s
env0_first_0:                 episode reward: -50.3500,                 loss: 1.5380
env0_second_0:                 episode reward: 50.3500,                 loss: 0.9792
env1_first_0:                 episode reward: -74.8000,                 loss: nan
env1_second_0:                 episode reward: 74.8000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 255.2,                last time consumption/overall running time: 231.5863s / 122970.6276 s
env0_first_0:                 episode reward: -70.7000,                 loss: 1.5357
env0_second_0:                 episode reward: 70.7000,                 loss: 0.9573
env1_first_0:                 episode reward: -66.2500,                 loss: nan
env1_second_0:                 episode reward: 66.2500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 274.35,                last time consumption/overall running time: 249.2819s / 123219.9095 s
env0_first_0:                 episode reward: -88.4500,                 loss: 1.5854
env0_second_0:                 episode reward: 88.4500,                 loss: 0.9640
env1_first_0:                 episode reward: -61.9500,                 loss: nan
env1_second_0:                 episode reward: 61.9500,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 339.15,                last time consumption/overall running time: 307.3376s / 123527.2471 s
env0_first_0:                 episode reward: -84.5500,                 loss: 1.5116
env0_second_0:                 episode reward: 84.5500,                 loss: 0.9716
env1_first_0:                 episode reward: -64.5000,                 loss: nan
env1_second_0:                 episode reward: 64.5000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 266.55,                last time consumption/overall running time: 242.5679s / 123769.8151 s
env0_first_0:                 episode reward: -74.8500,                 loss: 1.5432
env0_second_0:                 episode reward: 74.8500,                 loss: 1.0015
env1_first_0:                 episode reward: -61.1500,                 loss: nan
env1_second_0:                 episode reward: 61.1500,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 259.15,                last time consumption/overall running time: 236.5205s / 124006.3356 s
env0_first_0:                 episode reward: -91.0000,                 loss: 1.5641
env0_second_0:                 episode reward: 91.0000,                 loss: 0.9831
env1_first_0:                 episode reward: -70.2000,                 loss: nan
env1_second_0:                 episode reward: 70.2000,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 252.3,                last time consumption/overall running time: 229.4172s / 124235.7528 s
env0_first_0:                 episode reward: -56.0000,                 loss: 1.5861
env0_second_0:                 episode reward: 56.0000,                 loss: 0.9890
env1_first_0:                 episode reward: -74.5500,                 loss: nan
env1_second_0:                 episode reward: 74.5500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 263.9,                last time consumption/overall running time: 240.1266s / 124475.8794 s
env0_first_0:                 episode reward: -79.7000,                 loss: 1.6312
env0_second_0:                 episode reward: 79.7000,                 loss: 1.0319
env1_first_0:                 episode reward: -67.6000,                 loss: nan
env1_second_0:                 episode reward: 67.6000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 332.7,                last time consumption/overall running time: 300.5766s / 124776.4560 s
env0_first_0:                 episode reward: -76.6000,                 loss: 1.5603
env0_second_0:                 episode reward: 76.6000,                 loss: 1.0236
env1_first_0:                 episode reward: -55.0500,                 loss: nan
env1_second_0:                 episode reward: 55.0500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 654.7,                last time consumption/overall running time: 591.5194s / 125367.9754 s
env0_first_0:                 episode reward: -29.3000,                 loss: 1.3238
env0_second_0:                 episode reward: 29.3000,                 loss: 0.8131
env1_first_0:                 episode reward: -37.2000,                 loss: nan
env1_second_0:                 episode reward: 37.2000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 376.4,                last time consumption/overall running time: 340.6636s / 125708.6390 s
env0_first_0:                 episode reward: -52.8500,                 loss: 1.2508
env0_second_0:                 episode reward: 52.8500,                 loss: 0.7606
env1_first_0:                 episode reward: -39.4500,                 loss: nan
env1_second_0:                 episode reward: 39.4500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 285.6,                last time consumption/overall running time: 258.9828s / 125967.6218 s
env0_first_0:                 episode reward: -39.0000,                 loss: 1.2925
env0_second_0:                 episode reward: 39.0000,                 loss: 0.7663
env1_first_0:                 episode reward: -67.0000,                 loss: nan
env1_second_0:                 episode reward: 67.0000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 356.0,                last time consumption/overall running time: 322.6220s / 126290.2438 s
env0_first_0:                 episode reward: -37.2500,                 loss: 1.3293
env0_second_0:                 episode reward: 37.2500,                 loss: 0.8061
env1_first_0:                 episode reward: -28.9500,                 loss: nan
env1_second_0:                 episode reward: 28.9500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 358.1,                last time consumption/overall running time: 324.6842s / 126614.9280 s
env0_first_0:                 episode reward: -58.8500,                 loss: 1.4294
env0_second_0:                 episode reward: 58.8500,                 loss: 0.8923
env1_first_0:                 episode reward: -32.8500,                 loss: nan
env1_second_0:                 episode reward: 32.8500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 332.45,                last time consumption/overall running time: 301.3699s / 126916.2979 s
env0_first_0:                 episode reward: -11.0000,                 loss: 1.5364
env0_second_0:                 episode reward: 11.0000,                 loss: 0.9248
env1_first_0:                 episode reward: -30.3500,                 loss: nan
env1_second_0:                 episode reward: 30.3500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 314.45,                last time consumption/overall running time: 285.8225s / 127202.1204 s
env0_first_0:                 episode reward: -41.1500,                 loss: 1.6073
env0_second_0:                 episode reward: 41.1500,                 loss: 0.9897
env1_first_0:                 episode reward: -48.3500,                 loss: nan
env1_second_0:                 episode reward: 48.3500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 273.5,                last time consumption/overall running time: 248.1293s / 127450.2497 s
env0_first_0:                 episode reward: -47.9500,                 loss: 1.7015
env0_second_0:                 episode reward: 47.9500,                 loss: 1.0666
env1_first_0:                 episode reward: -65.6500,                 loss: nan
env1_second_0:                 episode reward: 65.6500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 260.65,                last time consumption/overall running time: 236.3670s / 127686.6168 s
env0_first_0:                 episode reward: -47.1000,                 loss: 1.7668
env0_second_0:                 episode reward: 47.1000,                 loss: 1.1474
env1_first_0:                 episode reward: -42.5500,                 loss: nan
env1_second_0:                 episode reward: 42.5500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 282.2,                last time consumption/overall running time: 255.9593s / 127942.5761 s
env0_first_0:                 episode reward: -74.7500,                 loss: 1.7453
env0_second_0:                 episode reward: 74.7500,                 loss: 1.1486
env1_first_0:                 episode reward: -61.9500,                 loss: nan
env1_second_0:                 episode reward: 61.9500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 292.0,                last time consumption/overall running time: 265.2597s / 128207.8358 s
env0_first_0:                 episode reward: -43.4500,                 loss: 1.7455
env0_second_0:                 episode reward: 43.4500,                 loss: 1.1564
env1_first_0:                 episode reward: -71.8000,                 loss: nan
env1_second_0:                 episode reward: 71.8000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 254.45,                last time consumption/overall running time: 231.2012s / 128439.0370 s
env0_first_0:                 episode reward: -68.5500,                 loss: 1.7158
env0_second_0:                 episode reward: 68.5500,                 loss: 1.1217
env1_first_0:                 episode reward: -50.8500,                 loss: nan
env1_second_0:                 episode reward: 50.8500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 266.55,                last time consumption/overall running time: 241.3159s / 128680.3529 s
env0_first_0:                 episode reward: -56.1000,                 loss: 1.6969
env0_second_0:                 episode reward: 56.1000,                 loss: 1.0795
env1_first_0:                 episode reward: -66.1000,                 loss: nan
env1_second_0:                 episode reward: 66.1000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 254.25,                last time consumption/overall running time: 230.8468s / 128911.1997 s
env0_first_0:                 episode reward: -53.9000,                 loss: 1.6259
env0_second_0:                 episode reward: 53.9000,                 loss: 1.0253
env1_first_0:                 episode reward: -76.6000,                 loss: nan
env1_second_0:                 episode reward: 76.6000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 290.3,                last time consumption/overall running time: 262.9629s / 129174.1626 s
env0_first_0:                 episode reward: -34.6000,                 loss: 1.4812
env0_second_0:                 episode reward: 34.6000,                 loss: 0.9976
env1_first_0:                 episode reward: -55.6000,                 loss: nan
env1_second_0:                 episode reward: 55.6000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 280.85,                last time consumption/overall running time: 255.3387s / 129429.5013 s
env0_first_0:                 episode reward: -73.3000,                 loss: 1.4168
env0_second_0:                 episode reward: 73.3000,                 loss: 0.9998
env1_first_0:                 episode reward: -51.2500,                 loss: nan
env1_second_0:                 episode reward: 51.2500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 278.1,                last time consumption/overall running time: 252.4872s / 129681.9885 s
env0_first_0:                 episode reward: -59.4000,                 loss: 1.3648
env0_second_0:                 episode reward: 59.4000,                 loss: 0.9517
env1_first_0:                 episode reward: -34.5500,                 loss: nan
env1_second_0:                 episode reward: 34.5500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 302.05,                last time consumption/overall running time: 274.2203s / 129956.2087 s
env0_first_0:                 episode reward: -68.7500,                 loss: 1.3769
env0_second_0:                 episode reward: 68.7500,                 loss: 0.9708
env1_first_0:                 episode reward: -43.8500,                 loss: nan
env1_second_0:                 episode reward: 43.8500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 314.5,                last time consumption/overall running time: 283.9245s / 130240.1332 s
env0_first_0:                 episode reward: -65.2500,                 loss: 1.3783
env0_second_0:                 episode reward: 65.2500,                 loss: 0.9727
env1_first_0:                 episode reward: -37.5500,                 loss: nan
env1_second_0:                 episode reward: 37.5500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 274.65,                last time consumption/overall running time: 248.7652s / 130488.8985 s
env0_first_0:                 episode reward: -24.2500,                 loss: 1.4539
env0_second_0:                 episode reward: 24.2500,                 loss: 0.9584
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 265.05,                last time consumption/overall running time: 240.7306s / 130729.6290 s
env0_first_0:                 episode reward: -63.6500,                 loss: 1.4530
env0_second_0:                 episode reward: 63.6500,                 loss: 0.9830
env1_first_0:                 episode reward: -83.7500,                 loss: nan
env1_second_0:                 episode reward: 83.7500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 257.0,                last time consumption/overall running time: 233.7943s / 130963.4234 s
env0_first_0:                 episode reward: -62.3500,                 loss: 1.5062
env0_second_0:                 episode reward: 62.3500,                 loss: 0.9728
env1_first_0:                 episode reward: -79.5500,                 loss: nan
env1_second_0:                 episode reward: 79.5500,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 268.05,                last time consumption/overall running time: 243.3137s / 131206.7371 s
env0_first_0:                 episode reward: -77.3000,                 loss: 1.4175
env0_second_0:                 episode reward: 77.3000,                 loss: 0.9444
env1_first_0:                 episode reward: -50.8500,                 loss: nan
env1_second_0:                 episode reward: 50.8500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 248.1,                last time consumption/overall running time: 225.3703s / 131432.1074 s
env0_first_0:                 episode reward: -69.4500,                 loss: 1.3685
env0_second_0:                 episode reward: 69.4500,                 loss: 0.9449
env1_first_0:                 episode reward: -80.5500,                 loss: nan
env1_second_0:                 episode reward: 80.5500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 295.25,                last time consumption/overall running time: 267.3931s / 131699.5005 s
env0_first_0:                 episode reward: -41.6500,                 loss: 1.3716
env0_second_0:                 episode reward: 41.6500,                 loss: 0.9162
env1_first_0:                 episode reward: -38.5500,                 loss: nan
env1_second_0:                 episode reward: 38.5500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 245.95,                last time consumption/overall running time: 222.9830s / 131922.4835 s
env0_first_0:                 episode reward: -75.0500,                 loss: 1.3348
env0_second_0:                 episode reward: 75.0500,                 loss: 0.8772
env1_first_0:                 episode reward: -60.1500,                 loss: nan
env1_second_0:                 episode reward: 60.1500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 263.0,                last time consumption/overall running time: 238.4537s / 132160.9372 s
env0_first_0:                 episode reward: -66.6000,                 loss: 1.2811
env0_second_0:                 episode reward: 66.6000,                 loss: 0.8647
env1_first_0:                 episode reward: -64.7000,                 loss: nan
env1_second_0:                 episode reward: 64.7000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 257.9,                last time consumption/overall running time: 234.7426s / 132395.6798 s
env0_first_0:                 episode reward: -69.4000,                 loss: 1.2578
env0_second_0:                 episode reward: 69.4000,                 loss: 0.8845
env1_first_0:                 episode reward: -79.3500,                 loss: nan
env1_second_0:                 episode reward: 79.3500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 252.6,                last time consumption/overall running time: 228.7313s / 132624.4111 s
env0_first_0:                 episode reward: -62.8500,                 loss: 1.2365
env0_second_0:                 episode reward: 62.8500,                 loss: 0.8387
env1_first_0:                 episode reward: -72.4000,                 loss: nan
env1_second_0:                 episode reward: 72.4000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 262.4,                last time consumption/overall running time: 237.6793s / 132862.0904 s
env0_first_0:                 episode reward: -63.4000,                 loss: 1.2019
env0_second_0:                 episode reward: 63.4000,                 loss: 0.8564
env1_first_0:                 episode reward: -45.5500,                 loss: nan
env1_second_0:                 episode reward: 45.5500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 252.15,                last time consumption/overall running time: 228.5319s / 133090.6223 s
env0_first_0:                 episode reward: -42.0000,                 loss: 1.2506
env0_second_0:                 episode reward: 42.0000,                 loss: 0.8667
env1_first_0:                 episode reward: -51.7000,                 loss: nan
env1_second_0:                 episode reward: 51.7000,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 289.45,                last time consumption/overall running time: 261.4801s / 133352.1023 s
env0_first_0:                 episode reward: -50.0000,                 loss: 1.3205
env0_second_0:                 episode reward: 50.0000,                 loss: 0.9392
env1_first_0:                 episode reward: -24.3000,                 loss: nan
env1_second_0:                 episode reward: 24.3000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 349.25,                last time consumption/overall running time: 316.7162s / 133668.8186 s
env0_first_0:                 episode reward: -60.9500,                 loss: 1.3344
env0_second_0:                 episode reward: 60.9500,                 loss: 0.9408
env1_first_0:                 episode reward: -34.3000,                 loss: nan
env1_second_0:                 episode reward: 34.3000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 290.5,                last time consumption/overall running time: 263.5873s / 133932.4058 s
env0_first_0:                 episode reward: -45.2500,                 loss: 1.3109
env0_second_0:                 episode reward: 45.2500,                 loss: 0.9392
env1_first_0:                 episode reward: -47.7500,                 loss: nan
env1_second_0:                 episode reward: 47.7500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 545.0,                last time consumption/overall running time: 494.6745s / 134427.0803 s
env0_first_0:                 episode reward: -46.1000,                 loss: 1.3000
env0_second_0:                 episode reward: 46.1000,                 loss: 0.9185
env1_first_0:                 episode reward: -42.3000,                 loss: nan
env1_second_0:                 episode reward: 42.3000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 346.4,                last time consumption/overall running time: 314.3539s / 134741.4342 s
env0_first_0:                 episode reward: -36.7000,                 loss: 1.3399
env0_second_0:                 episode reward: 36.7000,                 loss: 0.9345
env1_first_0:                 episode reward: -75.4000,                 loss: nan
env1_second_0:                 episode reward: 75.4000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 394.2,                last time consumption/overall running time: 357.5666s / 135099.0008 s
env0_first_0:                 episode reward: -51.2500,                 loss: 1.4667
env0_second_0:                 episode reward: 51.2500,                 loss: 1.0140
env1_first_0:                 episode reward: -69.5000,                 loss: nan
env1_second_0:                 episode reward: 69.5000,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 314.55,                last time consumption/overall running time: 285.1040s / 135384.1048 s
env0_first_0:                 episode reward: -50.1000,                 loss: 1.5528
env0_second_0:                 episode reward: 50.1000,                 loss: 1.0967
env1_first_0:                 episode reward: -53.4500,                 loss: nan
env1_second_0:                 episode reward: 53.4500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 283.75,                last time consumption/overall running time: 258.1215s / 135642.2264 s
env0_first_0:                 episode reward: -58.5000,                 loss: 1.5334
env0_second_0:                 episode reward: 58.5000,                 loss: 1.1293
env1_first_0:                 episode reward: -49.6000,                 loss: nan
env1_second_0:                 episode reward: 49.6000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 310.5,                last time consumption/overall running time: 281.9904s / 135924.2168 s
env0_first_0:                 episode reward: -46.6000,                 loss: 1.5740
env0_second_0:                 episode reward: 46.6000,                 loss: 1.1933
env1_first_0:                 episode reward: -32.1000,                 loss: nan
env1_second_0:                 episode reward: 32.1000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 467.4,                last time consumption/overall running time: 423.1870s / 136347.4037 s
env0_first_0:                 episode reward: -22.1000,                 loss: 1.6181
env0_second_0:                 episode reward: 22.1000,                 loss: 1.2518
env1_first_0:                 episode reward: -41.2000,                 loss: nan
env1_second_0:                 episode reward: 41.2000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 360.5,                last time consumption/overall running time: 326.5358s / 136673.9395 s
env0_first_0:                 episode reward: -59.6500,                 loss: 1.6077
env0_second_0:                 episode reward: 59.6500,                 loss: 1.2219
env1_first_0:                 episode reward: -34.7500,                 loss: nan
env1_second_0:                 episode reward: 34.7500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 268.45,                last time consumption/overall running time: 244.3744s / 136918.3139 s
env0_first_0:                 episode reward: -61.4000,                 loss: 1.6622
env0_second_0:                 episode reward: 61.4000,                 loss: 1.2399
env1_first_0:                 episode reward: -39.3000,                 loss: nan
env1_second_0:                 episode reward: 39.3000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 275.1,                last time consumption/overall running time: 250.2533s / 137168.5672 s
env0_first_0:                 episode reward: -75.0500,                 loss: 1.6538
env0_second_0:                 episode reward: 75.0500,                 loss: 1.2194
env1_first_0:                 episode reward: -71.4500,                 loss: nan
env1_second_0:                 episode reward: 71.4500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 340.45,                last time consumption/overall running time: 309.4953s / 137478.0624 s
env0_first_0:                 episode reward: -34.7500,                 loss: 1.5933
env0_second_0:                 episode reward: 34.7500,                 loss: 1.1227
env1_first_0:                 episode reward: -10.4500,                 loss: nan
env1_second_0:                 episode reward: 10.4500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 279.15,                last time consumption/overall running time: 253.3239s / 137731.3864 s
env0_first_0:                 episode reward: -41.7500,                 loss: 1.5872
env0_second_0:                 episode reward: 41.7500,                 loss: 1.1364
env1_first_0:                 episode reward: -72.2000,                 loss: nan
env1_second_0:                 episode reward: 72.2000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 268.85,                last time consumption/overall running time: 243.1564s / 137974.5428 s
env0_first_0:                 episode reward: -46.4000,                 loss: 1.5779
env0_second_0:                 episode reward: 46.4000,                 loss: 1.1412
env1_first_0:                 episode reward: -74.1000,                 loss: nan
env1_second_0:                 episode reward: 74.1000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 298.5,                last time consumption/overall running time: 270.9984s / 138245.5412 s
env0_first_0:                 episode reward: -60.5500,                 loss: 1.5439
env0_second_0:                 episode reward: 60.5500,                 loss: 1.0961
env1_first_0:                 episode reward: -54.5500,                 loss: nan
env1_second_0:                 episode reward: 54.5500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 268.0,                last time consumption/overall running time: 243.8018s / 138489.3430 s
env0_first_0:                 episode reward: -60.2000,                 loss: 1.5441
env0_second_0:                 episode reward: 60.2000,                 loss: 1.0774
env1_first_0:                 episode reward: -48.4000,                 loss: nan
env1_second_0:                 episode reward: 48.4000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 354.2,                last time consumption/overall running time: 321.2210s / 138810.5640 s
env0_first_0:                 episode reward: -39.3000,                 loss: 1.6673
env0_second_0:                 episode reward: 39.3000,                 loss: 1.0956
env1_first_0:                 episode reward: -45.5500,                 loss: nan
env1_second_0:                 episode reward: 45.5500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 301.4,                last time consumption/overall running time: 273.0729s / 139083.6369 s
env0_first_0:                 episode reward: -80.6500,                 loss: 1.6271
env0_second_0:                 episode reward: 80.6500,                 loss: 1.1221
env1_first_0:                 episode reward: -52.5500,                 loss: nan
env1_second_0:                 episode reward: 52.5500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 321.8,                last time consumption/overall running time: 291.6350s / 139375.2719 s
env0_first_0:                 episode reward: -48.0000,                 loss: 1.6056
env0_second_0:                 episode reward: 48.0000,                 loss: 1.0614
env1_first_0:                 episode reward: -60.9000,                 loss: nan
env1_second_0:                 episode reward: 60.9000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 278.05,                last time consumption/overall running time: 251.8443s / 139627.1161 s
env0_first_0:                 episode reward: -69.3500,                 loss: 1.6282
env0_second_0:                 episode reward: 69.3500,                 loss: 1.0570
env1_first_0:                 episode reward: -63.3500,                 loss: nan
env1_second_0:                 episode reward: 63.3500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 284.0,                last time consumption/overall running time: 257.8226s / 139884.9387 s
env0_first_0:                 episode reward: -54.6000,                 loss: 1.6636
env0_second_0:                 episode reward: 54.6000,                 loss: 1.1072
env1_first_0:                 episode reward: -74.4000,                 loss: nan
env1_second_0:                 episode reward: 74.4000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 297.0,                last time consumption/overall running time: 269.3060s / 140154.2447 s
env0_first_0:                 episode reward: -64.3000,                 loss: 1.6212
env0_second_0:                 episode reward: 64.3000,                 loss: 1.1161
env1_first_0:                 episode reward: -66.9500,                 loss: nan
env1_second_0:                 episode reward: 66.9500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 291.1,                last time consumption/overall running time: 263.8258s / 140418.0705 s
env0_first_0:                 episode reward: -39.8500,                 loss: 1.6910
env0_second_0:                 episode reward: 39.8500,                 loss: 1.1075
env1_first_0:                 episode reward: -50.0000,                 loss: nan
env1_second_0:                 episode reward: 50.0000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 272.5,                last time consumption/overall running time: 246.7393s / 140664.8098 sLoad boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: -69.6000,                 loss: 1.7192
env0_second_0:                 episode reward: 69.6000,                 loss: 1.1136
env1_first_0:                 episode reward: -67.3500,                 loss: nan
env1_second_0:                 episode reward: 67.3500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 248.6,                last time consumption/overall running time: 224.9914s / 140889.8013 s
env0_first_0:                 episode reward: -48.8000,                 loss: 1.6797
env0_second_0:                 episode reward: 48.8000,                 loss: 1.1048
env1_first_0:                 episode reward: -68.7000,                 loss: nan
env1_second_0:                 episode reward: 68.7000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 303.4,                last time consumption/overall running time: 274.5769s / 141164.3781 s
env0_first_0:                 episode reward: -64.1000,                 loss: 1.7115
env0_second_0:                 episode reward: 64.1000,                 loss: 1.1163
env1_first_0:                 episode reward: -48.6500,                 loss: nan
env1_second_0:                 episode reward: 48.6500,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 295.2,                last time consumption/overall running time: 266.9422s / 141431.3204 s
env0_first_0:                 episode reward: -47.3500,                 loss: 1.7759
env0_second_0:                 episode reward: 47.3500,                 loss: 1.1499
env1_first_0:                 episode reward: -36.8500,                 loss: nan
env1_second_0:                 episode reward: 36.8500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 252.95,                last time consumption/overall running time: 228.7740s / 141660.0943 s
env0_first_0:                 episode reward: -51.0500,                 loss: 1.7767
env0_second_0:                 episode reward: 51.0500,                 loss: 1.1203
env1_first_0:                 episode reward: -42.8500,                 loss: nan
env1_second_0:                 episode reward: 42.8500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 256.95,                last time consumption/overall running time: 234.0011s / 141894.0955 s
env0_first_0:                 episode reward: -56.0500,                 loss: 1.7497
env0_second_0:                 episode reward: 56.0500,                 loss: 1.1457
env1_first_0:                 episode reward: -49.9000,                 loss: nan
env1_second_0:                 episode reward: 49.9000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 265.75,                last time consumption/overall running time: 241.0009s / 142135.0963 s
env0_first_0:                 episode reward: -64.4000,                 loss: 1.7749
env0_second_0:                 episode reward: 64.4000,                 loss: 1.1092
env1_first_0:                 episode reward: -83.9000,                 loss: nan
env1_second_0:                 episode reward: 83.9000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 334.25,                last time consumption/overall running time: 302.5712s / 142437.6676 s
env0_first_0:                 episode reward: -65.7500,                 loss: 1.7022
env0_second_0:                 episode reward: 65.7500,                 loss: 1.0673
env1_first_0:                 episode reward: -67.0500,                 loss: nan
env1_second_0:                 episode reward: 67.0500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 290.15,                last time consumption/overall running time: 263.9874s / 142701.6550 s
env0_first_0:                 episode reward: -52.4000,                 loss: 1.5270
env0_second_0:                 episode reward: 52.4000,                 loss: 1.0336
env1_first_0:                 episode reward: -59.4500,                 loss: nan
env1_second_0:                 episode reward: 59.4500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 271.1,                last time consumption/overall running time: 246.2282s / 142947.8832 s
env0_first_0:                 episode reward: -47.1000,                 loss: 1.5283
env0_second_0:                 episode reward: 47.1000,                 loss: 1.0377
env1_first_0:                 episode reward: -65.2000,                 loss: nan
env1_second_0:                 episode reward: 65.2000,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 310.8,                last time consumption/overall running time: 281.6553s / 143229.5385 s
env0_first_0:                 episode reward: -50.0000,                 loss: 1.6058
env0_second_0:                 episode reward: 50.0000,                 loss: 1.0647
env1_first_0:                 episode reward: -60.2500,                 loss: nan
env1_second_0:                 episode reward: 60.2500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 326.05,                last time consumption/overall running time: 295.5447s / 143525.0832 s
env0_first_0:                 episode reward: -69.9000,                 loss: 1.6468
env0_second_0:                 episode reward: 69.9000,                 loss: 1.0740
env1_first_0:                 episode reward: -14.3500,                 loss: nan
env1_second_0:                 episode reward: 14.3500,                 loss: nan
