pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 42.0, (1,), float32) action space: Discrete(6)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f6b94029da0>
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 1.0, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.0, 'eps_decay': 10000}, 'num_process': 1, 'batch_size': 640, 'max_episodes': 50100, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 2.0, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2.
Episode: 1/50100 (0.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6673s / 0.6673 s
agent0:                 episode reward: -2.2009,                 loss: nan
agent1:                 episode reward: 2.2009,                 loss: nan
Episode: 21/50100 (0.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0857s / 0.7529 s
agent0:                 episode reward: -0.4937,                 loss: nan
agent1:                 episode reward: 0.4937,                 loss: nan
Episode: 41/50100 (0.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0874s / 0.8404 s
agent0:                 episode reward: -0.1408,                 loss: nan
agent1:                 episode reward: 0.1408,                 loss: nan
Episode: 61/50100 (0.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0889s / 0.9293 s
agent0:                 episode reward: 0.4266,                 loss: nan
agent1:                 episode reward: -0.4266,                 loss: nan
Episode: 81/50100 (0.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0949s / 1.0242 s
agent0:                 episode reward: -0.4623,                 loss: nan
agent1:                 episode reward: 0.4623,                 loss: nan
Episode: 101/50100 (0.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0918s / 1.1160 s
agent0:                 episode reward: -0.1103,                 loss: nan
agent1:                 episode reward: 0.1103,                 loss: nan
Episode: 121/50100 (0.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 0.5228s / 1.6388 s
agent0:                 episode reward: 0.0457,                 loss: 0.4355
agent1:                 episode reward: -0.0457,                 loss: nan
Episode: 141/50100 (0.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6805s / 2.3193 s
agent0:                 episode reward: 0.0116,                 loss: 0.4080
agent1:                 episode reward: -0.0116,                 loss: nan
Episode: 161/50100 (0.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6681s / 2.9874 s
agent0:                 episode reward: 0.1167,                 loss: 0.3959
agent1:                 episode reward: -0.1167,                 loss: nan
Episode: 181/50100 (0.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6915s / 3.6789 s
agent0:                 episode reward: 0.3863,                 loss: 0.3824
agent1:                 episode reward: -0.3863,                 loss: nan
Episode: 201/50100 (0.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6748s / 4.3538 s
agent0:                 episode reward: 0.2727,                 loss: 0.3640
agent1:                 episode reward: -0.2727,                 loss: nan
Episode: 221/50100 (0.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6886s / 5.0423 s
agent0:                 episode reward: 0.0677,                 loss: 0.3441
agent1:                 episode reward: -0.0677,                 loss: nan
Episode: 241/50100 (0.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6890s / 5.7313 s
agent0:                 episode reward: 0.0868,                 loss: 0.3265
agent1:                 episode reward: -0.0868,                 loss: nan
Episode: 261/50100 (0.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6889s / 6.4202 s
agent0:                 episode reward: 0.1360,                 loss: 0.3120
agent1:                 episode reward: -0.1360,                 loss: nan
Episode: 281/50100 (0.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7057s / 7.1259 s
agent0:                 episode reward: -0.5359,                 loss: 0.3409
agent1:                 episode reward: 0.5359,                 loss: nan
Episode: 301/50100 (0.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 0.6973s / 7.8232 s
agent0:                 episode reward: -0.2965,                 loss: 0.3761
agent1:                 episode reward: 0.2965,                 loss: nan
Episode: 321/50100 (0.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 0.2889s / 8.1121 s
agent0:                 episode reward: 0.5842,                 loss: 0.3695
agent1:                 episode reward: -0.5842,                 loss: nan
Score delta: 2.090367028056692, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/201_0.
Episode: 341/50100 (0.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 0.1151s / 8.2272 s
agent0:                 episode reward: 0.3241,                 loss: nan
agent1:                 episode reward: -0.3241,                 loss: nan
Episode: 361/50100 (0.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 0.1126s / 8.3397 s
agent0:                 episode reward: 0.1121,                 loss: nan
agent1:                 episode reward: -0.1121,                 loss: nan
Episode: 381/50100 (0.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0985s / 8.4382 s
agent0:                 episode reward: 0.0953,                 loss: nan
agent1:                 episode reward: -0.0953,                 loss: nan
Episode: 401/50100 (0.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 0.0898s / 8.5280 s
agent0:                 episode reward: 0.0587,                 loss: nan
agent1:                 episode reward: -0.0587,                 loss: nan
Episode: 421/50100 (0.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 0.3607s / 8.8887 s
agent0:                 episode reward: -0.2187,                 loss: nan
agent1:                 episode reward: 0.2187,                 loss: 0.4658
Episode: 441/50100 (0.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7926s / 9.6813 s
agent0:                 episode reward: 0.2263,                 loss: nan
agent1:                 episode reward: -0.2263,                 loss: 0.4438
Episode: 461/50100 (0.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7722s / 10.4535 s
agent0:                 episode reward: 0.0378,                 loss: nan
agent1:                 episode reward: -0.0378,                 loss: 0.4407
Episode: 481/50100 (0.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8076s / 11.2611 s
agent0:                 episode reward: 0.2130,                 loss: nan
agent1:                 episode reward: -0.2130,                 loss: 0.4384
Episode: 501/50100 (1.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8428s / 12.1038 s
agent0:                 episode reward: -0.1646,                 loss: nan
agent1:                 episode reward: 0.1646,                 loss: 0.4368
Episode: 521/50100 (1.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7656s / 12.8694 s
agent0:                 episode reward: -0.2210,                 loss: nan
agent1:                 episode reward: 0.2210,                 loss: 0.4381
Episode: 541/50100 (1.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7711s / 13.6405 s
agent0:                 episode reward: -0.1118,                 loss: nan
agent1:                 episode reward: 0.1118,                 loss: 0.4368
Episode: 561/50100 (1.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7689s / 14.4094 s
agent0:                 episode reward: -0.2607,                 loss: nan
agent1:                 episode reward: 0.2607,                 loss: 0.4363
Episode: 581/50100 (1.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7737s / 15.1831 s
agent0:                 episode reward: 0.2360,                 loss: nan
agent1:                 episode reward: -0.2360,                 loss: 0.4370
Episode: 601/50100 (1.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7950s / 15.9781 s
agent0:                 episode reward: -0.3977,                 loss: 0.3655
agent1:                 episode reward: 0.3977,                 loss: 0.4398
Score delta: 2.038731032551361, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/381_1.
Episode: 621/50100 (1.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8068s / 16.7849 s
agent0:                 episode reward: -0.5025,                 loss: 0.3667
agent1:                 episode reward: 0.5025,                 loss: nan
Episode: 641/50100 (1.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8459s / 17.6308 s
agent0:                 episode reward: -0.2254,                 loss: 0.3690
agent1:                 episode reward: 0.2254,                 loss: nan
Episode: 661/50100 (1.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8001s / 18.4308 s
agent0:                 episode reward: -0.4907,                 loss: 0.3686
agent1:                 episode reward: 0.4907,                 loss: nan
Episode: 681/50100 (1.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8097s / 19.2405 s
agent0:                 episode reward: -0.2900,                 loss: 0.3676
agent1:                 episode reward: 0.2900,                 loss: nan
Episode: 701/50100 (1.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8187s / 20.0592 s
agent0:                 episode reward: -0.0623,                 loss: 0.3683
agent1:                 episode reward: 0.0623,                 loss: nan
Episode: 721/50100 (1.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8290s / 20.8883 s
agent0:                 episode reward: -0.5737,                 loss: 0.3682
agent1:                 episode reward: 0.5737,                 loss: nan
Episode: 741/50100 (1.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7925s / 21.6808 s
agent0:                 episode reward: -0.3434,                 loss: 0.3639
agent1:                 episode reward: 0.3434,                 loss: nan
Episode: 761/50100 (1.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7943s / 22.4751 s
agent0:                 episode reward: -0.3921,                 loss: 0.3465
agent1:                 episode reward: 0.3921,                 loss: nan
Episode: 781/50100 (1.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7934s / 23.2685 s
agent0:                 episode reward: 0.1729,                 loss: 0.3425
agent1:                 episode reward: -0.1729,                 loss: nan
Episode: 801/50100 (1.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8469s / 24.1155 s
agent0:                 episode reward: -0.0644,                 loss: 0.3422
agent1:                 episode reward: 0.0644,                 loss: nan
Episode: 821/50100 (1.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7670s / 24.8825 s
agent0:                 episode reward: 0.3663,                 loss: 0.3389
agent1:                 episode reward: -0.3663,                 loss: 0.4393
Score delta: 2.0101304034015763, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/601_0.
Episode: 841/50100 (1.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7811s / 25.6636 s
agent0:                 episode reward: -0.0494,                 loss: nan
agent1:                 episode reward: 0.0494,                 loss: 0.4369
Episode: 861/50100 (1.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7551s / 26.4187 s
agent0:                 episode reward: 0.0139,                 loss: nan
agent1:                 episode reward: -0.0139,                 loss: 0.4355
Episode: 881/50100 (1.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7755s / 27.1942 s
agent0:                 episode reward: 0.3837,                 loss: nan
agent1:                 episode reward: -0.3837,                 loss: 0.4356
Episode: 901/50100 (1.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7791s / 27.9734 s
agent0:                 episode reward: -0.1779,                 loss: nan
agent1:                 episode reward: 0.1779,                 loss: 0.4363
Episode: 921/50100 (1.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7717s / 28.7451 s
agent0:                 episode reward: -0.1209,                 loss: nan
agent1:                 episode reward: 0.1209,                 loss: 0.4362
Episode: 941/50100 (1.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7856s / 29.5307 s
agent0:                 episode reward: 0.1511,                 loss: nan
agent1:                 episode reward: -0.1511,                 loss: 0.4353
Episode: 961/50100 (1.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7797s / 30.3104 s
agent0:                 episode reward: -0.6936,                 loss: 0.3783
agent1:                 episode reward: 0.6936,                 loss: 0.4348
Score delta: 2.053261350826549, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/741_1.
Episode: 981/50100 (1.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7905s / 31.1009 s
agent0:                 episode reward: 0.0402,                 loss: 0.3782
agent1:                 episode reward: -0.0402,                 loss: nan
Episode: 1001/50100 (1.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7900s / 31.8909 s
agent0:                 episode reward: -0.5588,                 loss: 0.3755
agent1:                 episode reward: 0.5588,                 loss: nan
Episode: 1021/50100 (2.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7968s / 32.6877 s
agent0:                 episode reward: 0.0194,                 loss: 0.3749
agent1:                 episode reward: -0.0194,                 loss: nan
Episode: 1041/50100 (2.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7855s / 33.4732 s
agent0:                 episode reward: -0.3915,                 loss: 0.3843
agent1:                 episode reward: 0.3915,                 loss: nan
Episode: 1061/50100 (2.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8234s / 34.2966 s
agent0:                 episode reward: -0.0135,                 loss: 0.3698
agent1:                 episode reward: 0.0135,                 loss: nan
Episode: 1081/50100 (2.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7812s / 35.0779 s
agent0:                 episode reward: 0.0427,                 loss: 0.3675
agent1:                 episode reward: -0.0427,                 loss: nan
Episode: 1101/50100 (2.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7894s / 35.8673 s
agent0:                 episode reward: 0.1690,                 loss: 0.3616
agent1:                 episode reward: -0.1690,                 loss: nan
Episode: 1121/50100 (2.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7861s / 36.6534 s
agent0:                 episode reward: -0.2530,                 loss: 0.3582
agent1:                 episode reward: 0.2530,                 loss: nan
Episode: 1141/50100 (2.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8086s / 37.4620 s
agent0:                 episode reward: -0.5401,                 loss: 0.3553
agent1:                 episode reward: 0.5401,                 loss: nan
Episode: 1161/50100 (2.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7972s / 38.2592 s
agent0:                 episode reward: -0.4769,                 loss: 0.3571
agent1:                 episode reward: 0.4769,                 loss: nan
Episode: 1181/50100 (2.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7781s / 39.0372 s
agent0:                 episode reward: -0.5437,                 loss: 0.3549
agent1:                 episode reward: 0.5437,                 loss: nan
Episode: 1201/50100 (2.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8023s / 39.8396 s
agent0:                 episode reward: 0.2345,                 loss: 0.3574
agent1:                 episode reward: -0.2345,                 loss: nan
Episode: 1221/50100 (2.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8003s / 40.6398 s
agent0:                 episode reward: 0.2878,                 loss: 0.3572
agent1:                 episode reward: -0.2878,                 loss: nan
Episode: 1241/50100 (2.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7840s / 41.4239 s
agent0:                 episode reward: -0.1364,                 loss: 0.3528
agent1:                 episode reward: 0.1364,                 loss: nan
Episode: 1261/50100 (2.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7944s / 42.2183 s
agent0:                 episode reward: 0.1874,                 loss: 0.3519
agent1:                 episode reward: -0.1874,                 loss: nan
Episode: 1281/50100 (2.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8020s / 43.0202 s
agent0:                 episode reward: 0.1145,                 loss: 0.3513
agent1:                 episode reward: -0.1145,                 loss: nan
Episode: 1301/50100 (2.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7916s / 43.8119 s
agent0:                 episode reward: 0.0706,                 loss: 0.3521
agent1:                 episode reward: -0.0706,                 loss: nan
Episode: 1321/50100 (2.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8072s / 44.6191 s
agent0:                 episode reward: -0.4687,                 loss: 0.3541
agent1:                 episode reward: 0.4687,                 loss: nan
Episode: 1341/50100 (2.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8167s / 45.4358 s
agent0:                 episode reward: 0.1821,                 loss: 0.3510
agent1:                 episode reward: -0.1821,                 loss: nan
Episode: 1361/50100 (2.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8087s / 46.2445 s
agent0:                 episode reward: -0.8363,                 loss: 0.3511
agent1:                 episode reward: 0.8363,                 loss: nan
Episode: 1381/50100 (2.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8214s / 47.0658 s
agent0:                 episode reward: 0.4539,                 loss: 0.3942
agent1:                 episode reward: -0.4539,                 loss: nan
Episode: 1401/50100 (2.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8268s / 47.8927 s
agent0:                 episode reward: -0.5413,                 loss: 0.4035
agent1:                 episode reward: 0.5413,                 loss: nan
Episode: 1421/50100 (2.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8112s / 48.7039 s
agent0:                 episode reward: -0.2851,                 loss: 0.4029
agent1:                 episode reward: 0.2851,                 loss: nan
Episode: 1441/50100 (2.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7998s / 49.5037 s
agent0:                 episode reward: 0.5704,                 loss: 0.3990
agent1:                 episode reward: -0.5704,                 loss: 0.4398
Score delta: 2.194753913452705, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/1220_0.
Episode: 1461/50100 (2.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7685s / 50.2722 s
agent0:                 episode reward: 0.3598,                 loss: nan
agent1:                 episode reward: -0.3598,                 loss: 0.4424
Episode: 1481/50100 (2.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7706s / 51.0428 s
agent0:                 episode reward: 0.0660,                 loss: nan
agent1:                 episode reward: -0.0660,                 loss: 0.4424
Episode: 1501/50100 (2.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7689s / 51.8117 s
agent0:                 episode reward: 0.0478,                 loss: nan
agent1:                 episode reward: -0.0478,                 loss: 0.4423
Episode: 1521/50100 (3.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7928s / 52.6045 s
agent0:                 episode reward: 0.1649,                 loss: nan
agent1:                 episode reward: -0.1649,                 loss: 0.4420
Episode: 1541/50100 (3.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8017s / 53.4062 s
agent0:                 episode reward: -0.7699,                 loss: 0.3850
agent1:                 episode reward: 0.7699,                 loss: 0.4414
Score delta: 2.076619894612872, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/1320_1.
Episode: 1561/50100 (3.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8193s / 54.2255 s
agent0:                 episode reward: -0.4934,                 loss: 0.3780
agent1:                 episode reward: 0.4934,                 loss: nan
Episode: 1581/50100 (3.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8244s / 55.0498 s
agent0:                 episode reward: -0.4282,                 loss: 0.3776
agent1:                 episode reward: 0.4282,                 loss: nan
Episode: 1601/50100 (3.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8289s / 55.8788 s
agent0:                 episode reward: 0.1101,                 loss: 0.3736
agent1:                 episode reward: -0.1101,                 loss: nan
Episode: 1621/50100 (3.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8336s / 56.7124 s
agent0:                 episode reward: -0.5138,                 loss: 0.3743
agent1:                 episode reward: 0.5138,                 loss: nan
Episode: 1641/50100 (3.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8315s / 57.5439 s
agent0:                 episode reward: 0.0399,                 loss: 0.3854
agent1:                 episode reward: -0.0399,                 loss: nan
Episode: 1661/50100 (3.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8280s / 58.3719 s
agent0:                 episode reward: -0.4872,                 loss: 0.3770
agent1:                 episode reward: 0.4872,                 loss: nan
Episode: 1681/50100 (3.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8375s / 59.2093 s
agent0:                 episode reward: -0.0137,                 loss: 0.3705
agent1:                 episode reward: 0.0137,                 loss: nan
Episode: 1701/50100 (3.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8351s / 60.0444 s
agent0:                 episode reward: 0.1441,                 loss: 0.3684
agent1:                 episode reward: -0.1441,                 loss: nan
Episode: 1721/50100 (3.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8320s / 60.8764 s
agent0:                 episode reward: 0.0262,                 loss: 0.3658
agent1:                 episode reward: -0.0262,                 loss: nan
Episode: 1741/50100 (3.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8368s / 61.7132 s
agent0:                 episode reward: 0.0661,                 loss: 0.3628
agent1:                 episode reward: -0.0661,                 loss: nan
Episode: 1761/50100 (3.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8263s / 62.5395 s
agent0:                 episode reward: -0.0465,                 loss: 0.3639
agent1:                 episode reward: 0.0465,                 loss: nan
Episode: 1781/50100 (3.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8565s / 63.3960 s
agent0:                 episode reward: 0.2676,                 loss: 0.3624
agent1:                 episode reward: -0.2676,                 loss: nan
Episode: 1801/50100 (3.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8279s / 64.2239 s
agent0:                 episode reward: 0.0549,                 loss: 0.3627
agent1:                 episode reward: -0.0549,                 loss: nan
Episode: 1821/50100 (3.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8470s / 65.0709 s
agent0:                 episode reward: 0.1215,                 loss: 0.3596
agent1:                 episode reward: -0.1215,                 loss: nan
Episode: 1841/50100 (3.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8505s / 65.9214 s
agent0:                 episode reward: -0.4144,                 loss: 0.3566
agent1:                 episode reward: 0.4144,                 loss: nan
Episode: 1861/50100 (3.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8446s / 66.7661 s
agent0:                 episode reward: -0.3542,                 loss: 0.3555
agent1:                 episode reward: 0.3542,                 loss: nan
Episode: 1881/50100 (3.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8558s / 67.6219 s
agent0:                 episode reward: 0.4860,                 loss: 0.3539
agent1:                 episode reward: -0.4860,                 loss: nan
Episode: 1901/50100 (3.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8455s / 68.4674 s
agent0:                 episode reward: 0.0905,                 loss: 0.3537
agent1:                 episode reward: -0.0905,                 loss: nan
Episode: 1921/50100 (3.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8271s / 69.2945 s
agent0:                 episode reward: 0.1439,                 loss: 0.3536
agent1:                 episode reward: -0.1439,                 loss: nan
Episode: 1941/50100 (3.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8339s / 70.1284 s
agent0:                 episode reward: 0.0614,                 loss: 0.3502
agent1:                 episode reward: -0.0614,                 loss: nan
Episode: 1961/50100 (3.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8316s / 70.9600 s
agent0:                 episode reward: 0.2925,                 loss: 0.3505
agent1:                 episode reward: -0.2925,                 loss: nan
Episode: 1981/50100 (3.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8424s / 71.8024 s
agent0:                 episode reward: 0.2972,                 loss: 0.3897
agent1:                 episode reward: -0.2972,                 loss: nan
Episode: 2001/50100 (3.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8304s / 72.6328 s
agent0:                 episode reward: 0.0709,                 loss: 0.3963
agent1:                 episode reward: -0.0709,                 loss: nan
Episode: 2021/50100 (4.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8473s / 73.4802 s
agent0:                 episode reward: 0.4987,                 loss: 0.3966
agent1:                 episode reward: -0.4987,                 loss: nan
Episode: 2041/50100 (4.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8423s / 74.3225 s
agent0:                 episode reward: 0.4176,                 loss: 0.3940
agent1:                 episode reward: -0.4176,                 loss: nan
Episode: 2061/50100 (4.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8306s / 75.1531 s
agent0:                 episode reward: -0.1655,                 loss: 0.3932
agent1:                 episode reward: 0.1655,                 loss: nan
Episode: 2081/50100 (4.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8500s / 76.0031 s
agent0:                 episode reward: -0.0042,                 loss: 0.3966
agent1:                 episode reward: 0.0042,                 loss: nan
Episode: 2101/50100 (4.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9059s / 76.9090 s
agent0:                 episode reward: -0.3514,                 loss: 0.3941
agent1:                 episode reward: 0.3514,                 loss: nan
Episode: 2121/50100 (4.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8694s / 77.7784 s
agent0:                 episode reward: -0.0535,                 loss: 0.3930
agent1:                 episode reward: 0.0535,                 loss: nan
Episode: 2141/50100 (4.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8776s / 78.6560 s
agent0:                 episode reward: -0.0316,                 loss: 0.3948
agent1:                 episode reward: 0.0316,                 loss: nan
Episode: 2161/50100 (4.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8548s / 79.5108 s
agent0:                 episode reward: -0.4321,                 loss: 0.3784
agent1:                 episode reward: 0.4321,                 loss: nan
Episode: 2181/50100 (4.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8835s / 80.3943 s
agent0:                 episode reward: -0.1102,                 loss: 0.3763
agent1:                 episode reward: 0.1102,                 loss: nan
Episode: 2201/50100 (4.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9104s / 81.3046 s
agent0:                 episode reward: 0.5860,                 loss: 0.3782
agent1:                 episode reward: -0.5860,                 loss: nan
Episode: 2221/50100 (4.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8615s / 82.1661 s
agent0:                 episode reward: 0.2575,                 loss: 0.3765
agent1:                 episode reward: -0.2575,                 loss: nan
Episode: 2241/50100 (4.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8632s / 83.0294 s
agent0:                 episode reward: 0.4196,                 loss: 0.3762
agent1:                 episode reward: -0.4196,                 loss: nan
Episode: 2261/50100 (4.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8633s / 83.8926 s
agent0:                 episode reward: -0.1345,                 loss: 0.3740
agent1:                 episode reward: 0.1345,                 loss: nan
Episode: 2281/50100 (4.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8655s / 84.7582 s
agent0:                 episode reward: 0.3396,                 loss: 0.3753
agent1:                 episode reward: -0.3396,                 loss: nan
Episode: 2301/50100 (4.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8832s / 85.6414 s
agent0:                 episode reward: 0.1591,                 loss: 0.3756
agent1:                 episode reward: -0.1591,                 loss: nan
Episode: 2321/50100 (4.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8819s / 86.5233 s
agent0:                 episode reward: -0.3823,                 loss: 0.3222
agent1:                 episode reward: 0.3823,                 loss: nan
Episode: 2341/50100 (4.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8756s / 87.3989 s
agent0:                 episode reward: 0.6205,                 loss: 0.3209
agent1:                 episode reward: -0.6205,                 loss: nan
Episode: 2361/50100 (4.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9086s / 88.3074 s
agent0:                 episode reward: -0.5804,                 loss: 0.3201
agent1:                 episode reward: 0.5804,                 loss: nan
Episode: 2381/50100 (4.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8727s / 89.1801 s
agent0:                 episode reward: -0.0465,                 loss: 0.3180
agent1:                 episode reward: 0.0465,                 loss: nan
Episode: 2401/50100 (4.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8872s / 90.0673 s
agent0:                 episode reward: 0.2469,                 loss: 0.3202
agent1:                 episode reward: -0.2469,                 loss: nan
Episode: 2421/50100 (4.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8845s / 90.9518 s
agent0:                 episode reward: 0.3581,                 loss: 0.3195
agent1:                 episode reward: -0.3581,                 loss: nan
Episode: 2441/50100 (4.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8324s / 91.7842 s
agent0:                 episode reward: 0.3079,                 loss: 0.3212
agent1:                 episode reward: -0.3079,                 loss: 0.4435
Score delta: 2.0787705338109577, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/2217_0.
Episode: 2461/50100 (4.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8267s / 92.6109 s
agent0:                 episode reward: -0.0218,                 loss: nan
agent1:                 episode reward: 0.0218,                 loss: 0.4426
Episode: 2481/50100 (4.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8874s / 93.4983 s
agent0:                 episode reward: 0.2361,                 loss: nan
agent1:                 episode reward: -0.2361,                 loss: 0.4423
Episode: 2501/50100 (4.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8174s / 94.3157 s
agent0:                 episode reward: 0.2823,                 loss: nan
agent1:                 episode reward: -0.2823,                 loss: 0.4424
Episode: 2521/50100 (5.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8285s / 95.1441 s
agent0:                 episode reward: -0.1364,                 loss: nan
agent1:                 episode reward: 0.1364,                 loss: 0.4442
Episode: 2541/50100 (5.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8353s / 95.9795 s
agent0:                 episode reward: -0.1153,                 loss: nan
agent1:                 episode reward: 0.1153,                 loss: 0.4447
Episode: 2561/50100 (5.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8344s / 96.8139 s
agent0:                 episode reward: -0.4905,                 loss: nan
agent1:                 episode reward: 0.4905,                 loss: 0.4443
Episode: 2581/50100 (5.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8269s / 97.6408 s
agent0:                 episode reward: -0.1901,                 loss: nan
agent1:                 episode reward: 0.1901,                 loss: 0.4453
Episode: 2601/50100 (5.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8129s / 98.4537 s
agent0:                 episode reward: 0.2082,                 loss: nan
agent1:                 episode reward: -0.2082,                 loss: 0.4446
Episode: 2621/50100 (5.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8243s / 99.2780 s
agent0:                 episode reward: -0.2315,                 loss: nan
agent1:                 episode reward: 0.2315,                 loss: 0.4443
Episode: 2641/50100 (5.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8231s / 100.1010 s
agent0:                 episode reward: -0.1512,                 loss: nan
agent1:                 episode reward: 0.1512,                 loss: 0.4455
Episode: 2661/50100 (5.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8217s / 100.9228 s
agent0:                 episode reward: 0.2203,                 loss: nan
agent1:                 episode reward: -0.2203,                 loss: 0.4452
Episode: 2681/50100 (5.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8261s / 101.7489 s
agent0:                 episode reward: -0.0450,                 loss: nan
agent1:                 episode reward: 0.0450,                 loss: 0.4451
Episode: 2701/50100 (5.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8286s / 102.5775 s
agent0:                 episode reward: -0.3593,                 loss: nan
agent1:                 episode reward: 0.3593,                 loss: 0.4449
Episode: 2721/50100 (5.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8295s / 103.4070 s
agent0:                 episode reward: -0.4448,                 loss: nan
agent1:                 episode reward: 0.4448,                 loss: 0.4461
Episode: 2741/50100 (5.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8916s / 104.2986 s
agent0:                 episode reward: -0.3940,                 loss: 0.3198
agent1:                 episode reward: 0.3940,                 loss: 0.4471
Score delta: 2.215417985283621, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/2511_1.
Episode: 2761/50100 (5.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8877s / 105.1863 s
agent0:                 episode reward: -0.0918,                 loss: 0.3241
agent1:                 episode reward: 0.0918,                 loss: nan
Episode: 2781/50100 (5.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8895s / 106.0758 s
agent0:                 episode reward: 0.1874,                 loss: 0.3478
agent1:                 episode reward: -0.1874,                 loss: nan
Episode: 2801/50100 (5.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9354s / 107.0112 s
agent0:                 episode reward: -0.2325,                 loss: 0.3447
agent1:                 episode reward: 0.2325,                 loss: nan
Episode: 2821/50100 (5.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8713s / 107.8825 s
agent0:                 episode reward: 0.1869,                 loss: 0.3429
agent1:                 episode reward: -0.1869,                 loss: nan
Episode: 2841/50100 (5.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9024s / 108.7849 s
agent0:                 episode reward: -0.0098,                 loss: 0.3411
agent1:                 episode reward: 0.0098,                 loss: nan
Episode: 2861/50100 (5.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8770s / 109.6619 s
agent0:                 episode reward: 0.3115,                 loss: 0.3423
agent1:                 episode reward: -0.3115,                 loss: nan
Episode: 2881/50100 (5.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8771s / 110.5390 s
agent0:                 episode reward: 0.2410,                 loss: 0.3425
agent1:                 episode reward: -0.2410,                 loss: 0.4491
Score delta: 2.0865128451988952, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/2667_0.
Episode: 2901/50100 (5.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8028s / 111.3418 s
agent0:                 episode reward: -0.8446,                 loss: 0.3392
agent1:                 episode reward: 0.8446,                 loss: 0.4441
Score delta: 2.6497516419795004, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/2688_1.
Episode: 2921/50100 (5.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8886s / 112.2304 s
agent0:                 episode reward: -0.1492,                 loss: 0.3384
agent1:                 episode reward: 0.1492,                 loss: nan
Episode: 2941/50100 (5.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8798s / 113.1102 s
agent0:                 episode reward: -0.1315,                 loss: 0.3412
agent1:                 episode reward: 0.1315,                 loss: nan
Episode: 2961/50100 (5.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8774s / 113.9875 s
agent0:                 episode reward: -0.4077,                 loss: 0.3481
agent1:                 episode reward: 0.4077,                 loss: nan
Episode: 2981/50100 (5.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8752s / 114.8628 s
agent0:                 episode reward: -0.3770,                 loss: 0.3313
agent1:                 episode reward: 0.3770,                 loss: nan
Episode: 3001/50100 (5.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8457s / 115.7085 s
agent0:                 episode reward: 0.5849,                 loss: 0.3282
agent1:                 episode reward: -0.5849,                 loss: 0.4465
Score delta: 2.2975496389498424, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/2781_0.
Episode: 3021/50100 (6.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8160s / 116.5245 s
agent0:                 episode reward: -0.0796,                 loss: nan
agent1:                 episode reward: 0.0796,                 loss: 0.4467
Episode: 3041/50100 (6.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8233s / 117.3478 s
agent0:                 episode reward: -0.2526,                 loss: nan
agent1:                 episode reward: 0.2526,                 loss: 0.4468
Episode: 3061/50100 (6.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8193s / 118.1671 s
agent0:                 episode reward: -0.0842,                 loss: nan
agent1:                 episode reward: 0.0842,                 loss: 0.4457
Episode: 3081/50100 (6.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8690s / 119.0361 s
agent0:                 episode reward: -0.7652,                 loss: 0.3835
agent1:                 episode reward: 0.7652,                 loss: 0.4464
Score delta: 2.097830574247457, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/2860_1.
Episode: 3101/50100 (6.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9040s / 119.9401 s
agent0:                 episode reward: -0.8136,                 loss: 0.3729
agent1:                 episode reward: 0.8136,                 loss: nan
Episode: 3121/50100 (6.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9162s / 120.8564 s
agent0:                 episode reward: 0.4061,                 loss: 0.3701
agent1:                 episode reward: -0.4061,                 loss: nan
Episode: 3141/50100 (6.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8885s / 121.7448 s
agent0:                 episode reward: -0.7616,                 loss: 0.3673
agent1:                 episode reward: 0.7616,                 loss: nan
Episode: 3161/50100 (6.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9032s / 122.6480 s
agent0:                 episode reward: 0.4648,                 loss: 0.3641
agent1:                 episode reward: -0.4648,                 loss: nan
Episode: 3181/50100 (6.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9082s / 123.5562 s
agent0:                 episode reward: -0.8497,                 loss: 0.3660
agent1:                 episode reward: 0.8497,                 loss: nan
Episode: 3201/50100 (6.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9598s / 124.5160 s
agent0:                 episode reward: -0.4662,                 loss: 0.3762
agent1:                 episode reward: 0.4662,                 loss: nan
Episode: 3221/50100 (6.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8983s / 125.4143 s
agent0:                 episode reward: -0.1987,                 loss: 0.3689
agent1:                 episode reward: 0.1987,                 loss: nan
Episode: 3241/50100 (6.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8942s / 126.3085 s
agent0:                 episode reward: 0.3389,                 loss: 0.3638
agent1:                 episode reward: -0.3389,                 loss: nan
Episode: 3261/50100 (6.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9177s / 127.2262 s
agent0:                 episode reward: -0.1526,                 loss: 0.3611
agent1:                 episode reward: 0.1526,                 loss: nan
Episode: 3281/50100 (6.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9262s / 128.1524 s
agent0:                 episode reward: -0.3629,                 loss: 0.3602
agent1:                 episode reward: 0.3629,                 loss: nan
Episode: 3301/50100 (6.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9008s / 129.0532 s
agent0:                 episode reward: 0.1486,                 loss: 0.3576
agent1:                 episode reward: -0.1486,                 loss: nan
Episode: 3321/50100 (6.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9015s / 129.9547 s
agent0:                 episode reward: -0.0200,                 loss: 0.3564
agent1:                 episode reward: 0.0200,                 loss: nan
Episode: 3341/50100 (6.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8938s / 130.8485 s
agent0:                 episode reward: -0.5316,                 loss: 0.3548
agent1:                 episode reward: 0.5316,                 loss: nan
Episode: 3361/50100 (6.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9126s / 131.7612 s
agent0:                 episode reward: -0.1601,                 loss: 0.3539
agent1:                 episode reward: 0.1601,                 loss: nan
Episode: 3381/50100 (6.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9168s / 132.6779 s
agent0:                 episode reward: -0.3861,                 loss: 0.3326
agent1:                 episode reward: 0.3861,                 loss: nan
Episode: 3401/50100 (6.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9177s / 133.5957 s
agent0:                 episode reward: 0.1581,                 loss: 0.3225
agent1:                 episode reward: -0.1581,                 loss: nan
Episode: 3421/50100 (6.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9067s / 134.5024 s
agent0:                 episode reward: -0.1838,                 loss: 0.3199
agent1:                 episode reward: 0.1838,                 loss: nan
Episode: 3441/50100 (6.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9105s / 135.4129 s
agent0:                 episode reward: 0.2138,                 loss: 0.3168
agent1:                 episode reward: -0.2138,                 loss: nan
Episode: 3461/50100 (6.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8984s / 136.3114 s
agent0:                 episode reward: 0.4336,                 loss: 0.3164
agent1:                 episode reward: -0.4336,                 loss: nan
Episode: 3481/50100 (6.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9000s / 137.2114 s
agent0:                 episode reward: -0.4749,                 loss: 0.3135
agent1:                 episode reward: 0.4749,                 loss: nan
Episode: 3501/50100 (6.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9006s / 138.1121 s
agent0:                 episode reward: -0.1397,                 loss: 0.3158
agent1:                 episode reward: 0.1397,                 loss: nan
Episode: 3521/50100 (7.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9907s / 139.1028 s
agent0:                 episode reward: 0.1184,                 loss: 0.3169
agent1:                 episode reward: -0.1184,                 loss: nan
Episode: 3541/50100 (7.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9254s / 140.0282 s
agent0:                 episode reward: 0.0647,                 loss: 0.3654
agent1:                 episode reward: -0.0647,                 loss: nan
Episode: 3561/50100 (7.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9430s / 140.9712 s
agent0:                 episode reward: -0.2489,                 loss: 0.3773
agent1:                 episode reward: 0.2489,                 loss: nan
Episode: 3581/50100 (7.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9928s / 141.9640 s
agent0:                 episode reward: -0.2834,                 loss: 0.3775
agent1:                 episode reward: 0.2834,                 loss: nan
Episode: 3601/50100 (7.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9797s / 142.9437 s
agent0:                 episode reward: 0.0616,                 loss: 0.3773
agent1:                 episode reward: -0.0616,                 loss: nan
Episode: 3621/50100 (7.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9522s / 143.8959 s
agent0:                 episode reward: 0.2032,                 loss: 0.3772
agent1:                 episode reward: -0.2032,                 loss: nan
Episode: 3641/50100 (7.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9872s / 144.8831 s
agent0:                 episode reward: -0.2520,                 loss: 0.3762
agent1:                 episode reward: 0.2520,                 loss: nan
Episode: 3661/50100 (7.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0518s / 145.9349 s
agent0:                 episode reward: -0.0963,                 loss: 0.3762
agent1:                 episode reward: 0.0963,                 loss: nan
Episode: 3681/50100 (7.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9613s / 146.8962 s
agent0:                 episode reward: 0.0201,                 loss: 0.3756
agent1:                 episode reward: -0.0201,                 loss: nan
Episode: 3701/50100 (7.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9814s / 147.8776 s
agent0:                 episode reward: 0.0658,                 loss: 0.3893
agent1:                 episode reward: -0.0658,                 loss: nan
Episode: 3721/50100 (7.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9718s / 148.8493 s
agent0:                 episode reward: -0.2167,                 loss: 0.3960
agent1:                 episode reward: 0.2167,                 loss: nan
Episode: 3741/50100 (7.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9753s / 149.8246 s
agent0:                 episode reward: 0.7530,                 loss: 0.3937
agent1:                 episode reward: -0.7530,                 loss: nan
Episode: 3761/50100 (7.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8765s / 150.7012 s
agent0:                 episode reward: -0.2688,                 loss: 0.3922
agent1:                 episode reward: 0.2688,                 loss: 0.4444
Score delta: 2.2090831470367487, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/3531_0.
Episode: 3781/50100 (7.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8271s / 151.5283 s
agent0:                 episode reward: -0.1719,                 loss: nan
agent1:                 episode reward: 0.1719,                 loss: 0.4476
Episode: 3801/50100 (7.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9347s / 152.4630 s
agent0:                 episode reward: -0.6457,                 loss: 0.3754
agent1:                 episode reward: 0.6457,                 loss: 0.4483
Score delta: 2.108607419469159, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/3571_1.
Episode: 3821/50100 (7.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9446s / 153.4076 s
agent0:                 episode reward: -0.3630,                 loss: 0.3682
agent1:                 episode reward: 0.3630,                 loss: nan
Episode: 3841/50100 (7.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9368s / 154.3444 s
agent0:                 episode reward: -0.3758,                 loss: 0.3672
agent1:                 episode reward: 0.3758,                 loss: nan
Episode: 3861/50100 (7.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9558s / 155.3002 s
agent0:                 episode reward: -0.0836,                 loss: 0.3644
agent1:                 episode reward: 0.0836,                 loss: nan
Episode: 3881/50100 (7.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9807s / 156.2809 s
agent0:                 episode reward: -0.4142,                 loss: 0.3645
agent1:                 episode reward: 0.4142,                 loss: nan
Episode: 3901/50100 (7.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9812s / 157.2621 s
agent0:                 episode reward: 0.2550,                 loss: 0.3672
agent1:                 episode reward: -0.2550,                 loss: nan
Episode: 3921/50100 (7.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9463s / 158.2084 s
agent0:                 episode reward: -0.5686,                 loss: 0.3615
agent1:                 episode reward: 0.5686,                 loss: nan
Episode: 3941/50100 (7.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9328s / 159.1412 s
agent0:                 episode reward: -0.7731,                 loss: 0.3517
agent1:                 episode reward: 0.7731,                 loss: nan
Episode: 3961/50100 (7.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0069s / 160.1481 s
agent0:                 episode reward: 0.1515,                 loss: 0.3468
agent1:                 episode reward: -0.1515,                 loss: nan
Episode: 3981/50100 (7.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9380s / 161.0861 s
agent0:                 episode reward: 0.0249,                 loss: 0.3443
agent1:                 episode reward: -0.0249,                 loss: nan
Episode: 4001/50100 (7.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9200s / 162.0062 s
agent0:                 episode reward: 0.2020,                 loss: 0.3431
agent1:                 episode reward: -0.2020,                 loss: nan
Episode: 4021/50100 (8.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9421s / 162.9483 s
agent0:                 episode reward: -0.3816,                 loss: 0.3394
agent1:                 episode reward: 0.3816,                 loss: nan
Episode: 4041/50100 (8.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9375s / 163.8858 s
agent0:                 episode reward: 0.1463,                 loss: 0.3378
agent1:                 episode reward: -0.1463,                 loss: nan
Episode: 4061/50100 (8.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9479s / 164.8338 s
agent0:                 episode reward: 0.0689,                 loss: 0.3377
agent1:                 episode reward: -0.0689,                 loss: nan
Episode: 4081/50100 (8.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9686s / 165.8023 s
agent0:                 episode reward: -0.4675,                 loss: 0.3353
agent1:                 episode reward: 0.4675,                 loss: nan
Episode: 4101/50100 (8.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9491s / 166.7514 s
agent0:                 episode reward: 0.7018,                 loss: 0.3192
agent1:                 episode reward: -0.7018,                 loss: nan
Episode: 4121/50100 (8.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9473s / 167.6987 s
agent0:                 episode reward: 0.0345,                 loss: 0.3173
agent1:                 episode reward: -0.0345,                 loss: nan
Episode: 4141/50100 (8.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9369s / 168.6356 s
agent0:                 episode reward: -0.5464,                 loss: 0.3187
agent1:                 episode reward: 0.5464,                 loss: nan
Episode: 4161/50100 (8.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9459s / 169.5815 s
agent0:                 episode reward: -0.4545,                 loss: 0.3168
agent1:                 episode reward: 0.4545,                 loss: nan
Episode: 4181/50100 (8.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9279s / 170.5094 s
agent0:                 episode reward: 0.1579,                 loss: 0.3143
agent1:                 episode reward: -0.1579,                 loss: nan
Episode: 4201/50100 (8.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8411s / 171.3505 s
agent0:                 episode reward: 0.3416,                 loss: 0.3147
agent1:                 episode reward: -0.3416,                 loss: 0.4443
Score delta: 2.016279187952019, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/3973_0.
Episode: 4221/50100 (8.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8906s / 172.2411 s
agent0:                 episode reward: -0.1107,                 loss: 0.3398
agent1:                 episode reward: 0.1107,                 loss: 0.4433
Score delta: 2.083521918110696, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/3998_1.
Episode: 4241/50100 (8.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9312s / 173.1723 s
agent0:                 episode reward: -0.0546,                 loss: 0.3425
agent1:                 episode reward: 0.0546,                 loss: nan
Episode: 4261/50100 (8.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9232s / 174.0955 s
agent0:                 episode reward: 0.1643,                 loss: 0.3509
agent1:                 episode reward: -0.1643,                 loss: nan
Episode: 4281/50100 (8.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9373s / 175.0328 s
agent0:                 episode reward: 0.0305,                 loss: 0.3315
agent1:                 episode reward: -0.0305,                 loss: nan
Episode: 4301/50100 (8.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9325s / 175.9654 s
agent0:                 episode reward: -0.1405,                 loss: 0.3244
agent1:                 episode reward: 0.1405,                 loss: nan
Episode: 4321/50100 (8.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9366s / 176.9020 s
agent0:                 episode reward: -0.3163,                 loss: 0.3210
agent1:                 episode reward: 0.3163,                 loss: nan
Episode: 4341/50100 (8.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9749s / 177.8770 s
agent0:                 episode reward: 0.1543,                 loss: 0.3194
agent1:                 episode reward: -0.1543,                 loss: nan
Episode: 4361/50100 (8.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9387s / 178.8156 s
agent0:                 episode reward: 0.4020,                 loss: 0.3200
agent1:                 episode reward: -0.4020,                 loss: nan
Episode: 4381/50100 (8.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9425s / 179.7582 s
agent0:                 episode reward: 0.0723,                 loss: 0.3171
agent1:                 episode reward: -0.0723,                 loss: nan
Episode: 4401/50100 (8.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9296s / 180.6878 s
agent0:                 episode reward: 0.4591,                 loss: 0.3141
agent1:                 episode reward: -0.4591,                 loss: nan
Episode: 4421/50100 (8.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9533s / 181.6410 s
agent0:                 episode reward: -0.2019,                 loss: 0.3135
agent1:                 episode reward: 0.2019,                 loss: nan
Episode: 4441/50100 (8.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9443s / 182.5853 s
agent0:                 episode reward: -0.4509,                 loss: 0.3509
agent1:                 episode reward: 0.4509,                 loss: nan
Episode: 4461/50100 (8.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9474s / 183.5327 s
agent0:                 episode reward: 0.4924,                 loss: 0.3556
agent1:                 episode reward: -0.4924,                 loss: nan
Episode: 4481/50100 (8.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9337s / 184.4664 s
agent0:                 episode reward: -0.0355,                 loss: 0.3552
agent1:                 episode reward: 0.0355,                 loss: nan
Episode: 4501/50100 (8.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9513s / 185.4177 s
agent0:                 episode reward: -0.4201,                 loss: 0.3533
agent1:                 episode reward: 0.4201,                 loss: nan
Episode: 4521/50100 (9.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9380s / 186.3558 s
agent0:                 episode reward: 0.0431,                 loss: 0.3534
agent1:                 episode reward: -0.0431,                 loss: nan
Episode: 4541/50100 (9.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9455s / 187.3012 s
agent0:                 episode reward: -0.1299,                 loss: 0.3527
agent1:                 episode reward: 0.1299,                 loss: nan
Episode: 4561/50100 (9.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9440s / 188.2452 s
agent0:                 episode reward: 0.4263,                 loss: 0.3534
agent1:                 episode reward: -0.4263,                 loss: nan
Episode: 4581/50100 (9.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9504s / 189.1956 s
agent0:                 episode reward: 0.1890,                 loss: 0.3521
agent1:                 episode reward: -0.1890,                 loss: nan
Episode: 4601/50100 (9.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8315s / 190.0271 s
agent0:                 episode reward: -0.0718,                 loss: 0.3560
agent1:                 episode reward: 0.0718,                 loss: 0.4478
Score delta: 2.025290069921659, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/4372_0.
Episode: 4621/50100 (9.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8052s / 190.8323 s
agent0:                 episode reward: -0.4855,                 loss: nan
agent1:                 episode reward: 0.4855,                 loss: 0.4473
Episode: 4641/50100 (9.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9287s / 191.7609 s
agent0:                 episode reward: -0.4701,                 loss: 0.3412
agent1:                 episode reward: 0.4701,                 loss: 0.4466
Score delta: 2.5673814562151254, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/4413_1.
Episode: 4661/50100 (9.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9363s / 192.6972 s
agent0:                 episode reward: -0.1715,                 loss: 0.3111
agent1:                 episode reward: 0.1715,                 loss: nan
Episode: 4681/50100 (9.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9481s / 193.6454 s
agent0:                 episode reward: 0.2941,                 loss: 0.3037
agent1:                 episode reward: -0.2941,                 loss: nan
Episode: 4701/50100 (9.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9453s / 194.5907 s
agent0:                 episode reward: 0.0621,                 loss: 0.3030
agent1:                 episode reward: -0.0621,                 loss: nan
Episode: 4721/50100 (9.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9356s / 195.5263 s
agent0:                 episode reward: -0.3162,                 loss: 0.3014
agent1:                 episode reward: 0.3162,                 loss: nan
Episode: 4741/50100 (9.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9593s / 196.4855 s
agent0:                 episode reward: 0.2241,                 loss: 0.3021
agent1:                 episode reward: -0.2241,                 loss: nan
Episode: 4761/50100 (9.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9534s / 197.4389 s
agent0:                 episode reward: -0.4664,                 loss: 0.3021
agent1:                 episode reward: 0.4664,                 loss: nan
Episode: 4781/50100 (9.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9530s / 198.3919 s
agent0:                 episode reward: -0.1192,                 loss: 0.2992
agent1:                 episode reward: 0.1192,                 loss: nan
Episode: 4801/50100 (9.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9591s / 199.3511 s
agent0:                 episode reward: -0.5998,                 loss: 0.3067
agent1:                 episode reward: 0.5998,                 loss: nan
Episode: 4821/50100 (9.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9538s / 200.3049 s
agent0:                 episode reward: -0.0348,                 loss: 0.3841
agent1:                 episode reward: 0.0348,                 loss: nan
Episode: 4841/50100 (9.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9667s / 201.2716 s
agent0:                 episode reward: -0.0112,                 loss: 0.3825
agent1:                 episode reward: 0.0112,                 loss: nan
Episode: 4861/50100 (9.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9583s / 202.2300 s
agent0:                 episode reward: -0.2482,                 loss: 0.3823
agent1:                 episode reward: 0.2482,                 loss: nan
Episode: 4881/50100 (9.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9760s / 203.2059 s
agent0:                 episode reward: 0.2454,                 loss: 0.3812
agent1:                 episode reward: -0.2454,                 loss: nan
Episode: 4901/50100 (9.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9672s / 204.1731 s
agent0:                 episode reward: -0.0935,                 loss: 0.3798
agent1:                 episode reward: 0.0935,                 loss: nan
Episode: 4921/50100 (9.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9714s / 205.1445 s
agent0:                 episode reward: -0.0134,                 loss: 0.3814
agent1:                 episode reward: 0.0134,                 loss: nan
Episode: 4941/50100 (9.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9653s / 206.1098 s
agent0:                 episode reward: 0.1436,                 loss: 0.3821
agent1:                 episode reward: -0.1436,                 loss: nan
Episode: 4961/50100 (9.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9725s / 207.0823 s
agent0:                 episode reward: -0.0703,                 loss: 0.3807
agent1:                 episode reward: 0.0703,                 loss: nan
Episode: 4981/50100 (9.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9884s / 208.0707 s
agent0:                 episode reward: 0.1582,                 loss: 0.4053
agent1:                 episode reward: -0.1582,                 loss: nan
Episode: 5001/50100 (9.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9680s / 209.0387 s
agent0:                 episode reward: -0.1863,                 loss: 0.4055
agent1:                 episode reward: 0.1863,                 loss: nan
Episode: 5021/50100 (10.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9658s / 210.0044 s
agent0:                 episode reward: 0.0316,                 loss: 0.4054
agent1:                 episode reward: -0.0316,                 loss: nan
Episode: 5041/50100 (10.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9612s / 210.9656 s
agent0:                 episode reward: -0.1245,                 loss: 0.4046
agent1:                 episode reward: 0.1245,                 loss: nan
Episode: 5061/50100 (10.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8646s / 211.8303 s
agent0:                 episode reward: 0.3285,                 loss: 0.4070
agent1:                 episode reward: -0.3285,                 loss: 0.4460
Score delta: 2.1272800324905474, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/4835_0.
Episode: 5081/50100 (10.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8004s / 212.6306 s
agent0:                 episode reward: -0.2970,                 loss: nan
agent1:                 episode reward: 0.2970,                 loss: 0.4458
Episode: 5101/50100 (10.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8046s / 213.4352 s
agent0:                 episode reward: -0.2152,                 loss: nan
agent1:                 episode reward: 0.2152,                 loss: 0.4454
Episode: 5121/50100 (10.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8015s / 214.2367 s
agent0:                 episode reward: -0.3087,                 loss: nan
agent1:                 episode reward: 0.3087,                 loss: 0.4445
Episode: 5141/50100 (10.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 0.7991s / 215.0358 s
agent0:                 episode reward: -0.3780,                 loss: nan
agent1:                 episode reward: 0.3780,                 loss: 0.4455
Episode: 5161/50100 (10.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9398s / 215.9756 s
agent0:                 episode reward: -0.4125,                 loss: 0.3966
agent1:                 episode reward: 0.4125,                 loss: 0.4458
Score delta: 2.0025914302929886, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/4933_1.
Episode: 5181/50100 (10.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9612s / 216.9368 s
agent0:                 episode reward: 0.1130,                 loss: 0.3951
agent1:                 episode reward: -0.1130,                 loss: nan
Episode: 5201/50100 (10.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9688s / 217.9056 s
agent0:                 episode reward: 0.1923,                 loss: 0.3978
agent1:                 episode reward: -0.1923,                 loss: nan
Episode: 5221/50100 (10.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8442s / 218.7498 s
agent0:                 episode reward: -0.2648,                 loss: 0.3961
agent1:                 episode reward: 0.2648,                 loss: 0.4448
Score delta: 2.249169840596312, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/4993_0.
Episode: 5241/50100 (10.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8119s / 219.5617 s
agent0:                 episode reward: 0.5959,                 loss: nan
agent1:                 episode reward: -0.5959,                 loss: 0.4437
Episode: 5261/50100 (10.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8286s / 220.3903 s
agent0:                 episode reward: -0.1736,                 loss: nan
agent1:                 episode reward: 0.1736,                 loss: 0.4450
Episode: 5281/50100 (10.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8434s / 221.2337 s
agent0:                 episode reward: -0.2569,                 loss: nan
agent1:                 episode reward: 0.2569,                 loss: 0.4444
Episode: 5301/50100 (10.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8462s / 222.0799 s
agent0:                 episode reward: -0.1989,                 loss: nan
agent1:                 episode reward: 0.1989,                 loss: 0.4437
Episode: 5321/50100 (10.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8373s / 222.9172 s
agent0:                 episode reward: 0.1368,                 loss: nan
agent1:                 episode reward: -0.1368,                 loss: 0.4443
Episode: 5341/50100 (10.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9295s / 223.8466 s
agent0:                 episode reward: -0.8653,                 loss: 0.3969
agent1:                 episode reward: 0.8653,                 loss: 0.4442
Score delta: 2.2660953819646013, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/5115_1.
Episode: 5361/50100 (10.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9779s / 224.8245 s
agent0:                 episode reward: 0.0894,                 loss: 0.3853
agent1:                 episode reward: -0.0894,                 loss: nan
Episode: 5381/50100 (10.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9638s / 225.7883 s
agent0:                 episode reward: -0.0700,                 loss: 0.3551
agent1:                 episode reward: 0.0700,                 loss: nan
Episode: 5401/50100 (10.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9976s / 226.7859 s
agent0:                 episode reward: -0.1556,                 loss: 0.3535
agent1:                 episode reward: 0.1556,                 loss: nan
Episode: 5421/50100 (10.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9818s / 227.7677 s
agent0:                 episode reward: -0.0960,                 loss: 0.3531
agent1:                 episode reward: 0.0960,                 loss: nan
Episode: 5441/50100 (10.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9606s / 228.7283 s
agent0:                 episode reward: -0.5373,                 loss: 0.3517
agent1:                 episode reward: 0.5373,                 loss: nan
Episode: 5461/50100 (10.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9812s / 229.7095 s
agent0:                 episode reward: 0.1311,                 loss: 0.3521
agent1:                 episode reward: -0.1311,                 loss: nan
Episode: 5481/50100 (10.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9674s / 230.6769 s
agent0:                 episode reward: 0.1700,                 loss: 0.3516
agent1:                 episode reward: -0.1700,                 loss: nan
Episode: 5501/50100 (10.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9689s / 231.6458 s
agent0:                 episode reward: -0.2582,                 loss: 0.3521
agent1:                 episode reward: 0.2582,                 loss: nan
Episode: 5521/50100 (11.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9797s / 232.6255 s
agent0:                 episode reward: -0.4328,                 loss: 0.3567
agent1:                 episode reward: 0.4328,                 loss: nan
Episode: 5541/50100 (11.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9639s / 233.5894 s
agent0:                 episode reward: 0.4875,                 loss: 0.3720
agent1:                 episode reward: -0.4875,                 loss: nan
Episode: 5561/50100 (11.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9880s / 234.5774 s
agent0:                 episode reward: -0.5940,                 loss: 0.3694
agent1:                 episode reward: 0.5940,                 loss: nan
Episode: 5581/50100 (11.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9887s / 235.5661 s
agent0:                 episode reward: 0.0730,                 loss: 0.3714
agent1:                 episode reward: -0.0730,                 loss: nan
Episode: 5601/50100 (11.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9769s / 236.5430 s
agent0:                 episode reward: 0.2546,                 loss: 0.3656
agent1:                 episode reward: -0.2546,                 loss: nan
Episode: 5621/50100 (11.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8367s / 237.3797 s
agent0:                 episode reward: 0.0205,                 loss: 0.3636
agent1:                 episode reward: -0.0205,                 loss: 0.4451
Score delta: 2.002467867305402, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/5390_0.
Episode: 5641/50100 (11.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8227s / 238.2024 s
agent0:                 episode reward: -0.1887,                 loss: nan
agent1:                 episode reward: 0.1887,                 loss: 0.4459
Episode: 5661/50100 (11.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8295s / 239.0319 s
agent0:                 episode reward: -0.1950,                 loss: nan
agent1:                 episode reward: 0.1950,                 loss: 0.4456
Episode: 5681/50100 (11.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8605s / 239.8924 s
agent0:                 episode reward: -0.4673,                 loss: 0.3161
agent1:                 episode reward: 0.4673,                 loss: 0.4450
Score delta: 2.0196494284861393, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/5465_1.
Episode: 5701/50100 (11.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9983s / 240.8907 s
agent0:                 episode reward: 0.0014,                 loss: 0.3192
agent1:                 episode reward: -0.0014,                 loss: nan
Episode: 5721/50100 (11.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9972s / 241.8879 s
agent0:                 episode reward: 0.0758,                 loss: 0.3185
agent1:                 episode reward: -0.0758,                 loss: nan
Episode: 5741/50100 (11.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0067s / 242.8946 s
agent0:                 episode reward: 0.3297,                 loss: 0.3160
agent1:                 episode reward: -0.3297,                 loss: nan
Episode: 5761/50100 (11.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0110s / 243.9056 s
agent0:                 episode reward: -0.0952,                 loss: 0.3163
agent1:                 episode reward: 0.0952,                 loss: nan
Episode: 5781/50100 (11.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9844s / 244.8900 s
agent0:                 episode reward: -0.4159,                 loss: 0.3552
agent1:                 episode reward: 0.4159,                 loss: nan
Episode: 5801/50100 (11.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0089s / 245.8989 s
agent0:                 episode reward: 0.2997,                 loss: 0.3491
agent1:                 episode reward: -0.2997,                 loss: nan
Episode: 5821/50100 (11.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9995s / 246.8984 s
agent0:                 episode reward: -0.0617,                 loss: 0.3494
agent1:                 episode reward: 0.0617,                 loss: nan
Episode: 5841/50100 (11.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0715s / 247.9700 s
agent0:                 episode reward: 0.1299,                 loss: 0.3487
agent1:                 episode reward: -0.1299,                 loss: nan
Episode: 5861/50100 (11.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0208s / 248.9908 s
agent0:                 episode reward: -0.3637,                 loss: 0.3501
agent1:                 episode reward: 0.3637,                 loss: nan
Episode: 5881/50100 (11.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0199s / 250.0107 s
agent0:                 episode reward: -0.4387,                 loss: 0.3482
agent1:                 episode reward: 0.4387,                 loss: nan
Episode: 5901/50100 (11.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0503s / 251.0610 s
agent0:                 episode reward: -0.0666,                 loss: 0.3468
agent1:                 episode reward: 0.0666,                 loss: nan
Episode: 5921/50100 (11.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0160s / 252.0770 s
agent0:                 episode reward: 0.5245,                 loss: 0.3477
agent1:                 episode reward: -0.5245,                 loss: 0.4480
Score delta: 2.089916138124142, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/5705_0.
Episode: 5941/50100 (11.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8500s / 252.9269 s
agent0:                 episode reward: -0.6147,                 loss: nan
agent1:                 episode reward: 0.6147,                 loss: 0.4460
Episode: 5961/50100 (11.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8738s / 253.8007 s
agent0:                 episode reward: -0.2148,                 loss: nan
agent1:                 episode reward: 0.2148,                 loss: 0.4446
Episode: 5981/50100 (11.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8467s / 254.6474 s
agent0:                 episode reward: -0.3462,                 loss: nan
agent1:                 episode reward: 0.3462,                 loss: 0.4441
Episode: 6001/50100 (11.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8656s / 255.5130 s
agent0:                 episode reward: -0.7484,                 loss: 0.3536
agent1:                 episode reward: 0.7484,                 loss: 0.4439
Score delta: 2.4830615404782352, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/5785_1.
Episode: 6021/50100 (12.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9986s / 256.5116 s
agent0:                 episode reward: -0.4367,                 loss: 0.3401
agent1:                 episode reward: 0.4367,                 loss: nan
Episode: 6041/50100 (12.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9883s / 257.4999 s
agent0:                 episode reward: -0.0065,                 loss: 0.3260
agent1:                 episode reward: 0.0065,                 loss: nan
Episode: 6061/50100 (12.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9862s / 258.4861 s
agent0:                 episode reward: 0.0188,                 loss: 0.3229
agent1:                 episode reward: -0.0188,                 loss: nan
Episode: 6081/50100 (12.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0069s / 259.4930 s
agent0:                 episode reward: -0.4140,                 loss: 0.3206
agent1:                 episode reward: 0.4140,                 loss: nan
Episode: 6101/50100 (12.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0046s / 260.4976 s
agent0:                 episode reward: 0.1532,                 loss: 0.3223
agent1:                 episode reward: -0.1532,                 loss: nan
Episode: 6121/50100 (12.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0131s / 261.5106 s
agent0:                 episode reward: -0.2447,                 loss: 0.3205
agent1:                 episode reward: 0.2447,                 loss: nan
Episode: 6141/50100 (12.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0274s / 262.5381 s
agent0:                 episode reward: 0.3227,                 loss: 0.3184
agent1:                 episode reward: -0.3227,                 loss: nan
Episode: 6161/50100 (12.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0227s / 263.5607 s
agent0:                 episode reward: 0.0514,                 loss: 0.3189
agent1:                 episode reward: -0.0514,                 loss: nan
Episode: 6181/50100 (12.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0756s / 264.6363 s
agent0:                 episode reward: -0.0928,                 loss: 0.3427
agent1:                 episode reward: 0.0928,                 loss: nan
Episode: 6201/50100 (12.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0093s / 265.6456 s
agent0:                 episode reward: -0.3053,                 loss: 0.3852
agent1:                 episode reward: 0.3053,                 loss: nan
Episode: 6221/50100 (12.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0101s / 266.6557 s
agent0:                 episode reward: -0.2729,                 loss: 0.3848
agent1:                 episode reward: 0.2729,                 loss: nan
Episode: 6241/50100 (12.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0155s / 267.6712 s
agent0:                 episode reward: -0.3817,                 loss: 0.3832
agent1:                 episode reward: 0.3817,                 loss: nan
Episode: 6261/50100 (12.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0065s / 268.6777 s
agent0:                 episode reward: 0.0235,                 loss: 0.3812
agent1:                 episode reward: -0.0235,                 loss: nan
Episode: 6281/50100 (12.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0072s / 269.6849 s
agent0:                 episode reward: 0.2115,                 loss: 0.3818
agent1:                 episode reward: -0.2115,                 loss: nan
Episode: 6301/50100 (12.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0032s / 270.6880 s
agent0:                 episode reward: -0.0770,                 loss: 0.3789
agent1:                 episode reward: 0.0770,                 loss: nan
Episode: 6321/50100 (12.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0470s / 271.7351 s
agent0:                 episode reward: 0.2714,                 loss: 0.3794
agent1:                 episode reward: -0.2714,                 loss: nan
Episode: 6341/50100 (12.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0450s / 272.7801 s
agent0:                 episode reward: 0.0895,                 loss: 0.3825
agent1:                 episode reward: -0.0895,                 loss: nan
Episode: 6361/50100 (12.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0286s / 273.8087 s
agent0:                 episode reward: -0.0263,                 loss: 0.4209
agent1:                 episode reward: 0.0263,                 loss: nan
Episode: 6381/50100 (12.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0436s / 274.8523 s
agent0:                 episode reward: -0.0607,                 loss: 0.4144
agent1:                 episode reward: 0.0607,                 loss: nan
Episode: 6401/50100 (12.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0039s / 275.8562 s
agent0:                 episode reward: -0.3278,                 loss: 0.4145
agent1:                 episode reward: 0.3278,                 loss: nan
Episode: 6421/50100 (12.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0354s / 276.8917 s
agent0:                 episode reward: -0.2489,                 loss: 0.4155
agent1:                 episode reward: 0.2489,                 loss: nan
Episode: 6441/50100 (12.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0267s / 277.9184 s
agent0:                 episode reward: 0.3287,                 loss: 0.4128
agent1:                 episode reward: -0.3287,                 loss: nan
Episode: 6461/50100 (12.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0537s / 278.9721 s
agent0:                 episode reward: -0.0489,                 loss: 0.4144
agent1:                 episode reward: 0.0489,                 loss: nan
Episode: 6481/50100 (12.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0706s / 280.0427 s
agent0:                 episode reward: -0.1117,                 loss: 0.4133
agent1:                 episode reward: 0.1117,                 loss: nan
Episode: 6501/50100 (12.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0143s / 281.0570 s
agent0:                 episode reward: -0.2493,                 loss: 0.4131
agent1:                 episode reward: 0.2493,                 loss: nan
Episode: 6521/50100 (13.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0215s / 282.0785 s
agent0:                 episode reward: -0.1389,                 loss: 0.3925
agent1:                 episode reward: 0.1389,                 loss: nan
Episode: 6541/50100 (13.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0219s / 283.1004 s
agent0:                 episode reward: -0.0021,                 loss: 0.3727
agent1:                 episode reward: 0.0021,                 loss: nan
Episode: 6561/50100 (13.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0121s / 284.1125 s
agent0:                 episode reward: 0.0617,                 loss: 0.3711
agent1:                 episode reward: -0.0617,                 loss: nan
Episode: 6581/50100 (13.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0252s / 285.1377 s
agent0:                 episode reward: 0.0853,                 loss: 0.3700
agent1:                 episode reward: -0.0853,                 loss: nan
Episode: 6601/50100 (13.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0276s / 286.1653 s
agent0:                 episode reward: 0.0695,                 loss: 0.3710
agent1:                 episode reward: -0.0695,                 loss: nan
Episode: 6621/50100 (13.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0617s / 287.2269 s
agent0:                 episode reward: -0.0873,                 loss: 0.3697
agent1:                 episode reward: 0.0873,                 loss: nan
Episode: 6641/50100 (13.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0464s / 288.2734 s
agent0:                 episode reward: 0.4865,                 loss: 0.3685
agent1:                 episode reward: -0.4865,                 loss: nan
Episode: 6661/50100 (13.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0971s / 289.3705 s
agent0:                 episode reward: -0.1717,                 loss: 0.3689
agent1:                 episode reward: 0.1717,                 loss: nan
Episode: 6681/50100 (13.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0302s / 290.4006 s
agent0:                 episode reward: -0.3597,                 loss: 0.3690
agent1:                 episode reward: 0.3597,                 loss: nan
Episode: 6701/50100 (13.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0179s / 291.4186 s
agent0:                 episode reward: -0.0444,                 loss: 0.3636
agent1:                 episode reward: 0.0444,                 loss: nan
Episode: 6721/50100 (13.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0260s / 292.4446 s
agent0:                 episode reward: 0.2089,                 loss: 0.3631
agent1:                 episode reward: -0.2089,                 loss: nan
Episode: 6741/50100 (13.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0290s / 293.4736 s
agent0:                 episode reward: 0.2357,                 loss: 0.3614
agent1:                 episode reward: -0.2357,                 loss: nan
Episode: 6761/50100 (13.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0151s / 294.4888 s
agent0:                 episode reward: -0.1744,                 loss: 0.3602
agent1:                 episode reward: 0.1744,                 loss: nan
Episode: 6781/50100 (13.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0923s / 295.5811 s
agent0:                 episode reward: -0.7930,                 loss: 0.3618
agent1:                 episode reward: 0.7930,                 loss: nan
Episode: 6801/50100 (13.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0529s / 296.6340 s
agent0:                 episode reward: -0.1421,                 loss: 0.3607
agent1:                 episode reward: 0.1421,                 loss: nan
Episode: 6821/50100 (13.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0492s / 297.6832 s
agent0:                 episode reward: -0.4408,                 loss: 0.3579
agent1:                 episode reward: 0.4408,                 loss: nan
Episode: 6841/50100 (13.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0365s / 298.7197 s
agent0:                 episode reward: 0.4562,                 loss: 0.3581
agent1:                 episode reward: -0.4562,                 loss: nan
Episode: 6861/50100 (13.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9330s / 299.6526 s
agent0:                 episode reward: 0.0343,                 loss: 0.4020
agent1:                 episode reward: -0.0343,                 loss: 0.4465
Score delta: 2.08029531623841, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/6631_0.
Episode: 6881/50100 (13.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8244s / 300.4770 s
agent0:                 episode reward: -0.4605,                 loss: nan
agent1:                 episode reward: 0.4605,                 loss: 0.4456
Episode: 6901/50100 (13.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8159s / 301.2930 s
agent0:                 episode reward: -0.2879,                 loss: nan
agent1:                 episode reward: 0.2879,                 loss: 0.4463
Episode: 6921/50100 (13.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8224s / 302.1153 s
agent0:                 episode reward: -0.0413,                 loss: nan
agent1:                 episode reward: 0.0413,                 loss: 0.4457
Episode: 6941/50100 (13.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8285s / 302.9438 s
agent0:                 episode reward: -0.4616,                 loss: nan
agent1:                 episode reward: 0.4616,                 loss: 0.4449
Episode: 6961/50100 (13.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8365s / 303.7804 s
agent0:                 episode reward: -0.0961,                 loss: nan
agent1:                 episode reward: 0.0961,                 loss: 0.4453
Episode: 6981/50100 (13.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8413s / 304.6216 s
agent0:                 episode reward: -0.2626,                 loss: nan
agent1:                 episode reward: 0.2626,                 loss: 0.4445
Episode: 7001/50100 (13.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8328s / 305.4544 s
agent0:                 episode reward: -0.2140,                 loss: nan
agent1:                 episode reward: 0.2140,                 loss: 0.4452
Episode: 7021/50100 (14.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8348s / 306.2892 s
agent0:                 episode reward: -0.3312,                 loss: nan
agent1:                 episode reward: 0.3312,                 loss: 0.4436
Episode: 7041/50100 (14.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8301s / 307.1193 s
agent0:                 episode reward: -0.2122,                 loss: nan
agent1:                 episode reward: 0.2122,                 loss: 0.4451
Episode: 7061/50100 (14.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8459s / 307.9652 s
agent0:                 episode reward: -0.4608,                 loss: nan
agent1:                 episode reward: 0.4608,                 loss: 0.4421
Episode: 7081/50100 (14.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8400s / 308.8052 s
agent0:                 episode reward: -0.6635,                 loss: nan
agent1:                 episode reward: 0.6635,                 loss: 0.4392
Episode: 7101/50100 (14.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8856s / 309.6909 s
agent0:                 episode reward: -0.6855,                 loss: 0.3481
agent1:                 episode reward: 0.6855,                 loss: 0.4391
Score delta: 2.5317339027050094, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/6886_1.
Episode: 7121/50100 (14.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0397s / 310.7305 s
agent0:                 episode reward: -0.4901,                 loss: 0.3489
agent1:                 episode reward: 0.4901,                 loss: nan
Episode: 7141/50100 (14.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0412s / 311.7717 s
agent0:                 episode reward: 0.4493,                 loss: 0.3475
agent1:                 episode reward: -0.4493,                 loss: nan
Episode: 7161/50100 (14.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0253s / 312.7970 s
agent0:                 episode reward: 0.3123,                 loss: 0.3470
agent1:                 episode reward: -0.3123,                 loss: nan
Episode: 7181/50100 (14.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0309s / 313.8280 s
agent0:                 episode reward: 0.2050,                 loss: 0.3468
agent1:                 episode reward: -0.2050,                 loss: nan
Episode: 7201/50100 (14.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0374s / 314.8654 s
agent0:                 episode reward: -0.0255,                 loss: 0.3449
agent1:                 episode reward: 0.0255,                 loss: nan
Episode: 7221/50100 (14.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0265s / 315.8918 s
agent0:                 episode reward: 0.2081,                 loss: 0.3472
agent1:                 episode reward: -0.2081,                 loss: nan
Episode: 7241/50100 (14.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0321s / 316.9239 s
agent0:                 episode reward: -0.1586,                 loss: 0.3472
agent1:                 episode reward: 0.1586,                 loss: nan
Episode: 7261/50100 (14.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0330s / 317.9569 s
agent0:                 episode reward: 0.0777,                 loss: 0.3462
agent1:                 episode reward: -0.0777,                 loss: nan
Episode: 7281/50100 (14.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0434s / 319.0004 s
agent0:                 episode reward: 0.2458,                 loss: 0.3937
agent1:                 episode reward: -0.2458,                 loss: nan
Episode: 7301/50100 (14.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0337s / 320.0340 s
agent0:                 episode reward: -0.1103,                 loss: 0.3938
agent1:                 episode reward: 0.1103,                 loss: nan
Episode: 7321/50100 (14.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0496s / 321.0836 s
agent0:                 episode reward: 0.1227,                 loss: 0.3938
agent1:                 episode reward: -0.1227,                 loss: nan
Episode: 7341/50100 (14.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0480s / 322.1316 s
agent0:                 episode reward: 0.0461,                 loss: 0.3934
agent1:                 episode reward: -0.0461,                 loss: nan
Episode: 7361/50100 (14.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0625s / 323.1942 s
agent0:                 episode reward: 0.0255,                 loss: 0.3942
agent1:                 episode reward: -0.0255,                 loss: nan
Episode: 7381/50100 (14.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0430s / 324.2372 s
agent0:                 episode reward: 0.1275,                 loss: 0.3943
agent1:                 episode reward: -0.1275,                 loss: nan
Episode: 7401/50100 (14.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0410s / 325.2782 s
agent0:                 episode reward: 0.2460,                 loss: 0.3935
agent1:                 episode reward: -0.2460,                 loss: nan
Episode: 7421/50100 (14.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0511s / 326.3292 s
agent0:                 episode reward: 0.2684,                 loss: 0.3944
agent1:                 episode reward: -0.2684,                 loss: nan
Episode: 7441/50100 (14.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0365s / 327.3657 s
agent0:                 episode reward: -0.4114,                 loss: 0.4065
agent1:                 episode reward: 0.4114,                 loss: nan
Episode: 7461/50100 (14.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0533s / 328.4190 s
agent0:                 episode reward: -0.0913,                 loss: 0.4064
agent1:                 episode reward: 0.0913,                 loss: nan
Episode: 7481/50100 (14.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0549s / 329.4739 s
agent0:                 episode reward: -0.0814,                 loss: 0.4077
agent1:                 episode reward: 0.0814,                 loss: nan
Episode: 7501/50100 (14.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0588s / 330.5327 s
agent0:                 episode reward: 0.1632,                 loss: 0.4054
agent1:                 episode reward: -0.1632,                 loss: nan
Episode: 7521/50100 (15.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0549s / 331.5876 s
agent0:                 episode reward: 0.2573,                 loss: 0.4052
agent1:                 episode reward: -0.2573,                 loss: nan
Episode: 7541/50100 (15.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0519s / 332.6395 s
agent0:                 episode reward: 0.0932,                 loss: 0.4050
agent1:                 episode reward: -0.0932,                 loss: nan
Episode: 7561/50100 (15.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0585s / 333.6980 s
agent0:                 episode reward: -0.1264,                 loss: 0.4035
agent1:                 episode reward: 0.1264,                 loss: nan
Episode: 7581/50100 (15.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0625s / 334.7605 s
agent0:                 episode reward: -0.2148,                 loss: 0.4042
agent1:                 episode reward: 0.2148,                 loss: nan
Episode: 7601/50100 (15.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0542s / 335.8147 s
agent0:                 episode reward: -0.1862,                 loss: 0.3920
agent1:                 episode reward: 0.1862,                 loss: nan
Episode: 7621/50100 (15.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1203s / 336.9350 s
agent0:                 episode reward: 0.0627,                 loss: 0.3336
agent1:                 episode reward: -0.0627,                 loss: nan
Episode: 7641/50100 (15.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0788s / 338.0137 s
agent0:                 episode reward: -0.4309,                 loss: 0.3306
agent1:                 episode reward: 0.4309,                 loss: nan
Episode: 7661/50100 (15.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1171s / 339.1308 s
agent0:                 episode reward: 0.2322,                 loss: 0.3303
agent1:                 episode reward: -0.2322,                 loss: nan
Episode: 7681/50100 (15.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0677s / 340.1985 s
agent0:                 episode reward: -0.1710,                 loss: 0.3259
agent1:                 episode reward: 0.1710,                 loss: nan
Episode: 7701/50100 (15.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0588s / 341.2573 s
agent0:                 episode reward: 0.1882,                 loss: 0.3294
agent1:                 episode reward: -0.1882,                 loss: nan
Episode: 7721/50100 (15.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0717s / 342.3290 s
agent0:                 episode reward: 0.0967,                 loss: 0.3281
agent1:                 episode reward: -0.0967,                 loss: nan
Episode: 7741/50100 (15.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0797s / 343.4087 s
agent0:                 episode reward: -0.2909,                 loss: 0.3285
agent1:                 episode reward: 0.2909,                 loss: nan
Episode: 7761/50100 (15.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0883s / 344.4970 s
agent0:                 episode reward: 0.1610,                 loss: 0.3292
agent1:                 episode reward: -0.1610,                 loss: nan
Episode: 7781/50100 (15.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0824s / 345.5794 s
agent0:                 episode reward: 0.3287,                 loss: 0.3619
agent1:                 episode reward: -0.3287,                 loss: nan
Episode: 7801/50100 (15.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0870s / 346.6665 s
agent0:                 episode reward: -0.2543,                 loss: 0.3523
agent1:                 episode reward: 0.2543,                 loss: nan
Episode: 7821/50100 (15.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0973s / 347.7637 s
agent0:                 episode reward: 0.2663,                 loss: 0.3568
agent1:                 episode reward: -0.2663,                 loss: nan
Episode: 7841/50100 (15.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0565s / 348.8202 s
agent0:                 episode reward: 0.7354,                 loss: 0.3547
agent1:                 episode reward: -0.7354,                 loss: 0.4459
Score delta: 2.3502229944858586, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/7625_0.
Episode: 7861/50100 (15.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8477s / 349.6679 s
agent0:                 episode reward: -0.2834,                 loss: nan
agent1:                 episode reward: 0.2834,                 loss: 0.4444
Episode: 7881/50100 (15.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8592s / 350.5271 s
agent0:                 episode reward: -0.2140,                 loss: nan
agent1:                 episode reward: 0.2140,                 loss: 0.4454
Episode: 7901/50100 (15.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8598s / 351.3869 s
agent0:                 episode reward: -0.1085,                 loss: nan
agent1:                 episode reward: 0.1085,                 loss: 0.4452
Episode: 7921/50100 (15.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8662s / 352.2531 s
agent0:                 episode reward: -0.0947,                 loss: nan
agent1:                 episode reward: 0.0947,                 loss: 0.4452
Episode: 7941/50100 (15.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9138s / 353.1669 s
agent0:                 episode reward: -0.0420,                 loss: nan
agent1:                 episode reward: 0.0420,                 loss: 0.4456
Episode: 7961/50100 (15.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8724s / 354.0392 s
agent0:                 episode reward: 0.0512,                 loss: nan
agent1:                 episode reward: -0.0512,                 loss: 0.4452
Episode: 7981/50100 (15.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8728s / 354.9120 s
agent0:                 episode reward: 0.0325,                 loss: nan
agent1:                 episode reward: -0.0325,                 loss: 0.4450
Episode: 8001/50100 (15.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8789s / 355.7909 s
agent0:                 episode reward: -0.5723,                 loss: nan
agent1:                 episode reward: 0.5723,                 loss: 0.4443
Episode: 8021/50100 (16.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0260s / 356.8169 s
agent0:                 episode reward: -0.5094,                 loss: 0.3244
agent1:                 episode reward: 0.5094,                 loss: 0.4450
Score delta: 2.039194728518642, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/7797_1.
Episode: 8041/50100 (16.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1109s / 357.9278 s
agent0:                 episode reward: 0.1417,                 loss: 0.3231
agent1:                 episode reward: -0.1417,                 loss: nan
Episode: 8061/50100 (16.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0958s / 359.0236 s
agent0:                 episode reward: 0.0880,                 loss: 0.3197
agent1:                 episode reward: -0.0880,                 loss: nan
Episode: 8081/50100 (16.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1055s / 360.1291 s
agent0:                 episode reward: -0.6179,                 loss: 0.3204
agent1:                 episode reward: 0.6179,                 loss: nan
Episode: 8101/50100 (16.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1158s / 361.2449 s
agent0:                 episode reward: -0.2401,                 loss: 0.3197
agent1:                 episode reward: 0.2401,                 loss: nan
Episode: 8121/50100 (16.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1362s / 362.3811 s
agent0:                 episode reward: -0.0026,                 loss: 0.3595
agent1:                 episode reward: 0.0026,                 loss: nan
Episode: 8141/50100 (16.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1283s / 363.5094 s
agent0:                 episode reward: -0.5126,                 loss: 0.3530
agent1:                 episode reward: 0.5126,                 loss: nan
Episode: 8161/50100 (16.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1081s / 364.6175 s
agent0:                 episode reward: 0.0029,                 loss: 0.3543
agent1:                 episode reward: -0.0029,                 loss: nan
Episode: 8181/50100 (16.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1047s / 365.7223 s
agent0:                 episode reward: -0.0264,                 loss: 0.3525
agent1:                 episode reward: 0.0264,                 loss: nan
Episode: 8201/50100 (16.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1069s / 366.8292 s
agent0:                 episode reward: 0.2796,                 loss: 0.3515
agent1:                 episode reward: -0.2796,                 loss: nan
Episode: 8221/50100 (16.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1378s / 367.9670 s
agent0:                 episode reward: 0.0091,                 loss: 0.3517
agent1:                 episode reward: -0.0091,                 loss: nan
Episode: 8241/50100 (16.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1255s / 369.0925 s
agent0:                 episode reward: -0.4977,                 loss: 0.3490
agent1:                 episode reward: 0.4977,                 loss: nan
Episode: 8261/50100 (16.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1271s / 370.2195 s
agent0:                 episode reward: -0.1518,                 loss: 0.3512
agent1:                 episode reward: 0.1518,                 loss: nan
Episode: 8281/50100 (16.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1303s / 371.3498 s
agent0:                 episode reward: -0.2342,                 loss: 0.3417
agent1:                 episode reward: 0.2342,                 loss: nan
Episode: 8301/50100 (16.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1333s / 372.4831 s
agent0:                 episode reward: -0.0567,                 loss: 0.3111
agent1:                 episode reward: 0.0567,                 loss: nan
Episode: 8321/50100 (16.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1388s / 373.6219 s
agent0:                 episode reward: 0.2303,                 loss: 0.3054
agent1:                 episode reward: -0.2303,                 loss: nan
Episode: 8341/50100 (16.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1375s / 374.7594 s
agent0:                 episode reward: -0.2754,                 loss: 0.3032
agent1:                 episode reward: 0.2754,                 loss: nan
Episode: 8361/50100 (16.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1192s / 375.8786 s
agent0:                 episode reward: 0.0976,                 loss: 0.3007
agent1:                 episode reward: -0.0976,                 loss: nan
Episode: 8381/50100 (16.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1140s / 376.9926 s
agent0:                 episode reward: -0.4983,                 loss: 0.2980
agent1:                 episode reward: 0.4983,                 loss: nan
Episode: 8401/50100 (16.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1319s / 378.1245 s
agent0:                 episode reward: -0.1430,                 loss: 0.2984
agent1:                 episode reward: 0.1430,                 loss: nan
Episode: 8421/50100 (16.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2101s / 379.3347 s
agent0:                 episode reward: -0.0824,                 loss: 0.2970
agent1:                 episode reward: 0.0824,                 loss: nan
Episode: 8441/50100 (16.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1334s / 380.4681 s
agent0:                 episode reward: -0.0914,                 loss: 0.3138
agent1:                 episode reward: 0.0914,                 loss: nan
Episode: 8461/50100 (16.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0214s / 381.4895 s
agent0:                 episode reward: 0.4424,                 loss: 0.3490
agent1:                 episode reward: -0.4424,                 loss: 0.4509
Score delta: 2.47158221460768, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/8240_0.
Episode: 8481/50100 (16.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8762s / 382.3657 s
agent0:                 episode reward: -0.5288,                 loss: nan
agent1:                 episode reward: 0.5288,                 loss: 0.4463
Episode: 8501/50100 (16.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8770s / 383.2427 s
agent0:                 episode reward: 0.1057,                 loss: nan
agent1:                 episode reward: -0.1057,                 loss: 0.4450
Episode: 8521/50100 (17.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9026s / 384.1453 s
agent0:                 episode reward: -0.2774,                 loss: nan
agent1:                 episode reward: 0.2774,                 loss: 0.4457
Episode: 8541/50100 (17.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0885s / 385.2338 s
agent0:                 episode reward: 0.0799,                 loss: 0.3462
agent1:                 episode reward: -0.0799,                 loss: 0.4439
Score delta: 2.086815602644769, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/8313_1.
Episode: 8561/50100 (17.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1960s / 386.4298 s
agent0:                 episode reward: -0.1541,                 loss: 0.3474
agent1:                 episode reward: 0.1541,                 loss: nan
Episode: 8581/50100 (17.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1680s / 387.5979 s
agent0:                 episode reward: -0.1078,                 loss: 0.3458
agent1:                 episode reward: 0.1078,                 loss: nan
Episode: 8601/50100 (17.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1356s / 388.7335 s
agent0:                 episode reward: 0.2983,                 loss: 0.3460
agent1:                 episode reward: -0.2983,                 loss: nan
Episode: 8621/50100 (17.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1333s / 389.8668 s
agent0:                 episode reward: 0.0051,                 loss: 0.3443
agent1:                 episode reward: -0.0051,                 loss: nan
Episode: 8641/50100 (17.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1447s / 391.0115 s
agent0:                 episode reward: 0.2054,                 loss: 0.3457
agent1:                 episode reward: -0.2054,                 loss: nan
Episode: 8661/50100 (17.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1397s / 392.1511 s
agent0:                 episode reward: 0.2268,                 loss: 0.3440
agent1:                 episode reward: -0.2268,                 loss: nan
Episode: 8681/50100 (17.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1443s / 393.2955 s
agent0:                 episode reward: -0.3420,                 loss: 0.3708
agent1:                 episode reward: 0.3420,                 loss: nan
Episode: 8701/50100 (17.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1459s / 394.4414 s
agent0:                 episode reward: -0.4436,                 loss: 0.4134
agent1:                 episode reward: 0.4436,                 loss: nan
Episode: 8721/50100 (17.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1513s / 395.5927 s
agent0:                 episode reward: -0.2345,                 loss: 0.4131
agent1:                 episode reward: 0.2345,                 loss: nan
Episode: 8741/50100 (17.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1562s / 396.7489 s
agent0:                 episode reward: -0.4253,                 loss: 0.4132
agent1:                 episode reward: 0.4253,                 loss: nan
Episode: 8761/50100 (17.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1623s / 397.9113 s
agent0:                 episode reward: 0.1804,                 loss: 0.4103
agent1:                 episode reward: -0.1804,                 loss: nan
Episode: 8781/50100 (17.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1627s / 399.0739 s
agent0:                 episode reward: -0.3863,                 loss: 0.4126
agent1:                 episode reward: 0.3863,                 loss: nan
Episode: 8801/50100 (17.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1521s / 400.2260 s
agent0:                 episode reward: 0.5576,                 loss: 0.4114
agent1:                 episode reward: -0.5576,                 loss: 0.4494
Score delta: 2.0913668552354863, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/8588_0.
Episode: 8821/50100 (17.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8819s / 401.1080 s
agent0:                 episode reward: 0.0266,                 loss: nan
agent1:                 episode reward: -0.0266,                 loss: 0.4470
Episode: 8841/50100 (17.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8740s / 401.9820 s
agent0:                 episode reward: -0.0718,                 loss: nan
agent1:                 episode reward: 0.0718,                 loss: 0.4466
Episode: 8861/50100 (17.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8944s / 402.8764 s
agent0:                 episode reward: 0.3102,                 loss: nan
agent1:                 episode reward: -0.3102,                 loss: 0.4443
Episode: 8881/50100 (17.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8807s / 403.7571 s
agent0:                 episode reward: 0.0954,                 loss: nan
agent1:                 episode reward: -0.0954,                 loss: 0.4440
Episode: 8901/50100 (17.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8810s / 404.6381 s
agent0:                 episode reward: -0.4048,                 loss: nan
agent1:                 episode reward: 0.4048,                 loss: 0.4432
Episode: 8921/50100 (17.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8792s / 405.5173 s
agent0:                 episode reward: 0.0003,                 loss: nan
agent1:                 episode reward: -0.0003,                 loss: 0.4443
Episode: 8941/50100 (17.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8885s / 406.4058 s
agent0:                 episode reward: -0.0924,                 loss: nan
agent1:                 episode reward: 0.0924,                 loss: 0.4438
Episode: 8961/50100 (17.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8950s / 407.3008 s
agent0:                 episode reward: -0.4225,                 loss: nan
agent1:                 episode reward: 0.4225,                 loss: 0.4441
Episode: 8981/50100 (17.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9044s / 408.2052 s
agent0:                 episode reward: 0.5229,                 loss: nan
agent1:                 episode reward: -0.5229,                 loss: 0.4432
Episode: 9001/50100 (17.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9215s / 409.1266 s
agent0:                 episode reward: -0.5279,                 loss: nan
agent1:                 episode reward: 0.5279,                 loss: 0.4430
Episode: 9021/50100 (18.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1493s / 410.2760 s
agent0:                 episode reward: -0.4391,                 loss: 0.4130
agent1:                 episode reward: 0.4391,                 loss: 0.4450
Score delta: 2.069224240338956, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/8792_1.
Episode: 9041/50100 (18.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2474s / 411.5234 s
agent0:                 episode reward: -0.0443,                 loss: 0.4114
agent1:                 episode reward: 0.0443,                 loss: nan
Episode: 9061/50100 (18.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2044s / 412.7278 s
agent0:                 episode reward: -0.3137,                 loss: 0.4339
agent1:                 episode reward: 0.3137,                 loss: nan
Episode: 9081/50100 (18.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2520s / 413.9798 s
agent0:                 episode reward: -0.2077,                 loss: 0.4298
agent1:                 episode reward: 0.2077,                 loss: nan
Episode: 9101/50100 (18.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1642s / 415.1440 s
agent0:                 episode reward: -0.0323,                 loss: 0.4298
agent1:                 episode reward: 0.0323,                 loss: nan
Episode: 9121/50100 (18.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1727s / 416.3167 s
agent0:                 episode reward: -0.0147,                 loss: 0.4287
agent1:                 episode reward: 0.0147,                 loss: nan
Episode: 9141/50100 (18.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1869s / 417.5036 s
agent0:                 episode reward: -0.5223,                 loss: 0.4302
agent1:                 episode reward: 0.5223,                 loss: nan
Episode: 9161/50100 (18.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1443s / 418.6480 s
agent0:                 episode reward: -0.1610,                 loss: 0.4291
agent1:                 episode reward: 0.1610,                 loss: nan
Episode: 9181/50100 (18.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1400s / 419.7880 s
agent0:                 episode reward: -0.0318,                 loss: 0.4290
agent1:                 episode reward: 0.0318,                 loss: nan
Episode: 9201/50100 (18.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1488s / 420.9368 s
agent0:                 episode reward: -0.0515,                 loss: 0.4284
agent1:                 episode reward: 0.0515,                 loss: nan
Episode: 9221/50100 (18.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1493s / 422.0861 s
agent0:                 episode reward: 0.1221,                 loss: 0.4074
agent1:                 episode reward: -0.1221,                 loss: nan
Episode: 9241/50100 (18.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1504s / 423.2365 s
agent0:                 episode reward: -0.2962,                 loss: 0.3785
agent1:                 episode reward: 0.2962,                 loss: nan
Episode: 9261/50100 (18.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1447s / 424.3812 s
agent0:                 episode reward: -0.2720,                 loss: 0.3774
agent1:                 episode reward: 0.2720,                 loss: nan
Episode: 9281/50100 (18.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1699s / 425.5510 s
agent0:                 episode reward: -0.1041,                 loss: 0.3754
agent1:                 episode reward: 0.1041,                 loss: nan
Episode: 9301/50100 (18.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1472s / 426.6983 s
agent0:                 episode reward: -0.8268,                 loss: 0.3771
agent1:                 episode reward: 0.8268,                 loss: nan
Episode: 9321/50100 (18.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1618s / 427.8600 s
agent0:                 episode reward: -0.1353,                 loss: 0.3752
agent1:                 episode reward: 0.1353,                 loss: nan
Episode: 9341/50100 (18.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1385s / 428.9985 s
agent0:                 episode reward: 0.2027,                 loss: 0.3763
agent1:                 episode reward: -0.2027,                 loss: nan
Episode: 9361/50100 (18.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1490s / 430.1476 s
agent0:                 episode reward: -0.0949,                 loss: 0.3753
agent1:                 episode reward: 0.0949,                 loss: nan
Episode: 9381/50100 (18.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1398s / 431.2873 s
agent0:                 episode reward: 0.5442,                 loss: 0.3777
agent1:                 episode reward: -0.5442,                 loss: nan
Episode: 9401/50100 (18.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9147s / 432.2020 s
agent0:                 episode reward: 0.2168,                 loss: 0.3761
agent1:                 episode reward: -0.2168,                 loss: 0.4479
Score delta: 2.1902049931406653, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/9171_0.
Episode: 9421/50100 (18.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8844s / 433.0864 s
agent0:                 episode reward: -0.1288,                 loss: nan
agent1:                 episode reward: 0.1288,                 loss: 0.4455
Episode: 9441/50100 (18.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8924s / 433.9787 s
agent0:                 episode reward: -0.3382,                 loss: nan
agent1:                 episode reward: 0.3382,                 loss: 0.4462
Episode: 9461/50100 (18.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9186s / 434.8974 s
agent0:                 episode reward: -0.2424,                 loss: nan
agent1:                 episode reward: 0.2424,                 loss: 0.4451
Episode: 9481/50100 (18.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8913s / 435.7887 s
agent0:                 episode reward: -0.3152,                 loss: nan
agent1:                 episode reward: 0.3152,                 loss: 0.4456
Episode: 9501/50100 (18.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9093s / 436.6980 s
agent0:                 episode reward: -0.6818,                 loss: 0.4170
agent1:                 episode reward: 0.6818,                 loss: 0.4459
Score delta: 2.07804350422631, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/9287_1.
Episode: 9521/50100 (19.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1506s / 437.8486 s
agent0:                 episode reward: -0.0429,                 loss: 0.4108
agent1:                 episode reward: 0.0429,                 loss: nan
Episode: 9541/50100 (19.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1452s / 438.9938 s
agent0:                 episode reward: -0.0384,                 loss: 0.4082
agent1:                 episode reward: 0.0384,                 loss: nan
Episode: 9561/50100 (19.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1441s / 440.1379 s
agent0:                 episode reward: -0.3414,                 loss: 0.4089
agent1:                 episode reward: 0.3414,                 loss: nan
Episode: 9581/50100 (19.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1530s / 441.2909 s
agent0:                 episode reward: -0.1709,                 loss: 0.4090
agent1:                 episode reward: 0.1709,                 loss: nan
Episode: 9601/50100 (19.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1532s / 442.4441 s
agent0:                 episode reward: -0.1443,                 loss: 0.4083
agent1:                 episode reward: 0.1443,                 loss: nan
Episode: 9621/50100 (19.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1535s / 443.5976 s
agent0:                 episode reward: -0.1982,                 loss: 0.4094
agent1:                 episode reward: 0.1982,                 loss: nan
Episode: 9641/50100 (19.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1667s / 444.7644 s
agent0:                 episode reward: -0.1723,                 loss: 0.4082
agent1:                 episode reward: 0.1723,                 loss: nan
Episode: 9661/50100 (19.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1640s / 445.9284 s
agent0:                 episode reward: -0.3012,                 loss: 0.4095
agent1:                 episode reward: 0.3012,                 loss: nan
Episode: 9681/50100 (19.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1520s / 447.0804 s
agent0:                 episode reward: -0.3405,                 loss: 0.4196
agent1:                 episode reward: 0.3405,                 loss: nan
Episode: 9701/50100 (19.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1382s / 448.2186 s
agent0:                 episode reward: -0.1854,                 loss: 0.4129
agent1:                 episode reward: 0.1854,                 loss: nan
Episode: 9721/50100 (19.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1458s / 449.3644 s
agent0:                 episode reward: -0.1432,                 loss: 0.4105
agent1:                 episode reward: 0.1432,                 loss: nan
Episode: 9741/50100 (19.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2010s / 450.5654 s
agent0:                 episode reward: -0.2721,                 loss: 0.4096
agent1:                 episode reward: 0.2721,                 loss: nan
Episode: 9761/50100 (19.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1776s / 451.7430 s
agent0:                 episode reward: -0.1997,                 loss: 0.4111
agent1:                 episode reward: 0.1997,                 loss: nan
Episode: 9781/50100 (19.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1732s / 452.9163 s
agent0:                 episode reward: 0.0908,                 loss: 0.4106
agent1:                 episode reward: -0.0908,                 loss: nan
Episode: 9801/50100 (19.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1637s / 454.0800 s
agent0:                 episode reward: 0.0351,                 loss: 0.4098
agent1:                 episode reward: -0.0351,                 loss: nan
Episode: 9821/50100 (19.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1838s / 455.2638 s
agent0:                 episode reward: -0.1985,                 loss: 0.4099
agent1:                 episode reward: 0.1985,                 loss: nan
Episode: 9841/50100 (19.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9199s / 456.1837 s
agent0:                 episode reward: 0.3187,                 loss: 0.4079
agent1:                 episode reward: -0.3187,                 loss: 0.4508
Score delta: 2.0300774294372586, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/9610_0.
Episode: 9861/50100 (19.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9010s / 457.0847 s
agent0:                 episode reward: -0.1356,                 loss: nan
agent1:                 episode reward: 0.1356,                 loss: 0.4460
Episode: 9881/50100 (19.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9120s / 457.9967 s
agent0:                 episode reward: -0.7584,                 loss: 0.4035
agent1:                 episode reward: 0.7584,                 loss: 0.4461
Score delta: 2.022305794854299, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/9668_1.
Episode: 9901/50100 (19.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1769s / 459.1736 s
agent0:                 episode reward: 0.1000,                 loss: 0.3697
agent1:                 episode reward: -0.1000,                 loss: nan
Episode: 9921/50100 (19.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2344s / 460.4080 s
agent0:                 episode reward: -0.2905,                 loss: 0.3469
agent1:                 episode reward: 0.2905,                 loss: nan
Episode: 9941/50100 (19.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1697s / 461.5778 s
agent0:                 episode reward: -0.4930,                 loss: 0.3493
agent1:                 episode reward: 0.4930,                 loss: nan
Episode: 9961/50100 (19.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1748s / 462.7526 s
agent0:                 episode reward: -0.0664,                 loss: 0.3460
agent1:                 episode reward: 0.0664,                 loss: nan
Episode: 9981/50100 (19.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1761s / 463.9286 s
agent0:                 episode reward: -0.2350,                 loss: 0.3482
agent1:                 episode reward: 0.2350,                 loss: nan
Episode: 10001/50100 (19.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2337s / 465.1623 s
agent0:                 episode reward: 0.1474,                 loss: 0.3478
agent1:                 episode reward: -0.1474,                 loss: nan
Episode: 10021/50100 (20.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2034s / 466.3657 s
agent0:                 episode reward: -0.2360,                 loss: 0.3477
agent1:                 episode reward: 0.2360,                 loss: nan
Episode: 10041/50100 (20.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2005s / 467.5662 s
agent0:                 episode reward: 0.0112,                 loss: 0.3479
agent1:                 episode reward: -0.0112,                 loss: nan
Episode: 10061/50100 (20.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1996s / 468.7657 s
agent0:                 episode reward: -0.0338,                 loss: 0.3528
agent1:                 episode reward: 0.0338,                 loss: nan
Episode: 10081/50100 (20.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2004s / 469.9661 s
agent0:                 episode reward: -0.1117,                 loss: 0.3520
agent1:                 episode reward: 0.1117,                 loss: nan
Episode: 10101/50100 (20.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2252s / 471.1914 s
agent0:                 episode reward: 0.3529,                 loss: 0.3542
agent1:                 episode reward: -0.3529,                 loss: nan
Episode: 10121/50100 (20.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2082s / 472.3996 s
agent0:                 episode reward: -0.2391,                 loss: 0.3518
agent1:                 episode reward: 0.2391,                 loss: nan
Episode: 10141/50100 (20.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1786s / 473.5782 s
agent0:                 episode reward: 0.3726,                 loss: 0.3530
agent1:                 episode reward: -0.3726,                 loss: nan
Episode: 10161/50100 (20.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1759s / 474.7541 s
agent0:                 episode reward: 0.3123,                 loss: 0.3521
agent1:                 episode reward: -0.3123,                 loss: nan
Episode: 10181/50100 (20.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1867s / 475.9408 s
agent0:                 episode reward: -0.2770,                 loss: 0.3536
agent1:                 episode reward: 0.2770,                 loss: nan
Episode: 10201/50100 (20.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1897s / 477.1305 s
agent0:                 episode reward: -0.0173,                 loss: 0.3521
agent1:                 episode reward: 0.0173,                 loss: nan
Episode: 10221/50100 (20.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1894s / 478.3199 s
agent0:                 episode reward: -0.2989,                 loss: 0.3587
agent1:                 episode reward: 0.2989,                 loss: nan
Episode: 10241/50100 (20.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1908s / 479.5107 s
agent0:                 episode reward: -0.1839,                 loss: 0.3599
agent1:                 episode reward: 0.1839,                 loss: nan
Episode: 10261/50100 (20.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1941s / 480.7048 s
agent0:                 episode reward: -0.1580,                 loss: 0.3549
agent1:                 episode reward: 0.1580,                 loss: nan
Episode: 10281/50100 (20.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1868s / 481.8915 s
agent0:                 episode reward: 0.2059,                 loss: 0.3540
agent1:                 episode reward: -0.2059,                 loss: nan
Episode: 10301/50100 (20.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1798s / 483.0713 s
agent0:                 episode reward: 0.2610,                 loss: 0.3518
agent1:                 episode reward: -0.2610,                 loss: nan
Episode: 10321/50100 (20.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1915s / 484.2628 s
agent0:                 episode reward: -0.3648,                 loss: 0.3545
agent1:                 episode reward: 0.3648,                 loss: nan
Episode: 10341/50100 (20.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1920s / 485.4548 s
agent0:                 episode reward: -0.1616,                 loss: 0.3517
agent1:                 episode reward: 0.1616,                 loss: nan
Episode: 10361/50100 (20.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1861s / 486.6409 s
agent0:                 episode reward: 0.0670,                 loss: 0.3517
agent1:                 episode reward: -0.0670,                 loss: nan
Episode: 10381/50100 (20.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1948s / 487.8357 s
agent0:                 episode reward: -0.2795,                 loss: 0.3482
agent1:                 episode reward: 0.2795,                 loss: nan
Episode: 10401/50100 (20.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1939s / 489.0295 s
agent0:                 episode reward: -0.0501,                 loss: 0.3634
agent1:                 episode reward: 0.0501,                 loss: nan
Episode: 10421/50100 (20.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2130s / 490.2426 s
agent0:                 episode reward: -0.0298,                 loss: 0.3603
agent1:                 episode reward: 0.0298,                 loss: nan
Episode: 10441/50100 (20.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2150s / 491.4576 s
agent0:                 episode reward: -0.2896,                 loss: 0.3598
agent1:                 episode reward: 0.2896,                 loss: nan
Episode: 10461/50100 (20.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1950s / 492.6526 s
agent0:                 episode reward: -0.3330,                 loss: 0.3571
agent1:                 episode reward: 0.3330,                 loss: nan
Episode: 10481/50100 (20.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2083s / 493.8610 s
agent0:                 episode reward: -0.1488,                 loss: 0.3605
agent1:                 episode reward: 0.1488,                 loss: nan
Episode: 10501/50100 (20.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2078s / 495.0687 s
agent0:                 episode reward: 0.2507,                 loss: 0.3574
agent1:                 episode reward: -0.2507,                 loss: nan
Episode: 10521/50100 (21.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2023s / 496.2711 s
agent0:                 episode reward: 0.0011,                 loss: 0.3579
agent1:                 episode reward: -0.0011,                 loss: nan
Episode: 10541/50100 (21.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2029s / 497.4739 s
agent0:                 episode reward: -0.2335,                 loss: 0.3552
agent1:                 episode reward: 0.2335,                 loss: nan
Episode: 10561/50100 (21.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2135s / 498.6875 s
agent0:                 episode reward: 0.0356,                 loss: 0.3862
agent1:                 episode reward: -0.0356,                 loss: nan
Episode: 10581/50100 (21.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2100s / 499.8975 s
agent0:                 episode reward: -0.1330,                 loss: 0.4200
agent1:                 episode reward: 0.1330,                 loss: nan
Episode: 10601/50100 (21.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2009s / 501.0984 s
agent0:                 episode reward: 0.1479,                 loss: 0.4192
agent1:                 episode reward: -0.1479,                 loss: nan
Episode: 10621/50100 (21.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2188s / 502.3172 s
agent0:                 episode reward: -0.0977,                 loss: 0.4202
agent1:                 episode reward: 0.0977,                 loss: nan
Episode: 10641/50100 (21.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2211s / 503.5383 s
agent0:                 episode reward: 0.1176,                 loss: 0.4200
agent1:                 episode reward: -0.1176,                 loss: nan
Episode: 10661/50100 (21.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2139s / 504.7523 s
agent0:                 episode reward: -0.4816,                 loss: 0.4203
agent1:                 episode reward: 0.4816,                 loss: nan
Episode: 10681/50100 (21.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2207s / 505.9730 s
agent0:                 episode reward: -0.0270,                 loss: 0.4191
agent1:                 episode reward: 0.0270,                 loss: nan
Episode: 10701/50100 (21.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2267s / 507.1998 s
agent0:                 episode reward: -0.4359,                 loss: 0.4201
agent1:                 episode reward: 0.4359,                 loss: nan
Episode: 10721/50100 (21.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2091s / 508.4089 s
agent0:                 episode reward: 0.0796,                 loss: 0.4235
agent1:                 episode reward: -0.0796,                 loss: nan
Episode: 10741/50100 (21.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2263s / 509.6353 s
agent0:                 episode reward: 0.5034,                 loss: 0.4413
agent1:                 episode reward: -0.5034,                 loss: nan
Episode: 10761/50100 (21.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2195s / 510.8548 s
agent0:                 episode reward: -0.5599,                 loss: 0.4392
agent1:                 episode reward: 0.5599,                 loss: nan
Episode: 10781/50100 (21.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2146s / 512.0694 s
agent0:                 episode reward: -0.3386,                 loss: 0.4398
agent1:                 episode reward: 0.3386,                 loss: nan
Episode: 10801/50100 (21.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2309s / 513.3003 s
agent0:                 episode reward: 0.2713,                 loss: 0.4386
agent1:                 episode reward: -0.2713,                 loss: nan
Episode: 10821/50100 (21.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2281s / 514.5285 s
agent0:                 episode reward: 0.0047,                 loss: 0.4378
agent1:                 episode reward: -0.0047,                 loss: nan
Episode: 10841/50100 (21.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2488s / 515.7773 s
agent0:                 episode reward: -0.2713,                 loss: 0.4387
agent1:                 episode reward: 0.2713,                 loss: nan
Episode: 10861/50100 (21.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2206s / 516.9979 s
agent0:                 episode reward: -0.1168,                 loss: 0.4377
agent1:                 episode reward: 0.1168,                 loss: nan
Episode: 10881/50100 (21.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2203s / 518.2182 s
agent0:                 episode reward: -0.1695,                 loss: 0.4364
agent1:                 episode reward: 0.1695,                 loss: nan
Episode: 10901/50100 (21.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2303s / 519.4485 s
agent0:                 episode reward: -0.2351,                 loss: 0.4265
agent1:                 episode reward: 0.2351,                 loss: nan
Episode: 10921/50100 (21.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2238s / 520.6723 s
agent0:                 episode reward: 0.0715,                 loss: 0.4213
agent1:                 episode reward: -0.0715,                 loss: nan
Episode: 10941/50100 (21.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2289s / 521.9012 s
agent0:                 episode reward: -0.2355,                 loss: 0.4208
agent1:                 episode reward: 0.2355,                 loss: nan
Episode: 10961/50100 (21.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2261s / 523.1272 s
agent0:                 episode reward: -0.0145,                 loss: 0.4198
agent1:                 episode reward: 0.0145,                 loss: nan
Episode: 10981/50100 (21.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2228s / 524.3500 s
agent0:                 episode reward: 0.4326,                 loss: 0.4210
agent1:                 episode reward: -0.4326,                 loss: nan
Episode: 11001/50100 (21.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2592s / 525.6092 s
agent0:                 episode reward: -0.0663,                 loss: 0.4199
agent1:                 episode reward: 0.0663,                 loss: nan
Episode: 11021/50100 (21.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2706s / 526.8798 s
agent0:                 episode reward: 0.1025,                 loss: 0.4207
agent1:                 episode reward: -0.1025,                 loss: nan
Episode: 11041/50100 (22.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2583s / 528.1381 s
agent0:                 episode reward: -0.2275,                 loss: 0.4200
agent1:                 episode reward: 0.2275,                 loss: nan
Episode: 11061/50100 (22.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2321s / 529.3702 s
agent0:                 episode reward: 0.6335,                 loss: 0.4135
agent1:                 episode reward: -0.6335,                 loss: 0.4544
Score delta: 2.475466337094244, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/10848_0.
Episode: 11081/50100 (22.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0093s / 530.3795 s
agent0:                 episode reward: -0.6238,                 loss: nan
agent1:                 episode reward: 0.6238,                 loss: 0.4493
Episode: 11101/50100 (22.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9157s / 531.2952 s
agent0:                 episode reward: -0.0530,                 loss: nan
agent1:                 episode reward: 0.0530,                 loss: 0.4473
Episode: 11121/50100 (22.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9443s / 532.2395 s
agent0:                 episode reward: -0.0232,                 loss: nan
agent1:                 episode reward: 0.0232,                 loss: 0.4464
Episode: 11141/50100 (22.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9384s / 533.1779 s
agent0:                 episode reward: 0.1424,                 loss: nan
agent1:                 episode reward: -0.1424,                 loss: 0.4460
Episode: 11161/50100 (22.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9322s / 534.1101 s
agent0:                 episode reward: -0.2406,                 loss: nan
agent1:                 episode reward: 0.2406,                 loss: 0.4461
Episode: 11181/50100 (22.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9363s / 535.0465 s
agent0:                 episode reward: -0.5496,                 loss: nan
agent1:                 episode reward: 0.5496,                 loss: 0.4452
Episode: 11201/50100 (22.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9547s / 536.0012 s
agent0:                 episode reward: -0.3430,                 loss: nan
agent1:                 episode reward: 0.3430,                 loss: 0.4445
Episode: 11221/50100 (22.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2446s / 537.2457 s
agent0:                 episode reward: -0.2802,                 loss: 0.4115
agent1:                 episode reward: 0.2802,                 loss: 0.4461
Score delta: 2.1966369251761764, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/10991_1.
Episode: 11241/50100 (22.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2475s / 538.4933 s
agent0:                 episode reward: 0.0734,                 loss: 0.4124
agent1:                 episode reward: -0.0734,                 loss: nan
Episode: 11261/50100 (22.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2467s / 539.7399 s
agent0:                 episode reward: -0.0278,                 loss: 0.4097
agent1:                 episode reward: 0.0278,                 loss: nan
Episode: 11281/50100 (22.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2473s / 540.9872 s
agent0:                 episode reward: -0.0053,                 loss: 0.4093
agent1:                 episode reward: 0.0053,                 loss: nan
Episode: 11301/50100 (22.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2276s / 542.2148 s
agent0:                 episode reward: 0.3680,                 loss: 0.4096
agent1:                 episode reward: -0.3680,                 loss: nan
Episode: 11321/50100 (22.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2523s / 543.4671 s
agent0:                 episode reward: -0.9634,                 loss: 0.4080
agent1:                 episode reward: 0.9634,                 loss: nan
Episode: 11341/50100 (22.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2541s / 544.7212 s
agent0:                 episode reward: -0.1096,                 loss: 0.4081
agent1:                 episode reward: 0.1096,                 loss: nan
Episode: 11361/50100 (22.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2552s / 545.9764 s
agent0:                 episode reward: -0.4172,                 loss: 0.4081
agent1:                 episode reward: 0.4172,                 loss: nan
Episode: 11381/50100 (22.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2661s / 547.2425 s
agent0:                 episode reward: -0.3780,                 loss: 0.4325
agent1:                 episode reward: 0.3780,                 loss: nan
Episode: 11401/50100 (22.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2677s / 548.5102 s
agent0:                 episode reward: 0.2186,                 loss: 0.4271
agent1:                 episode reward: -0.2186,                 loss: nan
Episode: 11421/50100 (22.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2329s / 549.7431 s
agent0:                 episode reward: -0.2150,                 loss: 0.4255
agent1:                 episode reward: 0.2150,                 loss: nan
Episode: 11441/50100 (22.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2455s / 550.9886 s
agent0:                 episode reward: -0.0620,                 loss: 0.4268
agent1:                 episode reward: 0.0620,                 loss: nan
Episode: 11461/50100 (22.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2497s / 552.2383 s
agent0:                 episode reward: 0.0845,                 loss: 0.4254
agent1:                 episode reward: -0.0845,                 loss: nan
Episode: 11481/50100 (22.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3168s / 553.5551 s
agent0:                 episode reward: -0.9202,                 loss: 0.4268
agent1:                 episode reward: 0.9202,                 loss: nan
Episode: 11501/50100 (22.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2819s / 554.8369 s
agent0:                 episode reward: -0.1511,                 loss: 0.4267
agent1:                 episode reward: 0.1511,                 loss: nan
Episode: 11521/50100 (22.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2964s / 556.1333 s
agent0:                 episode reward: 0.1402,                 loss: 0.4265
agent1:                 episode reward: -0.1402,                 loss: nan
Episode: 11541/50100 (23.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2881s / 557.4214 s
agent0:                 episode reward: -0.2698,                 loss: 0.4133
agent1:                 episode reward: 0.2698,                 loss: nan
Episode: 11561/50100 (23.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2876s / 558.7091 s
agent0:                 episode reward: 0.0832,                 loss: 0.4008
agent1:                 episode reward: -0.0832,                 loss: nan
Episode: 11581/50100 (23.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2874s / 559.9965 s
agent0:                 episode reward: 0.5303,                 loss: 0.3998
agent1:                 episode reward: -0.5303,                 loss: nan
Episode: 11601/50100 (23.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9880s / 560.9845 s
agent0:                 episode reward: 0.3533,                 loss: 0.4027
agent1:                 episode reward: -0.3533,                 loss: 0.4508
Score delta: 2.215599058622437, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/11372_0.
Episode: 11621/50100 (23.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9312s / 561.9158 s
agent0:                 episode reward: 0.7122,                 loss: nan
agent1:                 episode reward: -0.7122,                 loss: 0.4473
Episode: 11641/50100 (23.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9292s / 562.8450 s
agent0:                 episode reward: 0.1506,                 loss: nan
agent1:                 episode reward: -0.1506,                 loss: 0.4458
Episode: 11661/50100 (23.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9811s / 563.8261 s
agent0:                 episode reward: -0.2903,                 loss: nan
agent1:                 episode reward: 0.2903,                 loss: 0.4463
Episode: 11681/50100 (23.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0925s / 564.9186 s
agent0:                 episode reward: -0.5159,                 loss: 0.4126
agent1:                 episode reward: 0.5159,                 loss: 0.4469
Score delta: 2.098600781735407, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/11459_1.
Episode: 11701/50100 (23.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2845s / 566.2031 s
agent0:                 episode reward: 0.2393,                 loss: 0.4108
agent1:                 episode reward: -0.2393,                 loss: nan
Episode: 11721/50100 (23.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2689s / 567.4720 s
agent0:                 episode reward: 0.2496,                 loss: 0.4121
agent1:                 episode reward: -0.2496,                 loss: nan
Episode: 11741/50100 (23.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2650s / 568.7370 s
agent0:                 episode reward: 0.1852,                 loss: 0.4119
agent1:                 episode reward: -0.1852,                 loss: nan
Episode: 11761/50100 (23.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0140s / 569.7510 s
agent0:                 episode reward: -0.0954,                 loss: 0.4089
agent1:                 episode reward: 0.0954,                 loss: 0.4478
Score delta: 2.2036311551594943, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/11533_0.
Episode: 11781/50100 (23.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9019s / 570.6529 s
agent0:                 episode reward: -0.0251,                 loss: nan
agent1:                 episode reward: 0.0251,                 loss: 0.4484
Episode: 11801/50100 (23.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8851s / 571.5380 s
agent0:                 episode reward: -0.3035,                 loss: nan
agent1:                 episode reward: 0.3035,                 loss: 0.4476
Episode: 11821/50100 (23.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8911s / 572.4291 s
agent0:                 episode reward: -0.4945,                 loss: nan
agent1:                 episode reward: 0.4945,                 loss: 0.4473
Episode: 11841/50100 (23.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1814s / 573.6105 s
agent0:                 episode reward: -0.1423,                 loss: 0.3478
agent1:                 episode reward: 0.1423,                 loss: 0.4469
Score delta: 2.6311893490702554, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/11614_1.
Episode: 11861/50100 (23.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2760s / 574.8865 s
agent0:                 episode reward: 0.2394,                 loss: 0.3459
agent1:                 episode reward: -0.2394,                 loss: nan
Episode: 11881/50100 (23.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2855s / 576.1720 s
agent0:                 episode reward: -0.5176,                 loss: 0.3267
agent1:                 episode reward: 0.5176,                 loss: nan
Episode: 11901/50100 (23.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2920s / 577.4641 s
agent0:                 episode reward: 0.1775,                 loss: 0.3108
agent1:                 episode reward: -0.1775,                 loss: nan
Episode: 11921/50100 (23.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2679s / 578.7320 s
agent0:                 episode reward: 0.3861,                 loss: 0.3071
agent1:                 episode reward: -0.3861,                 loss: nan
Episode: 11941/50100 (23.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2983s / 580.0303 s
agent0:                 episode reward: -0.3230,                 loss: 0.3011
agent1:                 episode reward: 0.3230,                 loss: nan
Episode: 11961/50100 (23.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2803s / 581.3106 s
agent0:                 episode reward: -0.1160,                 loss: 0.2990
agent1:                 episode reward: 0.1160,                 loss: nan
Episode: 11981/50100 (23.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2969s / 582.6075 s
agent0:                 episode reward: -0.0955,                 loss: 0.2977
agent1:                 episode reward: 0.0955,                 loss: nan
Episode: 12001/50100 (23.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2881s / 583.8956 s
agent0:                 episode reward: -0.2097,                 loss: 0.2970
agent1:                 episode reward: 0.2097,                 loss: nan
Episode: 12021/50100 (23.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3561s / 585.2517 s
agent0:                 episode reward: -0.0907,                 loss: 0.2990
agent1:                 episode reward: 0.0907,                 loss: nan
Episode: 12041/50100 (24.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3150s / 586.5667 s
agent0:                 episode reward: 0.1357,                 loss: 0.3441
agent1:                 episode reward: -0.1357,                 loss: nan
Episode: 12061/50100 (24.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3193s / 587.8860 s
agent0:                 episode reward: 0.2737,                 loss: 0.3787
agent1:                 episode reward: -0.2737,                 loss: nan
Episode: 12081/50100 (24.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3165s / 589.2025 s
agent0:                 episode reward: 0.5215,                 loss: 0.3817
agent1:                 episode reward: -0.5215,                 loss: nan
Episode: 12101/50100 (24.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3830s / 590.5855 s
agent0:                 episode reward: -0.1949,                 loss: 0.3810
agent1:                 episode reward: 0.1949,                 loss: nan
Episode: 12121/50100 (24.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1561s / 591.7416 s
agent0:                 episode reward: 0.6210,                 loss: 0.3786
agent1:                 episode reward: -0.6210,                 loss: 0.4448
Score delta: 2.128659981637864, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/11902_0.
Episode: 12141/50100 (24.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8908s / 592.6324 s
agent0:                 episode reward: -0.0969,                 loss: nan
agent1:                 episode reward: 0.0969,                 loss: 0.4405
Episode: 12161/50100 (24.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 0.8973s / 593.5297 s
agent0:                 episode reward: -0.5419,                 loss: nan
agent1:                 episode reward: 0.5419,                 loss: 0.4351
Episode: 12181/50100 (24.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9151s / 594.4447 s
agent0:                 episode reward: -0.2090,                 loss: nan
agent1:                 episode reward: 0.2090,                 loss: 0.4348
Episode: 12201/50100 (24.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9207s / 595.3654 s
agent0:                 episode reward: -0.5151,                 loss: nan
agent1:                 episode reward: 0.5151,                 loss: 0.4346
Episode: 12221/50100 (24.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1859s / 596.5514 s
agent0:                 episode reward: -0.5904,                 loss: 0.3513
agent1:                 episode reward: 0.5904,                 loss: 0.4330
Score delta: 2.0052846668377478, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/11995_1.
Episode: 12241/50100 (24.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2913s / 597.8426 s
agent0:                 episode reward: -0.3143,                 loss: 0.3466
agent1:                 episode reward: 0.3143,                 loss: nan
Episode: 12261/50100 (24.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2918s / 599.1344 s
agent0:                 episode reward: 0.0408,                 loss: 0.3437
agent1:                 episode reward: -0.0408,                 loss: nan
Episode: 12281/50100 (24.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2967s / 600.4311 s
agent0:                 episode reward: 0.2489,                 loss: 0.3408
agent1:                 episode reward: -0.2489,                 loss: nan
Episode: 12301/50100 (24.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3015s / 601.7326 s
agent0:                 episode reward: 0.0103,                 loss: 0.3270
agent1:                 episode reward: -0.0103,                 loss: nan
Episode: 12321/50100 (24.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2919s / 603.0245 s
agent0:                 episode reward: -0.3130,                 loss: 0.2989
agent1:                 episode reward: 0.3130,                 loss: nan
Episode: 12341/50100 (24.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3151s / 604.3396 s
agent0:                 episode reward: -0.0339,                 loss: 0.2964
agent1:                 episode reward: 0.0339,                 loss: nan
Episode: 12361/50100 (24.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3730s / 605.7126 s
agent0:                 episode reward: -0.0227,                 loss: 0.2949
agent1:                 episode reward: 0.0227,                 loss: nan
Episode: 12381/50100 (24.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3174s / 607.0300 s
agent0:                 episode reward: 0.4034,                 loss: 0.2895
agent1:                 episode reward: -0.4034,                 loss: nan
Episode: 12401/50100 (24.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3604s / 608.3903 s
agent0:                 episode reward: -0.0821,                 loss: 0.2871
agent1:                 episode reward: 0.0821,                 loss: nan
Episode: 12421/50100 (24.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3001s / 609.6904 s
agent0:                 episode reward: 0.3984,                 loss: 0.2873
agent1:                 episode reward: -0.3984,                 loss: nan
Episode: 12441/50100 (24.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2726s / 610.9630 s
agent0:                 episode reward: 0.3421,                 loss: 0.2853
agent1:                 episode reward: -0.3421,                 loss: 0.4369
Score delta: 2.0809845937462255, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/12227_0.
Episode: 12461/50100 (24.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9015s / 611.8645 s
agent0:                 episode reward: -0.2307,                 loss: nan
agent1:                 episode reward: 0.2307,                 loss: 0.4327
Episode: 12481/50100 (24.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9053s / 612.7698 s
agent0:                 episode reward: -0.4500,                 loss: nan
agent1:                 episode reward: 0.4500,                 loss: 0.4323
Episode: 12501/50100 (24.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9080s / 613.6778 s
agent0:                 episode reward: -0.3385,                 loss: nan
agent1:                 episode reward: 0.3385,                 loss: 0.4342
Episode: 12521/50100 (24.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9159s / 614.5937 s
agent0:                 episode reward: -0.5010,                 loss: nan
agent1:                 episode reward: 0.5010,                 loss: 0.4334
Episode: 12541/50100 (25.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9247s / 615.5184 s
agent0:                 episode reward: -0.0467,                 loss: nan
agent1:                 episode reward: 0.0467,                 loss: 0.4308
Episode: 12561/50100 (25.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9130s / 616.4314 s
agent0:                 episode reward: -0.2358,                 loss: nan
agent1:                 episode reward: 0.2358,                 loss: 0.4274
Episode: 12581/50100 (25.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9156s / 617.3470 s
agent0:                 episode reward: -0.5366,                 loss: nan
agent1:                 episode reward: 0.5366,                 loss: 0.4267
Episode: 12601/50100 (25.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9185s / 618.2655 s
agent0:                 episode reward: -0.7162,                 loss: nan
agent1:                 episode reward: 0.7162,                 loss: 0.4276
Episode: 12621/50100 (25.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9414s / 619.2069 s
agent0:                 episode reward: -0.5427,                 loss: nan
agent1:                 episode reward: 0.5427,                 loss: 0.4262
Episode: 12641/50100 (25.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2990s / 620.5059 s
agent0:                 episode reward: 0.0944,                 loss: 0.3527
agent1:                 episode reward: -0.0944,                 loss: 0.4283
Score delta: 2.3421898169346598, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/12410_1.
Episode: 12661/50100 (25.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3073s / 621.8132 s
agent0:                 episode reward: 0.3940,                 loss: 0.3145
agent1:                 episode reward: -0.3940,                 loss: nan
Episode: 12681/50100 (25.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3087s / 623.1220 s
agent0:                 episode reward: 0.0475,                 loss: 0.3043
agent1:                 episode reward: -0.0475,                 loss: nan
Episode: 12701/50100 (25.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3130s / 624.4350 s
agent0:                 episode reward: -0.3143,                 loss: 0.3027
agent1:                 episode reward: 0.3143,                 loss: nan
Episode: 12721/50100 (25.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2701s / 625.7051 s
agent0:                 episode reward: 0.3851,                 loss: 0.2984
agent1:                 episode reward: -0.3851,                 loss: 0.4448
Score delta: 2.0259454346263936, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/12505_0.
Episode: 12741/50100 (25.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9360s / 626.6412 s
agent0:                 episode reward: -0.0895,                 loss: nan
agent1:                 episode reward: 0.0895,                 loss: 0.4466
Episode: 12761/50100 (25.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9546s / 627.5958 s
agent0:                 episode reward: -0.2378,                 loss: nan
agent1:                 episode reward: 0.2378,                 loss: 0.4451
Episode: 12781/50100 (25.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9432s / 628.5390 s
agent0:                 episode reward: 0.0032,                 loss: nan
agent1:                 episode reward: -0.0032,                 loss: 0.4465
Episode: 12801/50100 (25.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0352s / 629.5742 s
agent0:                 episode reward: -0.0878,                 loss: nan
agent1:                 episode reward: 0.0878,                 loss: 0.4435
Episode: 12821/50100 (25.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0160s / 630.5902 s
agent0:                 episode reward: -0.6771,                 loss: 0.3045
agent1:                 episode reward: 0.6771,                 loss: 0.4415
Score delta: 2.4459428490632407, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/12607_1.
Episode: 12841/50100 (25.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3638s / 631.9540 s
agent0:                 episode reward: 0.0496,                 loss: 0.2991
agent1:                 episode reward: -0.0496,                 loss: nan
Episode: 12861/50100 (25.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4166s / 633.3706 s
agent0:                 episode reward: 0.1574,                 loss: 0.2960
agent1:                 episode reward: -0.1574,                 loss: nan
Episode: 12881/50100 (25.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3215s / 634.6921 s
agent0:                 episode reward: -0.2961,                 loss: 0.2925
agent1:                 episode reward: 0.2961,                 loss: nan
Episode: 12901/50100 (25.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3442s / 636.0363 s
agent0:                 episode reward: -0.1461,                 loss: 0.2943
agent1:                 episode reward: 0.1461,                 loss: nan
Episode: 12921/50100 (25.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3507s / 637.3870 s
agent0:                 episode reward: 0.1901,                 loss: 0.3446
agent1:                 episode reward: -0.1901,                 loss: nan
Episode: 12941/50100 (25.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3191s / 638.7061 s
agent0:                 episode reward: -0.2840,                 loss: 0.3707
agent1:                 episode reward: 0.2840,                 loss: nan
Episode: 12961/50100 (25.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3328s / 640.0389 s
agent0:                 episode reward: 0.0273,                 loss: 0.3699
agent1:                 episode reward: -0.0273,                 loss: nan
Episode: 12981/50100 (25.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3365s / 641.3755 s
agent0:                 episode reward: -0.0485,                 loss: 0.3680
agent1:                 episode reward: 0.0485,                 loss: nan
Episode: 13001/50100 (25.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3493s / 642.7247 s
agent0:                 episode reward: 0.1460,                 loss: 0.3684
agent1:                 episode reward: -0.1460,                 loss: nan
Episode: 13021/50100 (25.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3358s / 644.0605 s
agent0:                 episode reward: -0.4032,                 loss: 0.3662
agent1:                 episode reward: 0.4032,                 loss: nan
Episode: 13041/50100 (26.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3419s / 645.4024 s
agent0:                 episode reward: -0.2637,                 loss: 0.3654
agent1:                 episode reward: 0.2637,                 loss: nan
Episode: 13061/50100 (26.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2012s / 646.6036 s
agent0:                 episode reward: 0.3847,                 loss: 0.3648
agent1:                 episode reward: -0.3847,                 loss: 0.4464
Score delta: 2.2894495282466742, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/12842_0.
Episode: 13081/50100 (26.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9347s / 647.5383 s
agent0:                 episode reward: -0.5447,                 loss: nan
agent1:                 episode reward: 0.5447,                 loss: 0.4463
Episode: 13101/50100 (26.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2861s / 648.8244 s
agent0:                 episode reward: 0.0399,                 loss: 0.4026
agent1:                 episode reward: -0.0399,                 loss: 0.4465
Score delta: 2.064064918540591, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/12872_1.
Episode: 13121/50100 (26.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3381s / 650.1626 s
agent0:                 episode reward: 0.3962,                 loss: 0.3891
agent1:                 episode reward: -0.3962,                 loss: nan
Episode: 13141/50100 (26.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3633s / 651.5258 s
agent0:                 episode reward: 0.5096,                 loss: 0.3811
agent1:                 episode reward: -0.5096,                 loss: nan
Episode: 13161/50100 (26.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3534s / 652.8792 s
agent0:                 episode reward: 0.2815,                 loss: 0.3789
agent1:                 episode reward: -0.2815,                 loss: nan
Episode: 13181/50100 (26.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0052s / 653.8844 s
agent0:                 episode reward: -0.0866,                 loss: 0.3803
agent1:                 episode reward: 0.0866,                 loss: 0.4513
Score delta: 2.141331630885227, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/12951_0.
Episode: 13201/50100 (26.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9365s / 654.8209 s
agent0:                 episode reward: 0.2116,                 loss: nan
agent1:                 episode reward: -0.2116,                 loss: 0.4490
Episode: 13221/50100 (26.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9734s / 655.7944 s
agent0:                 episode reward: -0.1717,                 loss: nan
agent1:                 episode reward: 0.1717,                 loss: 0.4503
Episode: 13241/50100 (26.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0248s / 656.8191 s
agent0:                 episode reward: -0.3688,                 loss: nan
agent1:                 episode reward: 0.3688,                 loss: 0.4485
Episode: 13261/50100 (26.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9571s / 657.7763 s
agent0:                 episode reward: 0.2707,                 loss: nan
agent1:                 episode reward: -0.2707,                 loss: 0.4487
Episode: 13281/50100 (26.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9523s / 658.7286 s
agent0:                 episode reward: -0.1038,                 loss: nan
agent1:                 episode reward: 0.1038,                 loss: 0.4472
Episode: 13301/50100 (26.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9666s / 659.6952 s
agent0:                 episode reward: -0.4587,                 loss: nan
agent1:                 episode reward: 0.4587,                 loss: 0.4441
Episode: 13321/50100 (26.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9424s / 660.6375 s
agent0:                 episode reward: -0.5986,                 loss: nan
agent1:                 episode reward: 0.5986,                 loss: 0.4429
Episode: 13341/50100 (26.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2959s / 661.9334 s
agent0:                 episode reward: -0.0974,                 loss: 0.3445
agent1:                 episode reward: 0.0974,                 loss: 0.4436
Score delta: 2.063590981987843, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/13113_1.
Episode: 13361/50100 (26.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3706s / 663.3040 s
agent0:                 episode reward: -0.2293,                 loss: 0.3410
agent1:                 episode reward: 0.2293,                 loss: nan
Episode: 13381/50100 (26.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3699s / 664.6739 s
agent0:                 episode reward: -0.1060,                 loss: 0.3410
agent1:                 episode reward: 0.1060,                 loss: nan
Episode: 13401/50100 (26.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3731s / 666.0470 s
agent0:                 episode reward: 0.2318,                 loss: 0.3389
agent1:                 episode reward: -0.2318,                 loss: nan
Episode: 13421/50100 (26.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3682s / 667.4152 s
agent0:                 episode reward: 0.0022,                 loss: 0.3406
agent1:                 episode reward: -0.0022,                 loss: nan
Episode: 13441/50100 (26.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3934s / 668.8085 s
agent0:                 episode reward: -0.0098,                 loss: 0.3668
agent1:                 episode reward: 0.0098,                 loss: nan
Episode: 13461/50100 (26.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3841s / 670.1926 s
agent0:                 episode reward: -0.6680,                 loss: 0.4074
agent1:                 episode reward: 0.6680,                 loss: nan
Episode: 13481/50100 (26.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4395s / 671.6321 s
agent0:                 episode reward: 0.1303,                 loss: 0.4049
agent1:                 episode reward: -0.1303,                 loss: nan
Episode: 13501/50100 (26.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3652s / 672.9973 s
agent0:                 episode reward: -0.3093,                 loss: 0.4052
agent1:                 episode reward: 0.3093,                 loss: nan
Episode: 13521/50100 (26.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3642s / 674.3615 s
agent0:                 episode reward: 0.0275,                 loss: 0.4040
agent1:                 episode reward: -0.0275,                 loss: nan
Episode: 13541/50100 (27.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3677s / 675.7292 s
agent0:                 episode reward: 0.0309,                 loss: 0.4034
agent1:                 episode reward: -0.0309,                 loss: nan
Episode: 13561/50100 (27.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3689s / 677.0981 s
agent0:                 episode reward: -0.4986,                 loss: 0.4046
agent1:                 episode reward: 0.4986,                 loss: nan
Episode: 13581/50100 (27.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3751s / 678.4732 s
agent0:                 episode reward: 0.2330,                 loss: 0.4033
agent1:                 episode reward: -0.2330,                 loss: nan
Episode: 13601/50100 (27.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3941s / 679.8673 s
agent0:                 episode reward: 0.3335,                 loss: 0.4075
agent1:                 episode reward: -0.3335,                 loss: nan
Episode: 13621/50100 (27.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3903s / 681.2576 s
agent0:                 episode reward: 0.3313,                 loss: 0.4075
agent1:                 episode reward: -0.3313,                 loss: nan
Episode: 13641/50100 (27.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4003s / 682.6579 s
agent0:                 episode reward: -0.1510,                 loss: 0.4052
agent1:                 episode reward: 0.1510,                 loss: nan
Episode: 13661/50100 (27.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4116s / 684.0694 s
agent0:                 episode reward: -0.3291,                 loss: 0.4066
agent1:                 episode reward: 0.3291,                 loss: nan
Episode: 13681/50100 (27.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4053s / 685.4747 s
agent0:                 episode reward: 0.2308,                 loss: 0.4051
agent1:                 episode reward: -0.2308,                 loss: nan
Episode: 13701/50100 (27.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3813s / 686.8560 s
agent0:                 episode reward: -0.4698,                 loss: 0.4041
agent1:                 episode reward: 0.4698,                 loss: nan
Episode: 13721/50100 (27.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4003s / 688.2563 s
agent0:                 episode reward: -0.1652,                 loss: 0.4028
agent1:                 episode reward: 0.1652,                 loss: nan
Episode: 13741/50100 (27.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3999s / 689.6562 s
agent0:                 episode reward: -0.3423,                 loss: 0.4030
agent1:                 episode reward: 0.3423,                 loss: nan
Episode: 13761/50100 (27.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4001s / 691.0563 s
agent0:                 episode reward: -0.2322,                 loss: 0.4025
agent1:                 episode reward: 0.2322,                 loss: nan
Episode: 13781/50100 (27.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3918s / 692.4481 s
agent0:                 episode reward: 0.2826,                 loss: 0.3983
agent1:                 episode reward: -0.2826,                 loss: nan
Episode: 13801/50100 (27.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3859s / 693.8340 s
agent0:                 episode reward: -0.4507,                 loss: 0.3911
agent1:                 episode reward: 0.4507,                 loss: nan
Episode: 13821/50100 (27.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3868s / 695.2208 s
agent0:                 episode reward: 0.1397,                 loss: 0.3906
agent1:                 episode reward: -0.1397,                 loss: nan
Episode: 13841/50100 (27.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4121s / 696.6329 s
agent0:                 episode reward: 0.0668,                 loss: 0.3916
agent1:                 episode reward: -0.0668,                 loss: nan
Episode: 13861/50100 (27.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4071s / 698.0401 s
agent0:                 episode reward: 0.3144,                 loss: 0.3906
agent1:                 episode reward: -0.3144,                 loss: nan
Episode: 13881/50100 (27.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4581s / 699.4982 s
agent0:                 episode reward: -0.5258,                 loss: 0.3908
agent1:                 episode reward: 0.5258,                 loss: nan
Episode: 13901/50100 (27.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4403s / 700.9385 s
agent0:                 episode reward: 0.1291,                 loss: 0.3874
agent1:                 episode reward: -0.1291,                 loss: nan
Episode: 13921/50100 (27.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4124s / 702.3510 s
agent0:                 episode reward: -0.3911,                 loss: 0.3887
agent1:                 episode reward: 0.3911,                 loss: nan
Episode: 13941/50100 (27.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4366s / 703.7876 s
agent0:                 episode reward: -0.0218,                 loss: 0.3948
agent1:                 episode reward: 0.0218,                 loss: nan
Episode: 13961/50100 (27.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4342s / 705.2218 s
agent0:                 episode reward: -0.1065,                 loss: 0.3922
agent1:                 episode reward: 0.1065,                 loss: nan
Episode: 13981/50100 (27.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4114s / 706.6332 s
agent0:                 episode reward: -0.1904,                 loss: 0.3913
agent1:                 episode reward: 0.1904,                 loss: nan
Episode: 14001/50100 (27.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4217s / 708.0549 s
agent0:                 episode reward: -0.1829,                 loss: 0.3899
agent1:                 episode reward: 0.1829,                 loss: nan
Episode: 14021/50100 (27.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4295s / 709.4844 s
agent0:                 episode reward: 0.3479,                 loss: 0.3922
agent1:                 episode reward: -0.3479,                 loss: nan
Episode: 14041/50100 (28.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4271s / 710.9115 s
agent0:                 episode reward: 0.2152,                 loss: 0.3884
agent1:                 episode reward: -0.2152,                 loss: nan
Episode: 14061/50100 (28.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4249s / 712.3365 s
agent0:                 episode reward: -0.1371,                 loss: 0.3919
agent1:                 episode reward: 0.1371,                 loss: nan
Episode: 14081/50100 (28.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4456s / 713.7821 s
agent0:                 episode reward: 0.1129,                 loss: 0.3889
agent1:                 episode reward: -0.1129,                 loss: nan
Episode: 14101/50100 (28.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2134s / 714.9955 s
agent0:                 episode reward: 0.1733,                 loss: 0.3912
agent1:                 episode reward: -0.1733,                 loss: 0.4528
Score delta: 2.0496972311138526, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/13881_0.
Episode: 14121/50100 (28.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9416s / 715.9371 s
agent0:                 episode reward: -0.3979,                 loss: nan
agent1:                 episode reward: 0.3979,                 loss: 0.4492
Episode: 14141/50100 (28.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9580s / 716.8951 s
agent0:                 episode reward: -0.0545,                 loss: nan
agent1:                 episode reward: 0.0545,                 loss: 0.4480
Episode: 14161/50100 (28.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9430s / 717.8381 s
agent0:                 episode reward: -0.3774,                 loss: nan
agent1:                 episode reward: 0.3774,                 loss: 0.4480
Episode: 14181/50100 (28.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9349s / 718.7730 s
agent0:                 episode reward: -0.6569,                 loss: nan
agent1:                 episode reward: 0.6569,                 loss: 0.4467
Episode: 14201/50100 (28.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9300s / 719.7030 s
agent0:                 episode reward: -0.2916,                 loss: nan
agent1:                 episode reward: 0.2916,                 loss: 0.4477
Episode: 14221/50100 (28.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9450s / 720.6480 s
agent0:                 episode reward: -0.2734,                 loss: nan
agent1:                 episode reward: 0.2734,                 loss: 0.4475
Episode: 14241/50100 (28.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1098s / 721.7578 s
agent0:                 episode reward: -0.5628,                 loss: 0.3746
agent1:                 episode reward: 0.5628,                 loss: 0.4452
Score delta: 2.0857674867304965, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/14022_1.
Episode: 14261/50100 (28.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3927s / 723.1505 s
agent0:                 episode reward: -0.1786,                 loss: 0.3522
agent1:                 episode reward: 0.1786,                 loss: nan
Episode: 14281/50100 (28.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4176s / 724.5680 s
agent0:                 episode reward: 0.4859,                 loss: 0.3402
agent1:                 episode reward: -0.4859,                 loss: nan
Episode: 14301/50100 (28.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4170s / 725.9851 s
agent0:                 episode reward: -0.2935,                 loss: 0.3370
agent1:                 episode reward: 0.2935,                 loss: nan
Episode: 14321/50100 (28.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4033s / 727.3884 s
agent0:                 episode reward: 0.2855,                 loss: 0.3333
agent1:                 episode reward: -0.2855,                 loss: nan
Episode: 14341/50100 (28.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4094s / 728.7977 s
agent0:                 episode reward: -0.0233,                 loss: 0.3315
agent1:                 episode reward: 0.0233,                 loss: nan
Episode: 14361/50100 (28.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4125s / 730.2102 s
agent0:                 episode reward: -0.1037,                 loss: 0.3320
agent1:                 episode reward: 0.1037,                 loss: nan
Episode: 14381/50100 (28.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4091s / 731.6193 s
agent0:                 episode reward: 0.0668,                 loss: 0.3301
agent1:                 episode reward: -0.0668,                 loss: nan
Episode: 14401/50100 (28.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4118s / 733.0311 s
agent0:                 episode reward: 0.0083,                 loss: 0.3275
agent1:                 episode reward: -0.0083,                 loss: nan
Episode: 14421/50100 (28.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4140s / 734.4451 s
agent0:                 episode reward: -0.3738,                 loss: 0.3298
agent1:                 episode reward: 0.3738,                 loss: nan
Episode: 14441/50100 (28.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4301s / 735.8752 s
agent0:                 episode reward: 0.0985,                 loss: 0.3239
agent1:                 episode reward: -0.0985,                 loss: nan
Episode: 14461/50100 (28.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4206s / 737.2958 s
agent0:                 episode reward: -0.5459,                 loss: 0.3216
agent1:                 episode reward: 0.5459,                 loss: nan
Episode: 14481/50100 (28.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4156s / 738.7114 s
agent0:                 episode reward: 0.3441,                 loss: 0.3200
agent1:                 episode reward: -0.3441,                 loss: nan
Episode: 14501/50100 (28.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4479s / 740.1593 s
agent0:                 episode reward: 0.2953,                 loss: 0.3207
agent1:                 episode reward: -0.2953,                 loss: nan
Episode: 14521/50100 (28.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4449s / 741.6042 s
agent0:                 episode reward: 0.5003,                 loss: 0.3192
agent1:                 episode reward: -0.5003,                 loss: nan
Episode: 14541/50100 (29.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4337s / 743.0379 s
agent0:                 episode reward: 0.3875,                 loss: 0.3206
agent1:                 episode reward: -0.3875,                 loss: nan
Episode: 14561/50100 (29.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4306s / 744.4685 s
agent0:                 episode reward: -0.4031,                 loss: 0.3186
agent1:                 episode reward: 0.4031,                 loss: nan
Episode: 14581/50100 (29.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4474s / 745.9158 s
agent0:                 episode reward: 0.1946,                 loss: 0.3434
agent1:                 episode reward: -0.1946,                 loss: nan
Episode: 14601/50100 (29.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4377s / 747.3535 s
agent0:                 episode reward: -0.0569,                 loss: 0.3927
agent1:                 episode reward: 0.0569,                 loss: nan
Episode: 14621/50100 (29.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4489s / 748.8024 s
agent0:                 episode reward: 0.1368,                 loss: 0.3902
agent1:                 episode reward: -0.1368,                 loss: nan
Episode: 14641/50100 (29.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4396s / 750.2420 s
agent0:                 episode reward: 0.0516,                 loss: 0.3880
agent1:                 episode reward: -0.0516,                 loss: nan
Episode: 14661/50100 (29.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4389s / 751.6809 s
agent0:                 episode reward: 0.2911,                 loss: 0.3895
agent1:                 episode reward: -0.2911,                 loss: nan
Episode: 14681/50100 (29.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4297s / 753.1105 s
agent0:                 episode reward: -0.4990,                 loss: 0.3881
agent1:                 episode reward: 0.4990,                 loss: nan
Episode: 14701/50100 (29.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4436s / 754.5541 s
agent0:                 episode reward: 0.5644,                 loss: 0.3897
agent1:                 episode reward: -0.5644,                 loss: nan
Episode: 14721/50100 (29.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4474s / 756.0015 s
agent0:                 episode reward: -0.4251,                 loss: 0.3882
agent1:                 episode reward: 0.4251,                 loss: nan
Episode: 14741/50100 (29.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4413s / 757.4428 s
agent0:                 episode reward: -0.6040,                 loss: 0.3890
agent1:                 episode reward: 0.6040,                 loss: nan
Episode: 14761/50100 (29.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4403s / 758.8831 s
agent0:                 episode reward: 0.4581,                 loss: 0.4437
agent1:                 episode reward: -0.4581,                 loss: nan
Episode: 14781/50100 (29.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4512s / 760.3344 s
agent0:                 episode reward: -0.4353,                 loss: 0.4404
agent1:                 episode reward: 0.4353,                 loss: nan
Episode: 14801/50100 (29.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4462s / 761.7806 s
agent0:                 episode reward: -0.0191,                 loss: 0.4400
agent1:                 episode reward: 0.0191,                 loss: nan
Episode: 14821/50100 (29.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4471s / 763.2277 s
agent0:                 episode reward: 0.3123,                 loss: 0.4396
agent1:                 episode reward: -0.3123,                 loss: nan
Episode: 14841/50100 (29.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4817s / 764.7094 s
agent0:                 episode reward: 0.0887,                 loss: 0.4389
agent1:                 episode reward: -0.0887,                 loss: nan
Episode: 14861/50100 (29.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4532s / 766.1626 s
agent0:                 episode reward: 0.0483,                 loss: 0.4384
agent1:                 episode reward: -0.0483,                 loss: nan
Episode: 14881/50100 (29.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4550s / 767.6176 s
agent0:                 episode reward: -0.0758,                 loss: 0.4382
agent1:                 episode reward: 0.0758,                 loss: nan
Episode: 14901/50100 (29.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4605s / 769.0781 s
agent0:                 episode reward: -0.3010,                 loss: 0.4401
agent1:                 episode reward: 0.3010,                 loss: nan
Episode: 14921/50100 (29.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4727s / 770.5508 s
agent0:                 episode reward: 0.3759,                 loss: 0.4300
agent1:                 episode reward: -0.3759,                 loss: nan
Episode: 14941/50100 (29.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4131s / 771.9639 s
agent0:                 episode reward: 0.3591,                 loss: 0.4244
agent1:                 episode reward: -0.3591,                 loss: 0.4464
Score delta: 2.026964065799657, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/14727_0.
Episode: 14961/50100 (29.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9522s / 772.9161 s
agent0:                 episode reward: -0.5065,                 loss: nan
agent1:                 episode reward: 0.5065,                 loss: 0.4466
Episode: 14981/50100 (29.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2499s / 774.1660 s
agent0:                 episode reward: -0.5853,                 loss: 0.2828
agent1:                 episode reward: 0.5853,                 loss: 0.4459
Score delta: 2.159480446727648, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/14758_1.
Episode: 15001/50100 (29.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4971s / 775.6631 s
agent0:                 episode reward: 0.2924,                 loss: 0.2817
agent1:                 episode reward: -0.2924,                 loss: nan
Episode: 15021/50100 (29.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5649s / 777.2280 s
agent0:                 episode reward: -0.0927,                 loss: 0.2813
agent1:                 episode reward: 0.0927,                 loss: nan
Episode: 15041/50100 (30.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4767s / 778.7048 s
agent0:                 episode reward: -0.1268,                 loss: 0.2789
agent1:                 episode reward: 0.1268,                 loss: nan
Episode: 15061/50100 (30.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4849s / 780.1897 s
agent0:                 episode reward: 0.2770,                 loss: 0.2787
agent1:                 episode reward: -0.2770,                 loss: nan
Episode: 15081/50100 (30.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4634s / 781.6531 s
agent0:                 episode reward: -0.5707,                 loss: 0.2795
agent1:                 episode reward: 0.5707,                 loss: nan
Episode: 15101/50100 (30.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4523s / 783.1054 s
agent0:                 episode reward: -0.5380,                 loss: 0.2767
agent1:                 episode reward: 0.5380,                 loss: nan
Episode: 15121/50100 (30.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4835s / 784.5889 s
agent0:                 episode reward: -0.4221,                 loss: 0.3526
agent1:                 episode reward: 0.4221,                 loss: nan
Episode: 15141/50100 (30.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5195s / 786.1084 s
agent0:                 episode reward: 0.6705,                 loss: 0.3705
agent1:                 episode reward: -0.6705,                 loss: nan
Episode: 15161/50100 (30.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5134s / 787.6217 s
agent0:                 episode reward: -0.4039,                 loss: 0.3697
agent1:                 episode reward: 0.4039,                 loss: nan
Episode: 15181/50100 (30.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3626s / 788.9844 s
agent0:                 episode reward: 0.4155,                 loss: 0.3663
agent1:                 episode reward: -0.4155,                 loss: 0.4498
Score delta: 2.016290432233137, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/14964_0.
Episode: 15201/50100 (30.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9908s / 789.9751 s
agent0:                 episode reward: -0.2664,                 loss: nan
agent1:                 episode reward: 0.2664,                 loss: 0.4495
Episode: 15221/50100 (30.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9901s / 790.9652 s
agent0:                 episode reward: -0.2021,                 loss: nan
agent1:                 episode reward: 0.2021,                 loss: 0.4499
Episode: 15241/50100 (30.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9688s / 791.9339 s
agent0:                 episode reward: -0.1275,                 loss: nan
agent1:                 episode reward: 0.1275,                 loss: 0.4494
Episode: 15261/50100 (30.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9757s / 792.9096 s
agent0:                 episode reward: -0.2340,                 loss: nan
agent1:                 episode reward: 0.2340,                 loss: 0.4489
Episode: 15281/50100 (30.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9704s / 793.8800 s
agent0:                 episode reward: -0.0409,                 loss: nan
agent1:                 episode reward: 0.0409,                 loss: 0.4491
Episode: 15301/50100 (30.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9836s / 794.8636 s
agent0:                 episode reward: -0.3277,                 loss: nan
agent1:                 episode reward: 0.3277,                 loss: 0.4451
Episode: 15321/50100 (30.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2040s / 796.0676 s
agent0:                 episode reward: -0.2067,                 loss: 0.4150
agent1:                 episode reward: 0.2067,                 loss: 0.4440
Score delta: 2.1811841879517404, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/15100_1.
Episode: 15341/50100 (30.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5070s / 797.5746 s
agent0:                 episode reward: -0.1097,                 loss: 0.4155
agent1:                 episode reward: 0.1097,                 loss: nan
Episode: 15361/50100 (30.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5130s / 799.0875 s
agent0:                 episode reward: -0.4634,                 loss: 0.4141
agent1:                 episode reward: 0.4634,                 loss: nan
Episode: 15381/50100 (30.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4858s / 800.5733 s
agent0:                 episode reward: 0.3036,                 loss: 0.4119
agent1:                 episode reward: -0.3036,                 loss: nan
Episode: 15401/50100 (30.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5040s / 802.0774 s
agent0:                 episode reward: 0.2694,                 loss: 0.4127
agent1:                 episode reward: -0.2694,                 loss: nan
Episode: 15421/50100 (30.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5048s / 803.5821 s
agent0:                 episode reward: -0.0634,                 loss: 0.3973
agent1:                 episode reward: 0.0634,                 loss: nan
Episode: 15441/50100 (30.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5371s / 805.1192 s
agent0:                 episode reward: 0.2677,                 loss: 0.3855
agent1:                 episode reward: -0.2677,                 loss: nan
Episode: 15461/50100 (30.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4984s / 806.6177 s
agent0:                 episode reward: -0.1935,                 loss: 0.3857
agent1:                 episode reward: 0.1935,                 loss: nan
Episode: 15481/50100 (30.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4695s / 808.0872 s
agent0:                 episode reward: -0.5065,                 loss: 0.3863
agent1:                 episode reward: 0.5065,                 loss: nan
Episode: 15501/50100 (30.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4942s / 809.5813 s
agent0:                 episode reward: 0.0683,                 loss: 0.3854
agent1:                 episode reward: -0.0683,                 loss: nan
Episode: 15521/50100 (30.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4784s / 811.0598 s
agent0:                 episode reward: -0.0231,                 loss: 0.3869
agent1:                 episode reward: 0.0231,                 loss: nan
Episode: 15541/50100 (31.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4907s / 812.5505 s
agent0:                 episode reward: -0.0481,                 loss: 0.3828
agent1:                 episode reward: 0.0481,                 loss: nan
Episode: 15561/50100 (31.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4736s / 814.0241 s
agent0:                 episode reward: -0.4075,                 loss: 0.3858
agent1:                 episode reward: 0.4075,                 loss: nan
Episode: 15581/50100 (31.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5024s / 815.5265 s
agent0:                 episode reward: 0.2168,                 loss: 0.3840
agent1:                 episode reward: -0.2168,                 loss: nan
Episode: 15601/50100 (31.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4995s / 817.0260 s
agent0:                 episode reward: 0.0124,                 loss: 0.3674
agent1:                 episode reward: -0.0124,                 loss: nan
Episode: 15621/50100 (31.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4959s / 818.5219 s
agent0:                 episode reward: 0.0346,                 loss: 0.3680
agent1:                 episode reward: -0.0346,                 loss: nan
Episode: 15641/50100 (31.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4805s / 820.0024 s
agent0:                 episode reward: -0.4705,                 loss: 0.3658
agent1:                 episode reward: 0.4705,                 loss: nan
Episode: 15661/50100 (31.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4867s / 821.4891 s
agent0:                 episode reward: 0.2563,                 loss: 0.3663
agent1:                 episode reward: -0.2563,                 loss: nan
Episode: 15681/50100 (31.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5050s / 822.9941 s
agent0:                 episode reward: 0.3705,                 loss: 0.3641
agent1:                 episode reward: -0.3705,                 loss: nan
Episode: 15701/50100 (31.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5244s / 824.5185 s
agent0:                 episode reward: -0.5317,                 loss: 0.3640
agent1:                 episode reward: 0.5317,                 loss: nan
Episode: 15721/50100 (31.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5130s / 826.0315 s
agent0:                 episode reward: -0.1283,                 loss: 0.3634
agent1:                 episode reward: 0.1283,                 loss: nan
Episode: 15741/50100 (31.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5244s / 827.5559 s
agent0:                 episode reward: -0.0506,                 loss: 0.3643
agent1:                 episode reward: 0.0506,                 loss: nan
Episode: 15761/50100 (31.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5335s / 829.0894 s
agent0:                 episode reward: 0.5011,                 loss: 0.3249
agent1:                 episode reward: -0.5011,                 loss: nan
Episode: 15781/50100 (31.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4188s / 830.5081 s
agent0:                 episode reward: 0.5011,                 loss: 0.3083
agent1:                 episode reward: -0.5011,                 loss: 0.4527
Score delta: 2.076336170129793, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/15564_0.
Episode: 15801/50100 (31.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9997s / 831.5078 s
agent0:                 episode reward: 0.2232,                 loss: nan
agent1:                 episode reward: -0.2232,                 loss: 0.4511
Episode: 15821/50100 (31.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9890s / 832.4968 s
agent0:                 episode reward: -0.0675,                 loss: nan
agent1:                 episode reward: 0.0675,                 loss: 0.4488
Episode: 15841/50100 (31.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9821s / 833.4789 s
agent0:                 episode reward: -0.3980,                 loss: nan
agent1:                 episode reward: 0.3980,                 loss: 0.4483
Episode: 15861/50100 (31.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0047s / 834.4836 s
agent0:                 episode reward: -0.0760,                 loss: nan
agent1:                 episode reward: 0.0760,                 loss: 0.4492
Episode: 15881/50100 (31.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0000s / 835.4837 s
agent0:                 episode reward: -0.5473,                 loss: nan
agent1:                 episode reward: 0.5473,                 loss: 0.4491
Episode: 15901/50100 (31.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0079s / 836.4915 s
agent0:                 episode reward: -0.0347,                 loss: nan
agent1:                 episode reward: 0.0347,                 loss: 0.4499
Episode: 15921/50100 (31.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9868s / 837.4783 s
agent0:                 episode reward: -0.1354,                 loss: nan
agent1:                 episode reward: 0.1354,                 loss: 0.4469
Episode: 15941/50100 (31.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9732s / 838.4516 s
agent0:                 episode reward: -0.6152,                 loss: nan
agent1:                 episode reward: 0.6152,                 loss: 0.4414
Episode: 15961/50100 (31.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9947s / 839.4462 s
agent0:                 episode reward: -0.0886,                 loss: nan
agent1:                 episode reward: 0.0886,                 loss: 0.4407
Episode: 15981/50100 (31.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9811s / 840.4273 s
agent0:                 episode reward: 0.0930,                 loss: nan
agent1:                 episode reward: -0.0930,                 loss: 0.4409
Episode: 16001/50100 (31.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9900s / 841.4173 s
agent0:                 episode reward: 0.4850,                 loss: nan
agent1:                 episode reward: -0.4850,                 loss: 0.4407
Episode: 16021/50100 (31.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0488s / 842.4661 s
agent0:                 episode reward: -0.2727,                 loss: 0.4007
agent1:                 episode reward: 0.2727,                 loss: 0.4412
Score delta: 2.241089772167423, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/15807_1.
Episode: 16041/50100 (32.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5049s / 843.9710 s
agent0:                 episode reward: -0.1919,                 loss: 0.3990
agent1:                 episode reward: 0.1919,                 loss: nan
Episode: 16061/50100 (32.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4942s / 845.4653 s
agent0:                 episode reward: -0.0194,                 loss: 0.3987
agent1:                 episode reward: 0.0194,                 loss: nan
Episode: 16081/50100 (32.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5141s / 846.9793 s
agent0:                 episode reward: -0.4368,                 loss: 0.3990
agent1:                 episode reward: 0.4368,                 loss: nan
Episode: 16101/50100 (32.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5522s / 848.5315 s
agent0:                 episode reward: 0.0740,                 loss: 0.3985
agent1:                 episode reward: -0.0740,                 loss: nan
Episode: 16121/50100 (32.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5384s / 850.0698 s
agent0:                 episode reward: -0.0616,                 loss: 0.3975
agent1:                 episode reward: 0.0616,                 loss: nan
Episode: 16141/50100 (32.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5498s / 851.6197 s
agent0:                 episode reward: -0.0499,                 loss: 0.3989
agent1:                 episode reward: 0.0499,                 loss: nan
Episode: 16161/50100 (32.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5339s / 853.1536 s
agent0:                 episode reward: -0.3554,                 loss: 0.3729
agent1:                 episode reward: 0.3554,                 loss: nan
Episode: 16181/50100 (32.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5386s / 854.6921 s
agent0:                 episode reward: 0.0986,                 loss: 0.3393
agent1:                 episode reward: -0.0986,                 loss: nan
Episode: 16201/50100 (32.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5838s / 856.2760 s
agent0:                 episode reward: 0.0800,                 loss: 0.3363
agent1:                 episode reward: -0.0800,                 loss: nan
Episode: 16221/50100 (32.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5864s / 857.8623 s
agent0:                 episode reward: -0.1909,                 loss: 0.3342
agent1:                 episode reward: 0.1909,                 loss: nan
Episode: 16241/50100 (32.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5690s / 859.4313 s
agent0:                 episode reward: 0.2535,                 loss: 0.3324
agent1:                 episode reward: -0.2535,                 loss: nan
Episode: 16261/50100 (32.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5958s / 861.0271 s
agent0:                 episode reward: -0.1493,                 loss: 0.3332
agent1:                 episode reward: 0.1493,                 loss: nan
Episode: 16281/50100 (32.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5820s / 862.6090 s
agent0:                 episode reward: -0.5112,                 loss: 0.3345
agent1:                 episode reward: 0.5112,                 loss: nan
Episode: 16301/50100 (32.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4302s / 864.0392 s
agent0:                 episode reward: 0.4970,                 loss: 0.3307
agent1:                 episode reward: -0.4970,                 loss: 0.4393
Score delta: 2.176287311261309, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/16083_0.
Episode: 16321/50100 (32.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0839s / 865.1232 s
agent0:                 episode reward: -0.4911,                 loss: nan
agent1:                 episode reward: 0.4911,                 loss: 0.4392
Episode: 16341/50100 (32.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0250s / 866.1481 s
agent0:                 episode reward: -0.3777,                 loss: nan
agent1:                 episode reward: 0.3777,                 loss: 0.4400
Episode: 16361/50100 (32.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5633s / 867.7114 s
agent0:                 episode reward: -0.1373,                 loss: 0.3900
agent1:                 episode reward: 0.1373,                 loss: 0.4418
Score delta: 2.01123934210781, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/16131_1.
Episode: 16381/50100 (32.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5861s / 869.2975 s
agent0:                 episode reward: 0.0676,                 loss: 0.3507
agent1:                 episode reward: -0.0676,                 loss: nan
Episode: 16401/50100 (32.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5811s / 870.8787 s
agent0:                 episode reward: 0.0123,                 loss: 0.3291
agent1:                 episode reward: -0.0123,                 loss: nan
Episode: 16421/50100 (32.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6440s / 872.5227 s
agent0:                 episode reward: 0.3884,                 loss: 0.3273
agent1:                 episode reward: -0.3884,                 loss: nan
Episode: 16441/50100 (32.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2304s / 873.7531 s
agent0:                 episode reward: -0.6090,                 loss: 0.3252
agent1:                 episode reward: 0.6090,                 loss: 0.4518
Score delta: 2.0040705104126486, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/16217_0.
Episode: 16461/50100 (32.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9957s / 874.7488 s
agent0:                 episode reward: -0.6483,                 loss: nan
agent1:                 episode reward: 0.6483,                 loss: 0.4491
Episode: 16481/50100 (32.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9923s / 875.7411 s
agent0:                 episode reward: 0.5069,                 loss: nan
agent1:                 episode reward: -0.5069,                 loss: 0.4483
Episode: 16501/50100 (32.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0101s / 876.7512 s
agent0:                 episode reward: -0.3658,                 loss: nan
agent1:                 episode reward: 0.3658,                 loss: 0.4481
Episode: 16521/50100 (32.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9962s / 877.7474 s
agent0:                 episode reward: -0.2964,                 loss: nan
agent1:                 episode reward: 0.2964,                 loss: 0.4491
Episode: 16541/50100 (33.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9941s / 878.7415 s
agent0:                 episode reward: 0.0057,                 loss: nan
agent1:                 episode reward: -0.0057,                 loss: 0.4495
Episode: 16561/50100 (33.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2070s / 879.9485 s
agent0:                 episode reward: -0.6813,                 loss: 0.3825
agent1:                 episode reward: 0.6813,                 loss: 0.4488
Score delta: 2.257678684515418, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/16342_1.
Episode: 16581/50100 (33.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5813s / 881.5298 s
agent0:                 episode reward: 0.4160,                 loss: 0.3782
agent1:                 episode reward: -0.4160,                 loss: nan
Episode: 16601/50100 (33.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5748s / 883.1046 s
agent0:                 episode reward: -0.0821,                 loss: 0.3778
agent1:                 episode reward: 0.0821,                 loss: nan
Episode: 16621/50100 (33.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5703s / 884.6749 s
agent0:                 episode reward: 0.2878,                 loss: 0.3778
agent1:                 episode reward: -0.2878,                 loss: nan
Episode: 16641/50100 (33.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5763s / 886.2512 s
agent0:                 episode reward: 0.1135,                 loss: 0.3771
agent1:                 episode reward: -0.1135,                 loss: nan
Episode: 16661/50100 (33.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5639s / 887.8151 s
agent0:                 episode reward: -0.1295,                 loss: 0.3710
agent1:                 episode reward: 0.1295,                 loss: nan
Episode: 16681/50100 (33.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5699s / 889.3851 s
agent0:                 episode reward: -0.1067,                 loss: 0.3120
agent1:                 episode reward: 0.1067,                 loss: nan
Episode: 16701/50100 (33.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5759s / 890.9610 s
agent0:                 episode reward: -0.5633,                 loss: 0.3064
agent1:                 episode reward: 0.5633,                 loss: nan
Episode: 16721/50100 (33.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5824s / 892.5434 s
agent0:                 episode reward: -0.2106,                 loss: 0.3083
agent1:                 episode reward: 0.2106,                 loss: nan
Episode: 16741/50100 (33.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5786s / 894.1220 s
agent0:                 episode reward: -0.2426,                 loss: 0.3062
agent1:                 episode reward: 0.2426,                 loss: nan
Episode: 16761/50100 (33.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5592s / 895.6811 s
agent0:                 episode reward: -0.2179,                 loss: 0.3031
agent1:                 episode reward: 0.2179,                 loss: nan
Episode: 16781/50100 (33.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5816s / 897.2627 s
agent0:                 episode reward: 0.1489,                 loss: 0.3027
agent1:                 episode reward: -0.1489,                 loss: nan
Episode: 16801/50100 (33.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6059s / 898.8687 s
agent0:                 episode reward: -0.4116,                 loss: 0.3012
agent1:                 episode reward: 0.4116,                 loss: nan
Episode: 16821/50100 (33.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6237s / 900.4924 s
agent0:                 episode reward: -0.3484,                 loss: 0.2981
agent1:                 episode reward: 0.3484,                 loss: nan
Episode: 16841/50100 (33.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5913s / 902.0837 s
agent0:                 episode reward: 0.1299,                 loss: 0.3495
agent1:                 episode reward: -0.1299,                 loss: nan
Episode: 16861/50100 (33.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6305s / 903.7142 s
agent0:                 episode reward: -0.0813,                 loss: 0.3551
agent1:                 episode reward: 0.0813,                 loss: nan
Episode: 16881/50100 (33.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6245s / 905.3387 s
agent0:                 episode reward: 0.1072,                 loss: 0.3523
agent1:                 episode reward: -0.1072,                 loss: nan
Episode: 16901/50100 (33.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6161s / 906.9548 s
agent0:                 episode reward: 0.0406,                 loss: 0.3533
agent1:                 episode reward: -0.0406,                 loss: nan
Episode: 16921/50100 (33.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6098s / 908.5646 s
agent0:                 episode reward: -0.0785,                 loss: 0.3542
agent1:                 episode reward: 0.0785,                 loss: nan
Episode: 16941/50100 (33.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5868s / 910.1514 s
agent0:                 episode reward: 0.1835,                 loss: 0.3514
agent1:                 episode reward: -0.1835,                 loss: nan
Episode: 16961/50100 (33.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6110s / 911.7624 s
agent0:                 episode reward: 0.3480,                 loss: 0.3515
agent1:                 episode reward: -0.3480,                 loss: nan
Episode: 16981/50100 (33.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6037s / 913.3661 s
agent0:                 episode reward: 0.1992,                 loss: 0.3509
agent1:                 episode reward: -0.1992,                 loss: nan
Episode: 17001/50100 (33.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6057s / 914.9717 s
agent0:                 episode reward: -0.5037,                 loss: 0.3887
agent1:                 episode reward: 0.5037,                 loss: nan
Episode: 17021/50100 (33.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6264s / 916.5982 s
agent0:                 episode reward: 0.4293,                 loss: 0.4129
agent1:                 episode reward: -0.4293,                 loss: nan
Episode: 17041/50100 (34.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1026s / 917.7008 s
agent0:                 episode reward: -0.5071,                 loss: 0.4078
agent1:                 episode reward: 0.5071,                 loss: 0.4438
Score delta: 2.2654891708038214, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/16811_0.
Episode: 17061/50100 (34.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0113s / 918.7121 s
agent0:                 episode reward: -0.2819,                 loss: nan
agent1:                 episode reward: 0.2819,                 loss: 0.4447
Episode: 17081/50100 (34.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4881s / 920.2002 s
agent0:                 episode reward: -0.4296,                 loss: 0.3053
agent1:                 episode reward: 0.4296,                 loss: 0.4433
Score delta: 2.0298419828117167, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/16854_1.
Episode: 17101/50100 (34.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6314s / 921.8316 s
agent0:                 episode reward: -0.0420,                 loss: 0.3032
agent1:                 episode reward: 0.0420,                 loss: nan
Episode: 17121/50100 (34.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6898s / 923.5215 s
agent0:                 episode reward: -0.3912,                 loss: 0.3007
agent1:                 episode reward: 0.3912,                 loss: nan
Episode: 17141/50100 (34.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6600s / 925.1814 s
agent0:                 episode reward: -0.4610,                 loss: 0.2997
agent1:                 episode reward: 0.4610,                 loss: nan
Episode: 17161/50100 (34.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6292s / 926.8106 s
agent0:                 episode reward: 0.7528,                 loss: 0.2981
agent1:                 episode reward: -0.7528,                 loss: nan
Score delta: 2.2765069822563637, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/16949_0.
Episode: 17181/50100 (34.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0209s / 927.8316 s
agent0:                 episode reward: -0.4749,                 loss: nan
agent1:                 episode reward: 0.4749,                 loss: 0.4490
Episode: 17201/50100 (34.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0208s / 928.8524 s
agent0:                 episode reward: 0.0104,                 loss: nan
agent1:                 episode reward: -0.0104,                 loss: 0.4475
Episode: 17221/50100 (34.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0779s / 929.9303 s
agent0:                 episode reward: -0.0512,                 loss: nan
agent1:                 episode reward: 0.0512,                 loss: 0.4462
Episode: 17241/50100 (34.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0130s / 930.9433 s
agent0:                 episode reward: -0.0210,                 loss: nan
agent1:                 episode reward: 0.0210,                 loss: 0.4470
Episode: 17261/50100 (34.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3506s / 932.2939 s
agent0:                 episode reward: -0.4546,                 loss: 0.4086
agent1:                 episode reward: 0.4546,                 loss: 0.4463
Score delta: 2.3198996882231406, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/17038_1.
Episode: 17281/50100 (34.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6274s / 933.9213 s
agent0:                 episode reward: 0.4965,                 loss: 0.4041
agent1:                 episode reward: -0.4965,                 loss: nan
Episode: 17301/50100 (34.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6285s / 935.5498 s
agent0:                 episode reward: 0.1541,                 loss: 0.3779
agent1:                 episode reward: -0.1541,                 loss: nan
Episode: 17321/50100 (34.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6197s / 937.1695 s
agent0:                 episode reward: -0.4628,                 loss: 0.3520
agent1:                 episode reward: 0.4628,                 loss: nan
Episode: 17341/50100 (34.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6154s / 938.7849 s
agent0:                 episode reward: 0.0818,                 loss: 0.3500
agent1:                 episode reward: -0.0818,                 loss: nan
Episode: 17361/50100 (34.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6307s / 940.4157 s
agent0:                 episode reward: 0.1391,                 loss: 0.3485
agent1:                 episode reward: -0.1391,                 loss: nan
Episode: 17381/50100 (34.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6309s / 942.0466 s
agent0:                 episode reward: 0.0873,                 loss: 0.3487
agent1:                 episode reward: -0.0873,                 loss: nan
Episode: 17401/50100 (34.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6303s / 943.6769 s
agent0:                 episode reward: 0.0069,                 loss: 0.3480
agent1:                 episode reward: -0.0069,                 loss: nan
Episode: 17421/50100 (34.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6118s / 945.2886 s
agent0:                 episode reward: -1.1461,                 loss: 0.3488
agent1:                 episode reward: 1.1461,                 loss: nan
Episode: 17441/50100 (34.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6377s / 946.9263 s
agent0:                 episode reward: -0.3877,                 loss: 0.3469
agent1:                 episode reward: 0.3877,                 loss: nan
Episode: 17461/50100 (34.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6399s / 948.5662 s
agent0:                 episode reward: 0.4207,                 loss: 0.3461
agent1:                 episode reward: -0.4207,                 loss: nan
Episode: 17481/50100 (34.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7085s / 950.2747 s
agent0:                 episode reward: -0.0260,                 loss: 0.3349
agent1:                 episode reward: 0.0260,                 loss: nan
Episode: 17501/50100 (34.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6201s / 951.8948 s
agent0:                 episode reward: -0.3089,                 loss: 0.3359
agent1:                 episode reward: 0.3089,                 loss: nan
Episode: 17521/50100 (34.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6294s / 953.5243 s
agent0:                 episode reward: -0.0423,                 loss: 0.3345
agent1:                 episode reward: 0.0423,                 loss: nan
Episode: 17541/50100 (35.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6202s / 955.1444 s
agent0:                 episode reward: -0.2513,                 loss: 0.3328
agent1:                 episode reward: 0.2513,                 loss: nan
Episode: 17561/50100 (35.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6742s / 956.8186 s
agent0:                 episode reward: 0.2468,                 loss: 0.3337
agent1:                 episode reward: -0.2468,                 loss: nan
Episode: 17581/50100 (35.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5277s / 958.3464 s
agent0:                 episode reward: 0.4134,                 loss: 0.3336
agent1:                 episode reward: -0.4134,                 loss: 0.4496
Score delta: 2.3500145326873523, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/17364_0.
Episode: 17601/50100 (35.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0286s / 959.3749 s
agent0:                 episode reward: -0.2718,                 loss: nan
agent1:                 episode reward: 0.2718,                 loss: 0.4501
Episode: 17621/50100 (35.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0523s / 960.4272 s
agent0:                 episode reward: -0.0409,                 loss: nan
agent1:                 episode reward: 0.0409,                 loss: 0.4502
Episode: 17641/50100 (35.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3728s / 961.8000 s
agent0:                 episode reward: -0.5470,                 loss: 0.3748
agent1:                 episode reward: 0.5470,                 loss: 0.4496
Score delta: 2.1073034662656656, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/17418_1.
Episode: 17661/50100 (35.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6558s / 963.4557 s
agent0:                 episode reward: 0.3072,                 loss: 0.3726
agent1:                 episode reward: -0.3072,                 loss: nan
Episode: 17681/50100 (35.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6494s / 965.1051 s
agent0:                 episode reward: 0.2087,                 loss: 0.3872
agent1:                 episode reward: -0.2087,                 loss: nan
Episode: 17701/50100 (35.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6607s / 966.7659 s
agent0:                 episode reward: 0.2669,                 loss: 0.4238
agent1:                 episode reward: -0.2669,                 loss: nan
Episode: 17721/50100 (35.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6461s / 968.4120 s
agent0:                 episode reward: 0.2389,                 loss: 0.4214
agent1:                 episode reward: -0.2389,                 loss: nan
Episode: 17741/50100 (35.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4019s / 969.8138 s
agent0:                 episode reward: 0.5409,                 loss: 0.4207
agent1:                 episode reward: -0.5409,                 loss: 0.4438
Score delta: 2.2443522519715815, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/17521_0.
Episode: 17761/50100 (35.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0184s / 970.8322 s
agent0:                 episode reward: 0.1960,                 loss: nan
agent1:                 episode reward: -0.1960,                 loss: 0.4442
Episode: 17781/50100 (35.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9963s / 971.8285 s
agent0:                 episode reward: -0.3681,                 loss: nan
agent1:                 episode reward: 0.3681,                 loss: 0.4404
Episode: 17801/50100 (35.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 0.9901s / 972.8187 s
agent0:                 episode reward: -0.5950,                 loss: nan
agent1:                 episode reward: 0.5950,                 loss: 0.4326
Episode: 17821/50100 (35.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0187s / 973.8374 s
agent0:                 episode reward: -0.3774,                 loss: nan
agent1:                 episode reward: 0.3774,                 loss: 0.4346
Episode: 17841/50100 (35.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0013s / 974.8387 s
agent0:                 episode reward: -0.7308,                 loss: nan
agent1:                 episode reward: 0.7308,                 loss: 0.4325
Episode: 17861/50100 (35.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5966s / 976.4353 s
agent0:                 episode reward: -0.2037,                 loss: 0.4133
agent1:                 episode reward: 0.2037,                 loss: 0.4345
Score delta: 2.3107260794149997, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/17631_1.
Episode: 17881/50100 (35.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6492s / 978.0845 s
agent0:                 episode reward: 0.4920,                 loss: 0.4117
agent1:                 episode reward: -0.4920,                 loss: nan
Episode: 17901/50100 (35.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6377s / 979.7222 s
agent0:                 episode reward: 0.3269,                 loss: 0.4113
agent1:                 episode reward: -0.3269,                 loss: nan
Episode: 17921/50100 (35.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6609s / 981.3831 s
agent0:                 episode reward: 0.4572,                 loss: 0.4118
agent1:                 episode reward: -0.4572,                 loss: nan
Episode: 17941/50100 (35.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6330s / 983.0161 s
agent0:                 episode reward: 0.4454,                 loss: 0.4101
agent1:                 episode reward: -0.4454,                 loss: nan
Episode: 17961/50100 (35.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6465s / 984.6627 s
agent0:                 episode reward: 0.0500,                 loss: 0.4142
agent1:                 episode reward: -0.0500,                 loss: nan
Episode: 17981/50100 (35.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6700s / 986.3327 s
agent0:                 episode reward: -0.2683,                 loss: 0.4049
agent1:                 episode reward: 0.2683,                 loss: nan
Episode: 18001/50100 (35.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6496s / 987.9823 s
agent0:                 episode reward: 0.3162,                 loss: 0.4052
agent1:                 episode reward: -0.3162,                 loss: nan
Episode: 18021/50100 (35.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2723s / 989.2546 s
agent0:                 episode reward: 0.1162,                 loss: 0.4054
agent1:                 episode reward: -0.1162,                 loss: 0.4545
Score delta: 2.1028022843710867, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/17797_0.
Episode: 18041/50100 (36.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0114s / 990.2659 s
agent0:                 episode reward: 0.2373,                 loss: nan
agent1:                 episode reward: -0.2373,                 loss: 0.4513
Episode: 18061/50100 (36.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0195s / 991.2854 s
agent0:                 episode reward: 0.1630,                 loss: nan
agent1:                 episode reward: -0.1630,                 loss: 0.4509
Episode: 18081/50100 (36.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0659s / 992.3513 s
agent0:                 episode reward: -0.4018,                 loss: 0.2867
agent1:                 episode reward: 0.4018,                 loss: 0.4497
Score delta: 2.0792911029716445, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/17867_1.
Episode: 18101/50100 (36.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6502s / 994.0014 s
agent0:                 episode reward: -0.6482,                 loss: 0.2792
agent1:                 episode reward: 0.6482,                 loss: nan
Episode: 18121/50100 (36.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6576s / 995.6590 s
agent0:                 episode reward: -0.2344,                 loss: 0.2798
agent1:                 episode reward: 0.2344,                 loss: nan
Episode: 18141/50100 (36.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7206s / 997.3796 s
agent0:                 episode reward: 0.1431,                 loss: 0.2789
agent1:                 episode reward: -0.1431,                 loss: nan
Episode: 18161/50100 (36.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6897s / 999.0693 s
agent0:                 episode reward: -0.4165,                 loss: 0.2778
agent1:                 episode reward: 0.4165,                 loss: nan
Episode: 18181/50100 (36.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7477s / 1000.8170 s
agent0:                 episode reward: -0.1337,                 loss: 0.2770
agent1:                 episode reward: 0.1337,                 loss: nan
Episode: 18201/50100 (36.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3674s / 1002.1844 s
agent0:                 episode reward: 0.6715,                 loss: 0.2872
agent1:                 episode reward: -0.6715,                 loss: 0.4482
Score delta: 2.018922804296756, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/17979_0.
Episode: 18221/50100 (36.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0404s / 1003.2248 s
agent0:                 episode reward: -0.2760,                 loss: nan
agent1:                 episode reward: 0.2760,                 loss: 0.4455
Episode: 18241/50100 (36.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0219s / 1004.2468 s
agent0:                 episode reward: -0.3236,                 loss: nan
agent1:                 episode reward: 0.3236,                 loss: 0.4390
Episode: 18261/50100 (36.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0120s / 1005.2588 s
agent0:                 episode reward: -0.3496,                 loss: nan
agent1:                 episode reward: 0.3496,                 loss: 0.4386
Episode: 18281/50100 (36.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1231s / 1006.3819 s
agent0:                 episode reward: -0.6723,                 loss: 0.3409
agent1:                 episode reward: 0.6723,                 loss: 0.4392
Score delta: 2.3612852259760544, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/18066_1.
Episode: 18301/50100 (36.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6794s / 1008.0613 s
agent0:                 episode reward: -0.2044,                 loss: 0.3371
agent1:                 episode reward: 0.2044,                 loss: nan
Episode: 18321/50100 (36.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6681s / 1009.7294 s
agent0:                 episode reward: 0.2952,                 loss: 0.3381
agent1:                 episode reward: -0.2952,                 loss: nan
Episode: 18341/50100 (36.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6740s / 1011.4034 s
agent0:                 episode reward: 0.0858,                 loss: 0.3370
agent1:                 episode reward: -0.0858,                 loss: nan
Episode: 18361/50100 (36.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6737s / 1013.0771 s
agent0:                 episode reward: -0.3014,                 loss: 0.3349
agent1:                 episode reward: 0.3014,                 loss: nan
Episode: 18381/50100 (36.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6835s / 1014.7606 s
agent0:                 episode reward: 0.1209,                 loss: 0.3362
agent1:                 episode reward: -0.1209,                 loss: nan
Episode: 18401/50100 (36.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6877s / 1016.4483 s
agent0:                 episode reward: 0.3964,                 loss: 0.3332
agent1:                 episode reward: -0.3964,                 loss: nan
Episode: 18421/50100 (36.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6894s / 1018.1378 s
agent0:                 episode reward: 0.1688,                 loss: 0.3376
agent1:                 episode reward: -0.1688,                 loss: nan
Episode: 18441/50100 (36.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7422s / 1019.8800 s
agent0:                 episode reward: -0.0210,                 loss: 0.3362
agent1:                 episode reward: 0.0210,                 loss: nan
Episode: 18461/50100 (36.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7146s / 1021.5946 s
agent0:                 episode reward: 0.2028,                 loss: 0.4026
agent1:                 episode reward: -0.2028,                 loss: nan
Episode: 18481/50100 (36.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7461s / 1023.3407 s
agent0:                 episode reward: 0.0026,                 loss: 0.4098
agent1:                 episode reward: -0.0026,                 loss: nan
Episode: 18501/50100 (36.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6815s / 1025.0222 s
agent0:                 episode reward: 0.0562,                 loss: 0.4075
agent1:                 episode reward: -0.0562,                 loss: nan
Episode: 18521/50100 (36.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6930s / 1026.7152 s
agent0:                 episode reward: -0.5407,                 loss: 0.4079
agent1:                 episode reward: 0.5407,                 loss: nan
Episode: 18541/50100 (37.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6775s / 1028.3927 s
agent0:                 episode reward: 0.2469,                 loss: 0.4068
agent1:                 episode reward: -0.2469,                 loss: nan
Episode: 18561/50100 (37.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7016s / 1030.0943 s
agent0:                 episode reward: -0.1591,                 loss: 0.4067
agent1:                 episode reward: 0.1591,                 loss: nan
Episode: 18581/50100 (37.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6896s / 1031.7839 s
agent0:                 episode reward: 0.0306,                 loss: 0.4081
agent1:                 episode reward: -0.0306,                 loss: nan
Episode: 18601/50100 (37.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7704s / 1033.5544 s
agent0:                 episode reward: 0.4590,                 loss: 0.4090
agent1:                 episode reward: -0.4590,                 loss: nan
Episode: 18621/50100 (37.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7395s / 1035.2938 s
agent0:                 episode reward: 0.0356,                 loss: 0.4250
agent1:                 episode reward: -0.0356,                 loss: nan
Episode: 18641/50100 (37.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7738s / 1037.0676 s
agent0:                 episode reward: 0.1160,                 loss: 0.4299
agent1:                 episode reward: -0.1160,                 loss: nan
Episode: 18661/50100 (37.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7140s / 1038.7817 s
agent0:                 episode reward: -0.0034,                 loss: 0.4307
agent1:                 episode reward: 0.0034,                 loss: nan
Episode: 18681/50100 (37.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7014s / 1040.4831 s
agent0:                 episode reward: 0.1174,                 loss: 0.4309
agent1:                 episode reward: -0.1174,                 loss: nan
Episode: 18701/50100 (37.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7102s / 1042.1933 s
agent0:                 episode reward: 0.1915,                 loss: 0.4297
agent1:                 episode reward: -0.1915,                 loss: nan
Episode: 18721/50100 (37.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7351s / 1043.9284 s
agent0:                 episode reward: -0.0796,                 loss: 0.4298
agent1:                 episode reward: 0.0796,                 loss: nan
Episode: 18741/50100 (37.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7093s / 1045.6377 s
agent0:                 episode reward: 0.0145,                 loss: 0.4282
agent1:                 episode reward: -0.0145,                 loss: nan
Episode: 18761/50100 (37.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7243s / 1047.3620 s
agent0:                 episode reward: -0.0621,                 loss: 0.4297
agent1:                 episode reward: 0.0621,                 loss: nan
Episode: 18781/50100 (37.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6992s / 1049.0612 s
agent0:                 episode reward: -0.2716,                 loss: 0.4281
agent1:                 episode reward: 0.2716,                 loss: nan
Episode: 18801/50100 (37.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7038s / 1050.7650 s
agent0:                 episode reward: -0.0943,                 loss: 0.4224
agent1:                 episode reward: 0.0943,                 loss: nan
Episode: 18821/50100 (37.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3223s / 1052.0872 s
agent0:                 episode reward: 0.1630,                 loss: 0.4210
agent1:                 episode reward: -0.1630,                 loss: 0.4506
Score delta: 2.0291539936670993, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/18597_0.
Episode: 18841/50100 (37.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0098s / 1053.0970 s
agent0:                 episode reward: -0.2027,                 loss: nan
agent1:                 episode reward: 0.2027,                 loss: 0.4506
Episode: 18861/50100 (37.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0154s / 1054.1124 s
agent0:                 episode reward: -0.0596,                 loss: nan
agent1:                 episode reward: 0.0596,                 loss: 0.4503
Episode: 18881/50100 (37.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0796s / 1055.1920 s
agent0:                 episode reward: -0.1988,                 loss: 0.3635
agent1:                 episode reward: 0.1988,                 loss: 0.4496
Score delta: 2.013781208531669, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/18667_1.
Episode: 18901/50100 (37.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7169s / 1056.9089 s
agent0:                 episode reward: 0.0483,                 loss: 0.3557
agent1:                 episode reward: -0.0483,                 loss: nan
Episode: 18921/50100 (37.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7096s / 1058.6185 s
agent0:                 episode reward: -0.1484,                 loss: 0.3565
agent1:                 episode reward: 0.1484,                 loss: nan
Episode: 18941/50100 (37.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7544s / 1060.3730 s
agent0:                 episode reward: -0.2288,                 loss: 0.3557
agent1:                 episode reward: 0.2288,                 loss: nan
Episode: 18961/50100 (37.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7427s / 1062.1157 s
agent0:                 episode reward: -0.4370,                 loss: 0.3542
agent1:                 episode reward: 0.4370,                 loss: nan
Episode: 18981/50100 (37.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7810s / 1063.8967 s
agent0:                 episode reward: 0.1405,                 loss: 0.3518
agent1:                 episode reward: -0.1405,                 loss: nan
Episode: 19001/50100 (37.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7124s / 1065.6092 s
agent0:                 episode reward: -0.3980,                 loss: 0.3521
agent1:                 episode reward: 0.3980,                 loss: nan
Episode: 19021/50100 (37.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7119s / 1067.3211 s
agent0:                 episode reward: -0.1779,                 loss: 0.3461
agent1:                 episode reward: 0.1779,                 loss: nan
Episode: 19041/50100 (38.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7397s / 1069.0607 s
agent0:                 episode reward: -0.4881,                 loss: 0.3063
agent1:                 episode reward: 0.4881,                 loss: nan
Episode: 19061/50100 (38.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7417s / 1070.8024 s
agent0:                 episode reward: -0.1302,                 loss: 0.3028
agent1:                 episode reward: 0.1302,                 loss: nan
Episode: 19081/50100 (38.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7301s / 1072.5325 s
agent0:                 episode reward: -0.1614,                 loss: 0.3022
agent1:                 episode reward: 0.1614,                 loss: nan
Episode: 19101/50100 (38.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7469s / 1074.2794 s
agent0:                 episode reward: -0.2268,                 loss: 0.2993
agent1:                 episode reward: 0.2268,                 loss: nan
Episode: 19121/50100 (38.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7186s / 1075.9980 s
agent0:                 episode reward: -0.2317,                 loss: 0.2967
agent1:                 episode reward: 0.2317,                 loss: nan
Episode: 19141/50100 (38.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7298s / 1077.7278 s
agent0:                 episode reward: -0.4163,                 loss: 0.2985
agent1:                 episode reward: 0.4163,                 loss: nan
Episode: 19161/50100 (38.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7380s / 1079.4658 s
agent0:                 episode reward: 0.3098,                 loss: 0.2964
agent1:                 episode reward: -0.3098,                 loss: nan
Episode: 19181/50100 (38.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7510s / 1081.2168 s
agent0:                 episode reward: -0.5776,                 loss: 0.2999
agent1:                 episode reward: 0.5776,                 loss: nan
Episode: 19201/50100 (38.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7229s / 1082.9397 s
agent0:                 episode reward: 0.2827,                 loss: 0.2939
agent1:                 episode reward: -0.2827,                 loss: nan
Episode: 19221/50100 (38.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7464s / 1084.6861 s
agent0:                 episode reward: 0.1133,                 loss: 0.2885
agent1:                 episode reward: -0.1133,                 loss: nan
Episode: 19241/50100 (38.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7288s / 1086.4149 s
agent0:                 episode reward: -0.0152,                 loss: 0.2877
agent1:                 episode reward: 0.0152,                 loss: nan
Episode: 19261/50100 (38.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7682s / 1088.1831 s
agent0:                 episode reward: -0.1483,                 loss: 0.2914
agent1:                 episode reward: 0.1483,                 loss: nan
Episode: 19281/50100 (38.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7305s / 1089.9137 s
agent0:                 episode reward: -0.0624,                 loss: 0.2882
agent1:                 episode reward: 0.0624,                 loss: nan
Episode: 19301/50100 (38.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7685s / 1091.6821 s
agent0:                 episode reward: 0.1219,                 loss: 0.2889
agent1:                 episode reward: -0.1219,                 loss: nan
Episode: 19321/50100 (38.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7370s / 1093.4192 s
agent0:                 episode reward: -0.3180,                 loss: 0.2875
agent1:                 episode reward: 0.3180,                 loss: nan
Episode: 19341/50100 (38.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7927s / 1095.2119 s
agent0:                 episode reward: 0.0143,                 loss: 0.2886
agent1:                 episode reward: -0.0143,                 loss: nan
Episode: 19361/50100 (38.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7671s / 1096.9789 s
agent0:                 episode reward: 0.6026,                 loss: 0.3622
agent1:                 episode reward: -0.6026,                 loss: nan
Episode: 19381/50100 (38.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7709s / 1098.7499 s
agent0:                 episode reward: 0.1038,                 loss: 0.3887
agent1:                 episode reward: -0.1038,                 loss: nan
Episode: 19401/50100 (38.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7834s / 1100.5333 s
agent0:                 episode reward: 0.0754,                 loss: 0.3894
agent1:                 episode reward: -0.0754,                 loss: nan
Episode: 19421/50100 (38.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7626s / 1102.2959 s
agent0:                 episode reward: -0.5045,                 loss: 0.3892
agent1:                 episode reward: 0.5045,                 loss: nan
Episode: 19441/50100 (38.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7606s / 1104.0565 s
agent0:                 episode reward: -0.1226,                 loss: 0.3872
agent1:                 episode reward: 0.1226,                 loss: nan
Episode: 19461/50100 (38.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8152s / 1105.8717 s
agent0:                 episode reward: -0.1041,                 loss: 0.3902
agent1:                 episode reward: 0.1041,                 loss: nan
Episode: 19481/50100 (38.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8175s / 1107.6892 s
agent0:                 episode reward: 0.3810,                 loss: 0.3891
agent1:                 episode reward: -0.3810,                 loss: nan
Score delta: 2.4057743195594896, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/19269_0.
Episode: 19501/50100 (38.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0230s / 1108.7122 s
agent0:                 episode reward: -0.6879,                 loss: nan
agent1:                 episode reward: 0.6879,                 loss: 0.4483
Episode: 19521/50100 (38.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0210s / 1109.7332 s
agent0:                 episode reward: -0.1497,                 loss: nan
agent1:                 episode reward: 0.1497,                 loss: 0.4502
Episode: 19541/50100 (39.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0310s / 1110.7641 s
agent0:                 episode reward: -0.6810,                 loss: nan
agent1:                 episode reward: 0.6810,                 loss: 0.4506
Episode: 19561/50100 (39.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0210s / 1111.7852 s
agent0:                 episode reward: -0.6053,                 loss: nan
agent1:                 episode reward: 0.6053,                 loss: 0.4501
Score delta: 2.0972958562470687, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/19349_1.
Episode: 19581/50100 (39.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7660s / 1113.5512 s
agent0:                 episode reward: -0.0774,                 loss: 0.4044
agent1:                 episode reward: 0.0774,                 loss: nan
Episode: 19601/50100 (39.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7823s / 1115.3335 s
agent0:                 episode reward: -0.2793,                 loss: 0.3971
agent1:                 episode reward: 0.2793,                 loss: nan
Episode: 19621/50100 (39.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7853s / 1117.1188 s
agent0:                 episode reward: -0.2211,                 loss: 0.3749
agent1:                 episode reward: 0.2211,                 loss: nan
Episode: 19641/50100 (39.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7879s / 1118.9067 s
agent0:                 episode reward: -1.0998,                 loss: 0.3743
agent1:                 episode reward: 1.0998,                 loss: nan
Episode: 19661/50100 (39.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8029s / 1120.7096 s
agent0:                 episode reward: 0.5914,                 loss: 0.3722
agent1:                 episode reward: -0.5914,                 loss: nan
Episode: 19681/50100 (39.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7805s / 1122.4902 s
agent0:                 episode reward: -0.1309,                 loss: 0.3732
agent1:                 episode reward: 0.1309,                 loss: nan
Episode: 19701/50100 (39.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8097s / 1124.2999 s
agent0:                 episode reward: -0.0877,                 loss: 0.3713
agent1:                 episode reward: 0.0877,                 loss: nan
Episode: 19721/50100 (39.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7855s / 1126.0853 s
agent0:                 episode reward: -0.1085,                 loss: 0.3715
agent1:                 episode reward: 0.1085,                 loss: nan
Episode: 19741/50100 (39.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8686s / 1127.9540 s
agent0:                 episode reward: -0.1174,                 loss: 0.3723
agent1:                 episode reward: 0.1174,                 loss: nan
Episode: 19761/50100 (39.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8198s / 1129.7738 s
agent0:                 episode reward: -0.0729,                 loss: 0.3721
agent1:                 episode reward: 0.0729,                 loss: nan
Episode: 19781/50100 (39.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8572s / 1131.6310 s
agent0:                 episode reward: 0.1024,                 loss: 0.3849
agent1:                 episode reward: -0.1024,                 loss: nan
Episode: 19801/50100 (39.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7794s / 1133.4104 s
agent0:                 episode reward: 0.1957,                 loss: 0.3803
agent1:                 episode reward: -0.1957,                 loss: nan
Episode: 19821/50100 (39.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7909s / 1135.2013 s
agent0:                 episode reward: -0.1493,                 loss: 0.3802
agent1:                 episode reward: 0.1493,                 loss: nan
Episode: 19841/50100 (39.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7837s / 1136.9851 s
agent0:                 episode reward: 0.3926,                 loss: 0.3820
agent1:                 episode reward: -0.3926,                 loss: nan
Episode: 19861/50100 (39.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7808s / 1138.7658 s
agent0:                 episode reward: -0.0329,                 loss: 0.3810
agent1:                 episode reward: 0.0329,                 loss: nan
Episode: 19881/50100 (39.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7891s / 1140.5550 s
agent0:                 episode reward: -0.1305,                 loss: 0.3810
agent1:                 episode reward: 0.1305,                 loss: nan
Episode: 19901/50100 (39.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7845s / 1142.3395 s
agent0:                 episode reward: 0.4734,                 loss: 0.3801
agent1:                 episode reward: -0.4734,                 loss: nan
Episode: 19921/50100 (39.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7958s / 1144.1352 s
agent0:                 episode reward: 0.1906,                 loss: 0.3803
agent1:                 episode reward: -0.1906,                 loss: nan
Episode: 19941/50100 (39.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7907s / 1145.9259 s
agent0:                 episode reward: 0.3624,                 loss: 0.3501
agent1:                 episode reward: -0.3624,                 loss: nan
Episode: 19961/50100 (39.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8016s / 1147.7275 s
agent0:                 episode reward: 0.0926,                 loss: 0.3310
agent1:                 episode reward: -0.0926,                 loss: nan
Episode: 19981/50100 (39.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8017s / 1149.5293 s
agent0:                 episode reward: 0.0353,                 loss: 0.3312
agent1:                 episode reward: -0.0353,                 loss: nan
Episode: 20001/50100 (39.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7955s / 1151.3248 s
agent0:                 episode reward: -0.0306,                 loss: 0.3305
agent1:                 episode reward: 0.0306,                 loss: nan
Episode: 20021/50100 (39.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8092s / 1153.1340 s
agent0:                 episode reward: 0.1776,                 loss: 0.3290
agent1:                 episode reward: -0.1776,                 loss: nan
Episode: 20041/50100 (40.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7986s / 1154.9326 s
agent0:                 episode reward: -0.1057,                 loss: 0.3280
agent1:                 episode reward: 0.1057,                 loss: nan
Episode: 20061/50100 (40.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8154s / 1156.7480 s
agent0:                 episode reward: 0.0281,                 loss: 0.3273
agent1:                 episode reward: -0.0281,                 loss: nan
Episode: 20081/50100 (40.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8002s / 1158.5482 s
agent0:                 episode reward: 0.1378,                 loss: 0.3271
agent1:                 episode reward: -0.1378,                 loss: nan
Episode: 20101/50100 (40.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8125s / 1160.3607 s
agent0:                 episode reward: -0.0999,                 loss: 0.3294
agent1:                 episode reward: 0.0999,                 loss: nan
Episode: 20121/50100 (40.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8195s / 1162.1802 s
agent0:                 episode reward: 0.3440,                 loss: 0.3345
agent1:                 episode reward: -0.3440,                 loss: nan
Episode: 20141/50100 (40.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8848s / 1164.0650 s
agent0:                 episode reward: -0.0703,                 loss: 0.3334
agent1:                 episode reward: 0.0703,                 loss: nan
Episode: 20161/50100 (40.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8357s / 1165.9007 s
agent0:                 episode reward: 0.6996,                 loss: 0.3321
agent1:                 episode reward: -0.6996,                 loss: nan
Episode: 20181/50100 (40.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8425s / 1167.7433 s
agent0:                 episode reward: 0.2473,                 loss: 0.3309
agent1:                 episode reward: -0.2473,                 loss: nan
Episode: 20201/50100 (40.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8889s / 1169.6322 s
agent0:                 episode reward: 0.2473,                 loss: 0.3294
agent1:                 episode reward: -0.2473,                 loss: nan
Episode: 20221/50100 (40.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8170s / 1171.4492 s
agent0:                 episode reward: -0.2388,                 loss: 0.3305
agent1:                 episode reward: 0.2388,                 loss: nan
Episode: 20241/50100 (40.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8257s / 1173.2748 s
agent0:                 episode reward: 0.0919,                 loss: 0.3304
agent1:                 episode reward: -0.0919,                 loss: nan
Episode: 20261/50100 (40.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8225s / 1175.0974 s
agent0:                 episode reward: -0.2177,                 loss: 0.3312
agent1:                 episode reward: 0.2177,                 loss: nan
Episode: 20281/50100 (40.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8483s / 1176.9456 s
agent0:                 episode reward: 0.2863,                 loss: 0.4049
agent1:                 episode reward: -0.2863,                 loss: nan
Episode: 20301/50100 (40.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8312s / 1178.7768 s
agent0:                 episode reward: -0.2384,                 loss: 0.4031
agent1:                 episode reward: 0.2384,                 loss: nan
Episode: 20321/50100 (40.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8403s / 1180.6172 s
agent0:                 episode reward: 0.0192,                 loss: 0.4021
agent1:                 episode reward: -0.0192,                 loss: nan
Episode: 20341/50100 (40.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8156s / 1182.4328 s
agent0:                 episode reward: -0.4303,                 loss: 0.4040
agent1:                 episode reward: 0.4303,                 loss: nan
Episode: 20361/50100 (40.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8336s / 1184.2664 s
agent0:                 episode reward: -0.2974,                 loss: 0.4021
agent1:                 episode reward: 0.2974,                 loss: nan
Episode: 20381/50100 (40.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8252s / 1186.0917 s
agent0:                 episode reward: 0.5313,                 loss: 0.4014
agent1:                 episode reward: -0.5313,                 loss: nan
Episode: 20401/50100 (40.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8314s / 1187.9231 s
agent0:                 episode reward: -0.0112,                 loss: 0.4032
agent1:                 episode reward: 0.0112,                 loss: nan
Episode: 20421/50100 (40.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8270s / 1189.7501 s
agent0:                 episode reward: 0.3996,                 loss: 0.4044
agent1:                 episode reward: -0.3996,                 loss: nan
Episode: 20441/50100 (40.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8259s / 1191.5759 s
agent0:                 episode reward: 0.3647,                 loss: 0.4310
agent1:                 episode reward: -0.3647,                 loss: nan
Episode: 20461/50100 (40.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8363s / 1193.4123 s
agent0:                 episode reward: -0.3041,                 loss: 0.4376
agent1:                 episode reward: 0.3041,                 loss: nan
Episode: 20481/50100 (40.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8362s / 1195.2485 s
agent0:                 episode reward: 0.2602,                 loss: 0.4368
agent1:                 episode reward: -0.2602,                 loss: nan
Episode: 20501/50100 (40.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8359s / 1197.0844 s
agent0:                 episode reward: -0.0732,                 loss: 0.4370
agent1:                 episode reward: 0.0732,                 loss: nan
Episode: 20521/50100 (40.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8421s / 1198.9265 s
agent0:                 episode reward: 0.1493,                 loss: 0.4366
agent1:                 episode reward: -0.1493,                 loss: nan
Episode: 20541/50100 (41.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8650s / 1200.7915 s
agent0:                 episode reward: -0.1893,                 loss: 0.4368
agent1:                 episode reward: 0.1893,                 loss: nan
Episode: 20561/50100 (41.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8539s / 1202.6454 s
agent0:                 episode reward: -0.1843,                 loss: 0.4365
agent1:                 episode reward: 0.1843,                 loss: nan
Episode: 20581/50100 (41.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8655s / 1204.5109 s
agent0:                 episode reward: 0.1602,                 loss: 0.4360
agent1:                 episode reward: -0.1602,                 loss: nan
Episode: 20601/50100 (41.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8551s / 1206.3660 s
agent0:                 episode reward: 0.4313,                 loss: 0.4368
agent1:                 episode reward: -0.4313,                 loss: nan
Episode: 20621/50100 (41.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8541s / 1208.2201 s
agent0:                 episode reward: 0.1485,                 loss: 0.4302
agent1:                 episode reward: -0.1485,                 loss: nan
Episode: 20641/50100 (41.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8566s / 1210.0767 s
agent0:                 episode reward: -0.4205,                 loss: 0.4292
agent1:                 episode reward: 0.4205,                 loss: nan
Episode: 20661/50100 (41.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8551s / 1211.9318 s
agent0:                 episode reward: -0.1539,                 loss: 0.4277
agent1:                 episode reward: 0.1539,                 loss: nan
Episode: 20681/50100 (41.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6996s / 1213.6314 s
agent0:                 episode reward: 0.5505,                 loss: 0.4278
agent1:                 episode reward: -0.5505,                 loss: 0.4512
Score delta: 2.024084161842858, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/20465_0.
Episode: 20701/50100 (41.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0450s / 1214.6764 s
agent0:                 episode reward: 0.0254,                 loss: nan
agent1:                 episode reward: -0.0254,                 loss: 0.4493
Episode: 20721/50100 (41.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0228s / 1215.6992 s
agent0:                 episode reward: -0.7224,                 loss: nan
agent1:                 episode reward: 0.7224,                 loss: 0.4487
Episode: 20741/50100 (41.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0254s / 1216.7246 s
agent0:                 episode reward: -0.0004,                 loss: nan
agent1:                 episode reward: 0.0004,                 loss: 0.4496
Episode: 20761/50100 (41.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0310s / 1217.7556 s
agent0:                 episode reward: -0.4809,                 loss: nan
agent1:                 episode reward: 0.4809,                 loss: 0.4493
Episode: 20781/50100 (41.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0343s / 1218.7899 s
agent0:                 episode reward: 0.0645,                 loss: nan
agent1:                 episode reward: -0.0645,                 loss: 0.4488
Episode: 20801/50100 (41.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0663s / 1219.8562 s
agent0:                 episode reward: -0.2803,                 loss: nan
agent1:                 episode reward: 0.2803,                 loss: 0.4486
Episode: 20821/50100 (41.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1104s / 1220.9666 s
agent0:                 episode reward: 0.0267,                 loss: nan
agent1:                 episode reward: -0.0267,                 loss: 0.4496
Episode: 20841/50100 (41.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0514s / 1222.0180 s
agent0:                 episode reward: -0.1234,                 loss: nan
agent1:                 episode reward: 0.1234,                 loss: 0.4491
Episode: 20861/50100 (41.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7676s / 1223.7856 s
agent0:                 episode reward: -0.1545,                 loss: 0.3431
agent1:                 episode reward: 0.1545,                 loss: 0.4490
Score delta: 2.1550605389662616, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/20632_1.
Episode: 20881/50100 (41.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9261s / 1225.7117 s
agent0:                 episode reward: -0.4645,                 loss: 0.3410
agent1:                 episode reward: 0.4645,                 loss: nan
Episode: 20901/50100 (41.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8520s / 1227.5637 s
agent0:                 episode reward: -0.2287,                 loss: 0.3373
agent1:                 episode reward: 0.2287,                 loss: nan
Episode: 20921/50100 (41.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8704s / 1229.4341 s
agent0:                 episode reward: 0.0742,                 loss: 0.3401
agent1:                 episode reward: -0.0742,                 loss: nan
Episode: 20941/50100 (41.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8765s / 1231.3106 s
agent0:                 episode reward: -0.4459,                 loss: 0.3217
agent1:                 episode reward: 0.4459,                 loss: nan
Episode: 20961/50100 (41.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8790s / 1233.1896 s
agent0:                 episode reward: 0.0799,                 loss: 0.2894
agent1:                 episode reward: -0.0799,                 loss: nan
Episode: 20981/50100 (41.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8864s / 1235.0760 s
agent0:                 episode reward: -0.2331,                 loss: 0.2875
agent1:                 episode reward: 0.2331,                 loss: nan
Episode: 21001/50100 (41.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8839s / 1236.9599 s
agent0:                 episode reward: 0.2845,                 loss: 0.2862
agent1:                 episode reward: -0.2845,                 loss: nan
Episode: 21021/50100 (41.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8780s / 1238.8380 s
agent0:                 episode reward: -0.3954,                 loss: 0.2839
agent1:                 episode reward: 0.3954,                 loss: nan
Episode: 21041/50100 (41.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8843s / 1240.7223 s
agent0:                 episode reward: -0.2392,                 loss: 0.2850
agent1:                 episode reward: 0.2392,                 loss: nan
Episode: 21061/50100 (42.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8778s / 1242.6001 s
agent0:                 episode reward: 0.2721,                 loss: 0.2819
agent1:                 episode reward: -0.2721,                 loss: nan
Episode: 21081/50100 (42.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8834s / 1244.4835 s
agent0:                 episode reward: -0.2044,                 loss: 0.2817
agent1:                 episode reward: 0.2044,                 loss: nan
Episode: 21101/50100 (42.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8720s / 1246.3555 s
agent0:                 episode reward: -0.2999,                 loss: 0.2888
agent1:                 episode reward: 0.2999,                 loss: nan
Episode: 21121/50100 (42.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8858s / 1248.2413 s
agent0:                 episode reward: 0.0362,                 loss: 0.2965
agent1:                 episode reward: -0.0362,                 loss: nan
Episode: 21141/50100 (42.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8805s / 1250.1218 s
agent0:                 episode reward: -0.2597,                 loss: 0.2970
agent1:                 episode reward: 0.2597,                 loss: nan
Episode: 21161/50100 (42.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8720s / 1251.9938 s
agent0:                 episode reward: -0.4253,                 loss: 0.2978
agent1:                 episode reward: 0.4253,                 loss: nan
Episode: 21181/50100 (42.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8838s / 1253.8776 s
agent0:                 episode reward: 0.2348,                 loss: 0.2930
agent1:                 episode reward: -0.2348,                 loss: nan
Episode: 21201/50100 (42.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8727s / 1255.7503 s
agent0:                 episode reward: 0.0522,                 loss: 0.2944
agent1:                 episode reward: -0.0522,                 loss: nan
Episode: 21221/50100 (42.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3308s / 1257.0811 s
agent0:                 episode reward: 0.1960,                 loss: 0.2947
agent1:                 episode reward: -0.1960,                 loss: 0.4506
Score delta: 2.0805948146523483, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/20996_0.
Episode: 21241/50100 (42.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0310s / 1258.1121 s
agent0:                 episode reward: -0.4530,                 loss: nan
agent1:                 episode reward: 0.4530,                 loss: 0.4497
Episode: 21261/50100 (42.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0368s / 1259.1489 s
agent0:                 episode reward: -0.4043,                 loss: nan
agent1:                 episode reward: 0.4043,                 loss: 0.4500
Episode: 21281/50100 (42.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0561s / 1260.2050 s
agent0:                 episode reward: -0.0908,                 loss: nan
agent1:                 episode reward: 0.0908,                 loss: 0.4494
Episode: 21301/50100 (42.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5155s / 1261.7205 s
agent0:                 episode reward: -0.5955,                 loss: 0.3226
agent1:                 episode reward: 0.5955,                 loss: 0.4502
Score delta: 2.544173195150705, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/21078_1.
Episode: 21321/50100 (42.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8805s / 1263.6010 s
agent0:                 episode reward: -0.3655,                 loss: 0.3197
agent1:                 episode reward: 0.3655,                 loss: nan
Episode: 21341/50100 (42.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8796s / 1265.4807 s
agent0:                 episode reward: -0.2723,                 loss: 0.3198
agent1:                 episode reward: 0.2723,                 loss: nan
Episode: 21361/50100 (42.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8793s / 1267.3600 s
agent0:                 episode reward: -0.4047,                 loss: 0.3650
agent1:                 episode reward: 0.4047,                 loss: nan
Episode: 21381/50100 (42.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8988s / 1269.2588 s
agent0:                 episode reward: 0.2183,                 loss: 0.3645
agent1:                 episode reward: -0.2183,                 loss: nan
Episode: 21401/50100 (42.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2294s / 1270.4882 s
agent0:                 episode reward: 0.1715,                 loss: 0.3631
agent1:                 episode reward: -0.1715,                 loss: 0.4503
Score delta: 2.6425449829729146, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/21173_0.
Episode: 21421/50100 (42.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0408s / 1271.5290 s
agent0:                 episode reward: -0.2668,                 loss: nan
agent1:                 episode reward: 0.2668,                 loss: 0.4500
Episode: 21441/50100 (42.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3509s / 1272.8798 s
agent0:                 episode reward: -0.6324,                 loss: 0.3447
agent1:                 episode reward: 0.6324,                 loss: 0.4454
Score delta: 2.1560155086945394, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/21222_1.
Episode: 21461/50100 (42.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8965s / 1274.7764 s
agent0:                 episode reward: 0.2653,                 loss: 0.3468
agent1:                 episode reward: -0.2653,                 loss: nan
Episode: 21481/50100 (42.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8951s / 1276.6715 s
agent0:                 episode reward: -0.2733,                 loss: 0.3448
agent1:                 episode reward: 0.2733,                 loss: nan
Episode: 21501/50100 (42.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8895s / 1278.5610 s
agent0:                 episode reward: -0.4196,                 loss: 0.3458
agent1:                 episode reward: 0.4196,                 loss: nan
Episode: 21521/50100 (42.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9109s / 1280.4719 s
agent0:                 episode reward: -0.1112,                 loss: 0.3436
agent1:                 episode reward: 0.1112,                 loss: nan
Episode: 21541/50100 (42.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9138s / 1282.3857 s
agent0:                 episode reward: 0.4273,                 loss: 0.3451
agent1:                 episode reward: -0.4273,                 loss: nan
Episode: 21561/50100 (43.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9214s / 1284.3071 s
agent0:                 episode reward: -0.0298,                 loss: 0.3572
agent1:                 episode reward: 0.0298,                 loss: nan
Episode: 21581/50100 (43.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9174s / 1286.2245 s
agent0:                 episode reward: 0.1261,                 loss: 0.4181
agent1:                 episode reward: -0.1261,                 loss: nan
Episode: 21601/50100 (43.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9177s / 1288.1422 s
agent0:                 episode reward: 0.0532,                 loss: 0.4167
agent1:                 episode reward: -0.0532,                 loss: nan
Episode: 21621/50100 (43.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8684s / 1290.0106 s
agent0:                 episode reward: 0.4410,                 loss: 0.4138
agent1:                 episode reward: -0.4410,                 loss: 0.4503
Score delta: 2.263058822073529, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/21408_0.
Episode: 21641/50100 (43.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0677s / 1291.0783 s
agent0:                 episode reward: -0.0021,                 loss: nan
agent1:                 episode reward: 0.0021,                 loss: 0.4510
Episode: 21661/50100 (43.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0655s / 1292.1439 s
agent0:                 episode reward: -0.1982,                 loss: nan
agent1:                 episode reward: 0.1982,                 loss: 0.4510
Episode: 21681/50100 (43.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0683s / 1293.2121 s
agent0:                 episode reward: -0.4045,                 loss: nan
agent1:                 episode reward: 0.4045,                 loss: 0.4500
Episode: 21701/50100 (43.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0463s / 1294.2584 s
agent0:                 episode reward: 0.0717,                 loss: nan
agent1:                 episode reward: -0.0717,                 loss: 0.4502
Episode: 21721/50100 (43.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0480s / 1295.3064 s
agent0:                 episode reward: -0.5925,                 loss: nan
agent1:                 episode reward: 0.5925,                 loss: 0.4503
Episode: 21741/50100 (43.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5171s / 1296.8236 s
agent0:                 episode reward: -0.2992,                 loss: 0.2889
agent1:                 episode reward: 0.2992,                 loss: 0.4490
Score delta: 2.038343878541897, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/21518_1.
Episode: 21761/50100 (43.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8975s / 1298.7211 s
agent0:                 episode reward: -0.6304,                 loss: 0.2908
agent1:                 episode reward: 0.6304,                 loss: nan
Episode: 21781/50100 (43.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9441s / 1300.6652 s
agent0:                 episode reward: 0.1000,                 loss: 0.2854
agent1:                 episode reward: -0.1000,                 loss: nan
Episode: 21801/50100 (43.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9318s / 1302.5969 s
agent0:                 episode reward: 0.0789,                 loss: 0.2863
agent1:                 episode reward: -0.0789,                 loss: nan
Episode: 21821/50100 (43.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9129s / 1304.5098 s
agent0:                 episode reward: 0.4354,                 loss: 0.2855
agent1:                 episode reward: -0.4354,                 loss: nan
Episode: 21841/50100 (43.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9070s / 1306.4168 s
agent0:                 episode reward: -0.5505,                 loss: 0.3054
agent1:                 episode reward: 0.5505,                 loss: nan
Episode: 21861/50100 (43.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9101s / 1308.3269 s
agent0:                 episode reward: -0.3382,                 loss: 0.3531
agent1:                 episode reward: 0.3382,                 loss: nan
Episode: 21881/50100 (43.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8974s / 1310.2243 s
agent0:                 episode reward: -0.4077,                 loss: 0.3519
agent1:                 episode reward: 0.4077,                 loss: nan
Episode: 21901/50100 (43.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9112s / 1312.1355 s
agent0:                 episode reward: 0.1509,                 loss: 0.3541
agent1:                 episode reward: -0.1509,                 loss: nan
Episode: 21921/50100 (43.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9070s / 1314.0424 s
agent0:                 episode reward: -0.0195,                 loss: 0.3504
agent1:                 episode reward: 0.0195,                 loss: nan
Episode: 21941/50100 (43.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9007s / 1315.9432 s
agent0:                 episode reward: -0.3504,                 loss: 0.3513
agent1:                 episode reward: 0.3504,                 loss: nan
Episode: 21961/50100 (43.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9052s / 1317.8483 s
agent0:                 episode reward: -0.3137,                 loss: 0.3488
agent1:                 episode reward: 0.3137,                 loss: nan
Episode: 21981/50100 (43.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9189s / 1319.7673 s
agent0:                 episode reward: -0.1268,                 loss: 0.3519
agent1:                 episode reward: 0.1268,                 loss: nan
Episode: 22001/50100 (43.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4494s / 1321.2167 s
agent0:                 episode reward: -0.1641,                 loss: 0.3497
agent1:                 episode reward: 0.1641,                 loss: 0.4459
Score delta: 2.104957848297519, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/21778_0.
Episode: 22021/50100 (43.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0697s / 1322.2863 s
agent0:                 episode reward: -0.5699,                 loss: nan
agent1:                 episode reward: 0.5699,                 loss: 0.4443
Episode: 22041/50100 (43.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0614s / 1323.3478 s
agent0:                 episode reward: -0.1593,                 loss: nan
agent1:                 episode reward: 0.1593,                 loss: 0.4462
Episode: 22061/50100 (44.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0349s / 1324.3827 s
agent0:                 episode reward: -0.3151,                 loss: nan
agent1:                 episode reward: 0.3151,                 loss: 0.4483
Episode: 22081/50100 (44.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0577s / 1325.4404 s
agent0:                 episode reward: 0.4372,                 loss: nan
agent1:                 episode reward: -0.4372,                 loss: 0.4489
Episode: 22101/50100 (44.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0602s / 1326.5006 s
agent0:                 episode reward: 0.0889,                 loss: nan
agent1:                 episode reward: -0.0889,                 loss: 0.4489
Episode: 22121/50100 (44.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1758s / 1327.6764 s
agent0:                 episode reward: -0.7441,                 loss: 0.3026
agent1:                 episode reward: 0.7441,                 loss: 0.4481
Score delta: 2.2562905117042784, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/21906_1.
Episode: 22141/50100 (44.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9088s / 1329.5852 s
agent0:                 episode reward: -0.0208,                 loss: 0.3408
agent1:                 episode reward: 0.0208,                 loss: nan
Episode: 22161/50100 (44.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8865s / 1331.4717 s
agent0:                 episode reward: -0.3272,                 loss: 0.3682
agent1:                 episode reward: 0.3272,                 loss: nan
Episode: 22181/50100 (44.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9199s / 1333.3917 s
agent0:                 episode reward: -0.3811,                 loss: 0.3662
agent1:                 episode reward: 0.3811,                 loss: nan
Episode: 22201/50100 (44.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9183s / 1335.3099 s
agent0:                 episode reward: 0.4250,                 loss: 0.3649
agent1:                 episode reward: -0.4250,                 loss: nan
Episode: 22221/50100 (44.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9144s / 1337.2243 s
agent0:                 episode reward: -0.0568,                 loss: 0.3656
agent1:                 episode reward: 0.0568,                 loss: nan
Episode: 22241/50100 (44.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9219s / 1339.1462 s
agent0:                 episode reward: -0.0222,                 loss: 0.3641
agent1:                 episode reward: 0.0222,                 loss: nan
Episode: 22261/50100 (44.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9161s / 1341.0623 s
agent0:                 episode reward: 0.1660,                 loss: 0.3607
agent1:                 episode reward: -0.1660,                 loss: nan
Episode: 22281/50100 (44.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9112s / 1342.9735 s
agent0:                 episode reward: 0.0188,                 loss: 0.3617
agent1:                 episode reward: -0.0188,                 loss: nan
Episode: 22301/50100 (44.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9039s / 1344.8774 s
agent0:                 episode reward: 0.1132,                 loss: 0.3798
agent1:                 episode reward: -0.1132,                 loss: nan
Episode: 22321/50100 (44.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9034s / 1346.7808 s
agent0:                 episode reward: 0.0916,                 loss: 0.4324
agent1:                 episode reward: -0.0916,                 loss: nan
Episode: 22341/50100 (44.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9080s / 1348.6888 s
agent0:                 episode reward: 0.2483,                 loss: 0.4319
agent1:                 episode reward: -0.2483,                 loss: nan
Episode: 22361/50100 (44.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9136s / 1350.6024 s
agent0:                 episode reward: 0.4074,                 loss: 0.4301
agent1:                 episode reward: -0.4074,                 loss: nan
Episode: 22381/50100 (44.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8987s / 1352.5011 s
agent0:                 episode reward: -0.6230,                 loss: 0.4307
agent1:                 episode reward: 0.6230,                 loss: nan
Episode: 22401/50100 (44.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9111s / 1354.4122 s
agent0:                 episode reward: -0.3664,                 loss: 0.4312
agent1:                 episode reward: 0.3664,                 loss: nan
Episode: 22421/50100 (44.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9242s / 1356.3364 s
agent0:                 episode reward: 0.0636,                 loss: 0.4290
agent1:                 episode reward: -0.0636,                 loss: nan
Episode: 22441/50100 (44.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9279s / 1358.2643 s
agent0:                 episode reward: -0.1035,                 loss: 0.4313
agent1:                 episode reward: 0.1035,                 loss: nan
Episode: 22461/50100 (44.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9125s / 1360.1767 s
agent0:                 episode reward: -0.0272,                 loss: 0.4301
agent1:                 episode reward: 0.0272,                 loss: nan
Episode: 22481/50100 (44.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9152s / 1362.0920 s
agent0:                 episode reward: -0.1625,                 loss: 0.4400
agent1:                 episode reward: 0.1625,                 loss: nan
Episode: 22501/50100 (44.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9390s / 1364.0309 s
agent0:                 episode reward: -0.0633,                 loss: 0.4379
agent1:                 episode reward: 0.0633,                 loss: nan
Episode: 22521/50100 (44.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9618s / 1365.9927 s
agent0:                 episode reward: 0.2338,                 loss: 0.4384
agent1:                 episode reward: -0.2338,                 loss: nan
Episode: 22541/50100 (44.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9531s / 1367.9458 s
agent0:                 episode reward: -0.2500,                 loss: 0.4381
agent1:                 episode reward: 0.2500,                 loss: nan
Episode: 22561/50100 (45.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9402s / 1369.8860 s
agent0:                 episode reward: 0.2081,                 loss: 0.4386
agent1:                 episode reward: -0.2081,                 loss: nan
Episode: 22581/50100 (45.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9547s / 1371.8407 s
agent0:                 episode reward: 0.1340,                 loss: 0.4380
agent1:                 episode reward: -0.1340,                 loss: nan
Episode: 22601/50100 (45.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9619s / 1373.8026 s
agent0:                 episode reward: -0.1768,                 loss: 0.4370
agent1:                 episode reward: 0.1768,                 loss: nan
Episode: 22621/50100 (45.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9452s / 1375.7479 s
agent0:                 episode reward: -0.1118,                 loss: 0.4372
agent1:                 episode reward: 0.1118,                 loss: nan
Episode: 22641/50100 (45.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9258s / 1377.6736 s
agent0:                 episode reward: 0.0037,                 loss: 0.4255
agent1:                 episode reward: -0.0037,                 loss: nan
Episode: 22661/50100 (45.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9026s / 1379.5762 s
agent0:                 episode reward: -0.1987,                 loss: 0.4142
agent1:                 episode reward: 0.1987,                 loss: nan
Episode: 22681/50100 (45.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9254s / 1381.5016 s
agent0:                 episode reward: 0.1870,                 loss: 0.4144
agent1:                 episode reward: -0.1870,                 loss: nan
Episode: 22701/50100 (45.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1521s / 1382.6537 s
agent0:                 episode reward: -0.0393,                 loss: 0.4163
agent1:                 episode reward: 0.0393,                 loss: 0.4482
Score delta: 2.100959555015107, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/22471_0.
Episode: 22721/50100 (45.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0680s / 1383.7217 s
agent0:                 episode reward: -0.2845,                 loss: nan
agent1:                 episode reward: 0.2845,                 loss: 0.4468
Episode: 22741/50100 (45.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4695s / 1385.1912 s
agent0:                 episode reward: -0.5049,                 loss: 0.4179
agent1:                 episode reward: 0.5049,                 loss: 0.4466
Score delta: 2.3269299799270753, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/22520_1.
Episode: 22761/50100 (45.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9145s / 1387.1058 s
agent0:                 episode reward: -0.2916,                 loss: 0.4147
agent1:                 episode reward: 0.2916,                 loss: nan
Episode: 22781/50100 (45.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9182s / 1389.0240 s
agent0:                 episode reward: -0.0965,                 loss: 0.4145
agent1:                 episode reward: 0.0965,                 loss: nan
Episode: 22801/50100 (45.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9176s / 1390.9416 s
agent0:                 episode reward: -0.1467,                 loss: 0.4148
agent1:                 episode reward: 0.1467,                 loss: nan
Episode: 22821/50100 (45.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8971s / 1392.8387 s
agent0:                 episode reward: 0.1077,                 loss: 0.4135
agent1:                 episode reward: -0.1077,                 loss: nan
Episode: 22841/50100 (45.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9425s / 1394.7812 s
agent0:                 episode reward: -0.2622,                 loss: 0.4136
agent1:                 episode reward: 0.2622,                 loss: nan
Episode: 22861/50100 (45.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0158s / 1396.7971 s
agent0:                 episode reward: 0.4195,                 loss: 0.3912
agent1:                 episode reward: -0.4195,                 loss: nan
Episode: 22881/50100 (45.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9735s / 1398.7705 s
agent0:                 episode reward: 0.0719,                 loss: 0.3816
agent1:                 episode reward: -0.0719,                 loss: nan
Episode: 22901/50100 (45.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9621s / 1400.7327 s
agent0:                 episode reward: -0.0754,                 loss: 0.3806
agent1:                 episode reward: 0.0754,                 loss: nan
Episode: 22921/50100 (45.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9336s / 1402.6663 s
agent0:                 episode reward: -0.6377,                 loss: 0.3831
agent1:                 episode reward: 0.6377,                 loss: nan
Episode: 22941/50100 (45.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9254s / 1404.5917 s
agent0:                 episode reward: -0.4435,                 loss: 0.3838
agent1:                 episode reward: 0.4435,                 loss: nan
Episode: 22961/50100 (45.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9248s / 1406.5165 s
agent0:                 episode reward: -0.8242,                 loss: 0.3826
agent1:                 episode reward: 0.8242,                 loss: nan
Episode: 22981/50100 (45.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8288s / 1408.3453 s
agent0:                 episode reward: 0.2464,                 loss: 0.3821
agent1:                 episode reward: -0.2464,                 loss: 0.4435
Score delta: 2.079967595596103, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/22767_0.
Episode: 23001/50100 (45.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0670s / 1409.4123 s
agent0:                 episode reward: -0.4735,                 loss: nan
agent1:                 episode reward: 0.4735,                 loss: 0.4422
Episode: 23021/50100 (45.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0779s / 1410.4902 s
agent0:                 episode reward: 0.1730,                 loss: nan
agent1:                 episode reward: -0.1730,                 loss: 0.4405
Episode: 23041/50100 (45.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1660s / 1411.6562 s
agent0:                 episode reward: -0.3873,                 loss: 0.2706
agent1:                 episode reward: 0.3873,                 loss: 0.4381
Score delta: 2.4168562788124697, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/22827_1.
Episode: 23061/50100 (46.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9166s / 1413.5728 s
agent0:                 episode reward: 0.0961,                 loss: 0.2750
agent1:                 episode reward: -0.0961,                 loss: nan
Episode: 23081/50100 (46.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9627s / 1415.5355 s
agent0:                 episode reward: -0.3056,                 loss: 0.3064
agent1:                 episode reward: 0.3056,                 loss: nan
Episode: 23101/50100 (46.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9237s / 1417.4592 s
agent0:                 episode reward: -0.1262,                 loss: 0.3427
agent1:                 episode reward: 0.1262,                 loss: nan
Episode: 23121/50100 (46.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9241s / 1419.3833 s
agent0:                 episode reward: -0.3703,                 loss: 0.3432
agent1:                 episode reward: 0.3703,                 loss: nan
Episode: 23141/50100 (46.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9229s / 1421.3063 s
agent0:                 episode reward: -0.0503,                 loss: 0.3420
agent1:                 episode reward: 0.0503,                 loss: nan
Episode: 23161/50100 (46.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9256s / 1423.2318 s
agent0:                 episode reward: -0.4173,                 loss: 0.3408
agent1:                 episode reward: 0.4173,                 loss: nan
Episode: 23181/50100 (46.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9876s / 1425.2195 s
agent0:                 episode reward: -0.4700,                 loss: 0.3421
agent1:                 episode reward: 0.4700,                 loss: nan
Episode: 23201/50100 (46.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9742s / 1427.1937 s
agent0:                 episode reward: 0.0416,                 loss: 0.3392
agent1:                 episode reward: -0.0416,                 loss: nan
Episode: 23221/50100 (46.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9214s / 1429.1152 s
agent0:                 episode reward: 0.1099,                 loss: 0.3406
agent1:                 episode reward: -0.1099,                 loss: nan
Episode: 23241/50100 (46.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9251s / 1431.0403 s
agent0:                 episode reward: -0.2293,                 loss: 0.3543
agent1:                 episode reward: 0.2293,                 loss: nan
Episode: 23261/50100 (46.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9340s / 1432.9743 s
agent0:                 episode reward: -0.3696,                 loss: 0.4184
agent1:                 episode reward: 0.3696,                 loss: nan
Episode: 23281/50100 (46.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9283s / 1434.9025 s
agent0:                 episode reward: -0.3918,                 loss: 0.4181
agent1:                 episode reward: 0.3918,                 loss: nan
Episode: 23301/50100 (46.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9135s / 1436.8160 s
agent0:                 episode reward: -0.2996,                 loss: 0.4179
agent1:                 episode reward: 0.2996,                 loss: nan
Episode: 23321/50100 (46.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9178s / 1438.7338 s
agent0:                 episode reward: -0.0152,                 loss: 0.4163
agent1:                 episode reward: 0.0152,                 loss: nan
Episode: 23341/50100 (46.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9257s / 1440.6594 s
agent0:                 episode reward: 0.0834,                 loss: 0.4182
agent1:                 episode reward: -0.0834,                 loss: nan
Episode: 23361/50100 (46.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9125s / 1442.5720 s
agent0:                 episode reward: -0.0606,                 loss: 0.4182
agent1:                 episode reward: 0.0606,                 loss: nan
Episode: 23381/50100 (46.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9404s / 1444.5124 s
agent0:                 episode reward: 0.5823,                 loss: 0.4172
agent1:                 episode reward: -0.5823,                 loss: nan
Episode: 23401/50100 (46.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9234s / 1446.4358 s
agent0:                 episode reward: -0.1119,                 loss: 0.4156
agent1:                 episode reward: 0.1119,                 loss: nan
Episode: 23421/50100 (46.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9179s / 1448.3537 s
agent0:                 episode reward: -0.2507,                 loss: 0.4226
agent1:                 episode reward: 0.2507,                 loss: nan
Episode: 23441/50100 (46.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9070s / 1450.2606 s
agent0:                 episode reward: -0.1244,                 loss: 0.4186
agent1:                 episode reward: 0.1244,                 loss: nan
Episode: 23461/50100 (46.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9151s / 1452.1758 s
agent0:                 episode reward: 0.0925,                 loss: 0.4177
agent1:                 episode reward: -0.0925,                 loss: nan
Episode: 23481/50100 (46.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9120s / 1454.0877 s
agent0:                 episode reward: -0.2678,                 loss: 0.4183
agent1:                 episode reward: 0.2678,                 loss: nan
Episode: 23501/50100 (46.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9266s / 1456.0144 s
agent0:                 episode reward: -0.4033,                 loss: 0.4188
agent1:                 episode reward: 0.4033,                 loss: nan
Episode: 23521/50100 (46.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9361s / 1457.9505 s
agent0:                 episode reward: 0.3791,                 loss: 0.4180
agent1:                 episode reward: -0.3791,                 loss: nan
Episode: 23541/50100 (46.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9906s / 1459.9411 s
agent0:                 episode reward: 0.0516,                 loss: 0.4159
agent1:                 episode reward: -0.0516,                 loss: nan
Episode: 23561/50100 (47.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9339s / 1461.8750 s
agent0:                 episode reward: -0.0052,                 loss: 0.4157
agent1:                 episode reward: 0.0052,                 loss: nan
Episode: 23581/50100 (47.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2574s / 1463.1324 s
agent0:                 episode reward: 0.1278,                 loss: 0.4134
agent1:                 episode reward: -0.1278,                 loss: 0.4539
Score delta: 2.121760670490672, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/23353_0.
Episode: 23601/50100 (47.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0876s / 1464.2199 s
agent0:                 episode reward: -0.4869,                 loss: nan
agent1:                 episode reward: 0.4869,                 loss: 0.4518
Episode: 23621/50100 (47.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1139s / 1465.3338 s
agent0:                 episode reward: -0.3550,                 loss: nan
agent1:                 episode reward: 0.3550,                 loss: 0.4517
Episode: 23641/50100 (47.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7491s / 1467.0830 s
agent0:                 episode reward: -0.7527,                 loss: 0.3577
agent1:                 episode reward: 0.7527,                 loss: 0.4516
Score delta: 2.028954008976013, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/23414_1.
Episode: 23661/50100 (47.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9460s / 1469.0290 s
agent0:                 episode reward: -0.5863,                 loss: 0.3256
agent1:                 episode reward: 0.5863,                 loss: nan
Episode: 23681/50100 (47.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9550s / 1470.9840 s
agent0:                 episode reward: -0.5480,                 loss: 0.3179
agent1:                 episode reward: 0.5480,                 loss: nan
Episode: 23701/50100 (47.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6004s / 1472.5844 s
agent0:                 episode reward: 0.1024,                 loss: 0.3143
agent1:                 episode reward: -0.1024,                 loss: 0.4454
Score delta: 2.075976853950256, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/23480_0.
Episode: 23721/50100 (47.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1036s / 1473.6880 s
agent0:                 episode reward: -0.1552,                 loss: nan
agent1:                 episode reward: 0.1552,                 loss: 0.4457
Episode: 23741/50100 (47.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1058s / 1474.7938 s
agent0:                 episode reward: -0.1188,                 loss: nan
agent1:                 episode reward: 0.1188,                 loss: 0.4463
Episode: 23761/50100 (47.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1194s / 1475.9132 s
agent0:                 episode reward: -0.1370,                 loss: nan
agent1:                 episode reward: 0.1370,                 loss: 0.4448
Episode: 23781/50100 (47.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1239s / 1477.0371 s
agent0:                 episode reward: -0.1951,                 loss: nan
agent1:                 episode reward: 0.1951,                 loss: 0.4472
Episode: 23801/50100 (47.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1107s / 1478.1478 s
agent0:                 episode reward: -0.6802,                 loss: nan
agent1:                 episode reward: 0.6802,                 loss: 0.4499
Episode: 23821/50100 (47.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1065s / 1479.2543 s
agent0:                 episode reward: -0.5502,                 loss: nan
agent1:                 episode reward: 0.5502,                 loss: 0.4498
Episode: 23841/50100 (47.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1208s / 1480.3752 s
agent0:                 episode reward: -0.2344,                 loss: nan
agent1:                 episode reward: 0.2344,                 loss: 0.4491
Episode: 23861/50100 (47.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8374s / 1482.2126 s
agent0:                 episode reward: -0.2507,                 loss: 0.4280
agent1:                 episode reward: 0.2507,                 loss: 0.4503
Score delta: 2.1496000217043347, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/23633_1.
Episode: 23881/50100 (47.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9586s / 1484.1712 s
agent0:                 episode reward: -0.0299,                 loss: 0.4286
agent1:                 episode reward: 0.0299,                 loss: nan
Episode: 23901/50100 (47.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0205s / 1486.1917 s
agent0:                 episode reward: -0.1614,                 loss: 0.4295
agent1:                 episode reward: 0.1614,                 loss: nan
Episode: 23921/50100 (47.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9917s / 1488.1835 s
agent0:                 episode reward: -0.8092,                 loss: 0.4277
agent1:                 episode reward: 0.8092,                 loss: nan
Episode: 23941/50100 (47.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9420s / 1490.1255 s
agent0:                 episode reward: 0.0054,                 loss: 0.4281
agent1:                 episode reward: -0.0054,                 loss: nan
Episode: 23961/50100 (47.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9413s / 1492.0668 s
agent0:                 episode reward: -0.2297,                 loss: 0.4270
agent1:                 episode reward: 0.2297,                 loss: nan
Episode: 23981/50100 (47.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9418s / 1494.0086 s
agent0:                 episode reward: -0.6714,                 loss: 0.4227
agent1:                 episode reward: 0.6714,                 loss: nan
Episode: 24001/50100 (47.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9410s / 1495.9496 s
agent0:                 episode reward: 0.1426,                 loss: 0.4234
agent1:                 episode reward: -0.1426,                 loss: nan
Episode: 24021/50100 (47.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9559s / 1497.9055 s
agent0:                 episode reward: 0.0495,                 loss: 0.4219
agent1:                 episode reward: -0.0495,                 loss: nan
Episode: 24041/50100 (47.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9177s / 1499.8232 s
agent0:                 episode reward: 0.3029,                 loss: 0.4217
agent1:                 episode reward: -0.3029,                 loss: nan
Episode: 24061/50100 (48.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9960s / 1501.8192 s
agent0:                 episode reward: 0.3417,                 loss: 0.4227
agent1:                 episode reward: -0.3417,                 loss: nan
Episode: 24081/50100 (48.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9246s / 1503.7438 s
agent0:                 episode reward: -0.0994,                 loss: 0.4221
agent1:                 episode reward: 0.0994,                 loss: nan
Episode: 24101/50100 (48.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9402s / 1505.6840 s
agent0:                 episode reward: -0.0278,                 loss: 0.4216
agent1:                 episode reward: 0.0278,                 loss: nan
Episode: 24121/50100 (48.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9405s / 1507.6246 s
agent0:                 episode reward: -0.1691,                 loss: 0.4214
agent1:                 episode reward: 0.1691,                 loss: nan
Episode: 24141/50100 (48.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9748s / 1509.5994 s
agent0:                 episode reward: 0.1162,                 loss: 0.3906
agent1:                 episode reward: -0.1162,                 loss: nan
Episode: 24161/50100 (48.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9468s / 1511.5462 s
agent0:                 episode reward: -0.1528,                 loss: 0.3892
agent1:                 episode reward: 0.1528,                 loss: nan
Episode: 24181/50100 (48.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9481s / 1513.4943 s
agent0:                 episode reward: 0.0666,                 loss: 0.3879
agent1:                 episode reward: -0.0666,                 loss: nan
Episode: 24201/50100 (48.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9531s / 1515.4474 s
agent0:                 episode reward: -0.1571,                 loss: 0.3871
agent1:                 episode reward: 0.1571,                 loss: nan
Episode: 24221/50100 (48.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0081s / 1517.4554 s
agent0:                 episode reward: -0.4613,                 loss: 0.3856
agent1:                 episode reward: 0.4613,                 loss: nan
Episode: 24241/50100 (48.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9416s / 1519.3971 s
agent0:                 episode reward: -0.4904,                 loss: 0.3865
agent1:                 episode reward: 0.4904,                 loss: nan
Episode: 24261/50100 (48.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9476s / 1521.3447 s
agent0:                 episode reward: 0.1751,                 loss: 0.3855
agent1:                 episode reward: -0.1751,                 loss: nan
Episode: 24281/50100 (48.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9440s / 1523.2887 s
agent0:                 episode reward: -0.1492,                 loss: 0.3835
agent1:                 episode reward: 0.1492,                 loss: nan
Episode: 24301/50100 (48.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9371s / 1525.2259 s
agent0:                 episode reward: -0.6458,                 loss: 0.3703
agent1:                 episode reward: 0.6458,                 loss: nan
Episode: 24321/50100 (48.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9799s / 1527.2057 s
agent0:                 episode reward: -0.3414,                 loss: 0.3624
agent1:                 episode reward: 0.3414,                 loss: nan
Episode: 24341/50100 (48.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9402s / 1529.1459 s
agent0:                 episode reward: 0.1405,                 loss: 0.3616
agent1:                 episode reward: -0.1405,                 loss: nan
Episode: 24361/50100 (48.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9517s / 1531.0976 s
agent0:                 episode reward: -0.0112,                 loss: 0.3617
agent1:                 episode reward: 0.0112,                 loss: nan
Episode: 24381/50100 (48.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9438s / 1533.0414 s
agent0:                 episode reward: -0.5134,                 loss: 0.3611
agent1:                 episode reward: 0.5134,                 loss: nan
Episode: 24401/50100 (48.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6301s / 1534.6715 s
agent0:                 episode reward: 0.1327,                 loss: 0.3579
agent1:                 episode reward: -0.1327,                 loss: 0.4406
Score delta: 2.015122574260003, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/24182_0.
Episode: 24421/50100 (48.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1067s / 1535.7782 s
agent0:                 episode reward: -0.3875,                 loss: nan
agent1:                 episode reward: 0.3875,                 loss: 0.4407
Episode: 24441/50100 (48.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0874s / 1536.8656 s
agent0:                 episode reward: -0.5576,                 loss: nan
agent1:                 episode reward: 0.5576,                 loss: 0.4411
Episode: 24461/50100 (48.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1047s / 1537.9703 s
agent0:                 episode reward: -0.0753,                 loss: nan
agent1:                 episode reward: 0.0753,                 loss: 0.4407
Episode: 24481/50100 (48.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1128s / 1539.0831 s
agent0:                 episode reward: -0.4268,                 loss: nan
agent1:                 episode reward: 0.4268,                 loss: 0.4408
Episode: 24501/50100 (48.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6180s / 1540.7011 s
agent0:                 episode reward: -0.6205,                 loss: 0.2880
agent1:                 episode reward: 0.6205,                 loss: 0.4411
Score delta: 2.3052570404777972, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/24277_1.
Episode: 24521/50100 (48.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9400s / 1542.6411 s
agent0:                 episode reward: -0.1862,                 loss: 0.2866
agent1:                 episode reward: 0.1862,                 loss: nan
Episode: 24541/50100 (48.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9618s / 1544.6029 s
agent0:                 episode reward: -0.0413,                 loss: 0.2825
agent1:                 episode reward: 0.0413,                 loss: nan
Episode: 24561/50100 (49.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0158s / 1546.6186 s
agent0:                 episode reward: 0.2039,                 loss: 0.3466
agent1:                 episode reward: -0.2039,                 loss: nan
Episode: 24581/50100 (49.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9722s / 1548.5908 s
agent0:                 episode reward: 0.1149,                 loss: 0.3767
agent1:                 episode reward: -0.1149,                 loss: nan
Episode: 24601/50100 (49.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9906s / 1550.5814 s
agent0:                 episode reward: 0.2660,                 loss: 0.3776
agent1:                 episode reward: -0.2660,                 loss: nan
Episode: 24621/50100 (49.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0305s / 1552.6119 s
agent0:                 episode reward: -0.1515,                 loss: 0.3772
agent1:                 episode reward: 0.1515,                 loss: nan
Episode: 24641/50100 (49.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0426s / 1554.6545 s
agent0:                 episode reward: -0.3299,                 loss: 0.3774
agent1:                 episode reward: 0.3299,                 loss: nan
Episode: 24661/50100 (49.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9660s / 1556.6205 s
agent0:                 episode reward: 0.8307,                 loss: 0.3753
agent1:                 episode reward: -0.8307,                 loss: 0.4543
Score delta: 2.22027247929194, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/24448_0.
Episode: 24681/50100 (49.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1209s / 1557.7413 s
agent0:                 episode reward: -0.1048,                 loss: nan
agent1:                 episode reward: 0.1048,                 loss: 0.4532
Episode: 24701/50100 (49.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1602s / 1558.9015 s
agent0:                 episode reward: -0.6697,                 loss: nan
agent1:                 episode reward: 0.6697,                 loss: 0.4523
Episode: 24721/50100 (49.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5131s / 1560.4147 s
agent0:                 episode reward: -0.3856,                 loss: 0.3396
agent1:                 episode reward: 0.3856,                 loss: 0.4520
Score delta: 2.092219354688951, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/24501_1.
Episode: 24741/50100 (49.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9565s / 1562.3712 s
agent0:                 episode reward: 0.0465,                 loss: 0.3365
agent1:                 episode reward: -0.0465,                 loss: nan
Episode: 24761/50100 (49.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9543s / 1564.3255 s
agent0:                 episode reward: 0.4292,                 loss: 0.3352
agent1:                 episode reward: -0.4292,                 loss: nan
Episode: 24781/50100 (49.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9511s / 1566.2766 s
agent0:                 episode reward: -0.5909,                 loss: 0.3434
agent1:                 episode reward: 0.5909,                 loss: nan
Episode: 24801/50100 (49.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0033s / 1568.2799 s
agent0:                 episode reward: -0.0042,                 loss: 0.3332
agent1:                 episode reward: 0.0042,                 loss: nan
Episode: 24821/50100 (49.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9384s / 1570.2182 s
agent0:                 episode reward: -0.3048,                 loss: 0.3302
agent1:                 episode reward: 0.3048,                 loss: nan
Episode: 24841/50100 (49.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9202s / 1572.1384 s
agent0:                 episode reward: 0.0879,                 loss: 0.3286
agent1:                 episode reward: -0.0879,                 loss: nan
Episode: 24861/50100 (49.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9249s / 1574.0633 s
agent0:                 episode reward: -1.0724,                 loss: 0.3298
agent1:                 episode reward: 1.0724,                 loss: nan
Episode: 24881/50100 (49.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9222s / 1575.9856 s
agent0:                 episode reward: -0.2012,                 loss: 0.3262
agent1:                 episode reward: 0.2012,                 loss: nan
Episode: 24901/50100 (49.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9234s / 1577.9090 s
agent0:                 episode reward: 0.0243,                 loss: 0.3267
agent1:                 episode reward: -0.0243,                 loss: nan
Episode: 24921/50100 (49.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9554s / 1579.8644 s
agent0:                 episode reward: -0.6011,                 loss: 0.3261
agent1:                 episode reward: 0.6011,                 loss: nan
Episode: 24941/50100 (49.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9457s / 1581.8101 s
agent0:                 episode reward: -0.4052,                 loss: 0.3224
agent1:                 episode reward: 0.4052,                 loss: nan
Episode: 24961/50100 (49.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9421s / 1583.7522 s
agent0:                 episode reward: -0.4512,                 loss: 0.2924
agent1:                 episode reward: 0.4512,                 loss: nan
Episode: 24981/50100 (49.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9477s / 1585.6999 s
agent0:                 episode reward: -0.0839,                 loss: 0.2880
agent1:                 episode reward: 0.0839,                 loss: nan
Episode: 25001/50100 (49.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9486s / 1587.6484 s
agent0:                 episode reward: -0.1568,                 loss: 0.2880
agent1:                 episode reward: 0.1568,                 loss: nan
Episode: 25021/50100 (49.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9776s / 1589.6260 s
agent0:                 episode reward: -0.0887,                 loss: 0.2852
agent1:                 episode reward: 0.0887,                 loss: nan
Episode: 25041/50100 (49.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9387s / 1591.5647 s
agent0:                 episode reward: -0.5793,                 loss: 0.2841
agent1:                 episode reward: 0.5793,                 loss: nan
Episode: 25061/50100 (50.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9800s / 1593.5447 s
agent0:                 episode reward: -0.5143,                 loss: 0.2825
agent1:                 episode reward: 0.5143,                 loss: nan
Episode: 25081/50100 (50.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9548s / 1595.4995 s
agent0:                 episode reward: -0.1693,                 loss: 0.2820
agent1:                 episode reward: 0.1693,                 loss: nan
Episode: 25101/50100 (50.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9149s / 1597.4144 s
agent0:                 episode reward: -0.2656,                 loss: 0.2846
agent1:                 episode reward: 0.2656,                 loss: nan
Episode: 25121/50100 (50.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9316s / 1599.3460 s
agent0:                 episode reward: -0.2168,                 loss: 0.3675
agent1:                 episode reward: 0.2168,                 loss: nan
Episode: 25141/50100 (50.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9223s / 1601.2683 s
agent0:                 episode reward: 0.2023,                 loss: 0.3648
agent1:                 episode reward: -0.2023,                 loss: nan
Episode: 25161/50100 (50.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2909s / 1602.5592 s
agent0:                 episode reward: 0.0390,                 loss: 0.3642
agent1:                 episode reward: -0.0390,                 loss: 0.4522
Score delta: 2.0005534912936036, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/24933_0.
Episode: 25181/50100 (50.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 1.0934s / 1603.6526 s
agent0:                 episode reward: -0.5170,                 loss: nan
agent1:                 episode reward: 0.5170,                 loss: 0.4515
Episode: 25201/50100 (50.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1051s / 1604.7577 s
agent0:                 episode reward: -0.4866,                 loss: nan
agent1:                 episode reward: 0.4866,                 loss: 0.4507
Episode: 25221/50100 (50.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1054s / 1605.8631 s
agent0:                 episode reward: 0.2649,                 loss: nan
agent1:                 episode reward: -0.2649,                 loss: 0.4507
Episode: 25241/50100 (50.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1055s / 1606.9686 s
agent0:                 episode reward: -0.4775,                 loss: nan
agent1:                 episode reward: 0.4775,                 loss: 0.4494
Score delta: 2.250424410483455, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/25029_1.
Episode: 25261/50100 (50.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9376s / 1608.9061 s
agent0:                 episode reward: -0.2011,                 loss: 0.3583
agent1:                 episode reward: 0.2011,                 loss: nan
Episode: 25281/50100 (50.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9343s / 1610.8404 s
agent0:                 episode reward: -0.0281,                 loss: 0.3565
agent1:                 episode reward: 0.0281,                 loss: nan
Episode: 25301/50100 (50.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9384s / 1612.7788 s
agent0:                 episode reward: 0.0439,                 loss: 0.3541
agent1:                 episode reward: -0.0439,                 loss: nan
Episode: 25321/50100 (50.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9449s / 1614.7237 s
agent0:                 episode reward: -0.4339,                 loss: 0.3555
agent1:                 episode reward: 0.4339,                 loss: nan
Episode: 25341/50100 (50.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9305s / 1616.6542 s
agent0:                 episode reward: -0.6997,                 loss: 0.3543
agent1:                 episode reward: 0.6997,                 loss: nan
Episode: 25361/50100 (50.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9118s / 1618.5660 s
agent0:                 episode reward: -0.0420,                 loss: 0.3554
agent1:                 episode reward: 0.0420,                 loss: nan
Episode: 25381/50100 (50.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9368s / 1620.5028 s
agent0:                 episode reward: 0.5939,                 loss: 0.3238
agent1:                 episode reward: -0.5939,                 loss: nan
Episode: 25401/50100 (50.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9373s / 1622.4401 s
agent0:                 episode reward: -0.6221,                 loss: 0.3101
agent1:                 episode reward: 0.6221,                 loss: nan
Episode: 25421/50100 (50.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9462s / 1624.3863 s
agent0:                 episode reward: 0.1182,                 loss: 0.3081
agent1:                 episode reward: -0.1182,                 loss: nan
Episode: 25441/50100 (50.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9396s / 1626.3259 s
agent0:                 episode reward: -0.3038,                 loss: 0.3076
agent1:                 episode reward: 0.3038,                 loss: nan
Episode: 25461/50100 (50.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9543s / 1628.2802 s
agent0:                 episode reward: -0.0850,                 loss: 0.3094
agent1:                 episode reward: 0.0850,                 loss: nan
Episode: 25481/50100 (50.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9419s / 1630.2221 s
agent0:                 episode reward: 0.0700,                 loss: 0.3049
agent1:                 episode reward: -0.0700,                 loss: nan
Episode: 25501/50100 (50.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9628s / 1632.1849 s
agent0:                 episode reward: 0.0557,                 loss: 0.3041
agent1:                 episode reward: -0.0557,                 loss: nan
Episode: 25521/50100 (50.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9447s / 1634.1296 s
agent0:                 episode reward: -0.2662,                 loss: 0.3031
agent1:                 episode reward: 0.2662,                 loss: nan
Episode: 25541/50100 (50.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9747s / 1636.1043 s
agent0:                 episode reward: 0.1294,                 loss: 0.3052
agent1:                 episode reward: -0.1294,                 loss: nan
Episode: 25561/50100 (51.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9341s / 1638.0384 s
agent0:                 episode reward: 0.3379,                 loss: 0.2989
agent1:                 episode reward: -0.3379,                 loss: nan
Episode: 25581/50100 (51.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9362s / 1639.9746 s
agent0:                 episode reward: 0.2935,                 loss: 0.2996
agent1:                 episode reward: -0.2935,                 loss: nan
Episode: 25601/50100 (51.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9427s / 1641.9173 s
agent0:                 episode reward: -0.0943,                 loss: 0.2996
agent1:                 episode reward: 0.0943,                 loss: nan
Episode: 25621/50100 (51.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9612s / 1643.8785 s
agent0:                 episode reward: -0.0593,                 loss: 0.2979
agent1:                 episode reward: 0.0593,                 loss: nan
Episode: 25641/50100 (51.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9753s / 1645.8538 s
agent0:                 episode reward: 0.3325,                 loss: 0.2985
agent1:                 episode reward: -0.3325,                 loss: nan
Episode: 25661/50100 (51.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6829s / 1647.5367 s
agent0:                 episode reward: 0.2885,                 loss: 0.3006
agent1:                 episode reward: -0.2885,                 loss: 0.4495
Score delta: 2.089648672626054, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/25442_0.
Episode: 25681/50100 (51.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1286s / 1648.6654 s
agent0:                 episode reward: -0.2652,                 loss: nan
agent1:                 episode reward: 0.2652,                 loss: 0.4478
Episode: 25701/50100 (51.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1473s / 1649.8127 s
agent0:                 episode reward: -0.2740,                 loss: nan
agent1:                 episode reward: 0.2740,                 loss: 0.4470
Episode: 25721/50100 (51.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1366s / 1650.9492 s
agent0:                 episode reward: -0.5693,                 loss: nan
agent1:                 episode reward: 0.5693,                 loss: 0.4474
Episode: 25741/50100 (51.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9446s / 1652.8939 s
agent0:                 episode reward: -0.4017,                 loss: 0.3342
agent1:                 episode reward: 0.4017,                 loss: 0.4468
Score delta: 2.1751904948992995, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/25510_1.
Episode: 25761/50100 (51.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0125s / 1654.9064 s
agent0:                 episode reward: -0.3526,                 loss: 0.3290
agent1:                 episode reward: 0.3526,                 loss: nan
Episode: 25781/50100 (51.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9517s / 1656.8581 s
agent0:                 episode reward: -0.5054,                 loss: 0.3094
agent1:                 episode reward: 0.5054,                 loss: nan
Episode: 25801/50100 (51.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9731s / 1658.8312 s
agent0:                 episode reward: 0.0786,                 loss: 0.2941
agent1:                 episode reward: -0.0786,                 loss: nan
Episode: 25821/50100 (51.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9561s / 1660.7873 s
agent0:                 episode reward: 0.0704,                 loss: 0.2910
agent1:                 episode reward: -0.0704,                 loss: nan
Episode: 25841/50100 (51.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9530s / 1662.7404 s
agent0:                 episode reward: -0.1809,                 loss: 0.2853
agent1:                 episode reward: 0.1809,                 loss: nan
Episode: 25861/50100 (51.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9411s / 1664.6815 s
agent0:                 episode reward: -0.0715,                 loss: 0.2862
agent1:                 episode reward: 0.0715,                 loss: nan
Episode: 25881/50100 (51.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9347s / 1666.6162 s
agent0:                 episode reward: 0.2734,                 loss: 0.2847
agent1:                 episode reward: -0.2734,                 loss: nan
Episode: 25901/50100 (51.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9671s / 1668.5833 s
agent0:                 episode reward: -0.0237,                 loss: 0.2835
agent1:                 episode reward: 0.0237,                 loss: nan
Episode: 25921/50100 (51.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9147s / 1670.4980 s
agent0:                 episode reward: 0.0948,                 loss: 0.2828
agent1:                 episode reward: -0.0948,                 loss: nan
Episode: 25941/50100 (51.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9409s / 1672.4390 s
agent0:                 episode reward: -0.0630,                 loss: 0.3235
agent1:                 episode reward: 0.0630,                 loss: nan
Episode: 25961/50100 (51.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9298s / 1674.3687 s
agent0:                 episode reward: 0.0622,                 loss: 0.3617
agent1:                 episode reward: -0.0622,                 loss: nan
Episode: 25981/50100 (51.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9349s / 1676.3036 s
agent0:                 episode reward: -0.3923,                 loss: 0.3595
agent1:                 episode reward: 0.3923,                 loss: nan
Episode: 26001/50100 (51.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9495s / 1678.2531 s
agent0:                 episode reward: 0.2450,                 loss: 0.3595
agent1:                 episode reward: -0.2450,                 loss: nan
Episode: 26021/50100 (51.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9268s / 1680.1799 s
agent0:                 episode reward: -0.9334,                 loss: 0.3610
agent1:                 episode reward: 0.9334,                 loss: nan
Episode: 26041/50100 (51.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9352s / 1682.1151 s
agent0:                 episode reward: 0.4845,                 loss: 0.3595
agent1:                 episode reward: -0.4845,                 loss: nan
Episode: 26061/50100 (52.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9137s / 1684.0288 s
agent0:                 episode reward: -0.3720,                 loss: 0.3585
agent1:                 episode reward: 0.3720,                 loss: nan
Episode: 26081/50100 (52.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9938s / 1686.0227 s
agent0:                 episode reward: -0.2963,                 loss: 0.3585
agent1:                 episode reward: 0.2963,                 loss: nan
Episode: 26101/50100 (52.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9452s / 1687.9679 s
agent0:                 episode reward: -0.4388,                 loss: 0.3706
agent1:                 episode reward: 0.4388,                 loss: nan
Episode: 26121/50100 (52.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9453s / 1689.9132 s
agent0:                 episode reward: -0.1497,                 loss: 0.4058
agent1:                 episode reward: 0.1497,                 loss: nan
Episode: 26141/50100 (52.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9541s / 1691.8673 s
agent0:                 episode reward: -0.2307,                 loss: 0.4016
agent1:                 episode reward: 0.2307,                 loss: nan
Episode: 26161/50100 (52.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9929s / 1693.8602 s
agent0:                 episode reward: -0.0406,                 loss: 0.4026
agent1:                 episode reward: 0.0406,                 loss: nan
Episode: 26181/50100 (52.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9142s / 1695.7744 s
agent0:                 episode reward: -0.0731,                 loss: 0.4009
agent1:                 episode reward: 0.0731,                 loss: nan
Episode: 26201/50100 (52.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9323s / 1697.7067 s
agent0:                 episode reward: 0.2591,                 loss: 0.4018
agent1:                 episode reward: -0.2591,                 loss: nan
Episode: 26221/50100 (52.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9468s / 1699.6534 s
agent0:                 episode reward: -0.1060,                 loss: 0.4024
agent1:                 episode reward: 0.1060,                 loss: nan
Episode: 26241/50100 (52.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9420s / 1701.5954 s
agent0:                 episode reward: -0.1493,                 loss: 0.4015
agent1:                 episode reward: 0.1493,                 loss: nan
Episode: 26261/50100 (52.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0092s / 1703.6047 s
agent0:                 episode reward: -0.1916,                 loss: 0.4007
agent1:                 episode reward: 0.1916,                 loss: nan
Episode: 26281/50100 (52.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9429s / 1705.5476 s
agent0:                 episode reward: -0.1854,                 loss: 0.3723
agent1:                 episode reward: 0.1854,                 loss: nan
Episode: 26301/50100 (52.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9919s / 1707.5395 s
agent0:                 episode reward: 0.0711,                 loss: 0.3571
agent1:                 episode reward: -0.0711,                 loss: nan
Episode: 26321/50100 (52.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9290s / 1709.4685 s
agent0:                 episode reward: -0.7440,                 loss: 0.3576
agent1:                 episode reward: 0.7440,                 loss: nan
Episode: 26341/50100 (52.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9625s / 1711.4310 s
agent0:                 episode reward: -0.6809,                 loss: 0.3592
agent1:                 episode reward: 0.6809,                 loss: nan
Episode: 26361/50100 (52.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9944s / 1713.4254 s
agent0:                 episode reward: -0.3174,                 loss: 0.3601
agent1:                 episode reward: 0.3174,                 loss: nan
Episode: 26381/50100 (52.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9488s / 1715.3742 s
agent0:                 episode reward: 0.0685,                 loss: 0.3553
agent1:                 episode reward: -0.0685,                 loss: nan
Episode: 26401/50100 (52.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9522s / 1717.3264 s
agent0:                 episode reward: 0.0215,                 loss: 0.3556
agent1:                 episode reward: -0.0215,                 loss: nan
Episode: 26421/50100 (52.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9948s / 1719.3211 s
agent0:                 episode reward: -0.4895,                 loss: 0.3577
agent1:                 episode reward: 0.4895,                 loss: nan
Episode: 26441/50100 (52.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9283s / 1721.2495 s
agent0:                 episode reward: 0.2726,                 loss: 0.3647
agent1:                 episode reward: -0.2726,                 loss: nan
Episode: 26461/50100 (52.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9350s / 1723.1845 s
agent0:                 episode reward: -0.3115,                 loss: 0.3564
agent1:                 episode reward: 0.3115,                 loss: nan
Episode: 26481/50100 (52.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8306s / 1725.0151 s
agent0:                 episode reward: 0.5940,                 loss: 0.3561
agent1:                 episode reward: -0.5940,                 loss: 0.4490
Score delta: 2.036435263883002, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/26266_0.
Episode: 26501/50100 (52.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1413s / 1726.1564 s
agent0:                 episode reward: -0.4094,                 loss: nan
agent1:                 episode reward: 0.4094,                 loss: 0.4489
Episode: 26521/50100 (52.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1426s / 1727.2991 s
agent0:                 episode reward: -0.5609,                 loss: nan
agent1:                 episode reward: 0.5609,                 loss: 0.4490
Episode: 26541/50100 (52.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1360s / 1728.4351 s
agent0:                 episode reward: 0.0383,                 loss: nan
agent1:                 episode reward: -0.0383,                 loss: 0.4498
Episode: 26561/50100 (53.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1517s / 1729.5867 s
agent0:                 episode reward: -0.5182,                 loss: nan
agent1:                 episode reward: 0.5182,                 loss: 0.4489
Episode: 26581/50100 (53.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9526s / 1731.5394 s
agent0:                 episode reward: -0.4100,                 loss: 0.3956
agent1:                 episode reward: 0.4100,                 loss: 0.4499
Score delta: 2.1872291024592045, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/26350_1.
Episode: 26601/50100 (53.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9779s / 1733.5173 s
agent0:                 episode reward: 0.4335,                 loss: 0.3933
agent1:                 episode reward: -0.4335,                 loss: nan
Episode: 26621/50100 (53.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9683s / 1735.4856 s
agent0:                 episode reward: 0.0991,                 loss: 0.3925
agent1:                 episode reward: -0.0991,                 loss: nan
Episode: 26641/50100 (53.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9886s / 1737.4742 s
agent0:                 episode reward: -0.2721,                 loss: 0.3931
agent1:                 episode reward: 0.2721,                 loss: nan
Episode: 26661/50100 (53.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9988s / 1739.4730 s
agent0:                 episode reward: -0.1341,                 loss: 0.3924
agent1:                 episode reward: 0.1341,                 loss: nan
Episode: 26681/50100 (53.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9556s / 1741.4286 s
agent0:                 episode reward: 0.0852,                 loss: 0.3911
agent1:                 episode reward: -0.0852,                 loss: nan
Episode: 26701/50100 (53.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9537s / 1743.3823 s
agent0:                 episode reward: -0.0950,                 loss: 0.4381
agent1:                 episode reward: 0.0950,                 loss: nan
Episode: 26721/50100 (53.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0479s / 1745.4303 s
agent0:                 episode reward: 0.1259,                 loss: 0.4327
agent1:                 episode reward: -0.1259,                 loss: nan
Episode: 26741/50100 (53.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9763s / 1747.4066 s
agent0:                 episode reward: -0.4861,                 loss: 0.4338
agent1:                 episode reward: 0.4861,                 loss: nan
Episode: 26761/50100 (53.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0119s / 1749.4185 s
agent0:                 episode reward: -0.4224,                 loss: 0.4334
agent1:                 episode reward: 0.4224,                 loss: nan
Episode: 26781/50100 (53.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9350s / 1751.3535 s
agent0:                 episode reward: -0.2556,                 loss: 0.4331
agent1:                 episode reward: 0.2556,                 loss: nan
Episode: 26801/50100 (53.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9473s / 1753.3008 s
agent0:                 episode reward: -0.2186,                 loss: 0.4316
agent1:                 episode reward: 0.2186,                 loss: nan
Episode: 26821/50100 (53.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9393s / 1755.2401 s
agent0:                 episode reward: 0.0693,                 loss: 0.4331
agent1:                 episode reward: -0.0693,                 loss: nan
Episode: 26841/50100 (53.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9497s / 1757.1898 s
agent0:                 episode reward: 0.2153,                 loss: 0.4327
agent1:                 episode reward: -0.2153,                 loss: nan
Episode: 26861/50100 (53.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9396s / 1759.1294 s
agent0:                 episode reward: -0.1272,                 loss: 0.4269
agent1:                 episode reward: 0.1272,                 loss: nan
Episode: 26881/50100 (53.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9389s / 1761.0682 s
agent0:                 episode reward: 0.3887,                 loss: 0.4188
agent1:                 episode reward: -0.3887,                 loss: nan
Episode: 26901/50100 (53.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9514s / 1763.0196 s
agent0:                 episode reward: 0.2184,                 loss: 0.4193
agent1:                 episode reward: -0.2184,                 loss: nan
Episode: 26921/50100 (53.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9397s / 1764.9593 s
agent0:                 episode reward: 0.6465,                 loss: 0.4193
agent1:                 episode reward: -0.6465,                 loss: nan
Episode: 26941/50100 (53.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9600s / 1766.9193 s
agent0:                 episode reward: -0.2588,                 loss: 0.4193
agent1:                 episode reward: 0.2588,                 loss: nan
Episode: 26961/50100 (53.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9614s / 1768.8807 s
agent0:                 episode reward: 0.2432,                 loss: 0.4192
agent1:                 episode reward: -0.2432,                 loss: nan
Episode: 26981/50100 (53.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8833s / 1770.7640 s
agent0:                 episode reward: 0.4723,                 loss: 0.4202
agent1:                 episode reward: -0.4723,                 loss: 0.4467
Score delta: 2.088640642687495, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/26767_0.
Episode: 27001/50100 (53.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1621s / 1771.9260 s
agent0:                 episode reward: -0.4626,                 loss: nan
agent1:                 episode reward: 0.4626,                 loss: 0.4463
Episode: 27021/50100 (53.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1482s / 1773.0742 s
agent0:                 episode reward: -0.2350,                 loss: nan
agent1:                 episode reward: 0.2350,                 loss: 0.4468
Episode: 27041/50100 (53.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1362s / 1774.2104 s
agent0:                 episode reward: -0.4186,                 loss: nan
agent1:                 episode reward: 0.4186,                 loss: 0.4481
Episode: 27061/50100 (54.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1585s / 1775.3690 s
agent0:                 episode reward: 0.4977,                 loss: nan
agent1:                 episode reward: -0.4977,                 loss: 0.4483
Episode: 27081/50100 (54.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5158s / 1776.8848 s
agent0:                 episode reward: -0.5221,                 loss: 0.2953
agent1:                 episode reward: 0.5221,                 loss: 0.4482
Score delta: 2.156988050627435, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/26862_1.
Episode: 27101/50100 (54.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9685s / 1778.8533 s
agent0:                 episode reward: 0.0666,                 loss: 0.2945
agent1:                 episode reward: -0.0666,                 loss: nan
Episode: 27121/50100 (54.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9757s / 1780.8290 s
agent0:                 episode reward: 0.1044,                 loss: 0.3232
agent1:                 episode reward: -0.1044,                 loss: nan
Episode: 27141/50100 (54.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0154s / 1782.8444 s
agent0:                 episode reward: 0.1201,                 loss: 0.3518
agent1:                 episode reward: -0.1201,                 loss: nan
Episode: 27161/50100 (54.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9329s / 1784.7773 s
agent0:                 episode reward: 0.1352,                 loss: 0.3515
agent1:                 episode reward: -0.1352,                 loss: nan
Episode: 27181/50100 (54.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0252s / 1786.8025 s
agent0:                 episode reward: -0.1105,                 loss: 0.3526
agent1:                 episode reward: 0.1105,                 loss: nan
Episode: 27201/50100 (54.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9468s / 1788.7492 s
agent0:                 episode reward: -0.2383,                 loss: 0.3505
agent1:                 episode reward: 0.2383,                 loss: nan
Episode: 27221/50100 (54.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0134s / 1790.7627 s
agent0:                 episode reward: -0.3327,                 loss: 0.3495
agent1:                 episode reward: 0.3327,                 loss: nan
Episode: 27241/50100 (54.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9356s / 1792.6983 s
agent0:                 episode reward: 0.0918,                 loss: 0.3511
agent1:                 episode reward: -0.0918,                 loss: nan
Episode: 27261/50100 (54.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9564s / 1794.6547 s
agent0:                 episode reward: -0.3634,                 loss: 0.3509
agent1:                 episode reward: 0.3634,                 loss: nan
Episode: 27281/50100 (54.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9613s / 1796.6160 s
agent0:                 episode reward: 0.1001,                 loss: 0.3652
agent1:                 episode reward: -0.1001,                 loss: nan
Episode: 27301/50100 (54.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9567s / 1798.5727 s
agent0:                 episode reward: -0.6278,                 loss: 0.4102
agent1:                 episode reward: 0.6278,                 loss: nan
Episode: 27321/50100 (54.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9542s / 1800.5269 s
agent0:                 episode reward: 0.1001,                 loss: 0.4094
agent1:                 episode reward: -0.1001,                 loss: nan
Episode: 27341/50100 (54.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9518s / 1802.4787 s
agent0:                 episode reward: 0.0117,                 loss: 0.4094
agent1:                 episode reward: -0.0117,                 loss: nan
Episode: 27361/50100 (54.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9437s / 1804.4224 s
agent0:                 episode reward: 0.1243,                 loss: 0.4075
agent1:                 episode reward: -0.1243,                 loss: nan
Episode: 27381/50100 (54.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9434s / 1806.3658 s
agent0:                 episode reward: 0.1181,                 loss: 0.4093
agent1:                 episode reward: -0.1181,                 loss: nan
Episode: 27401/50100 (54.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9418s / 1808.3076 s
agent0:                 episode reward: 0.2347,                 loss: 0.4102
agent1:                 episode reward: -0.2347,                 loss: nan
Episode: 27421/50100 (54.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9352s / 1810.2428 s
agent0:                 episode reward: 0.2913,                 loss: 0.4080
agent1:                 episode reward: -0.2913,                 loss: nan
Episode: 27441/50100 (54.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3568s / 1811.5996 s
agent0:                 episode reward: -0.3484,                 loss: 0.4117
agent1:                 episode reward: 0.3484,                 loss: 0.4386
Score delta: 2.229212120654359, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/27214_0.
Episode: 27461/50100 (54.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1827s / 1812.7823 s
agent0:                 episode reward: -0.2958,                 loss: nan
agent1:                 episode reward: 0.2958,                 loss: 0.4385
Episode: 27481/50100 (54.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1635s / 1813.9458 s
agent0:                 episode reward: -0.7265,                 loss: nan
agent1:                 episode reward: 0.7265,                 loss: 0.4387
Episode: 27501/50100 (54.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1531s / 1815.0989 s
agent0:                 episode reward: -0.1788,                 loss: nan
agent1:                 episode reward: 0.1788,                 loss: 0.4385
Episode: 27521/50100 (54.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6173s / 1816.7162 s
agent0:                 episode reward: -0.4398,                 loss: 0.4070
agent1:                 episode reward: 0.4398,                 loss: 0.4369
Score delta: 2.4093572563149595, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/27297_1.
Episode: 27541/50100 (54.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9417s / 1818.6578 s
agent0:                 episode reward: -0.3123,                 loss: 0.3981
agent1:                 episode reward: 0.3123,                 loss: nan
Episode: 27561/50100 (55.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9625s / 1820.6203 s
agent0:                 episode reward: 0.1112,                 loss: 0.3872
agent1:                 episode reward: -0.1112,                 loss: nan
Episode: 27581/50100 (55.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9450s / 1822.5653 s
agent0:                 episode reward: -0.4263,                 loss: 0.3864
agent1:                 episode reward: 0.4263,                 loss: nan
Episode: 27601/50100 (55.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9518s / 1824.5171 s
agent0:                 episode reward: -0.1266,                 loss: 0.3850
agent1:                 episode reward: 0.1266,                 loss: nan
Episode: 27621/50100 (55.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9291s / 1826.4462 s
agent0:                 episode reward: 0.1368,                 loss: 0.3849
agent1:                 episode reward: -0.1368,                 loss: nan
Episode: 27641/50100 (55.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9709s / 1828.4172 s
agent0:                 episode reward: 0.2215,                 loss: 0.3864
agent1:                 episode reward: -0.2215,                 loss: nan
Episode: 27661/50100 (55.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9855s / 1830.4027 s
agent0:                 episode reward: 0.1710,                 loss: 0.3853
agent1:                 episode reward: -0.1710,                 loss: nan
Episode: 27681/50100 (55.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9590s / 1832.3617 s
agent0:                 episode reward: 0.0477,                 loss: 0.3845
agent1:                 episode reward: -0.0477,                 loss: nan
Episode: 27701/50100 (55.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9576s / 1834.3193 s
agent0:                 episode reward: 0.0984,                 loss: 0.3910
agent1:                 episode reward: -0.0984,                 loss: nan
Episode: 27721/50100 (55.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0051s / 1836.3244 s
agent0:                 episode reward: -0.0916,                 loss: 0.3949
agent1:                 episode reward: 0.0916,                 loss: nan
Episode: 27741/50100 (55.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9239s / 1838.2483 s
agent0:                 episode reward: -0.3337,                 loss: 0.3928
agent1:                 episode reward: 0.3337,                 loss: nan
Episode: 27761/50100 (55.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9432s / 1840.1914 s
agent0:                 episode reward: -0.0831,                 loss: 0.3927
agent1:                 episode reward: 0.0831,                 loss: nan
Episode: 27781/50100 (55.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9334s / 1842.1249 s
agent0:                 episode reward: -0.0400,                 loss: 0.3928
agent1:                 episode reward: 0.0400,                 loss: nan
Episode: 27801/50100 (55.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9353s / 1844.0602 s
agent0:                 episode reward: -0.1794,                 loss: 0.3900
agent1:                 episode reward: 0.1794,                 loss: nan
Episode: 27821/50100 (55.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9500s / 1846.0102 s
agent0:                 episode reward: 0.1527,                 loss: 0.3925
agent1:                 episode reward: -0.1527,                 loss: nan
Episode: 27841/50100 (55.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9362s / 1847.9463 s
agent0:                 episode reward: -0.3901,                 loss: 0.3925
agent1:                 episode reward: 0.3901,                 loss: nan
Episode: 27861/50100 (55.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9316s / 1849.8779 s
agent0:                 episode reward: 0.0581,                 loss: 0.3908
agent1:                 episode reward: -0.0581,                 loss: nan
Episode: 27881/50100 (55.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9328s / 1851.8107 s
agent0:                 episode reward: 0.1440,                 loss: 0.3308
agent1:                 episode reward: -0.1440,                 loss: nan
Episode: 27901/50100 (55.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9415s / 1853.7521 s
agent0:                 episode reward: 0.4580,                 loss: 0.3274
agent1:                 episode reward: -0.4580,                 loss: nan
Episode: 27921/50100 (55.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3788s / 1855.1309 s
agent0:                 episode reward: 0.2028,                 loss: 0.3240
agent1:                 episode reward: -0.2028,                 loss: 0.4472
Score delta: 2.454366531129788, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/27695_0.
Episode: 27941/50100 (55.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1425s / 1856.2734 s
agent0:                 episode reward: -0.0538,                 loss: nan
agent1:                 episode reward: 0.0538,                 loss: 0.4455
Episode: 27961/50100 (55.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1337s / 1857.4071 s
agent0:                 episode reward: -0.4457,                 loss: nan
agent1:                 episode reward: 0.4457,                 loss: 0.4444
Episode: 27981/50100 (55.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1473s / 1858.5544 s
agent0:                 episode reward: -0.4505,                 loss: nan
agent1:                 episode reward: 0.4505,                 loss: 0.4440
Episode: 28001/50100 (55.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1424s / 1859.6968 s
agent0:                 episode reward: -0.0622,                 loss: nan
agent1:                 episode reward: 0.0622,                 loss: 0.4436
Episode: 28021/50100 (55.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1432s / 1860.8401 s
agent0:                 episode reward: -0.0291,                 loss: nan
agent1:                 episode reward: 0.0291,                 loss: 0.4438
Episode: 28041/50100 (55.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5948s / 1862.4348 s
agent0:                 episode reward: -0.3097,                 loss: 0.3163
agent1:                 episode reward: 0.3097,                 loss: 0.4435
Score delta: 2.111535406989378, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/27818_1.
Episode: 28061/50100 (56.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9275s / 1864.3623 s
agent0:                 episode reward: -0.0449,                 loss: 0.3151
agent1:                 episode reward: 0.0449,                 loss: nan
Episode: 28081/50100 (56.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9321s / 1866.2944 s
agent0:                 episode reward: -0.4248,                 loss: 0.3130
agent1:                 episode reward: 0.4248,                 loss: nan
Episode: 28101/50100 (56.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6822s / 1867.9766 s
agent0:                 episode reward: 0.2853,                 loss: 0.3158
agent1:                 episode reward: -0.2853,                 loss: 0.4452
Score delta: 2.38258152317804, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/27881_0.
Episode: 28121/50100 (56.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1711s / 1869.1477 s
agent0:                 episode reward: -0.2926,                 loss: nan
agent1:                 episode reward: 0.2926,                 loss: 0.4429
Episode: 28141/50100 (56.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1744s / 1870.3222 s
agent0:                 episode reward: -0.8435,                 loss: nan
agent1:                 episode reward: 0.8435,                 loss: 0.4429
Episode: 28161/50100 (56.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1481s / 1871.4703 s
agent0:                 episode reward: -0.7367,                 loss: nan
agent1:                 episode reward: 0.7367,                 loss: 0.4450
Episode: 28181/50100 (56.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7966s / 1873.2669 s
agent0:                 episode reward: -1.0184,                 loss: 0.3743
agent1:                 episode reward: 1.0184,                 loss: 0.4462
Score delta: 2.5422562060249296, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/27953_1.
Episode: 28201/50100 (56.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9260s / 1875.1929 s
agent0:                 episode reward: 0.4558,                 loss: 0.3734
agent1:                 episode reward: -0.4558,                 loss: nan
Episode: 28221/50100 (56.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0000s / 1877.1928 s
agent0:                 episode reward: -0.1821,                 loss: 0.3745
agent1:                 episode reward: 0.1821,                 loss: nan
Episode: 28241/50100 (56.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5609s / 1878.7537 s
agent0:                 episode reward: 0.2543,                 loss: 0.4294
agent1:                 episode reward: -0.2543,                 loss: 0.4516
Score delta: 2.405251263855527, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/28019_0.
Episode: 28261/50100 (56.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1651s / 1879.9188 s
agent0:                 episode reward: -0.1029,                 loss: nan
agent1:                 episode reward: 0.1029,                 loss: 0.4507
Episode: 28281/50100 (56.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1627s / 1881.0816 s
agent0:                 episode reward: -0.2389,                 loss: nan
agent1:                 episode reward: 0.2389,                 loss: 0.4503
Episode: 28301/50100 (56.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1843s / 1882.2659 s
agent0:                 episode reward: -0.6945,                 loss: nan
agent1:                 episode reward: 0.6945,                 loss: 0.4500
Score delta: 2.0572622345852847, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/28089_1.
Episode: 28321/50100 (56.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0387s / 1884.3047 s
agent0:                 episode reward: -0.4885,                 loss: 0.3350
agent1:                 episode reward: 0.4885,                 loss: nan
Episode: 28341/50100 (56.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9996s / 1886.3042 s
agent0:                 episode reward: -0.3862,                 loss: 0.3272
agent1:                 episode reward: 0.3862,                 loss: nan
Episode: 28361/50100 (56.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9680s / 1888.2722 s
agent0:                 episode reward: -0.2453,                 loss: 0.3284
agent1:                 episode reward: 0.2453,                 loss: nan
Episode: 28381/50100 (56.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9513s / 1890.2235 s
agent0:                 episode reward: -0.1285,                 loss: 0.3259
agent1:                 episode reward: 0.1285,                 loss: nan
Episode: 28401/50100 (56.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9536s / 1892.1771 s
agent0:                 episode reward: -0.2940,                 loss: 0.3247
agent1:                 episode reward: 0.2940,                 loss: nan
Episode: 28421/50100 (56.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9654s / 1894.1425 s
agent0:                 episode reward: 0.2030,                 loss: 0.3239
agent1:                 episode reward: -0.2030,                 loss: nan
Episode: 28441/50100 (56.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9216s / 1896.0641 s
agent0:                 episode reward: 0.3744,                 loss: 0.3235
agent1:                 episode reward: -0.3744,                 loss: nan
Episode: 28461/50100 (56.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9334s / 1897.9975 s
agent0:                 episode reward: -0.2600,                 loss: 0.3277
agent1:                 episode reward: 0.2600,                 loss: nan
Episode: 28481/50100 (56.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9307s / 1899.9281 s
agent0:                 episode reward: -0.2408,                 loss: 0.3065
agent1:                 episode reward: 0.2408,                 loss: nan
Episode: 28501/50100 (56.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9402s / 1901.8684 s
agent0:                 episode reward: -0.0809,                 loss: 0.3001
agent1:                 episode reward: 0.0809,                 loss: nan
Episode: 28521/50100 (56.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9384s / 1903.8068 s
agent0:                 episode reward: -0.1967,                 loss: 0.3000
agent1:                 episode reward: 0.1967,                 loss: nan
Episode: 28541/50100 (56.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9382s / 1905.7449 s
agent0:                 episode reward: -0.1916,                 loss: 0.3001
agent1:                 episode reward: 0.1916,                 loss: nan
Episode: 28561/50100 (57.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9436s / 1907.6885 s
agent0:                 episode reward: -0.0968,                 loss: 0.2986
agent1:                 episode reward: 0.0968,                 loss: nan
Episode: 28581/50100 (57.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9380s / 1909.6266 s
agent0:                 episode reward: 0.1131,                 loss: 0.2995
agent1:                 episode reward: -0.1131,                 loss: nan
Episode: 28601/50100 (57.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9401s / 1911.5667 s
agent0:                 episode reward: 0.2821,                 loss: 0.2988
agent1:                 episode reward: -0.2821,                 loss: nan
Episode: 28621/50100 (57.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9499s / 1913.5166 s
agent0:                 episode reward: 0.0079,                 loss: 0.2976
agent1:                 episode reward: -0.0079,                 loss: nan
Episode: 28641/50100 (57.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9318s / 1915.4483 s
agent0:                 episode reward: -0.1584,                 loss: 0.3418
agent1:                 episode reward: 0.1584,                 loss: nan
Episode: 28661/50100 (57.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0004s / 1917.4487 s
agent0:                 episode reward: -0.0569,                 loss: 0.3475
agent1:                 episode reward: 0.0569,                 loss: nan
Episode: 28681/50100 (57.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9548s / 1919.4035 s
agent0:                 episode reward: 0.0681,                 loss: 0.3466
agent1:                 episode reward: -0.0681,                 loss: nan
Episode: 28701/50100 (57.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9452s / 1921.3487 s
agent0:                 episode reward: 0.0253,                 loss: 0.3466
agent1:                 episode reward: -0.0253,                 loss: nan
Episode: 28721/50100 (57.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0260s / 1923.3746 s
agent0:                 episode reward: 0.3603,                 loss: 0.3464
agent1:                 episode reward: -0.3603,                 loss: nan
Episode: 28741/50100 (57.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7652s / 1925.1398 s
agent0:                 episode reward: 0.3339,                 loss: 0.3443
agent1:                 episode reward: -0.3339,                 loss: 0.4465
Score delta: 2.0545481240268435, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/28524_0.
Episode: 28761/50100 (57.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1685s / 1926.3084 s
agent0:                 episode reward: -0.0721,                 loss: nan
agent1:                 episode reward: 0.0721,                 loss: 0.4439
Episode: 28781/50100 (57.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1746s / 1927.4829 s
agent0:                 episode reward: -0.1925,                 loss: nan
agent1:                 episode reward: 0.1925,                 loss: 0.4445
Episode: 28801/50100 (57.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1792s / 1928.6621 s
agent0:                 episode reward: -0.3742,                 loss: nan
agent1:                 episode reward: 0.3742,                 loss: 0.4436
Episode: 28821/50100 (57.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6086s / 1930.2707 s
agent0:                 episode reward: -0.6449,                 loss: 0.2977
agent1:                 episode reward: 0.6449,                 loss: 0.4449
Score delta: 2.153094906124829, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/28598_1.
Episode: 28841/50100 (57.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9240s / 1932.1947 s
agent0:                 episode reward: -0.4698,                 loss: 0.2956
agent1:                 episode reward: 0.4698,                 loss: nan
Episode: 28861/50100 (57.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9407s / 1934.1354 s
agent0:                 episode reward: -0.2534,                 loss: 0.2941
agent1:                 episode reward: 0.2534,                 loss: nan
Episode: 28881/50100 (57.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9330s / 1936.0684 s
agent0:                 episode reward: 0.2221,                 loss: 0.3438
agent1:                 episode reward: -0.2221,                 loss: nan
Episode: 28901/50100 (57.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9431s / 1938.0115 s
agent0:                 episode reward: 0.2258,                 loss: 0.3529
agent1:                 episode reward: -0.2258,                 loss: nan
Episode: 28921/50100 (57.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9565s / 1939.9680 s
agent0:                 episode reward: -0.0295,                 loss: 0.3536
agent1:                 episode reward: 0.0295,                 loss: nan
Episode: 28941/50100 (57.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5489s / 1941.5169 s
agent0:                 episode reward: 0.3533,                 loss: 0.3505
agent1:                 episode reward: -0.3533,                 loss: 0.4493
Score delta: 2.0753607893888706, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/28718_0.
Episode: 28961/50100 (57.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1913s / 1942.7082 s
agent0:                 episode reward: 0.1257,                 loss: nan
agent1:                 episode reward: -0.1257,                 loss: 0.4430
Episode: 28981/50100 (57.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1709s / 1943.8791 s
agent0:                 episode reward: -0.8024,                 loss: nan
agent1:                 episode reward: 0.8024,                 loss: 0.4406
Episode: 29001/50100 (57.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3407s / 1945.2198 s
agent0:                 episode reward: -0.5476,                 loss: 0.2935
agent1:                 episode reward: 0.5476,                 loss: 0.4385
Score delta: 2.0938142154309256, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/28785_1.
Episode: 29021/50100 (57.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9483s / 1947.1682 s
agent0:                 episode reward: -0.1550,                 loss: 0.2925
agent1:                 episode reward: 0.1550,                 loss: nan
Episode: 29041/50100 (57.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9487s / 1949.1169 s
agent0:                 episode reward: 0.3849,                 loss: 0.2899
agent1:                 episode reward: -0.3849,                 loss: nan
Episode: 29061/50100 (58.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9571s / 1951.0740 s
agent0:                 episode reward: 0.1069,                 loss: 0.2903
agent1:                 episode reward: -0.1069,                 loss: nan
Episode: 29081/50100 (58.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9374s / 1953.0114 s
agent0:                 episode reward: -0.3575,                 loss: 0.2892
agent1:                 episode reward: 0.3575,                 loss: nan
Episode: 29101/50100 (58.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9629s / 1954.9743 s
agent0:                 episode reward: -0.4534,                 loss: 0.2941
agent1:                 episode reward: 0.4534,                 loss: nan
Episode: 29121/50100 (58.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9743s / 1956.9485 s
agent0:                 episode reward: 0.2695,                 loss: 0.3400
agent1:                 episode reward: -0.2695,                 loss: nan
Episode: 29141/50100 (58.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9659s / 1958.9144 s
agent0:                 episode reward: -0.2493,                 loss: 0.3412
agent1:                 episode reward: 0.2493,                 loss: nan
Episode: 29161/50100 (58.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9605s / 1960.8749 s
agent0:                 episode reward: 0.1594,                 loss: 0.3404
agent1:                 episode reward: -0.1594,                 loss: nan
Episode: 29181/50100 (58.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9672s / 1962.8421 s
agent0:                 episode reward: 0.0894,                 loss: 0.3413
agent1:                 episode reward: -0.0894,                 loss: nan
Episode: 29201/50100 (58.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9227s / 1964.7648 s
agent0:                 episode reward: 0.3039,                 loss: 0.3390
agent1:                 episode reward: -0.3039,                 loss: nan
Episode: 29221/50100 (58.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9506s / 1966.7153 s
agent0:                 episode reward: -0.2809,                 loss: 0.3384
agent1:                 episode reward: 0.2809,                 loss: nan
Episode: 29241/50100 (58.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6015s / 1968.3168 s
agent0:                 episode reward: 0.4028,                 loss: 0.3391
agent1:                 episode reward: -0.4028,                 loss: 0.4531
Score delta: 2.1357901697076462, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/29020_0.
Episode: 29261/50100 (58.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1916s / 1969.5084 s
agent0:                 episode reward: 0.5054,                 loss: nan
agent1:                 episode reward: -0.5054,                 loss: 0.4518
Episode: 29281/50100 (58.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3015s / 1970.8099 s
agent0:                 episode reward: -0.4615,                 loss: nan
agent1:                 episode reward: 0.4615,                 loss: 0.4531
Episode: 29301/50100 (58.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3526s / 1972.1625 s
agent0:                 episode reward: -0.6153,                 loss: 0.3812
agent1:                 episode reward: 0.6153,                 loss: 0.4522
Score delta: 2.1785810804842707, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/29086_1.
Episode: 29321/50100 (58.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9733s / 1974.1359 s
agent0:                 episode reward: 0.1057,                 loss: 0.3825
agent1:                 episode reward: -0.1057,                 loss: nan
Episode: 29341/50100 (58.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0047s / 1976.1406 s
agent0:                 episode reward: -0.4936,                 loss: 0.3993
agent1:                 episode reward: 0.4936,                 loss: nan
Episode: 29361/50100 (58.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9883s / 1978.1289 s
agent0:                 episode reward: 0.6970,                 loss: 0.4111
agent1:                 episode reward: -0.6970,                 loss: nan
Episode: 29381/50100 (58.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9488s / 1980.0777 s
agent0:                 episode reward: 0.0242,                 loss: 0.4083
agent1:                 episode reward: -0.0242,                 loss: nan
Episode: 29401/50100 (58.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9342s / 1982.0119 s
agent0:                 episode reward: -0.2248,                 loss: 0.4079
agent1:                 episode reward: 0.2248,                 loss: nan
Episode: 29421/50100 (58.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9595s / 1983.9714 s
agent0:                 episode reward: 0.0035,                 loss: 0.4083
agent1:                 episode reward: -0.0035,                 loss: nan
Episode: 29441/50100 (58.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9483s / 1985.9197 s
agent0:                 episode reward: 0.3100,                 loss: 0.4073
agent1:                 episode reward: -0.3100,                 loss: nan
Episode: 29461/50100 (58.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9582s / 1987.8779 s
agent0:                 episode reward: -0.1060,                 loss: 0.4068
agent1:                 episode reward: 0.1060,                 loss: nan
Episode: 29481/50100 (58.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9552s / 1989.8330 s
agent0:                 episode reward: 0.0525,                 loss: 0.4055
agent1:                 episode reward: -0.0525,                 loss: nan
Episode: 29501/50100 (58.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9400s / 1991.7730 s
agent0:                 episode reward: 0.2531,                 loss: 0.4045
agent1:                 episode reward: -0.2531,                 loss: nan
Episode: 29521/50100 (58.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9400s / 1993.7131 s
agent0:                 episode reward: 0.3738,                 loss: 0.3713
agent1:                 episode reward: -0.3738,                 loss: nan
Episode: 29541/50100 (58.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9599s / 1995.6730 s
agent0:                 episode reward: 0.0371,                 loss: 0.3686
agent1:                 episode reward: -0.0371,                 loss: nan
Episode: 29561/50100 (59.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9272s / 1997.6002 s
agent0:                 episode reward: -0.4902,                 loss: 0.3681
agent1:                 episode reward: 0.4902,                 loss: nan
Episode: 29581/50100 (59.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9383s / 1999.5385 s
agent0:                 episode reward: 0.2658,                 loss: 0.3668
agent1:                 episode reward: -0.2658,                 loss: nan
Episode: 29601/50100 (59.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9432s / 2001.4817 s
agent0:                 episode reward: 0.2755,                 loss: 0.3681
agent1:                 episode reward: -0.2755,                 loss: nan
Episode: 29621/50100 (59.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9944s / 2003.4761 s
agent0:                 episode reward: 0.5330,                 loss: 0.3688
agent1:                 episode reward: -0.5330,                 loss: nan
Episode: 29641/50100 (59.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9643s / 2005.4404 s
agent0:                 episode reward: -0.5091,                 loss: 0.3669
agent1:                 episode reward: 0.5091,                 loss: nan
Episode: 29661/50100 (59.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9517s / 2007.3921 s
agent0:                 episode reward: -0.0701,                 loss: 0.3664
agent1:                 episode reward: 0.0701,                 loss: nan
Episode: 29681/50100 (59.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9863s / 2009.3784 s
agent0:                 episode reward: -0.0406,                 loss: 0.3713
agent1:                 episode reward: 0.0406,                 loss: nan
Score delta: 2.2105016830093462, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/29469_0.
Episode: 29701/50100 (59.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2996s / 2010.6780 s
agent0:                 episode reward: -0.1792,                 loss: nan
agent1:                 episode reward: 0.1792,                 loss: 0.4521
Episode: 29721/50100 (59.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1952s / 2011.8732 s
agent0:                 episode reward: 0.1764,                 loss: nan
agent1:                 episode reward: -0.1764,                 loss: 0.4513
Episode: 29741/50100 (59.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2064s / 2013.0796 s
agent0:                 episode reward: -0.1843,                 loss: nan
agent1:                 episode reward: 0.1843,                 loss: 0.4516
Episode: 29761/50100 (59.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2060s / 2014.2856 s
agent0:                 episode reward: -0.4572,                 loss: nan
agent1:                 episode reward: 0.4572,                 loss: 0.4522
Episode: 29781/50100 (59.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1838s / 2015.4694 s
agent0:                 episode reward: -0.0956,                 loss: nan
agent1:                 episode reward: 0.0956,                 loss: 0.4510
Episode: 29801/50100 (59.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3128s / 2016.7822 s
agent0:                 episode reward: -0.2555,                 loss: nan
agent1:                 episode reward: 0.2555,                 loss: 0.4509
Episode: 29821/50100 (59.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8314s / 2018.6136 s
agent0:                 episode reward: -0.0693,                 loss: 0.3567
agent1:                 episode reward: 0.0693,                 loss: 0.4513
Score delta: 2.015997937753519, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/29593_1.
Episode: 29841/50100 (59.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9915s / 2020.6051 s
agent0:                 episode reward: -0.2325,                 loss: 0.3559
agent1:                 episode reward: 0.2325,                 loss: nan
Episode: 29861/50100 (59.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9669s / 2022.5720 s
agent0:                 episode reward: -0.6618,                 loss: 0.3564
agent1:                 episode reward: 0.6618,                 loss: nan
Episode: 29881/50100 (59.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9508s / 2024.5227 s
agent0:                 episode reward: 0.1616,                 loss: 0.3546
agent1:                 episode reward: -0.1616,                 loss: nan
Episode: 29901/50100 (59.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9746s / 2026.4973 s
agent0:                 episode reward: 0.2245,                 loss: 0.3552
agent1:                 episode reward: -0.2245,                 loss: nan
Episode: 29921/50100 (59.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9714s / 2028.4687 s
agent0:                 episode reward: -0.2276,                 loss: 0.3551
agent1:                 episode reward: 0.2276,                 loss: nan
Episode: 29941/50100 (59.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9645s / 2030.4332 s
agent0:                 episode reward: 0.0575,                 loss: 0.3561
agent1:                 episode reward: -0.0575,                 loss: nan
Episode: 29961/50100 (59.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9532s / 2032.3864 s
agent0:                 episode reward: 0.4077,                 loss: 0.3625
agent1:                 episode reward: -0.4077,                 loss: nan
Episode: 29981/50100 (59.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9425s / 2034.3289 s
agent0:                 episode reward: 0.2112,                 loss: 0.3821
agent1:                 episode reward: -0.2112,                 loss: nan
Episode: 30001/50100 (59.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9479s / 2036.2768 s
agent0:                 episode reward: -0.0480,                 loss: 0.3819
agent1:                 episode reward: 0.0480,                 loss: nan
Episode: 30021/50100 (59.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9568s / 2038.2336 s
agent0:                 episode reward: 0.1048,                 loss: 0.3824
agent1:                 episode reward: -0.1048,                 loss: nan
Episode: 30041/50100 (59.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9506s / 2040.1842 s
agent0:                 episode reward: 0.1157,                 loss: 0.3826
agent1:                 episode reward: -0.1157,                 loss: nan
Episode: 30061/50100 (60.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9946s / 2042.1788 s
agent0:                 episode reward: 0.0083,                 loss: 0.3793
agent1:                 episode reward: -0.0083,                 loss: nan
Episode: 30081/50100 (60.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9125s / 2044.0913 s
agent0:                 episode reward: 0.3015,                 loss: 0.3819
agent1:                 episode reward: -0.3015,                 loss: nan
Episode: 30101/50100 (60.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9233s / 2046.0146 s
agent0:                 episode reward: 0.3038,                 loss: 0.3803
agent1:                 episode reward: -0.3038,                 loss: nan
Episode: 30121/50100 (60.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9395s / 2047.9541 s
agent0:                 episode reward: 0.4142,                 loss: 0.3800
agent1:                 episode reward: -0.4142,                 loss: nan
Episode: 30141/50100 (60.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9551s / 2049.9092 s
agent0:                 episode reward: -0.3483,                 loss: 0.4217
agent1:                 episode reward: 0.3483,                 loss: nan
Episode: 30161/50100 (60.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9320s / 2051.8413 s
agent0:                 episode reward: -0.0505,                 loss: 0.4249
agent1:                 episode reward: 0.0505,                 loss: nan
Episode: 30181/50100 (60.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9577s / 2053.7990 s
agent0:                 episode reward: 0.2926,                 loss: 0.4248
agent1:                 episode reward: -0.2926,                 loss: 0.4537
Score delta: 2.0937335216679123, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/29967_0.
Episode: 30201/50100 (60.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1888s / 2054.9878 s
agent0:                 episode reward: -0.8574,                 loss: nan
agent1:                 episode reward: 0.8574,                 loss: 0.4537
Episode: 30221/50100 (60.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1825s / 2056.1702 s
agent0:                 episode reward: -0.8011,                 loss: nan
agent1:                 episode reward: 0.8011,                 loss: 0.4536
Episode: 30241/50100 (60.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1954s / 2057.3657 s
agent0:                 episode reward: -0.3912,                 loss: nan
agent1:                 episode reward: 0.3912,                 loss: 0.4525
Episode: 30261/50100 (60.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2320s / 2058.5976 s
agent0:                 episode reward: 0.5491,                 loss: nan
agent1:                 episode reward: -0.5491,                 loss: 0.4524
Episode: 30281/50100 (60.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3302s / 2059.9278 s
agent0:                 episode reward: -0.8240,                 loss: 0.3403
agent1:                 episode reward: 0.8240,                 loss: 0.4521
Score delta: 2.2774831267803664, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/30066_1.
Episode: 30301/50100 (60.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9725s / 2061.9003 s
agent0:                 episode reward: 0.1090,                 loss: 0.3317
agent1:                 episode reward: -0.1090,                 loss: nan
Episode: 30321/50100 (60.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9541s / 2063.8544 s
agent0:                 episode reward: 0.4073,                 loss: 0.3303
agent1:                 episode reward: -0.4073,                 loss: nan
Episode: 30341/50100 (60.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9424s / 2065.7968 s
agent0:                 episode reward: 0.0969,                 loss: 0.3299
agent1:                 episode reward: -0.0969,                 loss: nan
Episode: 30361/50100 (60.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9576s / 2067.7544 s
agent0:                 episode reward: -0.4781,                 loss: 0.3281
agent1:                 episode reward: 0.4781,                 loss: nan
Episode: 30381/50100 (60.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9442s / 2069.6986 s
agent0:                 episode reward: -0.2403,                 loss: 0.3262
agent1:                 episode reward: 0.2403,                 loss: nan
Episode: 30401/50100 (60.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9651s / 2071.6637 s
agent0:                 episode reward: 0.6235,                 loss: 0.3662
agent1:                 episode reward: -0.6235,                 loss: nan
Episode: 30421/50100 (60.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9685s / 2073.6323 s
agent0:                 episode reward: 0.0230,                 loss: 0.3873
agent1:                 episode reward: -0.0230,                 loss: nan
Episode: 30441/50100 (60.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9744s / 2075.6066 s
agent0:                 episode reward: 0.2590,                 loss: 0.3864
agent1:                 episode reward: -0.2590,                 loss: nan
Episode: 30461/50100 (60.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9545s / 2077.5611 s
agent0:                 episode reward: -0.7635,                 loss: 0.3856
agent1:                 episode reward: 0.7635,                 loss: nan
Episode: 30481/50100 (60.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9686s / 2079.5297 s
agent0:                 episode reward: -0.0560,                 loss: 0.3875
agent1:                 episode reward: 0.0560,                 loss: nan
Episode: 30501/50100 (60.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9603s / 2081.4900 s
agent0:                 episode reward: -0.2575,                 loss: 0.3878
agent1:                 episode reward: 0.2575,                 loss: nan
Episode: 30521/50100 (60.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9584s / 2083.4485 s
agent0:                 episode reward: -0.1233,                 loss: 0.3844
agent1:                 episode reward: 0.1233,                 loss: nan
Episode: 30541/50100 (60.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4470s / 2084.8954 s
agent0:                 episode reward: 0.4397,                 loss: 0.3839
agent1:                 episode reward: -0.4397,                 loss: 0.4471
Score delta: 2.023912906255755, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/30315_0.
Episode: 30561/50100 (61.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 1.1932s / 2086.0886 s
agent0:                 episode reward: 0.1115,                 loss: nan
agent1:                 episode reward: -0.1115,                 loss: 0.4472
Episode: 30581/50100 (61.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2196s / 2087.3083 s
agent0:                 episode reward: -0.5470,                 loss: nan
agent1:                 episode reward: 0.5470,                 loss: 0.4459
Episode: 30601/50100 (61.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2113s / 2088.5195 s
agent0:                 episode reward: -0.2549,                 loss: nan
agent1:                 episode reward: 0.2549,                 loss: 0.4453
Episode: 30621/50100 (61.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2161s / 2089.7356 s
agent0:                 episode reward: -0.0906,                 loss: nan
agent1:                 episode reward: 0.0906,                 loss: 0.4453
Episode: 30641/50100 (61.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4211s / 2091.1567 s
agent0:                 episode reward: -0.8666,                 loss: 0.3300
agent1:                 episode reward: 0.8666,                 loss: 0.4461
Score delta: 2.4176754818996926, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/30424_1.
Episode: 30661/50100 (61.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9518s / 2093.1086 s
agent0:                 episode reward: 0.1578,                 loss: 0.3258
agent1:                 episode reward: -0.1578,                 loss: nan
Episode: 30681/50100 (61.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9577s / 2095.0663 s
agent0:                 episode reward: -0.5004,                 loss: 0.3467
agent1:                 episode reward: 0.5004,                 loss: nan
Episode: 30701/50100 (61.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9426s / 2097.0089 s
agent0:                 episode reward: -0.5776,                 loss: 0.3450
agent1:                 episode reward: 0.5776,                 loss: nan
Episode: 30721/50100 (61.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9491s / 2098.9580 s
agent0:                 episode reward: -0.2194,                 loss: 0.3441
agent1:                 episode reward: 0.2194,                 loss: nan
Episode: 30741/50100 (61.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4807s / 2100.4387 s
agent0:                 episode reward: 0.0243,                 loss: 0.3460
agent1:                 episode reward: -0.0243,                 loss: 0.4533
Score delta: 2.028460844349235, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/30516_0.
Episode: 30761/50100 (61.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2344s / 2101.6732 s
agent0:                 episode reward: -0.3741,                 loss: nan
agent1:                 episode reward: 0.3741,                 loss: 0.4535
Episode: 30781/50100 (61.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2142s / 2102.8874 s
agent0:                 episode reward: -0.1604,                 loss: nan
agent1:                 episode reward: 0.1604,                 loss: 0.4530
Episode: 30801/50100 (61.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4095s / 2104.2969 s
agent0:                 episode reward: -0.6561,                 loss: 0.3116
agent1:                 episode reward: 0.6561,                 loss: 0.4533
Score delta: 2.33701193817991, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/30584_1.
Episode: 30821/50100 (61.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9499s / 2106.2467 s
agent0:                 episode reward: -0.2921,                 loss: 0.3142
agent1:                 episode reward: 0.2921,                 loss: nan
Episode: 30841/50100 (61.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9493s / 2108.1961 s
agent0:                 episode reward: -0.3638,                 loss: 0.3133
agent1:                 episode reward: 0.3638,                 loss: nan
Episode: 30861/50100 (61.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9557s / 2110.1518 s
agent0:                 episode reward: -0.1464,                 loss: 0.3113
agent1:                 episode reward: 0.1464,                 loss: nan
Episode: 30881/50100 (61.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9549s / 2112.1067 s
agent0:                 episode reward: -0.1118,                 loss: 0.3113
agent1:                 episode reward: 0.1118,                 loss: nan
Episode: 30901/50100 (61.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9460s / 2114.0527 s
agent0:                 episode reward: 0.2568,                 loss: 0.3169
agent1:                 episode reward: -0.2568,                 loss: nan
Episode: 30921/50100 (61.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9728s / 2116.0255 s
agent0:                 episode reward: -0.2546,                 loss: 0.3490
agent1:                 episode reward: 0.2546,                 loss: nan
Episode: 30941/50100 (61.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9580s / 2117.9835 s
agent0:                 episode reward: 0.1706,                 loss: 0.3487
agent1:                 episode reward: -0.1706,                 loss: nan
Episode: 30961/50100 (61.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9827s / 2119.9662 s
agent0:                 episode reward: 0.2029,                 loss: 0.3511
agent1:                 episode reward: -0.2029,                 loss: nan
Episode: 30981/50100 (61.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5131s / 2121.4793 s
agent0:                 episode reward: 0.4721,                 loss: 0.3465
agent1:                 episode reward: -0.4721,                 loss: 0.4559
Score delta: 2.3260481802711217, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/30756_0.
Episode: 31001/50100 (61.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2318s / 2122.7111 s
agent0:                 episode reward: -0.7310,                 loss: nan
agent1:                 episode reward: 0.7310,                 loss: 0.4525
Episode: 31021/50100 (61.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2308s / 2123.9419 s
agent0:                 episode reward: -0.0300,                 loss: nan
agent1:                 episode reward: 0.0300,                 loss: 0.4514
Episode: 31041/50100 (61.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2191s / 2125.1610 s
agent0:                 episode reward: 0.1399,                 loss: nan
agent1:                 episode reward: -0.1399,                 loss: 0.4514
Episode: 31061/50100 (61.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2360s / 2126.3969 s
agent0:                 episode reward: -0.6415,                 loss: nan
agent1:                 episode reward: 0.6415,                 loss: 0.4511
Episode: 31081/50100 (62.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2348s / 2127.6318 s
agent0:                 episode reward: 0.1842,                 loss: nan
agent1:                 episode reward: -0.1842,                 loss: 0.4502
Episode: 31101/50100 (62.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6181s / 2129.2499 s
agent0:                 episode reward: -0.8049,                 loss: 0.3610
agent1:                 episode reward: 0.8049,                 loss: 0.4500
Score delta: 2.514139989344547, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/30878_1.
Episode: 31121/50100 (62.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9630s / 2131.2128 s
agent0:                 episode reward: -0.7636,                 loss: 0.3633
agent1:                 episode reward: 0.7636,                 loss: nan
Episode: 31141/50100 (62.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9926s / 2133.2054 s
agent0:                 episode reward: 0.5048,                 loss: 0.3626
agent1:                 episode reward: -0.5048,                 loss: nan
Episode: 31161/50100 (62.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9814s / 2135.1868 s
agent0:                 episode reward: 0.3139,                 loss: 0.3603
agent1:                 episode reward: -0.3139,                 loss: nan
Episode: 31181/50100 (62.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9700s / 2137.1569 s
agent0:                 episode reward: -0.1644,                 loss: 0.3626
agent1:                 episode reward: 0.1644,                 loss: nan
Episode: 31201/50100 (62.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9680s / 2139.1249 s
agent0:                 episode reward: -0.2074,                 loss: 0.4074
agent1:                 episode reward: 0.2074,                 loss: nan
Episode: 31221/50100 (62.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9665s / 2141.0914 s
agent0:                 episode reward: 0.1445,                 loss: 0.4277
agent1:                 episode reward: -0.1445,                 loss: nan
Episode: 31241/50100 (62.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9808s / 2143.0723 s
agent0:                 episode reward: -0.0784,                 loss: 0.4275
agent1:                 episode reward: 0.0784,                 loss: nan
Episode: 31261/50100 (62.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9828s / 2145.0551 s
agent0:                 episode reward: 0.1118,                 loss: 0.4272
agent1:                 episode reward: -0.1118,                 loss: nan
Episode: 31281/50100 (62.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9723s / 2147.0274 s
agent0:                 episode reward: -0.4605,                 loss: 0.4262
agent1:                 episode reward: 0.4605,                 loss: nan
Episode: 31301/50100 (62.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9585s / 2148.9859 s
agent0:                 episode reward: 0.6329,                 loss: 0.4262
agent1:                 episode reward: -0.6329,                 loss: nan
Episode: 31321/50100 (62.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9838s / 2150.9697 s
agent0:                 episode reward: -0.0124,                 loss: 0.4246
agent1:                 episode reward: 0.0124,                 loss: nan
Episode: 31341/50100 (62.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9688s / 2152.9385 s
agent0:                 episode reward: 0.1075,                 loss: 0.4271
agent1:                 episode reward: -0.1075,                 loss: nan
Episode: 31361/50100 (62.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9935s / 2154.9320 s
agent0:                 episode reward: 0.0101,                 loss: 0.4329
agent1:                 episode reward: -0.0101,                 loss: nan
Episode: 31381/50100 (62.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9680s / 2156.9000 s
agent0:                 episode reward: 0.2543,                 loss: 0.4379
agent1:                 episode reward: -0.2543,                 loss: nan
Episode: 31401/50100 (62.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0162s / 2158.9162 s
agent0:                 episode reward: -0.1735,                 loss: 0.4389
agent1:                 episode reward: 0.1735,                 loss: nan
Episode: 31421/50100 (62.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9937s / 2160.9099 s
agent0:                 episode reward: 0.2893,                 loss: 0.4392
agent1:                 episode reward: -0.2893,                 loss: nan
Episode: 31441/50100 (62.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9771s / 2162.8871 s
agent0:                 episode reward: -0.4059,                 loss: 0.4380
agent1:                 episode reward: 0.4059,                 loss: nan
Episode: 31461/50100 (62.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9982s / 2164.8853 s
agent0:                 episode reward: -0.1902,                 loss: 0.4376
agent1:                 episode reward: 0.1902,                 loss: nan
Episode: 31481/50100 (62.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9917s / 2166.8770 s
agent0:                 episode reward: -0.1987,                 loss: 0.4375
agent1:                 episode reward: 0.1987,                 loss: nan
Episode: 31501/50100 (62.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8742s / 2168.7512 s
agent0:                 episode reward: 0.3463,                 loss: 0.4384
agent1:                 episode reward: -0.3463,                 loss: 0.4472
Score delta: 2.3497734850765997, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/31286_0.
Episode: 31521/50100 (62.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3393s / 2170.0905 s
agent0:                 episode reward: -0.6067,                 loss: nan
agent1:                 episode reward: 0.6067,                 loss: 0.4452
Episode: 31541/50100 (62.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2599s / 2171.3504 s
agent0:                 episode reward: -0.1405,                 loss: nan
agent1:                 episode reward: 0.1405,                 loss: 0.4512
Episode: 31561/50100 (62.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2499s / 2172.6003 s
agent0:                 episode reward: -0.5494,                 loss: nan
agent1:                 episode reward: 0.5494,                 loss: 0.4509
Episode: 31581/50100 (63.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2414s / 2173.8416 s
agent0:                 episode reward: -0.3332,                 loss: nan
agent1:                 episode reward: 0.3332,                 loss: 0.4496
Episode: 31601/50100 (63.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3889s / 2175.2305 s
agent0:                 episode reward: -0.6437,                 loss: 0.3121
agent1:                 episode reward: 0.6437,                 loss: 0.4495
Score delta: 2.2351238896830807, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/31386_1.
Episode: 31621/50100 (63.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0101s / 2177.2406 s
agent0:                 episode reward: -0.3195,                 loss: 0.3153
agent1:                 episode reward: 0.3195,                 loss: nan
Episode: 31641/50100 (63.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9653s / 2179.2059 s
agent0:                 episode reward: -0.2541,                 loss: 0.3573
agent1:                 episode reward: 0.2541,                 loss: nan
Episode: 31661/50100 (63.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9563s / 2181.1622 s
agent0:                 episode reward: 0.1646,                 loss: 0.3548
agent1:                 episode reward: -0.1646,                 loss: nan
Episode: 31681/50100 (63.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9734s / 2183.1356 s
agent0:                 episode reward: -0.1708,                 loss: 0.3559
agent1:                 episode reward: 0.1708,                 loss: nan
Episode: 31701/50100 (63.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9666s / 2185.1022 s
agent0:                 episode reward: -0.5967,                 loss: 0.3569
agent1:                 episode reward: 0.5967,                 loss: nan
Episode: 31721/50100 (63.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0058s / 2187.1080 s
agent0:                 episode reward: -0.0426,                 loss: 0.3574
agent1:                 episode reward: 0.0426,                 loss: nan
Episode: 31741/50100 (63.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0291s / 2189.1371 s
agent0:                 episode reward: -0.2778,                 loss: 0.3539
agent1:                 episode reward: 0.2778,                 loss: nan
Episode: 31761/50100 (63.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9633s / 2191.1004 s
agent0:                 episode reward: -0.1341,                 loss: 0.3543
agent1:                 episode reward: 0.1341,                 loss: nan
Episode: 31781/50100 (63.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9781s / 2193.0784 s
agent0:                 episode reward: -0.0274,                 loss: 0.3541
agent1:                 episode reward: 0.0274,                 loss: nan
Episode: 31801/50100 (63.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9805s / 2195.0589 s
agent0:                 episode reward: 0.0483,                 loss: 0.4013
agent1:                 episode reward: -0.0483,                 loss: nan
Episode: 31821/50100 (63.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9566s / 2197.0156 s
agent0:                 episode reward: 0.0658,                 loss: 0.4214
agent1:                 episode reward: -0.0658,                 loss: nan
Episode: 31841/50100 (63.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9866s / 2199.0022 s
agent0:                 episode reward: -0.1311,                 loss: 0.4182
agent1:                 episode reward: 0.1311,                 loss: nan
Episode: 31861/50100 (63.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0005s / 2201.0027 s
agent0:                 episode reward: -0.2133,                 loss: 0.4177
agent1:                 episode reward: 0.2133,                 loss: nan
Episode: 31881/50100 (63.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0375s / 2203.0401 s
agent0:                 episode reward: 0.0971,                 loss: 0.4170
agent1:                 episode reward: -0.0971,                 loss: nan
Episode: 31901/50100 (63.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9685s / 2205.0087 s
agent0:                 episode reward: 0.0836,                 loss: 0.4174
agent1:                 episode reward: -0.0836,                 loss: nan
Episode: 31921/50100 (63.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9495s / 2206.9582 s
agent0:                 episode reward: -0.0375,                 loss: 0.4185
agent1:                 episode reward: 0.0375,                 loss: nan
Episode: 31941/50100 (63.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0575s / 2209.0157 s
agent0:                 episode reward: 0.2797,                 loss: 0.4201
agent1:                 episode reward: -0.2797,                 loss: nan
Episode: 31961/50100 (63.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9780s / 2210.9937 s
agent0:                 episode reward: 0.2419,                 loss: 0.4278
agent1:                 episode reward: -0.2419,                 loss: nan
Episode: 31981/50100 (63.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9568s / 2212.9505 s
agent0:                 episode reward: 0.2679,                 loss: 0.4412
agent1:                 episode reward: -0.2679,                 loss: nan
Episode: 32001/50100 (63.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0794s / 2215.0299 s
agent0:                 episode reward: -0.4437,                 loss: 0.4410
agent1:                 episode reward: 0.4437,                 loss: nan
Episode: 32021/50100 (63.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0198s / 2217.0497 s
agent0:                 episode reward: 0.3502,                 loss: 0.4400
agent1:                 episode reward: -0.3502,                 loss: nan
Episode: 32041/50100 (63.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3111s / 2218.3608 s
agent0:                 episode reward: -0.5407,                 loss: 0.4431
agent1:                 episode reward: 0.5407,                 loss: 0.4549
Score delta: 2.263386863017095, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/31811_0.
Episode: 32061/50100 (63.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2572s / 2219.6179 s
agent0:                 episode reward: -0.0386,                 loss: nan
agent1:                 episode reward: 0.0386,                 loss: 0.4538
Episode: 32081/50100 (64.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2837s / 2220.9016 s
agent0:                 episode reward: -0.6676,                 loss: nan
agent1:                 episode reward: 0.6676,                 loss: 0.4532
Episode: 32101/50100 (64.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3275s / 2222.2291 s
agent0:                 episode reward: -0.3120,                 loss: nan
agent1:                 episode reward: 0.3120,                 loss: 0.4528
Episode: 32121/50100 (64.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2721s / 2223.5011 s
agent0:                 episode reward: -0.2167,                 loss: nan
agent1:                 episode reward: 0.2167,                 loss: 0.4532
Episode: 32141/50100 (64.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7952s / 2225.2963 s
agent0:                 episode reward: -0.1499,                 loss: 0.3558
agent1:                 episode reward: 0.1499,                 loss: 0.4529
Score delta: 2.278746414566581, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/31914_1.
Episode: 32161/50100 (64.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9815s / 2227.2778 s
agent0:                 episode reward: 0.0780,                 loss: 0.3522
agent1:                 episode reward: -0.0780,                 loss: nan
Episode: 32181/50100 (64.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9651s / 2229.2429 s
agent0:                 episode reward: 0.3663,                 loss: 0.3514
agent1:                 episode reward: -0.3663,                 loss: nan
Episode: 32201/50100 (64.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9655s / 2231.2084 s
agent0:                 episode reward: -0.0358,                 loss: 0.3535
agent1:                 episode reward: 0.0358,                 loss: nan
Episode: 32221/50100 (64.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9649s / 2233.1733 s
agent0:                 episode reward: 0.1554,                 loss: 0.3526
agent1:                 episode reward: -0.1554,                 loss: nan
Episode: 32241/50100 (64.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0251s / 2235.1984 s
agent0:                 episode reward: 0.0433,                 loss: 0.4044
agent1:                 episode reward: -0.0433,                 loss: nan
Episode: 32261/50100 (64.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9411s / 2237.1395 s
agent0:                 episode reward: -0.1895,                 loss: 0.4084
agent1:                 episode reward: 0.1895,                 loss: nan
Episode: 32281/50100 (64.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9488s / 2239.0883 s
agent0:                 episode reward: -0.4589,                 loss: 0.4080
agent1:                 episode reward: 0.4589,                 loss: nan
Episode: 32301/50100 (64.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9573s / 2241.0456 s
agent0:                 episode reward: -0.3684,                 loss: 0.4075
agent1:                 episode reward: 0.3684,                 loss: nan
Episode: 32321/50100 (64.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9564s / 2243.0020 s
agent0:                 episode reward: 0.2313,                 loss: 0.4071
agent1:                 episode reward: -0.2313,                 loss: nan
Episode: 32341/50100 (64.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9576s / 2244.9596 s
agent0:                 episode reward: 0.0991,                 loss: 0.4070
agent1:                 episode reward: -0.0991,                 loss: nan
Episode: 32361/50100 (64.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9500s / 2246.9096 s
agent0:                 episode reward: -0.4674,                 loss: 0.4054
agent1:                 episode reward: 0.4674,                 loss: nan
Episode: 32381/50100 (64.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9643s / 2248.8740 s
agent0:                 episode reward: -0.3518,                 loss: 0.4064
agent1:                 episode reward: 0.3518,                 loss: nan
Episode: 32401/50100 (64.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9666s / 2250.8406 s
agent0:                 episode reward: -0.1836,                 loss: 0.4135
agent1:                 episode reward: 0.1836,                 loss: nan
Episode: 32421/50100 (64.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9576s / 2252.7982 s
agent0:                 episode reward: -0.6116,                 loss: 0.4121
agent1:                 episode reward: 0.6116,                 loss: nan
Episode: 32441/50100 (64.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9597s / 2254.7578 s
agent0:                 episode reward: 0.6028,                 loss: 0.4125
agent1:                 episode reward: -0.6028,                 loss: nan
Episode: 32461/50100 (64.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2894s / 2256.0472 s
agent0:                 episode reward: -0.1637,                 loss: 0.4119
agent1:                 episode reward: 0.1637,                 loss: 0.4513
Score delta: 2.016303269347869, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/32230_0.
Episode: 32481/50100 (64.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2627s / 2257.3099 s
agent0:                 episode reward: -0.1654,                 loss: nan
agent1:                 episode reward: 0.1654,                 loss: 0.4503
Episode: 32501/50100 (64.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2499s / 2258.5598 s
agent0:                 episode reward: -0.4450,                 loss: nan
agent1:                 episode reward: 0.4450,                 loss: 0.4505
Episode: 32521/50100 (64.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2508s / 2259.8106 s
agent0:                 episode reward: -0.0969,                 loss: nan
agent1:                 episode reward: 0.0969,                 loss: 0.4496
Episode: 32541/50100 (64.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2642s / 2261.0748 s
agent0:                 episode reward: -0.3243,                 loss: nan
agent1:                 episode reward: 0.3243,                 loss: 0.4495
Episode: 32561/50100 (64.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2514s / 2262.3261 s
agent0:                 episode reward: -0.2179,                 loss: nan
agent1:                 episode reward: 0.2179,                 loss: 0.4498
Episode: 32581/50100 (65.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2675s / 2263.5936 s
agent0:                 episode reward: -0.5648,                 loss: nan
agent1:                 episode reward: 0.5648,                 loss: 0.4499
Episode: 32601/50100 (65.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9109s / 2265.5045 s
agent0:                 episode reward: -0.4319,                 loss: 0.3593
agent1:                 episode reward: 0.4319,                 loss: 0.4496
Score delta: 2.0863907695536983, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/32373_1.
Episode: 32621/50100 (65.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9637s / 2267.4682 s
agent0:                 episode reward: 0.1347,                 loss: 0.3595
agent1:                 episode reward: -0.1347,                 loss: nan
Episode: 32641/50100 (65.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9692s / 2269.4375 s
agent0:                 episode reward: 0.1123,                 loss: 0.3558
agent1:                 episode reward: -0.1123,                 loss: nan
Episode: 32661/50100 (65.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0326s / 2271.4701 s
agent0:                 episode reward: -0.0410,                 loss: 0.3582
agent1:                 episode reward: 0.0410,                 loss: nan
Episode: 32681/50100 (65.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9710s / 2273.4411 s
agent0:                 episode reward: -0.3290,                 loss: 0.3583
agent1:                 episode reward: 0.3290,                 loss: nan
Episode: 32701/50100 (65.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9598s / 2275.4009 s
agent0:                 episode reward: -0.3011,                 loss: 0.3616
agent1:                 episode reward: 0.3011,                 loss: nan
Episode: 32721/50100 (65.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9706s / 2277.3715 s
agent0:                 episode reward: -0.3326,                 loss: 0.4220
agent1:                 episode reward: 0.3326,                 loss: nan
Episode: 32741/50100 (65.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5948s / 2278.9663 s
agent0:                 episode reward: 0.6823,                 loss: 0.4200
agent1:                 episode reward: -0.6823,                 loss: 0.4531
Score delta: 2.1061891410937497, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/32518_0.
Episode: 32761/50100 (65.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2690s / 2280.2352 s
agent0:                 episode reward: -1.0449,                 loss: nan
agent1:                 episode reward: 1.0449,                 loss: 0.4533
Episode: 32781/50100 (65.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2779s / 2281.5131 s
agent0:                 episode reward: -0.4969,                 loss: nan
agent1:                 episode reward: 0.4969,                 loss: 0.4524
Episode: 32801/50100 (65.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2673s / 2282.7804 s
agent0:                 episode reward: 0.0445,                 loss: nan
agent1:                 episode reward: -0.0445,                 loss: 0.4520
Episode: 32821/50100 (65.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2596s / 2284.0401 s
agent0:                 episode reward: -0.0349,                 loss: nan
agent1:                 episode reward: 0.0349,                 loss: 0.4517
Episode: 32841/50100 (65.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3000s / 2285.3401 s
agent0:                 episode reward: 0.0985,                 loss: nan
agent1:                 episode reward: -0.0985,                 loss: 0.4520
Episode: 32861/50100 (65.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3903s / 2286.7304 s
agent0:                 episode reward: -0.0696,                 loss: nan
agent1:                 episode reward: 0.0696,                 loss: 0.4510
Episode: 32881/50100 (65.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2794s / 2288.0098 s
agent0:                 episode reward: -0.3504,                 loss: nan
agent1:                 episode reward: 0.3504,                 loss: 0.4518
Episode: 32901/50100 (65.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2839s / 2289.2938 s
agent0:                 episode reward: 0.0233,                 loss: nan
agent1:                 episode reward: -0.0233,                 loss: 0.4515
Episode: 32921/50100 (65.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4440s / 2290.7377 s
agent0:                 episode reward: -0.4242,                 loss: 0.2928
agent1:                 episode reward: 0.4242,                 loss: 0.4510
Score delta: 2.054235938775951, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/32705_1.
Episode: 32941/50100 (65.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9992s / 2292.7369 s
agent0:                 episode reward: -0.3776,                 loss: 0.2829
agent1:                 episode reward: 0.3776,                 loss: nan
Episode: 32961/50100 (65.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9877s / 2294.7246 s
agent0:                 episode reward: 0.0816,                 loss: 0.2845
agent1:                 episode reward: -0.0816,                 loss: nan
Episode: 32981/50100 (65.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0533s / 2296.7779 s
agent0:                 episode reward: 0.2759,                 loss: 0.2822
agent1:                 episode reward: -0.2759,                 loss: nan
Episode: 33001/50100 (65.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9812s / 2298.7591 s
agent0:                 episode reward: -0.5584,                 loss: 0.2799
agent1:                 episode reward: 0.5584,                 loss: nan
Episode: 33021/50100 (65.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9714s / 2300.7305 s
agent0:                 episode reward: 0.2401,                 loss: 0.2821
agent1:                 episode reward: -0.2401,                 loss: nan
Episode: 33041/50100 (65.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9829s / 2302.7134 s
agent0:                 episode reward: -0.3934,                 loss: 0.2801
agent1:                 episode reward: 0.3934,                 loss: nan
Episode: 33061/50100 (65.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9788s / 2304.6922 s
agent0:                 episode reward: 0.3371,                 loss: 0.3063
agent1:                 episode reward: -0.3371,                 loss: nan
Episode: 33081/50100 (66.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9707s / 2306.6629 s
agent0:                 episode reward: 0.1334,                 loss: 0.3458
agent1:                 episode reward: -0.1334,                 loss: nan
Episode: 33101/50100 (66.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9759s / 2308.6389 s
agent0:                 episode reward: -0.1494,                 loss: 0.3448
agent1:                 episode reward: 0.1494,                 loss: nan
Episode: 33121/50100 (66.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9624s / 2310.6012 s
agent0:                 episode reward: 0.3898,                 loss: 0.3446
agent1:                 episode reward: -0.3898,                 loss: nan
Episode: 33141/50100 (66.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9433s / 2312.5446 s
agent0:                 episode reward: -0.1998,                 loss: 0.3453
agent1:                 episode reward: 0.1998,                 loss: nan
Episode: 33161/50100 (66.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9617s / 2314.5063 s
agent0:                 episode reward: -0.4092,                 loss: 0.3432
agent1:                 episode reward: 0.4092,                 loss: nan
Episode: 33181/50100 (66.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9827s / 2316.4890 s
agent0:                 episode reward: -0.0672,                 loss: 0.3438
agent1:                 episode reward: 0.0672,                 loss: nan
Episode: 33201/50100 (66.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9705s / 2318.4595 s
agent0:                 episode reward: -0.0038,                 loss: 0.3444
agent1:                 episode reward: 0.0038,                 loss: nan
Episode: 33221/50100 (66.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9930s / 2320.4525 s
agent0:                 episode reward: -0.4286,                 loss: 0.3449
agent1:                 episode reward: 0.4286,                 loss: nan
Episode: 33241/50100 (66.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9794s / 2322.4318 s
agent0:                 episode reward: 0.0871,                 loss: 0.3986
agent1:                 episode reward: -0.0871,                 loss: nan
Episode: 33261/50100 (66.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9594s / 2324.3912 s
agent0:                 episode reward: 0.0616,                 loss: 0.4001
agent1:                 episode reward: -0.0616,                 loss: nan
Episode: 33281/50100 (66.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9667s / 2326.3579 s
agent0:                 episode reward: -0.1484,                 loss: 0.3989
agent1:                 episode reward: 0.1484,                 loss: nan
Episode: 33301/50100 (66.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9716s / 2328.3295 s
agent0:                 episode reward: 0.1524,                 loss: 0.3997
agent1:                 episode reward: -0.1524,                 loss: nan
Episode: 33321/50100 (66.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9714s / 2330.3009 s
agent0:                 episode reward: 0.3013,                 loss: 0.3971
agent1:                 episode reward: -0.3013,                 loss: nan
Episode: 33341/50100 (66.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8221s / 2332.1229 s
agent0:                 episode reward: 0.1317,                 loss: 0.3985
agent1:                 episode reward: -0.1317,                 loss: 0.4482
Score delta: 2.2300908856990276, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/33125_0.
Episode: 33361/50100 (66.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2765s / 2333.3994 s
agent0:                 episode reward: -0.8865,                 loss: nan
agent1:                 episode reward: 0.8865,                 loss: 0.4459
Episode: 33381/50100 (66.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2793s / 2334.6788 s
agent0:                 episode reward: -0.4144,                 loss: nan
agent1:                 episode reward: 0.4144,                 loss: 0.4440
Episode: 33401/50100 (66.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2748s / 2335.9536 s
agent0:                 episode reward: -0.7030,                 loss: nan
agent1:                 episode reward: 0.7030,                 loss: 0.4446
Episode: 33421/50100 (66.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2802s / 2337.2337 s
agent0:                 episode reward: -0.2761,                 loss: nan
agent1:                 episode reward: 0.2761,                 loss: 0.4443
Episode: 33441/50100 (66.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2943s / 2338.5280 s
agent0:                 episode reward: -0.1607,                 loss: nan
agent1:                 episode reward: 0.1607,                 loss: 0.4426
Episode: 33461/50100 (66.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2869s / 2339.8150 s
agent0:                 episode reward: -0.4194,                 loss: nan
agent1:                 episode reward: 0.4194,                 loss: 0.4444
Episode: 33481/50100 (66.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2866s / 2341.1016 s
agent0:                 episode reward: -0.2406,                 loss: nan
agent1:                 episode reward: 0.2406,                 loss: 0.4439
Episode: 33501/50100 (66.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2857s / 2342.3873 s
agent0:                 episode reward: -0.2808,                 loss: nan
agent1:                 episode reward: 0.2808,                 loss: 0.4432
Episode: 33521/50100 (66.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2811s / 2343.6685 s
agent0:                 episode reward: -0.1724,                 loss: nan
agent1:                 episode reward: 0.1724,                 loss: 0.4446
Episode: 33541/50100 (66.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2864s / 2344.9548 s
agent0:                 episode reward: -0.2518,                 loss: nan
agent1:                 episode reward: 0.2518,                 loss: 0.4442
Episode: 33561/50100 (66.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3567s / 2346.3115 s
agent0:                 episode reward: -0.5288,                 loss: 0.4236
agent1:                 episode reward: 0.5288,                 loss: 0.4427
Score delta: 2.1110140756019073, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/33347_1.
Episode: 33581/50100 (67.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9806s / 2348.2921 s
agent0:                 episode reward: 0.1915,                 loss: 0.4210
agent1:                 episode reward: -0.1915,                 loss: nan
Episode: 33601/50100 (67.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9718s / 2350.2639 s
agent0:                 episode reward: -0.5331,                 loss: 0.4233
agent1:                 episode reward: 0.5331,                 loss: nan
Episode: 33621/50100 (67.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9598s / 2352.2238 s
agent0:                 episode reward: -0.4384,                 loss: 0.4319
agent1:                 episode reward: 0.4384,                 loss: nan
Episode: 33641/50100 (67.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9764s / 2354.2002 s
agent0:                 episode reward: -0.3923,                 loss: 0.4383
agent1:                 episode reward: 0.3923,                 loss: nan
Episode: 33661/50100 (67.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9595s / 2356.1598 s
agent0:                 episode reward: 0.1770,                 loss: 0.4348
agent1:                 episode reward: -0.1770,                 loss: nan
Episode: 33681/50100 (67.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4824s / 2357.6422 s
agent0:                 episode reward: 0.6228,                 loss: 0.4375
agent1:                 episode reward: -0.6228,                 loss: 0.4536
Score delta: 2.5094953927968136, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/33454_0.
Episode: 33701/50100 (67.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3019s / 2358.9440 s
agent0:                 episode reward: 0.0792,                 loss: nan
agent1:                 episode reward: -0.0792,                 loss: 0.4532
Episode: 33721/50100 (67.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2959s / 2360.2399 s
agent0:                 episode reward: -0.8126,                 loss: nan
agent1:                 episode reward: 0.8126,                 loss: 0.4505
Episode: 33741/50100 (67.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3162s / 2361.5561 s
agent0:                 episode reward: -0.5992,                 loss: nan
agent1:                 episode reward: 0.5992,                 loss: 0.4501
Episode: 33761/50100 (67.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7654s / 2363.3215 s
agent0:                 episode reward: -0.8490,                 loss: 0.3370
agent1:                 episode reward: 0.8490,                 loss: 0.4507
Score delta: 2.148298188653801, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/33535_1.
Episode: 33781/50100 (67.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0093s / 2365.3308 s
agent0:                 episode reward: -0.4064,                 loss: 0.3366
agent1:                 episode reward: 0.4064,                 loss: nan
Episode: 33801/50100 (67.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0049s / 2367.3357 s
agent0:                 episode reward: -0.1774,                 loss: 0.3365
agent1:                 episode reward: 0.1774,                 loss: nan
Episode: 33821/50100 (67.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9784s / 2369.3142 s
agent0:                 episode reward: 0.4417,                 loss: 0.3363
agent1:                 episode reward: -0.4417,                 loss: nan
Episode: 33841/50100 (67.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9889s / 2371.3031 s
agent0:                 episode reward: 0.2795,                 loss: 0.3379
agent1:                 episode reward: -0.2795,                 loss: nan
Episode: 33861/50100 (67.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9844s / 2373.2876 s
agent0:                 episode reward: 0.0503,                 loss: 0.3529
agent1:                 episode reward: -0.0503,                 loss: nan
Episode: 33881/50100 (67.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0481s / 2375.3356 s
agent0:                 episode reward: -0.6046,                 loss: 0.4086
agent1:                 episode reward: 0.6046,                 loss: nan
Episode: 33901/50100 (67.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9538s / 2377.2895 s
agent0:                 episode reward: -0.1401,                 loss: 0.4083
agent1:                 episode reward: 0.1401,                 loss: nan
Episode: 33921/50100 (67.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9595s / 2379.2490 s
agent0:                 episode reward: -0.3894,                 loss: 0.4083
agent1:                 episode reward: 0.3894,                 loss: nan
Episode: 33941/50100 (67.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9646s / 2381.2136 s
agent0:                 episode reward: -0.5074,                 loss: 0.4057
agent1:                 episode reward: 0.5074,                 loss: nan
Episode: 33961/50100 (67.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9706s / 2383.1842 s
agent0:                 episode reward: 0.1813,                 loss: 0.4074
agent1:                 episode reward: -0.1813,                 loss: nan
Episode: 33981/50100 (67.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9803s / 2385.1646 s
agent0:                 episode reward: -0.0958,                 loss: 0.4072
agent1:                 episode reward: 0.0958,                 loss: nan
Episode: 34001/50100 (67.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9714s / 2387.1360 s
agent0:                 episode reward: 0.1994,                 loss: 0.4069
agent1:                 episode reward: -0.1994,                 loss: nan
Episode: 34021/50100 (67.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9642s / 2389.1003 s
agent0:                 episode reward: -0.1710,                 loss: 0.4066
agent1:                 episode reward: 0.1710,                 loss: nan
Episode: 34041/50100 (67.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0007s / 2391.1010 s
agent0:                 episode reward: -0.1296,                 loss: 0.4219
agent1:                 episode reward: 0.1296,                 loss: nan
Episode: 34061/50100 (67.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0209s / 2393.1219 s
agent0:                 episode reward: -0.3673,                 loss: 0.4196
agent1:                 episode reward: 0.3673,                 loss: nan
Episode: 34081/50100 (68.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9858s / 2395.1076 s
agent0:                 episode reward: 0.2250,                 loss: 0.4167
agent1:                 episode reward: -0.2250,                 loss: nan
Episode: 34101/50100 (68.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7646s / 2396.8723 s
agent0:                 episode reward: 0.3932,                 loss: 0.4161
agent1:                 episode reward: -0.3932,                 loss: 0.4559
Score delta: 2.115293318529516, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/33882_0.
Episode: 34121/50100 (68.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3296s / 2398.2019 s
agent0:                 episode reward: -0.2881,                 loss: nan
agent1:                 episode reward: 0.2881,                 loss: 0.4529
Episode: 34141/50100 (68.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3254s / 2399.5272 s
agent0:                 episode reward: 0.0403,                 loss: nan
agent1:                 episode reward: -0.0403,                 loss: 0.4516
Episode: 34161/50100 (68.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3241s / 2400.8514 s
agent0:                 episode reward: -0.1977,                 loss: nan
agent1:                 episode reward: 0.1977,                 loss: 0.4520
Episode: 34181/50100 (68.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4514s / 2402.3028 s
agent0:                 episode reward: -0.6875,                 loss: 0.3423
agent1:                 episode reward: 0.6875,                 loss: 0.4511
Score delta: 2.5691701514184646, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/33965_1.
Episode: 34201/50100 (68.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0490s / 2404.3519 s
agent0:                 episode reward: -0.2095,                 loss: 0.3390
agent1:                 episode reward: 0.2095,                 loss: nan
Episode: 34221/50100 (68.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9937s / 2406.3456 s
agent0:                 episode reward: 0.5631,                 loss: 0.3360
agent1:                 episode reward: -0.5631,                 loss: nan
Episode: 34241/50100 (68.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9702s / 2408.3158 s
agent0:                 episode reward: -0.0180,                 loss: 0.3351
agent1:                 episode reward: 0.0180,                 loss: nan
Episode: 34261/50100 (68.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9678s / 2410.2836 s
agent0:                 episode reward: -0.0729,                 loss: 0.3348
agent1:                 episode reward: 0.0729,                 loss: nan
Episode: 34281/50100 (68.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9854s / 2412.2691 s
agent0:                 episode reward: -0.0951,                 loss: 0.3618
agent1:                 episode reward: 0.0951,                 loss: nan
Episode: 34301/50100 (68.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9847s / 2414.2538 s
agent0:                 episode reward: 0.0431,                 loss: 0.3950
agent1:                 episode reward: -0.0431,                 loss: nan
Episode: 34321/50100 (68.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9674s / 2416.2212 s
agent0:                 episode reward: 0.2263,                 loss: 0.3945
agent1:                 episode reward: -0.2263,                 loss: nan
Episode: 34341/50100 (68.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9801s / 2418.2012 s
agent0:                 episode reward: -0.2428,                 loss: 0.3952
agent1:                 episode reward: 0.2428,                 loss: nan
Episode: 34361/50100 (68.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9912s / 2420.1924 s
agent0:                 episode reward: 0.3772,                 loss: 0.3958
agent1:                 episode reward: -0.3772,                 loss: nan
Episode: 34381/50100 (68.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9717s / 2422.1641 s
agent0:                 episode reward: 0.3885,                 loss: 0.3951
agent1:                 episode reward: -0.3885,                 loss: nan
Episode: 34401/50100 (68.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9943s / 2424.1584 s
agent0:                 episode reward: -0.1308,                 loss: 0.3938
agent1:                 episode reward: 0.1308,                 loss: nan
Episode: 34421/50100 (68.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9874s / 2426.1457 s
agent0:                 episode reward: -0.1555,                 loss: 0.3938
agent1:                 episode reward: 0.1555,                 loss: nan
Episode: 34441/50100 (68.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9884s / 2428.1342 s
agent0:                 episode reward: -0.3404,                 loss: 0.3982
agent1:                 episode reward: 0.3404,                 loss: nan
Episode: 34461/50100 (68.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9921s / 2430.1263 s
agent0:                 episode reward: -0.0473,                 loss: 0.4392
agent1:                 episode reward: 0.0473,                 loss: nan
Episode: 34481/50100 (68.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9983s / 2432.1246 s
agent0:                 episode reward: -0.2196,                 loss: 0.4389
agent1:                 episode reward: 0.2196,                 loss: nan
Episode: 34501/50100 (68.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9944s / 2434.1190 s
agent0:                 episode reward: -0.1527,                 loss: 0.4394
agent1:                 episode reward: 0.1527,                 loss: nan
Episode: 34521/50100 (68.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9546s / 2436.0736 s
agent0:                 episode reward: -0.0478,                 loss: 0.4374
agent1:                 episode reward: 0.0478,                 loss: nan
Episode: 34541/50100 (68.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9695s / 2438.0431 s
agent0:                 episode reward: 0.1393,                 loss: 0.4395
agent1:                 episode reward: -0.1393,                 loss: nan
Episode: 34561/50100 (68.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9691s / 2440.0123 s
agent0:                 episode reward: -0.1085,                 loss: 0.4385
agent1:                 episode reward: 0.1085,                 loss: nan
Episode: 34581/50100 (69.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9686s / 2441.9809 s
agent0:                 episode reward: 0.4274,                 loss: 0.4391
agent1:                 episode reward: -0.4274,                 loss: nan
Episode: 34601/50100 (69.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9586s / 2443.9394 s
agent0:                 episode reward: -0.1260,                 loss: 0.4385
agent1:                 episode reward: 0.1260,                 loss: nan
Episode: 34621/50100 (69.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0003s / 2445.9397 s
agent0:                 episode reward: -0.0560,                 loss: 0.4334
agent1:                 episode reward: 0.0560,                 loss: nan
Episode: 34641/50100 (69.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9935s / 2447.9332 s
agent0:                 episode reward: -0.0163,                 loss: 0.4289
agent1:                 episode reward: 0.0163,                 loss: nan
Episode: 34661/50100 (69.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9668s / 2449.9000 s
agent0:                 episode reward: 0.0667,                 loss: 0.4297
agent1:                 episode reward: -0.0667,                 loss: nan
Episode: 34681/50100 (69.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9591s / 2451.8591 s
agent0:                 episode reward: -0.0056,                 loss: 0.4302
agent1:                 episode reward: 0.0056,                 loss: nan
Episode: 34701/50100 (69.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9662s / 2453.8253 s
agent0:                 episode reward: 0.1614,                 loss: 0.4285
agent1:                 episode reward: -0.1614,                 loss: nan
Episode: 34721/50100 (69.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9372s / 2455.7624 s
agent0:                 episode reward: 0.1003,                 loss: 0.4280
agent1:                 episode reward: -0.1003,                 loss: 0.4518
Score delta: 2.0153357227856707, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/34508_0.
Episode: 34741/50100 (69.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3033s / 2457.0658 s
agent0:                 episode reward: 0.1659,                 loss: nan
agent1:                 episode reward: -0.1659,                 loss: 0.4535
Episode: 34761/50100 (69.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3224s / 2458.3882 s
agent0:                 episode reward: 0.0288,                 loss: nan
agent1:                 episode reward: -0.0288,                 loss: 0.4527
Episode: 34781/50100 (69.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 1.2966s / 2459.6848 s
agent0:                 episode reward: -0.4480,                 loss: nan
agent1:                 episode reward: 0.4480,                 loss: 0.4530
Episode: 34801/50100 (69.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4110s / 2461.0958 s
agent0:                 episode reward: -0.6837,                 loss: 0.4399
agent1:                 episode reward: 0.6837,                 loss: 0.4524
Score delta: 2.4494106059603573, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/34586_1.
Episode: 34821/50100 (69.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9843s / 2463.0802 s
agent0:                 episode reward: -0.2855,                 loss: 0.4415
agent1:                 episode reward: 0.2855,                 loss: nan
Episode: 34841/50100 (69.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9814s / 2465.0615 s
agent0:                 episode reward: 0.0647,                 loss: 0.4409
agent1:                 episode reward: -0.0647,                 loss: nan
Episode: 34861/50100 (69.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9669s / 2467.0285 s
agent0:                 episode reward: 0.1618,                 loss: 0.4415
agent1:                 episode reward: -0.1618,                 loss: nan
Episode: 34881/50100 (69.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9596s / 2468.9881 s
agent0:                 episode reward: -0.1688,                 loss: 0.4407
agent1:                 episode reward: 0.1688,                 loss: nan
Episode: 34901/50100 (69.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9589s / 2470.9470 s
agent0:                 episode reward: 0.1768,                 loss: 0.4407
agent1:                 episode reward: -0.1768,                 loss: nan
Episode: 34921/50100 (69.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9850s / 2472.9320 s
agent0:                 episode reward: 0.0763,                 loss: 0.4403
agent1:                 episode reward: -0.0763,                 loss: nan
Episode: 34941/50100 (69.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9743s / 2474.9063 s
agent0:                 episode reward: 0.1797,                 loss: 0.4392
agent1:                 episode reward: -0.1797,                 loss: nan
Episode: 34961/50100 (69.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0614s / 2476.9677 s
agent0:                 episode reward: -0.1259,                 loss: 0.4402
agent1:                 episode reward: 0.1259,                 loss: nan
Episode: 34981/50100 (69.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6381s / 2478.6058 s
agent0:                 episode reward: -0.0747,                 loss: 0.4386
agent1:                 episode reward: 0.0747,                 loss: 0.4467
Score delta: 2.094273551860228, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/34758_0.
Episode: 35001/50100 (69.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3407s / 2479.9465 s
agent0:                 episode reward: -0.8126,                 loss: nan
agent1:                 episode reward: 0.8126,                 loss: 0.4447
Episode: 35021/50100 (69.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3313s / 2481.2778 s
agent0:                 episode reward: -0.6977,                 loss: nan
agent1:                 episode reward: 0.6977,                 loss: 0.4446
Episode: 35041/50100 (69.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3359s / 2482.6138 s
agent0:                 episode reward: -0.5199,                 loss: nan
agent1:                 episode reward: 0.5199,                 loss: 0.4443
Episode: 35061/50100 (69.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5765s / 2484.1903 s
agent0:                 episode reward: -0.5989,                 loss: 0.4092
agent1:                 episode reward: 0.5989,                 loss: 0.4431
Score delta: 2.2617576151929937, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/34842_1.
Episode: 35081/50100 (70.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9969s / 2486.1871 s
agent0:                 episode reward: 0.1097,                 loss: 0.4067
agent1:                 episode reward: -0.1097,                 loss: nan
Episode: 35101/50100 (70.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0260s / 2488.2131 s
agent0:                 episode reward: 0.2263,                 loss: 0.4085
agent1:                 episode reward: -0.2263,                 loss: nan
Episode: 35121/50100 (70.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0048s / 2490.2180 s
agent0:                 episode reward: -0.3741,                 loss: 0.4390
agent1:                 episode reward: 0.3741,                 loss: nan
Episode: 35141/50100 (70.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9709s / 2492.1888 s
agent0:                 episode reward: -0.1360,                 loss: 0.4372
agent1:                 episode reward: 0.1360,                 loss: nan
Episode: 35161/50100 (70.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6965s / 2493.8854 s
agent0:                 episode reward: 0.5565,                 loss: 0.4374
agent1:                 episode reward: -0.5565,                 loss: 0.4485
Score delta: 2.1544982112494666, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/34940_0.
Episode: 35181/50100 (70.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3312s / 2495.2166 s
agent0:                 episode reward: -0.4028,                 loss: nan
agent1:                 episode reward: 0.4028,                 loss: 0.4456
Episode: 35201/50100 (70.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3218s / 2496.5384 s
agent0:                 episode reward: -0.3086,                 loss: nan
agent1:                 episode reward: 0.3086,                 loss: 0.4446
Episode: 35221/50100 (70.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3249s / 2497.8633 s
agent0:                 episode reward: -0.5975,                 loss: nan
agent1:                 episode reward: 0.5975,                 loss: 0.4440
Episode: 35241/50100 (70.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3329s / 2499.1962 s
agent0:                 episode reward: 0.1072,                 loss: nan
agent1:                 episode reward: -0.1072,                 loss: 0.4445
Episode: 35261/50100 (70.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4256s / 2500.6218 s
agent0:                 episode reward: -0.4493,                 loss: nan
agent1:                 episode reward: 0.4493,                 loss: 0.4444
Episode: 35281/50100 (70.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3707s / 2501.9925 s
agent0:                 episode reward: -0.2326,                 loss: nan
agent1:                 episode reward: 0.2326,                 loss: 0.4446
Episode: 35301/50100 (70.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3793s / 2503.3719 s
agent0:                 episode reward: -0.0946,                 loss: nan
agent1:                 episode reward: 0.0946,                 loss: 0.4439
Episode: 35321/50100 (70.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7251s / 2505.0969 s
agent0:                 episode reward: -0.5320,                 loss: 0.3615
agent1:                 episode reward: 0.5320,                 loss: 0.4435
Score delta: 2.0178761761181176, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/35100_1.
Episode: 35341/50100 (70.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9729s / 2507.0699 s
agent0:                 episode reward: -0.6003,                 loss: 0.3610
agent1:                 episode reward: 0.6003,                 loss: nan
Episode: 35361/50100 (70.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9684s / 2509.0383 s
agent0:                 episode reward: 0.0354,                 loss: 0.3573
agent1:                 episode reward: -0.0354,                 loss: nan
Episode: 35381/50100 (70.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9696s / 2511.0079 s
agent0:                 episode reward: -0.1443,                 loss: 0.3560
agent1:                 episode reward: 0.1443,                 loss: nan
Episode: 35401/50100 (70.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9678s / 2512.9757 s
agent0:                 episode reward: -0.0845,                 loss: 0.3578
agent1:                 episode reward: 0.0845,                 loss: nan
Episode: 35421/50100 (70.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9875s / 2514.9632 s
agent0:                 episode reward: -0.1542,                 loss: 0.3545
agent1:                 episode reward: 0.1542,                 loss: nan
Episode: 35441/50100 (70.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9983s / 2516.9615 s
agent0:                 episode reward: 0.1167,                 loss: 0.3747
agent1:                 episode reward: -0.1167,                 loss: nan
Episode: 35461/50100 (70.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9919s / 2518.9534 s
agent0:                 episode reward: 0.4237,                 loss: 0.3838
agent1:                 episode reward: -0.4237,                 loss: nan
Episode: 35481/50100 (70.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9790s / 2520.9324 s
agent0:                 episode reward: -0.3456,                 loss: 0.3832
agent1:                 episode reward: 0.3456,                 loss: nan
Episode: 35501/50100 (70.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9908s / 2522.9232 s
agent0:                 episode reward: 0.2023,                 loss: 0.3821
agent1:                 episode reward: -0.2023,                 loss: nan
Episode: 35521/50100 (70.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9700s / 2524.8933 s
agent0:                 episode reward: 0.0436,                 loss: 0.3811
agent1:                 episode reward: -0.0436,                 loss: nan
Episode: 35541/50100 (70.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9596s / 2526.8528 s
agent0:                 episode reward: 0.5649,                 loss: 0.3847
agent1:                 episode reward: -0.5649,                 loss: 0.4615
Score delta: 2.119648631367196, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/35328_0.
Episode: 35561/50100 (70.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3522s / 2528.2050 s
agent0:                 episode reward: -0.0381,                 loss: nan
agent1:                 episode reward: 0.0381,                 loss: 0.4571
Episode: 35581/50100 (71.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3524s / 2529.5574 s
agent0:                 episode reward: -0.0783,                 loss: nan
agent1:                 episode reward: 0.0783,                 loss: 0.4554
Episode: 35601/50100 (71.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3587s / 2530.9160 s
agent0:                 episode reward: -0.4441,                 loss: nan
agent1:                 episode reward: 0.4441,                 loss: 0.4556
Episode: 35621/50100 (71.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3514s / 2532.2675 s
agent0:                 episode reward: -0.2150,                 loss: nan
agent1:                 episode reward: 0.2150,                 loss: 0.4532
Episode: 35641/50100 (71.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5798s / 2533.8473 s
agent0:                 episode reward: -0.1861,                 loss: 0.3449
agent1:                 episode reward: 0.1861,                 loss: 0.4531
Score delta: 2.0947886916034393, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/35422_1.
Episode: 35661/50100 (71.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9842s / 2535.8315 s
agent0:                 episode reward: 0.0031,                 loss: 0.3445
agent1:                 episode reward: -0.0031,                 loss: nan
Episode: 35681/50100 (71.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9848s / 2537.8163 s
agent0:                 episode reward: -0.2788,                 loss: 0.3431
agent1:                 episode reward: 0.2788,                 loss: nan
Episode: 35701/50100 (71.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0029s / 2539.8192 s
agent0:                 episode reward: -0.2159,                 loss: 0.3807
agent1:                 episode reward: 0.2159,                 loss: nan
Episode: 35721/50100 (71.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9950s / 2541.8142 s
agent0:                 episode reward: -0.0486,                 loss: 0.4037
agent1:                 episode reward: 0.0486,                 loss: nan
Episode: 35741/50100 (71.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9934s / 2543.8076 s
agent0:                 episode reward: -0.2279,                 loss: 0.4002
agent1:                 episode reward: 0.2279,                 loss: nan
Episode: 35761/50100 (71.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0422s / 2545.8498 s
agent0:                 episode reward: -0.2752,                 loss: 0.3995
agent1:                 episode reward: 0.2752,                 loss: nan
Episode: 35781/50100 (71.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9879s / 2547.8377 s
agent0:                 episode reward: -0.2293,                 loss: 0.4018
agent1:                 episode reward: 0.2293,                 loss: nan
Episode: 35801/50100 (71.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9881s / 2549.8258 s
agent0:                 episode reward: 0.0813,                 loss: 0.4037
agent1:                 episode reward: -0.0813,                 loss: nan
Episode: 35821/50100 (71.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0604s / 2551.8862 s
agent0:                 episode reward: -0.3913,                 loss: 0.4032
agent1:                 episode reward: 0.3913,                 loss: nan
Episode: 35841/50100 (71.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9953s / 2553.8815 s
agent0:                 episode reward: -0.5725,                 loss: 0.4013
agent1:                 episode reward: 0.5725,                 loss: nan
Episode: 35861/50100 (71.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9888s / 2555.8703 s
agent0:                 episode reward: -0.1204,                 loss: 0.4056
agent1:                 episode reward: 0.1204,                 loss: nan
Episode: 35881/50100 (71.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9845s / 2557.8548 s
agent0:                 episode reward: 0.6059,                 loss: 0.4149
agent1:                 episode reward: -0.6059,                 loss: nan
Episode: 35901/50100 (71.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9793s / 2559.8341 s
agent0:                 episode reward: -0.2142,                 loss: 0.4132
agent1:                 episode reward: 0.2142,                 loss: nan
Episode: 35921/50100 (71.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9769s / 2561.8110 s
agent0:                 episode reward: -0.1370,                 loss: 0.4137
agent1:                 episode reward: 0.1370,                 loss: nan
Episode: 35941/50100 (71.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8805s / 2563.6914 s
agent0:                 episode reward: 0.4026,                 loss: 0.4129
agent1:                 episode reward: -0.4026,                 loss: 0.4505
Score delta: 2.131742807636142, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/35725_0.
Episode: 35961/50100 (71.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3607s / 2565.0521 s
agent0:                 episode reward: 0.1476,                 loss: nan
agent1:                 episode reward: -0.1476,                 loss: 0.4500
Episode: 35981/50100 (71.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3554s / 2566.4075 s
agent0:                 episode reward: -0.3812,                 loss: nan
agent1:                 episode reward: 0.3812,                 loss: 0.4511
Episode: 36001/50100 (71.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3613s / 2567.7689 s
agent0:                 episode reward: -0.6020,                 loss: nan
agent1:                 episode reward: 0.6020,                 loss: 0.4508
Episode: 36021/50100 (71.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3814s / 2569.1503 s
agent0:                 episode reward: -0.6587,                 loss: nan
agent1:                 episode reward: 0.6587,                 loss: 0.4510
Score delta: 2.2251171738884463, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/35809_1.
Episode: 36041/50100 (71.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9932s / 2571.1435 s
agent0:                 episode reward: -0.1844,                 loss: 0.3691
agent1:                 episode reward: 0.1844,                 loss: nan
Episode: 36061/50100 (71.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0419s / 2573.1854 s
agent0:                 episode reward: 0.1209,                 loss: 0.3653
agent1:                 episode reward: -0.1209,                 loss: nan
Episode: 36081/50100 (72.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9748s / 2575.1602 s
agent0:                 episode reward: 0.1239,                 loss: 0.3675
agent1:                 episode reward: -0.1239,                 loss: nan
Episode: 36101/50100 (72.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0067s / 2577.1669 s
agent0:                 episode reward: 0.5030,                 loss: 0.3689
agent1:                 episode reward: -0.5030,                 loss: nan
Episode: 36121/50100 (72.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9878s / 2579.1547 s
agent0:                 episode reward: 0.2639,                 loss: 0.3337
agent1:                 episode reward: -0.2639,                 loss: nan
Episode: 36141/50100 (72.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9949s / 2581.1496 s
agent0:                 episode reward: 0.3425,                 loss: 0.3076
agent1:                 episode reward: -0.3425,                 loss: nan
Episode: 36161/50100 (72.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6132s / 2582.7628 s
agent0:                 episode reward: -0.1392,                 loss: 0.3026
agent1:                 episode reward: 0.1392,                 loss: 0.4525
Score delta: 2.0599052675542766, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/35936_0.
Episode: 36181/50100 (72.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3741s / 2584.1369 s
agent0:                 episode reward: -0.0891,                 loss: nan
agent1:                 episode reward: 0.0891,                 loss: 0.4528
Episode: 36201/50100 (72.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3598s / 2585.4967 s
agent0:                 episode reward: -0.2554,                 loss: nan
agent1:                 episode reward: 0.2554,                 loss: 0.4523
Episode: 36221/50100 (72.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3739s / 2586.8706 s
agent0:                 episode reward: -0.3468,                 loss: nan
agent1:                 episode reward: 0.3468,                 loss: 0.4525
Episode: 36241/50100 (72.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3760s / 2588.2466 s
agent0:                 episode reward: 0.1414,                 loss: nan
agent1:                 episode reward: -0.1414,                 loss: 0.4517
Episode: 36261/50100 (72.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5566s / 2589.8032 s
agent0:                 episode reward: -0.8169,                 loss: 0.4150
agent1:                 episode reward: 0.8169,                 loss: 0.4511
Score delta: 2.3013167828535583, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/36042_1.
Episode: 36281/50100 (72.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0014s / 2591.8046 s
agent0:                 episode reward: -0.3557,                 loss: 0.4101
agent1:                 episode reward: 0.3557,                 loss: nan
Episode: 36301/50100 (72.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0052s / 2593.8098 s
agent0:                 episode reward: -0.0017,                 loss: 0.4105
agent1:                 episode reward: 0.0017,                 loss: nan
Episode: 36321/50100 (72.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0357s / 2595.8455 s
agent0:                 episode reward: -0.3439,                 loss: 0.4102
agent1:                 episode reward: 0.3439,                 loss: nan
Episode: 36341/50100 (72.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0492s / 2597.8947 s
agent0:                 episode reward: 0.1345,                 loss: 0.4094
agent1:                 episode reward: -0.1345,                 loss: nan
Episode: 36361/50100 (72.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9905s / 2599.8851 s
agent0:                 episode reward: 0.0902,                 loss: 0.4089
agent1:                 episode reward: -0.0902,                 loss: nan
Episode: 36381/50100 (72.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0113s / 2601.8964 s
agent0:                 episode reward: 0.1008,                 loss: 0.4104
agent1:                 episode reward: -0.1008,                 loss: nan
Episode: 36401/50100 (72.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0025s / 2603.8989 s
agent0:                 episode reward: -0.0943,                 loss: 0.4118
agent1:                 episode reward: 0.0943,                 loss: nan
Episode: 36421/50100 (72.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0050s / 2605.9040 s
agent0:                 episode reward: -0.0816,                 loss: 0.4103
agent1:                 episode reward: 0.0816,                 loss: nan
Episode: 36441/50100 (72.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0107s / 2607.9147 s
agent0:                 episode reward: -0.2103,                 loss: 0.4110
agent1:                 episode reward: 0.2103,                 loss: nan
Episode: 36461/50100 (72.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9906s / 2609.9052 s
agent0:                 episode reward: -0.3833,                 loss: 0.4111
agent1:                 episode reward: 0.3833,                 loss: nan
Episode: 36481/50100 (72.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0087s / 2611.9139 s
agent0:                 episode reward: 0.4755,                 loss: 0.4098
agent1:                 episode reward: -0.4755,                 loss: nan
Episode: 36501/50100 (72.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0435s / 2613.9574 s
agent0:                 episode reward: -0.7408,                 loss: 0.4122
agent1:                 episode reward: 0.7408,                 loss: nan
Episode: 36521/50100 (72.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0211s / 2615.9785 s
agent0:                 episode reward: -0.0551,                 loss: 0.4099
agent1:                 episode reward: 0.0551,                 loss: nan
Episode: 36541/50100 (72.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9979s / 2617.9764 s
agent0:                 episode reward: 0.1243,                 loss: 0.4112
agent1:                 episode reward: -0.1243,                 loss: nan
Episode: 36561/50100 (72.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0077s / 2619.9842 s
agent0:                 episode reward: -0.2759,                 loss: 0.4084
agent1:                 episode reward: 0.2759,                 loss: nan
Episode: 36581/50100 (73.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9852s / 2621.9694 s
agent0:                 episode reward: -0.1668,                 loss: 0.4038
agent1:                 episode reward: 0.1668,                 loss: nan
Episode: 36601/50100 (73.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0241s / 2623.9935 s
agent0:                 episode reward: 0.2308,                 loss: 0.4020
agent1:                 episode reward: -0.2308,                 loss: nan
Episode: 36621/50100 (73.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0129s / 2626.0064 s
agent0:                 episode reward: 0.1390,                 loss: 0.4008
agent1:                 episode reward: -0.1390,                 loss: nan
Episode: 36641/50100 (73.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0667s / 2628.0731 s
agent0:                 episode reward: -0.1617,                 loss: 0.4009
agent1:                 episode reward: 0.1617,                 loss: nan
Episode: 36661/50100 (73.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0282s / 2630.1013 s
agent0:                 episode reward: 0.1243,                 loss: 0.3993
agent1:                 episode reward: -0.1243,                 loss: nan
Episode: 36681/50100 (73.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0753s / 2632.1766 s
agent0:                 episode reward: -0.6635,                 loss: 0.4008
agent1:                 episode reward: 0.6635,                 loss: nan
Episode: 36701/50100 (73.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9838s / 2634.1604 s
agent0:                 episode reward: -0.0077,                 loss: 0.4017
agent1:                 episode reward: 0.0077,                 loss: nan
Episode: 36721/50100 (73.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9841s / 2636.1445 s
agent0:                 episode reward: -0.0742,                 loss: 0.3819
agent1:                 episode reward: 0.0742,                 loss: nan
Episode: 36741/50100 (73.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0033s / 2638.1477 s
agent0:                 episode reward: -0.0339,                 loss: 0.3484
agent1:                 episode reward: 0.0339,                 loss: nan
Episode: 36761/50100 (73.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6101s / 2639.7578 s
agent0:                 episode reward: -0.3545,                 loss: 0.3441
agent1:                 episode reward: 0.3545,                 loss: 0.4567
Score delta: 2.192541672940847, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/36536_0.
Episode: 36781/50100 (73.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3815s / 2641.1393 s
agent0:                 episode reward: -0.4541,                 loss: nan
agent1:                 episode reward: 0.4541,                 loss: 0.4554
Episode: 36801/50100 (73.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3796s / 2642.5189 s
agent0:                 episode reward: -0.3532,                 loss: nan
agent1:                 episode reward: 0.3532,                 loss: 0.4542
Episode: 36821/50100 (73.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3760s / 2643.8949 s
agent0:                 episode reward: 0.1838,                 loss: nan
agent1:                 episode reward: -0.1838,                 loss: 0.4540
Episode: 36841/50100 (73.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3878s / 2645.2826 s
agent0:                 episode reward: 0.0583,                 loss: nan
agent1:                 episode reward: -0.0583,                 loss: 0.4532
Episode: 36861/50100 (73.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4496s / 2646.7322 s
agent0:                 episode reward: -0.0332,                 loss: nan
agent1:                 episode reward: 0.0332,                 loss: 0.4531
Episode: 36881/50100 (73.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4404s / 2648.1725 s
agent0:                 episode reward: -0.2179,                 loss: nan
agent1:                 episode reward: 0.2179,                 loss: 0.4529
Episode: 36901/50100 (73.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4069s / 2649.5795 s
agent0:                 episode reward: -0.3875,                 loss: nan
agent1:                 episode reward: 0.3875,                 loss: 0.4530
Episode: 36921/50100 (73.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8050s / 2651.3844 s
agent0:                 episode reward: -0.1235,                 loss: 0.3301
agent1:                 episode reward: 0.1235,                 loss: 0.4528
Score delta: 2.2599478358407117, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/36696_1.
Episode: 36941/50100 (73.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0064s / 2653.3908 s
agent0:                 episode reward: 0.1609,                 loss: 0.3278
agent1:                 episode reward: -0.1609,                 loss: nan
Episode: 36961/50100 (73.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0611s / 2655.4519 s
agent0:                 episode reward: 0.3537,                 loss: 0.3277
agent1:                 episode reward: -0.3537,                 loss: nan
Episode: 36981/50100 (73.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0289s / 2657.4808 s
agent0:                 episode reward: -0.1688,                 loss: 0.3279
agent1:                 episode reward: 0.1688,                 loss: nan
Episode: 37001/50100 (73.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0289s / 2659.5097 s
agent0:                 episode reward: -0.2279,                 loss: 0.3264
agent1:                 episode reward: 0.2279,                 loss: nan
Episode: 37021/50100 (73.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0111s / 2661.5208 s
agent0:                 episode reward: -0.2783,                 loss: 0.3269
agent1:                 episode reward: 0.2783,                 loss: nan
Episode: 37041/50100 (73.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0206s / 2663.5415 s
agent0:                 episode reward: -0.3056,                 loss: 0.3282
agent1:                 episode reward: 0.3056,                 loss: nan
Episode: 37061/50100 (73.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0139s / 2665.5554 s
agent0:                 episode reward: -0.6679,                 loss: 0.3414
agent1:                 episode reward: 0.6679,                 loss: nan
Episode: 37081/50100 (74.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0322s / 2667.5876 s
agent0:                 episode reward: -0.0504,                 loss: 0.3382
agent1:                 episode reward: 0.0504,                 loss: nan
Episode: 37101/50100 (74.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0322s / 2669.6198 s
agent0:                 episode reward: -0.1161,                 loss: 0.3377
agent1:                 episode reward: 0.1161,                 loss: nan
Episode: 37121/50100 (74.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0148s / 2671.6346 s
agent0:                 episode reward: -0.5961,                 loss: 0.3389
agent1:                 episode reward: 0.5961,                 loss: nan
Episode: 37141/50100 (74.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9960s / 2673.6306 s
agent0:                 episode reward: 0.7389,                 loss: 0.3359
agent1:                 episode reward: -0.7389,                 loss: 0.4610
Score delta: 2.019522326067115, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/36928_0.
Episode: 37161/50100 (74.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3929s / 2675.0235 s
agent0:                 episode reward: -0.0485,                 loss: nan
agent1:                 episode reward: 0.0485,                 loss: 0.4562
Episode: 37181/50100 (74.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4006s / 2676.4241 s
agent0:                 episode reward: -0.1628,                 loss: nan
agent1:                 episode reward: 0.1628,                 loss: 0.4539
Episode: 37201/50100 (74.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4133s / 2677.8375 s
agent0:                 episode reward: -0.4099,                 loss: nan
agent1:                 episode reward: 0.4099,                 loss: 0.4526
Episode: 37221/50100 (74.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4056s / 2679.2430 s
agent0:                 episode reward: -0.5977,                 loss: nan
agent1:                 episode reward: 0.5977,                 loss: 0.4518
Episode: 37241/50100 (74.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 1.3997s / 2680.6427 s
agent0:                 episode reward: -0.1965,                 loss: nan
agent1:                 episode reward: 0.1965,                 loss: 0.4520
Episode: 37261/50100 (74.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4340s / 2682.0768 s
agent0:                 episode reward: -0.6247,                 loss: nan
agent1:                 episode reward: 0.6247,                 loss: 0.4516
Episode: 37281/50100 (74.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9636s / 2684.0404 s
agent0:                 episode reward: -0.5906,                 loss: 0.4134
agent1:                 episode reward: 0.5906,                 loss: 0.4507
Score delta: 2.373301703007411, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/37053_1.
Episode: 37301/50100 (74.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0046s / 2686.0450 s
agent0:                 episode reward: -0.2374,                 loss: 0.4125
agent1:                 episode reward: 0.2374,                 loss: nan
Episode: 37321/50100 (74.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0022s / 2688.0472 s
agent0:                 episode reward: -0.2816,                 loss: 0.4121
agent1:                 episode reward: 0.2816,                 loss: nan
Episode: 37341/50100 (74.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0395s / 2690.0867 s
agent0:                 episode reward: 0.4694,                 loss: 0.4217
agent1:                 episode reward: -0.4694,                 loss: nan
Episode: 37361/50100 (74.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0627s / 2692.1494 s
agent0:                 episode reward: -0.1187,                 loss: 0.4240
agent1:                 episode reward: 0.1187,                 loss: nan
Episode: 37381/50100 (74.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0111s / 2694.1605 s
agent0:                 episode reward: -0.3299,                 loss: 0.4229
agent1:                 episode reward: 0.3299,                 loss: nan
Episode: 37401/50100 (74.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0430s / 2696.2035 s
agent0:                 episode reward: 0.1675,                 loss: 0.4234
agent1:                 episode reward: -0.1675,                 loss: nan
Episode: 37421/50100 (74.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5713s / 2697.7748 s
agent0:                 episode reward: -0.0180,                 loss: 0.4256
agent1:                 episode reward: 0.0180,                 loss: 0.4499
Score delta: 2.275934137463942, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/37192_0.
Episode: 37441/50100 (74.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4195s / 2699.1943 s
agent0:                 episode reward: -0.5473,                 loss: nan
agent1:                 episode reward: 0.5473,                 loss: 0.4494
Episode: 37461/50100 (74.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4129s / 2700.6072 s
agent0:                 episode reward: -0.4890,                 loss: nan
agent1:                 episode reward: 0.4890,                 loss: 0.4506
Episode: 37481/50100 (74.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4256s / 2702.0328 s
agent0:                 episode reward: -0.2454,                 loss: nan
agent1:                 episode reward: 0.2454,                 loss: 0.4523
Episode: 37501/50100 (74.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4121s / 2703.4449 s
agent0:                 episode reward: -0.8727,                 loss: nan
agent1:                 episode reward: 0.8727,                 loss: 0.4523
Episode: 37521/50100 (74.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4277s / 2704.8726 s
agent0:                 episode reward: -0.2577,                 loss: nan
agent1:                 episode reward: 0.2577,                 loss: 0.4514
Episode: 37541/50100 (74.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0387s / 2706.9113 s
agent0:                 episode reward: -0.2188,                 loss: 0.4306
agent1:                 episode reward: 0.2188,                 loss: 0.4499
Score delta: 2.1441274938818347, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/37311_1.
Episode: 37561/50100 (74.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0295s / 2708.9408 s
agent0:                 episode reward: 0.3957,                 loss: 0.4289
agent1:                 episode reward: -0.3957,                 loss: nan
Episode: 37581/50100 (75.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0162s / 2710.9570 s
agent0:                 episode reward: -0.5283,                 loss: 0.4285
agent1:                 episode reward: 0.5283,                 loss: nan
Episode: 37601/50100 (75.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9951s / 2712.9522 s
agent0:                 episode reward: -0.0569,                 loss: 0.4286
agent1:                 episode reward: 0.0569,                 loss: nan
Episode: 37621/50100 (75.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0423s / 2714.9945 s
agent0:                 episode reward: -0.0021,                 loss: 0.4298
agent1:                 episode reward: 0.0021,                 loss: nan
Episode: 37641/50100 (75.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9995s / 2716.9939 s
agent0:                 episode reward: -0.3999,                 loss: 0.4295
agent1:                 episode reward: 0.3999,                 loss: nan
Episode: 37661/50100 (75.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0172s / 2719.0111 s
agent0:                 episode reward: -0.2413,                 loss: 0.4280
agent1:                 episode reward: 0.2413,                 loss: nan
Episode: 37681/50100 (75.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0017s / 2721.0128 s
agent0:                 episode reward: 0.0733,                 loss: 0.4305
agent1:                 episode reward: -0.0733,                 loss: nan
Episode: 37701/50100 (75.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9931s / 2723.0059 s
agent0:                 episode reward: 0.3770,                 loss: 0.4280
agent1:                 episode reward: -0.3770,                 loss: nan
Episode: 37721/50100 (75.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9977s / 2725.0036 s
agent0:                 episode reward: 0.3538,                 loss: 0.4291
agent1:                 episode reward: -0.3538,                 loss: nan
Episode: 37741/50100 (75.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0234s / 2727.0270 s
agent0:                 episode reward: -0.5801,                 loss: 0.4271
agent1:                 episode reward: 0.5801,                 loss: nan
Episode: 37761/50100 (75.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0127s / 2729.0396 s
agent0:                 episode reward: 0.0323,                 loss: 0.4292
agent1:                 episode reward: -0.0323,                 loss: nan
Episode: 37781/50100 (75.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9333s / 2730.9730 s
agent0:                 episode reward: 0.1836,                 loss: 0.4275
agent1:                 episode reward: -0.1836,                 loss: 0.4521
Score delta: 2.294434969480336, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/37563_0.
Episode: 37801/50100 (75.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4171s / 2732.3900 s
agent0:                 episode reward: 0.1978,                 loss: nan
agent1:                 episode reward: -0.1978,                 loss: 0.4511
Episode: 37821/50100 (75.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4249s / 2733.8149 s
agent0:                 episode reward: -0.4631,                 loss: nan
agent1:                 episode reward: 0.4631,                 loss: 0.4505
Episode: 37841/50100 (75.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4426s / 2735.2575 s
agent0:                 episode reward: -0.2252,                 loss: nan
agent1:                 episode reward: 0.2252,                 loss: 0.4495
Episode: 37861/50100 (75.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5003s / 2736.7578 s
agent0:                 episode reward: -0.2519,                 loss: nan
agent1:                 episode reward: 0.2519,                 loss: 0.4501
Episode: 37881/50100 (75.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8481s / 2738.6059 s
agent0:                 episode reward: -0.6105,                 loss: 0.3816
agent1:                 episode reward: 0.6105,                 loss: 0.4510
Score delta: 2.5086728841532837, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/37656_1.
Episode: 37901/50100 (75.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9938s / 2740.5997 s
agent0:                 episode reward: 0.4444,                 loss: 0.4249
agent1:                 episode reward: -0.4444,                 loss: nan
Episode: 37921/50100 (75.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9871s / 2742.5868 s
agent0:                 episode reward: -0.2369,                 loss: 0.4225
agent1:                 episode reward: 0.2369,                 loss: nan
Episode: 37941/50100 (75.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9920s / 2744.5788 s
agent0:                 episode reward: -0.3782,                 loss: 0.4221
agent1:                 episode reward: 0.3782,                 loss: nan
Episode: 37961/50100 (75.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0889s / 2746.6677 s
agent0:                 episode reward: 0.1619,                 loss: 0.4239
agent1:                 episode reward: -0.1619,                 loss: nan
Episode: 37981/50100 (75.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9875s / 2748.6553 s
agent0:                 episode reward: 0.0524,                 loss: 0.4242
agent1:                 episode reward: -0.0524,                 loss: nan
Episode: 38001/50100 (75.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0068s / 2750.6621 s
agent0:                 episode reward: 0.1775,                 loss: 0.4233
agent1:                 episode reward: -0.1775,                 loss: nan
Episode: 38021/50100 (75.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0272s / 2752.6893 s
agent0:                 episode reward: -0.3010,                 loss: 0.4222
agent1:                 episode reward: 0.3010,                 loss: nan
Episode: 38041/50100 (75.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0256s / 2754.7149 s
agent0:                 episode reward: -0.2518,                 loss: 0.4231
agent1:                 episode reward: 0.2518,                 loss: nan
Episode: 38061/50100 (75.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0273s / 2756.7422 s
agent0:                 episode reward: 0.3915,                 loss: 0.4427
agent1:                 episode reward: -0.3915,                 loss: nan
Episode: 38081/50100 (76.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0600s / 2758.8021 s
agent0:                 episode reward: -0.0575,                 loss: 0.4427
agent1:                 episode reward: 0.0575,                 loss: nan
Episode: 38101/50100 (76.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0070s / 2760.8091 s
agent0:                 episode reward: -0.3045,                 loss: 0.4423
agent1:                 episode reward: 0.3045,                 loss: nan
Episode: 38121/50100 (76.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0036s / 2762.8127 s
agent0:                 episode reward: 0.1271,                 loss: 0.4424
agent1:                 episode reward: -0.1271,                 loss: nan
Episode: 38141/50100 (76.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0242s / 2764.8369 s
agent0:                 episode reward: -0.0086,                 loss: 0.4437
agent1:                 episode reward: 0.0086,                 loss: nan
Episode: 38161/50100 (76.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0067s / 2766.8436 s
agent0:                 episode reward: 0.1357,                 loss: 0.4416
agent1:                 episode reward: -0.1357,                 loss: nan
Episode: 38181/50100 (76.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9992s / 2768.8428 s
agent0:                 episode reward: -0.3396,                 loss: 0.4416
agent1:                 episode reward: 0.3396,                 loss: nan
Episode: 38201/50100 (76.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0122s / 2770.8550 s
agent0:                 episode reward: 0.5081,                 loss: 0.4433
agent1:                 episode reward: -0.5081,                 loss: nan
Episode: 38221/50100 (76.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0002s / 2772.8552 s
agent0:                 episode reward: 0.3373,                 loss: 0.4395
agent1:                 episode reward: -0.3373,                 loss: nan
Episode: 38241/50100 (76.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9946s / 2774.8497 s
agent0:                 episode reward: 0.0672,                 loss: 0.4348
agent1:                 episode reward: -0.0672,                 loss: nan
Episode: 38261/50100 (76.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0059s / 2776.8556 s
agent0:                 episode reward: -0.0796,                 loss: 0.4344
agent1:                 episode reward: 0.0796,                 loss: nan
Episode: 38281/50100 (76.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0074s / 2778.8630 s
agent0:                 episode reward: 0.5852,                 loss: 0.4351
agent1:                 episode reward: -0.5852,                 loss: nan
Episode: 38301/50100 (76.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0050s / 2780.8681 s
agent0:                 episode reward: -0.3004,                 loss: 0.4351
agent1:                 episode reward: 0.3004,                 loss: nan
Episode: 38321/50100 (76.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9913s / 2782.8594 s
agent0:                 episode reward: 0.3705,                 loss: 0.4337
agent1:                 episode reward: -0.3705,                 loss: nan
Episode: 38341/50100 (76.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0045s / 2784.8639 s
agent0:                 episode reward: 0.1167,                 loss: 0.4355
agent1:                 episode reward: -0.1167,                 loss: nan
Episode: 38361/50100 (76.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9984s / 2786.8623 s
agent0:                 episode reward: -0.0531,                 loss: 0.4353
agent1:                 episode reward: 0.0531,                 loss: nan
Episode: 38381/50100 (76.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0159s / 2788.8782 s
agent0:                 episode reward: 0.5678,                 loss: 0.4288
agent1:                 episode reward: -0.5678,                 loss: nan
Episode: 38401/50100 (76.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0361s / 2790.9142 s
agent0:                 episode reward: 0.2009,                 loss: 0.4043
agent1:                 episode reward: -0.2009,                 loss: nan
Episode: 38421/50100 (76.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0826s / 2792.9969 s
agent0:                 episode reward: 0.0335,                 loss: 0.4059
agent1:                 episode reward: -0.0335,                 loss: nan
Episode: 38441/50100 (76.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0002s / 2794.9971 s
agent0:                 episode reward: 0.2054,                 loss: 0.4060
agent1:                 episode reward: -0.2054,                 loss: nan
Episode: 38461/50100 (76.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0053s / 2797.0025 s
agent0:                 episode reward: -0.0189,                 loss: 0.4017
agent1:                 episode reward: 0.0189,                 loss: nan
Episode: 38481/50100 (76.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0144s / 2799.0168 s
agent0:                 episode reward: -0.0091,                 loss: 0.4046
agent1:                 episode reward: 0.0091,                 loss: nan
Episode: 38501/50100 (76.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5430s / 2800.5598 s
agent0:                 episode reward: 0.0714,                 loss: 0.4072
agent1:                 episode reward: -0.0714,                 loss: 0.4555
Score delta: 2.0212058992400204, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/38272_0.
Episode: 38521/50100 (76.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5116s / 2802.0714 s
agent0:                 episode reward: -0.4246,                 loss: nan
agent1:                 episode reward: 0.4246,                 loss: 0.4535
Episode: 38541/50100 (76.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4185s / 2803.4899 s
agent0:                 episode reward: -0.0440,                 loss: nan
agent1:                 episode reward: 0.0440,                 loss: 0.4529
Episode: 38561/50100 (76.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4316s / 2804.9216 s
agent0:                 episode reward: -0.3613,                 loss: nan
agent1:                 episode reward: 0.3613,                 loss: 0.4526
Episode: 38581/50100 (77.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4484s / 2806.3700 s
agent0:                 episode reward: 0.1546,                 loss: nan
agent1:                 episode reward: -0.1546,                 loss: 0.4525
Episode: 38601/50100 (77.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8281s / 2808.1981 s
agent0:                 episode reward: -0.8434,                 loss: 0.3801
agent1:                 episode reward: 0.8434,                 loss: 0.4517
Score delta: 2.2568062379105114, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/38376_1.
Episode: 38621/50100 (77.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0129s / 2810.2110 s
agent0:                 episode reward: 0.0820,                 loss: 0.3779
agent1:                 episode reward: -0.0820,                 loss: nan
Episode: 38641/50100 (77.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0289s / 2812.2399 s
agent0:                 episode reward: 0.0906,                 loss: 0.3789
agent1:                 episode reward: -0.0906,                 loss: nan
Episode: 38661/50100 (77.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0220s / 2814.2619 s
agent0:                 episode reward: 0.3871,                 loss: 0.4094
agent1:                 episode reward: -0.3871,                 loss: nan
Episode: 38681/50100 (77.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0345s / 2816.2964 s
agent0:                 episode reward: -0.5893,                 loss: 0.4211
agent1:                 episode reward: 0.5893,                 loss: nan
Episode: 38701/50100 (77.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9415s / 2818.2380 s
agent0:                 episode reward: 0.3552,                 loss: 0.4222
agent1:                 episode reward: -0.3552,                 loss: 0.4434
Score delta: 2.329322441866581, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/38486_0.
Episode: 38721/50100 (77.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4479s / 2819.6859 s
agent0:                 episode reward: -0.0286,                 loss: nan
agent1:                 episode reward: 0.0286,                 loss: 0.4440
Episode: 38741/50100 (77.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4289s / 2821.1148 s
agent0:                 episode reward: 0.3142,                 loss: nan
agent1:                 episode reward: -0.3142,                 loss: 0.4427
Episode: 38761/50100 (77.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4497s / 2822.5645 s
agent0:                 episode reward: -0.8950,                 loss: nan
agent1:                 episode reward: 0.8950,                 loss: 0.4421
Episode: 38781/50100 (77.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4425s / 2824.0070 s
agent0:                 episode reward: -0.2143,                 loss: nan
agent1:                 episode reward: 0.2143,                 loss: 0.4451
Episode: 38801/50100 (77.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5608s / 2825.5678 s
agent0:                 episode reward: -0.5598,                 loss: 0.4118
agent1:                 episode reward: 0.5598,                 loss: 0.4466
Score delta: 2.014466090056607, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/38585_1.
Episode: 38821/50100 (77.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9973s / 2827.5651 s
agent0:                 episode reward: 0.2484,                 loss: 0.4084
agent1:                 episode reward: -0.2484,                 loss: nan
Episode: 38841/50100 (77.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1122s / 2829.6773 s
agent0:                 episode reward: 0.2636,                 loss: 0.4101
agent1:                 episode reward: -0.2636,                 loss: nan
Episode: 38861/50100 (77.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0249s / 2831.7022 s
agent0:                 episode reward: 0.0924,                 loss: 0.4083
agent1:                 episode reward: -0.0924,                 loss: nan
Episode: 38881/50100 (77.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0269s / 2833.7291 s
agent0:                 episode reward: -0.4598,                 loss: 0.4087
agent1:                 episode reward: 0.4598,                 loss: nan
Episode: 38901/50100 (77.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0292s / 2835.7584 s
agent0:                 episode reward: -0.2830,                 loss: 0.4086
agent1:                 episode reward: 0.2830,                 loss: nan
Episode: 38921/50100 (77.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0276s / 2837.7860 s
agent0:                 episode reward: 0.3891,                 loss: 0.4063
agent1:                 episode reward: -0.3891,                 loss: nan
Episode: 38941/50100 (77.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0798s / 2839.8658 s
agent0:                 episode reward: 0.1111,                 loss: 0.3934
agent1:                 episode reward: -0.1111,                 loss: nan
Episode: 38961/50100 (77.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0464s / 2841.9121 s
agent0:                 episode reward: -0.2733,                 loss: 0.3888
agent1:                 episode reward: 0.2733,                 loss: nan
Episode: 38981/50100 (77.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0250s / 2843.9371 s
agent0:                 episode reward: -0.2073,                 loss: 0.3877
agent1:                 episode reward: 0.2073,                 loss: nan
Episode: 39001/50100 (77.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0190s / 2845.9561 s
agent0:                 episode reward: 0.2641,                 loss: 0.3884
agent1:                 episode reward: -0.2641,                 loss: nan
Episode: 39021/50100 (77.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8238s / 2847.7799 s
agent0:                 episode reward: 0.6169,                 loss: 0.3868
agent1:                 episode reward: -0.6169,                 loss: 0.4527
Score delta: 2.290294783121042, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/38802_0.
Episode: 39041/50100 (77.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4441s / 2849.2240 s
agent0:                 episode reward: -0.4948,                 loss: nan
agent1:                 episode reward: 0.4948,                 loss: 0.4514
Episode: 39061/50100 (77.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4592s / 2850.6832 s
agent0:                 episode reward: 0.2083,                 loss: nan
agent1:                 episode reward: -0.2083,                 loss: 0.4512
Episode: 39081/50100 (78.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4872s / 2852.1704 s
agent0:                 episode reward: -0.3865,                 loss: nan
agent1:                 episode reward: 0.3865,                 loss: 0.4510
Episode: 39101/50100 (78.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5526s / 2853.7229 s
agent0:                 episode reward: -0.1411,                 loss: nan
agent1:                 episode reward: 0.1411,                 loss: 0.4511
Episode: 39121/50100 (78.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9089s / 2855.6318 s
agent0:                 episode reward: -0.2708,                 loss: 0.4345
agent1:                 episode reward: 0.2708,                 loss: 0.4506
Score delta: 2.4832607652673344, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/38893_1.
Episode: 39141/50100 (78.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0141s / 2857.6459 s
agent0:                 episode reward: -0.2076,                 loss: 0.4354
agent1:                 episode reward: 0.2076,                 loss: nan
Episode: 39161/50100 (78.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0191s / 2859.6651 s
agent0:                 episode reward: -0.0135,                 loss: 0.4348
agent1:                 episode reward: 0.0135,                 loss: nan
Episode: 39181/50100 (78.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0116s / 2861.6767 s
agent0:                 episode reward: 0.1270,                 loss: 0.4340
agent1:                 episode reward: -0.1270,                 loss: nan
Episode: 39201/50100 (78.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0923s / 2863.7690 s
agent0:                 episode reward: -0.4327,                 loss: 0.4319
agent1:                 episode reward: 0.4327,                 loss: nan
Episode: 39221/50100 (78.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0253s / 2865.7943 s
agent0:                 episode reward: -0.2651,                 loss: 0.4307
agent1:                 episode reward: 0.2651,                 loss: nan
Episode: 39241/50100 (78.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0204s / 2867.8147 s
agent0:                 episode reward: -0.1516,                 loss: 0.4302
agent1:                 episode reward: 0.1516,                 loss: nan
Episode: 39261/50100 (78.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0048s / 2869.8195 s
agent0:                 episode reward: -0.4385,                 loss: 0.4299
agent1:                 episode reward: 0.4385,                 loss: nan
Episode: 39281/50100 (78.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0217s / 2871.8412 s
agent0:                 episode reward: -0.0794,                 loss: 0.4314
agent1:                 episode reward: 0.0794,                 loss: nan
Episode: 39301/50100 (78.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0065s / 2873.8477 s
agent0:                 episode reward: 0.0297,                 loss: 0.4300
agent1:                 episode reward: -0.0297,                 loss: nan
Episode: 39321/50100 (78.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0301s / 2875.8778 s
agent0:                 episode reward: 0.2559,                 loss: 0.4296
agent1:                 episode reward: -0.2559,                 loss: nan
Episode: 39341/50100 (78.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9968s / 2877.8746 s
agent0:                 episode reward: -0.4572,                 loss: 0.4265
agent1:                 episode reward: 0.4572,                 loss: nan
Episode: 39361/50100 (78.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0105s / 2879.8851 s
agent0:                 episode reward: -0.3741,                 loss: 0.4024
agent1:                 episode reward: 0.3741,                 loss: nan
Episode: 39381/50100 (78.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0071s / 2881.8922 s
agent0:                 episode reward: -0.1197,                 loss: 0.4027
agent1:                 episode reward: 0.1197,                 loss: nan
Episode: 39401/50100 (78.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9999s / 2883.8920 s
agent0:                 episode reward: 0.0618,                 loss: 0.4011
agent1:                 episode reward: -0.0618,                 loss: nan
Episode: 39421/50100 (78.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0786s / 2885.9707 s
agent0:                 episode reward: 0.2060,                 loss: 0.4007
agent1:                 episode reward: -0.2060,                 loss: nan
Episode: 39441/50100 (78.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9971s / 2887.9677 s
agent0:                 episode reward: -0.0727,                 loss: 0.3983
agent1:                 episode reward: 0.0727,                 loss: nan
Episode: 39461/50100 (78.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0135s / 2889.9812 s
agent0:                 episode reward: -0.2552,                 loss: 0.4001
agent1:                 episode reward: 0.2552,                 loss: nan
Episode: 39481/50100 (78.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0261s / 2892.0073 s
agent0:                 episode reward: -0.0743,                 loss: 0.3993
agent1:                 episode reward: 0.0743,                 loss: nan
Episode: 39501/50100 (78.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0235s / 2894.0308 s
agent0:                 episode reward: 0.4489,                 loss: 0.3999
agent1:                 episode reward: -0.4489,                 loss: nan
Episode: 39521/50100 (78.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0294s / 2896.0602 s
agent0:                 episode reward: 0.2038,                 loss: 0.3601
agent1:                 episode reward: -0.2038,                 loss: nan
Episode: 39541/50100 (78.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6435s / 2897.7037 s
agent0:                 episode reward: -0.0380,                 loss: 0.3504
agent1:                 episode reward: 0.0380,                 loss: 0.4511
Score delta: 2.525788448417622, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/39315_0.
Episode: 39561/50100 (78.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4612s / 2899.1649 s
agent0:                 episode reward: 0.0357,                 loss: nan
agent1:                 episode reward: -0.0357,                 loss: 0.4512
Episode: 39581/50100 (79.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4548s / 2900.6197 s
agent0:                 episode reward: -0.0836,                 loss: nan
agent1:                 episode reward: 0.0836,                 loss: 0.4511
Episode: 39601/50100 (79.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4598s / 2902.0795 s
agent0:                 episode reward: -0.0797,                 loss: nan
agent1:                 episode reward: 0.0797,                 loss: 0.4505
Episode: 39621/50100 (79.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5758s / 2903.6553 s
agent0:                 episode reward: -0.2914,                 loss: 0.4152
agent1:                 episode reward: 0.2914,                 loss: 0.4498
Score delta: 2.057089775681167, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/39405_1.
Episode: 39641/50100 (79.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0201s / 2905.6755 s
agent0:                 episode reward: 0.0331,                 loss: 0.4158
agent1:                 episode reward: -0.0331,                 loss: nan
Episode: 39661/50100 (79.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0107s / 2907.6861 s
agent0:                 episode reward: -0.1981,                 loss: 0.4154
agent1:                 episode reward: 0.1981,                 loss: nan
Episode: 39681/50100 (79.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0300s / 2909.7161 s
agent0:                 episode reward: -0.2300,                 loss: 0.4132
agent1:                 episode reward: 0.2300,                 loss: nan
Episode: 39701/50100 (79.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0162s / 2911.7323 s
agent0:                 episode reward: 0.0748,                 loss: 0.4140
agent1:                 episode reward: -0.0748,                 loss: nan
Episode: 39721/50100 (79.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0447s / 2913.7770 s
agent0:                 episode reward: 0.2605,                 loss: 0.4112
agent1:                 episode reward: -0.2605,                 loss: nan
Episode: 39741/50100 (79.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6588s / 2915.4358 s
agent0:                 episode reward: 0.3379,                 loss: 0.4114
agent1:                 episode reward: -0.3379,                 loss: 0.4578
Score delta: 2.100754920736369, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/39512_0.
Episode: 39761/50100 (79.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4706s / 2916.9064 s
agent0:                 episode reward: -0.6834,                 loss: nan
agent1:                 episode reward: 0.6834,                 loss: 0.4562
Episode: 39781/50100 (79.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4569s / 2918.3633 s
agent0:                 episode reward: -0.0343,                 loss: nan
agent1:                 episode reward: 0.0343,                 loss: 0.4552
Episode: 39801/50100 (79.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4598s / 2919.8231 s
agent0:                 episode reward: -0.5283,                 loss: nan
agent1:                 episode reward: 0.5283,                 loss: 0.4542
Episode: 39821/50100 (79.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4622s / 2921.2854 s
agent0:                 episode reward: -0.2152,                 loss: nan
agent1:                 episode reward: 0.2152,                 loss: 0.4542
Episode: 39841/50100 (79.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6815s / 2922.9669 s
agent0:                 episode reward: -0.7015,                 loss: 0.3447
agent1:                 episode reward: 0.7015,                 loss: 0.4527
Score delta: 2.3315597995884034, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/39621_1.
Episode: 39861/50100 (79.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0157s / 2924.9825 s
agent0:                 episode reward: 0.4812,                 loss: 0.3455
agent1:                 episode reward: -0.4812,                 loss: nan
Episode: 39881/50100 (79.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0145s / 2926.9971 s
agent0:                 episode reward: 0.3248,                 loss: 0.3844
agent1:                 episode reward: -0.3248,                 loss: nan
Episode: 39901/50100 (79.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0225s / 2929.0196 s
agent0:                 episode reward: -0.4558,                 loss: 0.4047
agent1:                 episode reward: 0.4558,                 loss: nan
Episode: 39921/50100 (79.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0063s / 2931.0259 s
agent0:                 episode reward: 0.1712,                 loss: 0.4047
agent1:                 episode reward: -0.1712,                 loss: nan
Episode: 39941/50100 (79.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0202s / 2933.0461 s
agent0:                 episode reward: 0.3072,                 loss: 0.4062
agent1:                 episode reward: -0.3072,                 loss: nan
Episode: 39961/50100 (79.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0023s / 2935.0484 s
agent0:                 episode reward: -0.0648,                 loss: 0.4039
agent1:                 episode reward: 0.0648,                 loss: nan
Episode: 39981/50100 (79.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0015s / 2937.0499 s
agent0:                 episode reward: -0.2310,                 loss: 0.4046
agent1:                 episode reward: 0.2310,                 loss: nan
Episode: 40001/50100 (79.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0236s / 2939.0736 s
agent0:                 episode reward: 0.3593,                 loss: 0.4037
agent1:                 episode reward: -0.3593,                 loss: nan
Episode: 40021/50100 (79.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0430s / 2941.1165 s
agent0:                 episode reward: -0.2664,                 loss: 0.4035
agent1:                 episode reward: 0.2664,                 loss: nan
Episode: 40041/50100 (79.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0824s / 2943.1989 s
agent0:                 episode reward: -0.4942,                 loss: 0.4092
agent1:                 episode reward: 0.4942,                 loss: nan
Episode: 40061/50100 (79.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0203s / 2945.2192 s
agent0:                 episode reward: -0.0763,                 loss: 0.4033
agent1:                 episode reward: 0.0763,                 loss: nan
Episode: 40081/50100 (80.0020%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9178s / 2947.1370 s
agent0:                 episode reward: 0.7267,                 loss: 0.4026
agent1:                 episode reward: -0.7267,                 loss: 0.4553
Score delta: 2.3008952487936014, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/39863_0.
Episode: 40101/50100 (80.0419%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5364s / 2948.6734 s
agent0:                 episode reward: -0.5256,                 loss: nan
agent1:                 episode reward: 0.5256,                 loss: 0.4506
Episode: 40121/50100 (80.0818%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4645s / 2950.1379 s
agent0:                 episode reward: -0.2622,                 loss: nan
agent1:                 episode reward: 0.2622,                 loss: 0.4488
Episode: 40141/50100 (80.1218%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4745s / 2951.6124 s
agent0:                 episode reward: -0.5801,                 loss: nan
agent1:                 episode reward: 0.5801,                 loss: 0.4485
Episode: 40161/50100 (80.1617%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4759s / 2953.0883 s
agent0:                 episode reward: 0.3769,                 loss: nan
agent1:                 episode reward: -0.3769,                 loss: 0.4483
Episode: 40181/50100 (80.2016%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4670s / 2954.5553 s
agent0:                 episode reward: -0.2970,                 loss: nan
agent1:                 episode reward: 0.2970,                 loss: 0.4477
Episode: 40201/50100 (80.2415%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8476s / 2956.4030 s
agent0:                 episode reward: 0.2231,                 loss: 0.4456
agent1:                 episode reward: -0.2231,                 loss: 0.4471
Score delta: 2.231422561029597, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/39975_1.
Episode: 40221/50100 (80.2814%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0360s / 2958.4389 s
agent0:                 episode reward: -0.0470,                 loss: 0.4436
agent1:                 episode reward: 0.0470,                 loss: nan
Episode: 40241/50100 (80.3214%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0185s / 2960.4575 s
agent0:                 episode reward: 0.4275,                 loss: 0.4460
agent1:                 episode reward: -0.4275,                 loss: nan
Episode: 40261/50100 (80.3613%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0174s / 2962.4749 s
agent0:                 episode reward: 0.3000,                 loss: 0.4426
agent1:                 episode reward: -0.3000,                 loss: nan
Episode: 40281/50100 (80.4012%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0900s / 2964.5649 s
agent0:                 episode reward: -0.0021,                 loss: 0.4428
agent1:                 episode reward: 0.0021,                 loss: nan
Episode: 40301/50100 (80.4411%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0350s / 2966.5999 s
agent0:                 episode reward: 0.0943,                 loss: 0.4427
agent1:                 episode reward: -0.0943,                 loss: nan
Episode: 40321/50100 (80.4810%),                 avg. length: 5.0,                last time consumption/overall running time: 2.2008s / 2968.8007 s
agent0:                 episode reward: 0.5418,                 loss: 0.4422
agent1:                 episode reward: -0.5418,                 loss: nan
Episode: 40341/50100 (80.5210%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0158s / 2970.8165 s
agent0:                 episode reward: -0.2162,                 loss: 0.4388
agent1:                 episode reward: 0.2162,                 loss: nan
Episode: 40361/50100 (80.5609%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0178s / 2972.8343 s
agent0:                 episode reward: 0.3219,                 loss: 0.4386
agent1:                 episode reward: -0.3219,                 loss: nan
Episode: 40381/50100 (80.6008%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0481s / 2974.8824 s
agent0:                 episode reward: -0.1002,                 loss: 0.4385
agent1:                 episode reward: 0.1002,                 loss: nan
Episode: 40401/50100 (80.6407%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0216s / 2976.9040 s
agent0:                 episode reward: 0.1628,                 loss: 0.4382
agent1:                 episode reward: -0.1628,                 loss: nan
Episode: 40421/50100 (80.6806%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0223s / 2978.9263 s
agent0:                 episode reward: 0.3919,                 loss: 0.4395
agent1:                 episode reward: -0.3919,                 loss: nan
Episode: 40441/50100 (80.7206%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1091s / 2981.0354 s
agent0:                 episode reward: -0.1072,                 loss: 0.4387
agent1:                 episode reward: 0.1072,                 loss: nan
Episode: 40461/50100 (80.7605%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7408s / 2982.7763 s
agent0:                 episode reward: 0.3831,                 loss: 0.4390
agent1:                 episode reward: -0.3831,                 loss: 0.4549
Score delta: 2.1324066275972546, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/40238_0.
Episode: 40481/50100 (80.8004%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4772s / 2984.2535 s
agent0:                 episode reward: -0.2714,                 loss: nan
agent1:                 episode reward: 0.2714,                 loss: 0.4524
Episode: 40501/50100 (80.8403%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4792s / 2985.7327 s
agent0:                 episode reward: -0.4300,                 loss: nan
agent1:                 episode reward: 0.4300,                 loss: 0.4521
Episode: 40521/50100 (80.8802%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5623s / 2987.2950 s
agent0:                 episode reward: 0.7793,                 loss: nan
agent1:                 episode reward: -0.7793,                 loss: 0.4516
Episode: 40541/50100 (80.9202%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5364s / 2988.8313 s
agent0:                 episode reward: -0.7611,                 loss: nan
agent1:                 episode reward: 0.7611,                 loss: 0.4487
Episode: 40561/50100 (80.9601%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4837s / 2990.3151 s
agent0:                 episode reward: -0.3876,                 loss: nan
agent1:                 episode reward: 0.3876,                 loss: 0.4483
Episode: 40581/50100 (81.0000%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4878s / 2991.8029 s
agent0:                 episode reward: -0.5277,                 loss: nan
agent1:                 episode reward: 0.5277,                 loss: 0.4471
Episode: 40601/50100 (81.0399%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9674s / 2993.7702 s
agent0:                 episode reward: -0.3185,                 loss: 0.4265
agent1:                 episode reward: 0.3185,                 loss: 0.4482
Score delta: 2.068516231968636, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/40372_1.
Episode: 40621/50100 (81.0798%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0394s / 2995.8096 s
agent0:                 episode reward: -0.1908,                 loss: 0.4283
agent1:                 episode reward: 0.1908,                 loss: nan
Episode: 40641/50100 (81.1198%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0444s / 2997.8540 s
agent0:                 episode reward: -0.2421,                 loss: 0.4264
agent1:                 episode reward: 0.2421,                 loss: nan
Episode: 40661/50100 (81.1597%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0860s / 2999.9399 s
agent0:                 episode reward: -0.4688,                 loss: 0.4260
agent1:                 episode reward: 0.4688,                 loss: nan
Episode: 40681/50100 (81.1996%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0720s / 3002.0119 s
agent0:                 episode reward: 0.0614,                 loss: 0.4269
agent1:                 episode reward: -0.0614,                 loss: nan
Episode: 40701/50100 (81.2395%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9970s / 3004.0089 s
agent0:                 episode reward: -0.2107,                 loss: 0.4263
agent1:                 episode reward: 0.2107,                 loss: nan
Episode: 40721/50100 (81.2794%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0119s / 3006.0209 s
agent0:                 episode reward: -0.5103,                 loss: 0.4266
agent1:                 episode reward: 0.5103,                 loss: nan
Episode: 40741/50100 (81.3194%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0125s / 3008.0333 s
agent0:                 episode reward: 0.5745,                 loss: 0.4247
agent1:                 episode reward: -0.5745,                 loss: nan
Episode: 40761/50100 (81.3593%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4978s / 3009.5312 s
agent0:                 episode reward: -0.3741,                 loss: 0.4222
agent1:                 episode reward: 0.3741,                 loss: 0.4549
Score delta: 2.227325695266709, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/40530_0.
Episode: 40781/50100 (81.3992%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4592s / 3010.9903 s
agent0:                 episode reward: -0.4279,                 loss: nan
agent1:                 episode reward: 0.4279,                 loss: 0.4534
Episode: 40801/50100 (81.4391%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4914s / 3012.4817 s
agent0:                 episode reward: -0.7336,                 loss: nan
agent1:                 episode reward: 0.7336,                 loss: 0.4528
Episode: 40821/50100 (81.4790%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4784s / 3013.9600 s
agent0:                 episode reward: -0.7763,                 loss: nan
agent1:                 episode reward: 0.7763,                 loss: 0.4531
Episode: 40841/50100 (81.5190%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4950s / 3015.4550 s
agent0:                 episode reward: -0.5631,                 loss: nan
agent1:                 episode reward: 0.5631,                 loss: 0.4530
Episode: 40861/50100 (81.5589%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4990s / 3016.9540 s
agent0:                 episode reward: -0.1206,                 loss: nan
agent1:                 episode reward: 0.1206,                 loss: 0.4512
Episode: 40881/50100 (81.5988%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4952s / 3018.4493 s
agent0:                 episode reward: -0.2538,                 loss: nan
agent1:                 episode reward: 0.2538,                 loss: 0.4508
Episode: 40901/50100 (81.6387%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5134s / 3019.9627 s
agent0:                 episode reward: -0.1518,                 loss: nan
agent1:                 episode reward: 0.1518,                 loss: 0.4514
Episode: 40921/50100 (81.6786%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5019s / 3021.4646 s
agent0:                 episode reward: -0.4876,                 loss: nan
agent1:                 episode reward: 0.4876,                 loss: 0.4509
Episode: 40941/50100 (81.7186%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5294s / 3022.9940 s
agent0:                 episode reward: -0.0481,                 loss: nan
agent1:                 episode reward: 0.0481,                 loss: 0.4513
Episode: 40961/50100 (81.7585%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5123s / 3024.5063 s
agent0:                 episode reward: -0.2391,                 loss: nan
agent1:                 episode reward: 0.2391,                 loss: 0.4514
Episode: 40981/50100 (81.7984%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5225s / 3026.0288 s
agent0:                 episode reward: 0.1488,                 loss: nan
agent1:                 episode reward: -0.1488,                 loss: 0.4509
Episode: 41001/50100 (81.8383%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5120s / 3027.5408 s
agent0:                 episode reward: -0.5599,                 loss: nan
agent1:                 episode reward: 0.5599,                 loss: 0.4506
Episode: 41021/50100 (81.8782%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8656s / 3029.4064 s
agent0:                 episode reward: -0.3290,                 loss: 0.2807
agent1:                 episode reward: 0.3290,                 loss: 0.4507
Score delta: 2.160062301603302, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/40796_1.
Episode: 41041/50100 (81.9182%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0312s / 3031.4376 s
agent0:                 episode reward: -0.1159,                 loss: 0.2798
agent1:                 episode reward: 0.1159,                 loss: nan
Episode: 41061/50100 (81.9581%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0374s / 3033.4750 s
agent0:                 episode reward: 0.0366,                 loss: 0.3383
agent1:                 episode reward: -0.0366,                 loss: nan
Episode: 41081/50100 (81.9980%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0277s / 3035.5027 s
agent0:                 episode reward: -0.1902,                 loss: 0.3684
agent1:                 episode reward: 0.1902,                 loss: nan
Episode: 41101/50100 (82.0379%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1212s / 3037.6239 s
agent0:                 episode reward: 0.2091,                 loss: 0.3672
agent1:                 episode reward: -0.2091,                 loss: nan
Episode: 41121/50100 (82.0778%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0284s / 3039.6524 s
agent0:                 episode reward: -0.7847,                 loss: 0.3635
agent1:                 episode reward: 0.7847,                 loss: nan
Episode: 41141/50100 (82.1178%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0202s / 3041.6725 s
agent0:                 episode reward: -0.0942,                 loss: 0.3628
agent1:                 episode reward: 0.0942,                 loss: nan
Episode: 41161/50100 (82.1577%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0334s / 3043.7059 s
agent0:                 episode reward: 0.0327,                 loss: 0.3624
agent1:                 episode reward: -0.0327,                 loss: nan
Episode: 41181/50100 (82.1976%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1082s / 3045.8141 s
agent0:                 episode reward: -0.0421,                 loss: 0.3644
agent1:                 episode reward: 0.0421,                 loss: nan
Episode: 41201/50100 (82.2375%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0378s / 3047.8520 s
agent0:                 episode reward: 0.1439,                 loss: 0.3639
agent1:                 episode reward: -0.1439,                 loss: nan
Episode: 41221/50100 (82.2774%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0357s / 3049.8877 s
agent0:                 episode reward: -0.2660,                 loss: 0.3884
agent1:                 episode reward: 0.2660,                 loss: nan
Episode: 41241/50100 (82.3174%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1081s / 3051.9957 s
agent0:                 episode reward: -0.3109,                 loss: 0.4334
agent1:                 episode reward: 0.3109,                 loss: nan
Episode: 41261/50100 (82.3573%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0519s / 3054.0476 s
agent0:                 episode reward: 0.0149,                 loss: 0.4316
agent1:                 episode reward: -0.0149,                 loss: nan
Episode: 41281/50100 (82.3972%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0490s / 3056.0966 s
agent0:                 episode reward: -0.1284,                 loss: 0.4324
agent1:                 episode reward: 0.1284,                 loss: nan
Episode: 41301/50100 (82.4371%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0229s / 3058.1195 s
agent0:                 episode reward: -0.1551,                 loss: 0.4318
agent1:                 episode reward: 0.1551,                 loss: nan
Episode: 41321/50100 (82.4770%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0221s / 3060.1416 s
agent0:                 episode reward: 0.2123,                 loss: 0.4323
agent1:                 episode reward: -0.2123,                 loss: nan
Episode: 41341/50100 (82.5170%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9960s / 3062.1376 s
agent0:                 episode reward: 0.0194,                 loss: 0.4309
agent1:                 episode reward: -0.0194,                 loss: nan
Episode: 41361/50100 (82.5569%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0104s / 3064.1480 s
agent0:                 episode reward: 0.0864,                 loss: 0.4311
agent1:                 episode reward: -0.0864,                 loss: nan
Episode: 41381/50100 (82.5968%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9968s / 3066.1448 s
agent0:                 episode reward: 0.0424,                 loss: 0.4323
agent1:                 episode reward: -0.0424,                 loss: nan
Episode: 41401/50100 (82.6367%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0341s / 3068.1789 s
agent0:                 episode reward: -0.2316,                 loss: 0.4396
agent1:                 episode reward: 0.2316,                 loss: nan
Episode: 41421/50100 (82.6766%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0137s / 3070.1926 s
agent0:                 episode reward: -0.4510,                 loss: 0.4376
agent1:                 episode reward: 0.4510,                 loss: nan
Episode: 41441/50100 (82.7166%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0160s / 3072.2086 s
agent0:                 episode reward: -0.3208,                 loss: 0.4370
agent1:                 episode reward: 0.3208,                 loss: nan
Episode: 41461/50100 (82.7565%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9957s / 3074.2044 s
agent0:                 episode reward: 0.5942,                 loss: 0.4371
agent1:                 episode reward: -0.5942,                 loss: nan
Episode: 41481/50100 (82.7964%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0152s / 3076.2196 s
agent0:                 episode reward: -0.5917,                 loss: 0.4360
agent1:                 episode reward: 0.5917,                 loss: nan
Episode: 41501/50100 (82.8363%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0737s / 3078.2933 s
agent0:                 episode reward: -0.1741,                 loss: 0.4365
agent1:                 episode reward: 0.1741,                 loss: nan
Episode: 41521/50100 (82.8762%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0166s / 3080.3099 s
agent0:                 episode reward: 0.0951,                 loss: 0.4356
agent1:                 episode reward: -0.0951,                 loss: nan
Episode: 41541/50100 (82.9162%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0126s / 3082.3225 s
agent0:                 episode reward: 0.4114,                 loss: 0.4368
agent1:                 episode reward: -0.4114,                 loss: nan
Episode: 41561/50100 (82.9561%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0684s / 3084.3908 s
agent0:                 episode reward: -0.1805,                 loss: 0.4323
agent1:                 episode reward: 0.1805,                 loss: nan
Episode: 41581/50100 (82.9960%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0096s / 3086.4004 s
agent0:                 episode reward: -0.0788,                 loss: 0.4277
agent1:                 episode reward: 0.0788,                 loss: nan
Episode: 41601/50100 (83.0359%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9888s / 3088.3892 s
agent0:                 episode reward: 0.0324,                 loss: 0.4270
agent1:                 episode reward: -0.0324,                 loss: nan
Episode: 41621/50100 (83.0758%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0014s / 3090.3906 s
agent0:                 episode reward: 0.0765,                 loss: 0.4273
agent1:                 episode reward: -0.0765,                 loss: nan
Episode: 41641/50100 (83.1158%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6292s / 3092.0198 s
agent0:                 episode reward: -0.0438,                 loss: 0.4287
agent1:                 episode reward: 0.0438,                 loss: 0.4484
Score delta: 2.4676806515548204, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/41414_0.
Episode: 41661/50100 (83.1557%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4700s / 3093.4898 s
agent0:                 episode reward: -0.4456,                 loss: nan
agent1:                 episode reward: 0.4456,                 loss: 0.4471
Episode: 41681/50100 (83.1956%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4826s / 3094.9725 s
agent0:                 episode reward: 0.0361,                 loss: nan
agent1:                 episode reward: -0.0361,                 loss: 0.4470
Episode: 41701/50100 (83.2355%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4911s / 3096.4636 s
agent0:                 episode reward: -0.1192,                 loss: nan
agent1:                 episode reward: 0.1192,                 loss: 0.4469
Episode: 41721/50100 (83.2754%),                 avg. length: 5.0,                last time consumption/overall running time: 1.4856s / 3097.9491 s
agent0:                 episode reward: -0.2332,                 loss: nan
agent1:                 episode reward: 0.2332,                 loss: 0.4465
Episode: 41741/50100 (83.3154%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6899s / 3099.6390 s
agent0:                 episode reward: -0.6275,                 loss: 0.4135
agent1:                 episode reward: 0.6275,                 loss: 0.4459
Score delta: 2.0180560557239255, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/41522_1.
Episode: 41761/50100 (83.3553%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9930s / 3101.6321 s
agent0:                 episode reward: 0.1220,                 loss: 0.4122
agent1:                 episode reward: -0.1220,                 loss: nan
Episode: 41781/50100 (83.3952%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9937s / 3103.6258 s
agent0:                 episode reward: -0.2460,                 loss: 0.4121
agent1:                 episode reward: 0.2460,                 loss: nan
Episode: 41801/50100 (83.4351%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9948s / 3105.6206 s
agent0:                 episode reward: 0.0454,                 loss: 0.4081
agent1:                 episode reward: -0.0454,                 loss: nan
Episode: 41821/50100 (83.4750%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0227s / 3107.6434 s
agent0:                 episode reward: -0.2769,                 loss: 0.4088
agent1:                 episode reward: 0.2769,                 loss: nan
Episode: 41841/50100 (83.5150%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0112s / 3109.6546 s
agent0:                 episode reward: 0.0673,                 loss: 0.4015
agent1:                 episode reward: -0.0673,                 loss: nan
Episode: 41861/50100 (83.5549%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9899s / 3111.6445 s
agent0:                 episode reward: 0.2829,                 loss: 0.3945
agent1:                 episode reward: -0.2829,                 loss: nan
Episode: 41881/50100 (83.5948%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0091s / 3113.6536 s
agent0:                 episode reward: -0.2307,                 loss: 0.3946
agent1:                 episode reward: 0.2307,                 loss: nan
Episode: 41901/50100 (83.6347%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9854s / 3115.6390 s
agent0:                 episode reward: 0.1573,                 loss: 0.3947
agent1:                 episode reward: -0.1573,                 loss: nan
Episode: 41921/50100 (83.6747%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9961s / 3117.6352 s
agent0:                 episode reward: -0.0836,                 loss: 0.3933
agent1:                 episode reward: 0.0836,                 loss: nan
Episode: 41941/50100 (83.7146%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9733s / 3119.6084 s
agent0:                 episode reward: -0.4412,                 loss: 0.3930
agent1:                 episode reward: 0.4412,                 loss: nan
Episode: 41961/50100 (83.7545%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9757s / 3121.5841 s
agent0:                 episode reward: -0.0019,                 loss: 0.3916
agent1:                 episode reward: 0.0019,                 loss: nan
Episode: 41981/50100 (83.7944%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9853s / 3123.5694 s
agent0:                 episode reward: -0.1750,                 loss: 0.3918
agent1:                 episode reward: 0.1750,                 loss: nan
Episode: 42001/50100 (83.8343%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0698s / 3125.6392 s
agent0:                 episode reward: -0.0982,                 loss: 0.3675
agent1:                 episode reward: 0.0982,                 loss: nan
Episode: 42021/50100 (83.8743%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0247s / 3127.6639 s
agent0:                 episode reward: 0.0487,                 loss: 0.3397
agent1:                 episode reward: -0.0487,                 loss: nan
Episode: 42041/50100 (83.9142%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0167s / 3129.6806 s
agent0:                 episode reward: 0.2924,                 loss: 0.3401
agent1:                 episode reward: -0.2924,                 loss: nan
Episode: 42061/50100 (83.9541%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0616s / 3131.7422 s
agent0:                 episode reward: -0.3954,                 loss: 0.3388
agent1:                 episode reward: 0.3954,                 loss: nan
Episode: 42081/50100 (83.9940%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0385s / 3133.7807 s
agent0:                 episode reward: 0.4236,                 loss: 0.3386
agent1:                 episode reward: -0.4236,                 loss: nan
Episode: 42101/50100 (84.0339%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0186s / 3135.7993 s
agent0:                 episode reward: 0.0967,                 loss: 0.3395
agent1:                 episode reward: -0.0967,                 loss: nan
Episode: 42121/50100 (84.0739%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9963s / 3137.7956 s
agent0:                 episode reward: -0.4271,                 loss: 0.3372
agent1:                 episode reward: 0.4271,                 loss: nan
Episode: 42141/50100 (84.1138%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0254s / 3139.8210 s
agent0:                 episode reward: 0.1667,                 loss: 0.3367
agent1:                 episode reward: -0.1667,                 loss: nan
Episode: 42161/50100 (84.1537%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0077s / 3141.8286 s
agent0:                 episode reward: -0.5227,                 loss: 0.3486
agent1:                 episode reward: 0.5227,                 loss: nan
Episode: 42181/50100 (84.1936%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0261s / 3143.8547 s
agent0:                 episode reward: 0.0085,                 loss: 0.3766
agent1:                 episode reward: -0.0085,                 loss: nan
Episode: 42201/50100 (84.2335%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0012s / 3145.8559 s
agent0:                 episode reward: 0.0247,                 loss: 0.3765
agent1:                 episode reward: -0.0247,                 loss: nan
Episode: 42221/50100 (84.2735%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0249s / 3147.8807 s
agent0:                 episode reward: -0.4623,                 loss: 0.3742
agent1:                 episode reward: 0.4623,                 loss: nan
Episode: 42241/50100 (84.3134%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0802s / 3149.9609 s
agent0:                 episode reward: 0.1277,                 loss: 0.3733
agent1:                 episode reward: -0.1277,                 loss: nan
Episode: 42261/50100 (84.3533%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0234s / 3151.9844 s
agent0:                 episode reward: 0.3320,                 loss: 0.3752
agent1:                 episode reward: -0.3320,                 loss: nan
Episode: 42281/50100 (84.3932%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0233s / 3154.0077 s
agent0:                 episode reward: -0.1929,                 loss: 0.3738
agent1:                 episode reward: 0.1929,                 loss: nan
Episode: 42301/50100 (84.4331%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0251s / 3156.0328 s
agent0:                 episode reward: 0.0022,                 loss: 0.3729
agent1:                 episode reward: -0.0022,                 loss: nan
Episode: 42321/50100 (84.4731%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0500s / 3158.0828 s
agent0:                 episode reward: 0.4195,                 loss: 0.3728
agent1:                 episode reward: -0.4195,                 loss: nan
Episode: 42341/50100 (84.5130%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0416s / 3160.1244 s
agent0:                 episode reward: -0.0049,                 loss: 0.4178
agent1:                 episode reward: 0.0049,                 loss: nan
Episode: 42361/50100 (84.5529%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0499s / 3162.1744 s
agent0:                 episode reward: -0.4041,                 loss: 0.4207
agent1:                 episode reward: 0.4041,                 loss: nan
Episode: 42381/50100 (84.5928%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0467s / 3164.2210 s
agent0:                 episode reward: 0.7135,                 loss: 0.4195
agent1:                 episode reward: -0.7135,                 loss: nan
Episode: 42401/50100 (84.6327%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0577s / 3166.2787 s
agent0:                 episode reward: 0.1434,                 loss: 0.4231
agent1:                 episode reward: -0.1434,                 loss: nan
Episode: 42421/50100 (84.6727%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1322s / 3168.4109 s
agent0:                 episode reward: 0.0221,                 loss: 0.4213
agent1:                 episode reward: -0.0221,                 loss: nan
Episode: 42441/50100 (84.7126%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0510s / 3170.4619 s
agent0:                 episode reward: 0.1100,                 loss: 0.4204
agent1:                 episode reward: -0.1100,                 loss: nan
Episode: 42461/50100 (84.7525%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0294s / 3172.4913 s
agent0:                 episode reward: 0.3928,                 loss: 0.4220
agent1:                 episode reward: -0.3928,                 loss: nan
Episode: 42481/50100 (84.7924%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0412s / 3174.5325 s
agent0:                 episode reward: -0.2154,                 loss: 0.4220
agent1:                 episode reward: 0.2154,                 loss: nan
Episode: 42501/50100 (84.8323%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0296s / 3176.5621 s
agent0:                 episode reward: -0.2710,                 loss: 0.4365
agent1:                 episode reward: 0.2710,                 loss: nan
Episode: 42521/50100 (84.8723%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0261s / 3178.5882 s
agent0:                 episode reward: -0.0782,                 loss: 0.4431
agent1:                 episode reward: 0.0782,                 loss: nan
Episode: 42541/50100 (84.9122%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0181s / 3180.6063 s
agent0:                 episode reward: 0.3906,                 loss: 0.4431
agent1:                 episode reward: -0.3906,                 loss: nan
Episode: 42561/50100 (84.9521%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0398s / 3182.6462 s
agent0:                 episode reward: -0.1003,                 loss: 0.4426
agent1:                 episode reward: 0.1003,                 loss: nan
Episode: 42581/50100 (84.9920%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0151s / 3184.6613 s
agent0:                 episode reward: 0.0248,                 loss: 0.4422
agent1:                 episode reward: -0.0248,                 loss: nan
Episode: 42601/50100 (85.0319%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0286s / 3186.6899 s
agent0:                 episode reward: -0.1306,                 loss: 0.4420
agent1:                 episode reward: 0.1306,                 loss: nan
Episode: 42621/50100 (85.0719%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0512s / 3188.7411 s
agent0:                 episode reward: 0.0120,                 loss: 0.4417
agent1:                 episode reward: -0.0120,                 loss: nan
Episode: 42641/50100 (85.1118%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0972s / 3190.8383 s
agent0:                 episode reward: 0.2209,                 loss: 0.4421
agent1:                 episode reward: -0.2209,                 loss: nan
Episode: 42661/50100 (85.1517%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0450s / 3192.8833 s
agent0:                 episode reward: -0.2839,                 loss: 0.4430
agent1:                 episode reward: 0.2839,                 loss: nan
Episode: 42681/50100 (85.1916%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0408s / 3194.9241 s
agent0:                 episode reward: 0.0928,                 loss: 0.4405
agent1:                 episode reward: -0.0928,                 loss: nan
Episode: 42701/50100 (85.2315%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1201s / 3197.0442 s
agent0:                 episode reward: -0.1727,                 loss: 0.4398
agent1:                 episode reward: 0.1727,                 loss: nan
Episode: 42721/50100 (85.2715%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0304s / 3199.0746 s
agent0:                 episode reward: -0.5932,                 loss: 0.4404
agent1:                 episode reward: 0.5932,                 loss: nan
Episode: 42741/50100 (85.3114%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0276s / 3201.1023 s
agent0:                 episode reward: 0.2647,                 loss: 0.4392
agent1:                 episode reward: -0.2647,                 loss: nan
Episode: 42761/50100 (85.3513%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0151s / 3203.1174 s
agent0:                 episode reward: -0.4345,                 loss: 0.4403
agent1:                 episode reward: 0.4345,                 loss: nan
Episode: 42781/50100 (85.3912%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0404s / 3205.1578 s
agent0:                 episode reward: 0.2131,                 loss: 0.4399
agent1:                 episode reward: -0.2131,                 loss: nan
Episode: 42801/50100 (85.4311%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0281s / 3207.1859 s
agent0:                 episode reward: 0.1623,                 loss: 0.4393
agent1:                 episode reward: -0.1623,                 loss: nan
Episode: 42821/50100 (85.4711%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0136s / 3209.1995 s
agent0:                 episode reward: 0.2348,                 loss: 0.4398
agent1:                 episode reward: -0.2348,                 loss: nan
Episode: 42841/50100 (85.5110%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0270s / 3211.2265 s
agent0:                 episode reward: -0.3251,                 loss: 0.4243
agent1:                 episode reward: 0.3251,                 loss: nan
Episode: 42861/50100 (85.5509%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0105s / 3213.2370 s
agent0:                 episode reward: 0.1430,                 loss: 0.4206
agent1:                 episode reward: -0.1430,                 loss: nan
Episode: 42881/50100 (85.5908%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0267s / 3215.2636 s
agent0:                 episode reward: -0.5619,                 loss: 0.4218
agent1:                 episode reward: 0.5619,                 loss: nan
Episode: 42901/50100 (85.6307%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0168s / 3217.2805 s
agent0:                 episode reward: -0.5254,                 loss: 0.4191
agent1:                 episode reward: 0.5254,                 loss: nan
Episode: 42921/50100 (85.6707%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0060s / 3219.2865 s
agent0:                 episode reward: -0.1299,                 loss: 0.4210
agent1:                 episode reward: 0.1299,                 loss: nan
Episode: 42941/50100 (85.7106%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1046s / 3221.3911 s
agent0:                 episode reward: -0.0452,                 loss: 0.4190
agent1:                 episode reward: 0.0452,                 loss: nan
Episode: 42961/50100 (85.7505%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0246s / 3223.4156 s
agent0:                 episode reward: 0.1802,                 loss: 0.4215
agent1:                 episode reward: -0.1802,                 loss: nan
Episode: 42981/50100 (85.7904%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8461s / 3225.2617 s
agent0:                 episode reward: 0.8418,                 loss: 0.4214
agent1:                 episode reward: -0.8418,                 loss: 0.4592
Score delta: 2.2976511646546527, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/42762_0.
Episode: 43001/50100 (85.8303%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5060s / 3226.7677 s
agent0:                 episode reward: -0.4905,                 loss: nan
agent1:                 episode reward: 0.4905,                 loss: 0.4568
Episode: 43021/50100 (85.8703%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5495s / 3228.3172 s
agent0:                 episode reward: -0.0722,                 loss: nan
agent1:                 episode reward: 0.0722,                 loss: 0.4560
Episode: 43041/50100 (85.9102%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5720s / 3229.8892 s
agent0:                 episode reward: 0.2144,                 loss: nan
agent1:                 episode reward: -0.2144,                 loss: 0.4549
Episode: 43061/50100 (85.9501%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5009s / 3231.3901 s
agent0:                 episode reward: -0.3055,                 loss: nan
agent1:                 episode reward: 0.3055,                 loss: 0.4545
Episode: 43081/50100 (85.9900%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5155s / 3232.9057 s
agent0:                 episode reward: -0.0186,                 loss: nan
agent1:                 episode reward: 0.0186,                 loss: 0.4550
Episode: 43101/50100 (86.0299%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6591s / 3234.5648 s
agent0:                 episode reward: -0.4804,                 loss: 0.3541
agent1:                 episode reward: 0.4804,                 loss: 0.4545
Score delta: 2.0088807154851915, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/42884_1.
Episode: 43121/50100 (86.0699%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0726s / 3236.6374 s
agent0:                 episode reward: -0.0205,                 loss: 0.3334
agent1:                 episode reward: 0.0205,                 loss: nan
Episode: 43141/50100 (86.1098%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0030s / 3238.6404 s
agent0:                 episode reward: -0.3388,                 loss: 0.3162
agent1:                 episode reward: 0.3388,                 loss: nan
Episode: 43161/50100 (86.1497%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0009s / 3240.6413 s
agent0:                 episode reward: -0.2253,                 loss: 0.3138
agent1:                 episode reward: 0.2253,                 loss: nan
Episode: 43181/50100 (86.1896%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0126s / 3242.6539 s
agent0:                 episode reward: -0.8232,                 loss: 0.3105
agent1:                 episode reward: 0.8232,                 loss: nan
Episode: 43201/50100 (86.2295%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0219s / 3244.6758 s
agent0:                 episode reward: 0.1785,                 loss: 0.3118
agent1:                 episode reward: -0.1785,                 loss: nan
Episode: 43221/50100 (86.2695%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9881s / 3246.6638 s
agent0:                 episode reward: -0.0936,                 loss: 0.3100
agent1:                 episode reward: 0.0936,                 loss: nan
Episode: 43241/50100 (86.3094%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0070s / 3248.6708 s
agent0:                 episode reward: -0.1290,                 loss: 0.3109
agent1:                 episode reward: 0.1290,                 loss: nan
Episode: 43261/50100 (86.3493%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0008s / 3250.6716 s
agent0:                 episode reward: 0.7447,                 loss: 0.3112
agent1:                 episode reward: -0.7447,                 loss: nan
Episode: 43281/50100 (86.3892%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5635s / 3252.2352 s
agent0:                 episode reward: 0.2402,                 loss: 0.3088
agent1:                 episode reward: -0.2402,                 loss: 0.4558
Score delta: 2.482332388566385, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/43051_0.
Episode: 43301/50100 (86.4291%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5871s / 3253.8223 s
agent0:                 episode reward: -0.5579,                 loss: nan
agent1:                 episode reward: 0.5579,                 loss: 0.4557
Episode: 43321/50100 (86.4691%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5567s / 3255.3790 s
agent0:                 episode reward: -0.1503,                 loss: nan
agent1:                 episode reward: 0.1503,                 loss: 0.4536
Episode: 43341/50100 (86.5090%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5147s / 3256.8936 s
agent0:                 episode reward: -0.6681,                 loss: nan
agent1:                 episode reward: 0.6681,                 loss: 0.4544
Episode: 43361/50100 (86.5489%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5121s / 3258.4057 s
agent0:                 episode reward: -0.8275,                 loss: nan
agent1:                 episode reward: 0.8275,                 loss: 0.4534
Episode: 43381/50100 (86.5888%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5193s / 3259.9250 s
agent0:                 episode reward: 0.1138,                 loss: nan
agent1:                 episode reward: -0.1138,                 loss: 0.4538
Episode: 43401/50100 (86.6287%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5161s / 3261.4411 s
agent0:                 episode reward: -0.0294,                 loss: nan
agent1:                 episode reward: 0.0294,                 loss: 0.4528
Episode: 43421/50100 (86.6687%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5256s / 3262.9667 s
agent0:                 episode reward: 0.1054,                 loss: nan
agent1:                 episode reward: -0.1054,                 loss: 0.4538
Episode: 43441/50100 (86.7086%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5125s / 3264.4793 s
agent0:                 episode reward: -0.1109,                 loss: nan
agent1:                 episode reward: 0.1109,                 loss: 0.4533
Episode: 43461/50100 (86.7485%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5245s / 3266.0038 s
agent0:                 episode reward: -0.4092,                 loss: nan
agent1:                 episode reward: 0.4092,                 loss: 0.4529
Episode: 43481/50100 (86.7884%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5200s / 3267.5238 s
agent0:                 episode reward: -0.0151,                 loss: nan
agent1:                 episode reward: 0.0151,                 loss: 0.4519
Episode: 43501/50100 (86.8283%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5346s / 3269.0584 s
agent0:                 episode reward: -0.0352,                 loss: nan
agent1:                 episode reward: 0.0352,                 loss: 0.4531
Episode: 43521/50100 (86.8683%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5372s / 3270.5956 s
agent0:                 episode reward: -0.3268,                 loss: nan
agent1:                 episode reward: 0.3268,                 loss: 0.4524
Episode: 43541/50100 (86.9082%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5155s / 3272.1112 s
agent0:                 episode reward: -0.5039,                 loss: nan
agent1:                 episode reward: 0.5039,                 loss: 0.4534
Score delta: 2.0127721753080543, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/43329_1.
Episode: 43561/50100 (86.9481%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0114s / 3274.1226 s
agent0:                 episode reward: -0.2777,                 loss: 0.3886
agent1:                 episode reward: 0.2777,                 loss: nan
Episode: 43581/50100 (86.9880%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0036s / 3276.1262 s
agent0:                 episode reward: 0.3394,                 loss: 0.3395
agent1:                 episode reward: -0.3394,                 loss: nan
Episode: 43601/50100 (87.0279%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0527s / 3278.1789 s
agent0:                 episode reward: 0.1673,                 loss: 0.3372
agent1:                 episode reward: -0.1673,                 loss: nan
Episode: 43621/50100 (87.0679%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0406s / 3280.2195 s
agent0:                 episode reward: -0.1235,                 loss: 0.3339
agent1:                 episode reward: 0.1235,                 loss: nan
Episode: 43641/50100 (87.1078%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0446s / 3282.2641 s
agent0:                 episode reward: -0.4373,                 loss: 0.3334
agent1:                 episode reward: 0.4373,                 loss: nan
Episode: 43661/50100 (87.1477%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0370s / 3284.3011 s
agent0:                 episode reward: 0.2169,                 loss: 0.3335
agent1:                 episode reward: -0.2169,                 loss: nan
Episode: 43681/50100 (87.1876%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0069s / 3286.3080 s
agent0:                 episode reward: 0.4196,                 loss: 0.3329
agent1:                 episode reward: -0.4196,                 loss: nan
Episode: 43701/50100 (87.2275%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9972s / 3288.3052 s
agent0:                 episode reward: -0.3918,                 loss: 0.3333
agent1:                 episode reward: 0.3918,                 loss: nan
Episode: 43721/50100 (87.2675%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0172s / 3290.3224 s
agent0:                 episode reward: -0.3544,                 loss: 0.3312
agent1:                 episode reward: 0.3544,                 loss: nan
Episode: 43741/50100 (87.3074%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0051s / 3292.3275 s
agent0:                 episode reward: 0.3484,                 loss: 0.3484
agent1:                 episode reward: -0.3484,                 loss: nan
Episode: 43761/50100 (87.3473%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0008s / 3294.3284 s
agent0:                 episode reward: 0.0815,                 loss: 0.3465
agent1:                 episode reward: -0.0815,                 loss: nan
Episode: 43781/50100 (87.3872%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0973s / 3296.4257 s
agent0:                 episode reward: -0.1393,                 loss: 0.3433
agent1:                 episode reward: 0.1393,                 loss: nan
Episode: 43801/50100 (87.4271%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0029s / 3298.4286 s
agent0:                 episode reward: -0.1655,                 loss: 0.3431
agent1:                 episode reward: 0.1655,                 loss: nan
Episode: 43821/50100 (87.4671%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0041s / 3300.4327 s
agent0:                 episode reward: -0.0499,                 loss: 0.3412
agent1:                 episode reward: 0.0499,                 loss: nan
Episode: 43841/50100 (87.5070%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0059s / 3302.4386 s
agent0:                 episode reward: 0.5308,                 loss: 0.3449
agent1:                 episode reward: -0.5308,                 loss: nan
Episode: 43861/50100 (87.5469%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0348s / 3304.4733 s
agent0:                 episode reward: 0.1729,                 loss: 0.3415
agent1:                 episode reward: -0.1729,                 loss: nan
Episode: 43881/50100 (87.5868%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9778s / 3306.4511 s
agent0:                 episode reward: 0.4178,                 loss: 0.3420
agent1:                 episode reward: -0.4178,                 loss: 0.4391
Score delta: 2.0950220575848633, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/43664_0.
Episode: 43901/50100 (87.6267%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5386s / 3307.9897 s
agent0:                 episode reward: 0.2493,                 loss: nan
agent1:                 episode reward: -0.2493,                 loss: 0.4388
Episode: 43921/50100 (87.6667%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5308s / 3309.5205 s
agent0:                 episode reward: -0.2461,                 loss: nan
agent1:                 episode reward: 0.2461,                 loss: 0.4387
Episode: 43941/50100 (87.7066%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5371s / 3311.0576 s
agent0:                 episode reward: -0.2472,                 loss: nan
agent1:                 episode reward: 0.2472,                 loss: 0.4376
Episode: 43961/50100 (87.7465%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5338s / 3312.5914 s
agent0:                 episode reward: -0.6741,                 loss: nan
agent1:                 episode reward: 0.6741,                 loss: 0.4368
Episode: 43981/50100 (87.7864%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5504s / 3314.1418 s
agent0:                 episode reward: 0.0334,                 loss: nan
agent1:                 episode reward: -0.0334,                 loss: 0.4375
Episode: 44001/50100 (87.8263%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5413s / 3315.6831 s
agent0:                 episode reward: -0.6529,                 loss: nan
agent1:                 episode reward: 0.6529,                 loss: 0.4368
Episode: 44021/50100 (87.8663%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7643s / 3317.4474 s
agent0:                 episode reward: -0.7848,                 loss: 0.3483
agent1:                 episode reward: 0.7848,                 loss: 0.4355
Score delta: 2.199735017139207, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/43800_1.
Episode: 44041/50100 (87.9062%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0070s / 3319.4544 s
agent0:                 episode reward: 0.0661,                 loss: 0.4028
agent1:                 episode reward: -0.0661,                 loss: nan
Episode: 44061/50100 (87.9461%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0552s / 3321.5097 s
agent0:                 episode reward: -0.0564,                 loss: 0.4143
agent1:                 episode reward: 0.0564,                 loss: nan
Episode: 44081/50100 (87.9860%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0396s / 3323.5493 s
agent0:                 episode reward: -0.0780,                 loss: 0.4141
agent1:                 episode reward: 0.0780,                 loss: nan
Episode: 44101/50100 (88.0259%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0720s / 3325.6213 s
agent0:                 episode reward: -0.1793,                 loss: 0.4134
agent1:                 episode reward: 0.1793,                 loss: nan
Episode: 44121/50100 (88.0659%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0142s / 3327.6356 s
agent0:                 episode reward: -0.1838,                 loss: 0.4136
agent1:                 episode reward: 0.1838,                 loss: nan
Episode: 44141/50100 (88.1058%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0058s / 3329.6414 s
agent0:                 episode reward: -0.4614,                 loss: 0.4130
agent1:                 episode reward: 0.4614,                 loss: nan
Episode: 44161/50100 (88.1457%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0101s / 3331.6515 s
agent0:                 episode reward: 0.3461,                 loss: 0.4119
agent1:                 episode reward: -0.3461,                 loss: nan
Episode: 44181/50100 (88.1856%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0139s / 3333.6653 s
agent0:                 episode reward: 0.1474,                 loss: 0.4117
agent1:                 episode reward: -0.1474,                 loss: nan
Episode: 44201/50100 (88.2255%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0110s / 3335.6763 s
agent0:                 episode reward: 0.4201,                 loss: 0.4249
agent1:                 episode reward: -0.4201,                 loss: nan
Episode: 44221/50100 (88.2655%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1008s / 3337.7771 s
agent0:                 episode reward: 0.2252,                 loss: 0.4264
agent1:                 episode reward: -0.2252,                 loss: nan
Episode: 44241/50100 (88.3054%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0104s / 3339.7875 s
agent0:                 episode reward: 0.6889,                 loss: 0.4259
agent1:                 episode reward: -0.6889,                 loss: nan
Episode: 44261/50100 (88.3453%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0641s / 3341.8516 s
agent0:                 episode reward: -0.2739,                 loss: 0.4250
agent1:                 episode reward: 0.2739,                 loss: nan
Episode: 44281/50100 (88.3852%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0580s / 3343.9096 s
agent0:                 episode reward: 0.1281,                 loss: 0.4252
agent1:                 episode reward: -0.1281,                 loss: nan
Episode: 44301/50100 (88.4251%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0147s / 3345.9244 s
agent0:                 episode reward: 0.1210,                 loss: 0.4243
agent1:                 episode reward: -0.1210,                 loss: nan
Episode: 44321/50100 (88.4651%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0062s / 3347.9306 s
agent0:                 episode reward: -0.2045,                 loss: 0.4253
agent1:                 episode reward: 0.2045,                 loss: nan
Episode: 44341/50100 (88.5050%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0220s / 3349.9525 s
agent0:                 episode reward: 0.0183,                 loss: 0.4248
agent1:                 episode reward: -0.0183,                 loss: nan
Episode: 44361/50100 (88.5449%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1054s / 3352.0579 s
agent0:                 episode reward: 0.2149,                 loss: 0.4260
agent1:                 episode reward: -0.2149,                 loss: nan
Episode: 44381/50100 (88.5848%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9764s / 3354.0343 s
agent0:                 episode reward: 0.4702,                 loss: 0.4183
agent1:                 episode reward: -0.4702,                 loss: 0.4507
Score delta: 2.105525480354152, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/44167_0.
Episode: 44401/50100 (88.6248%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5749s / 3355.6092 s
agent0:                 episode reward: -0.2607,                 loss: nan
agent1:                 episode reward: 0.2607,                 loss: 0.4527
Episode: 44421/50100 (88.6647%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5631s / 3357.1723 s
agent0:                 episode reward: -0.2488,                 loss: nan
agent1:                 episode reward: 0.2488,                 loss: 0.4529
Episode: 44441/50100 (88.7046%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5505s / 3358.7228 s
agent0:                 episode reward: -0.1332,                 loss: nan
agent1:                 episode reward: 0.1332,                 loss: 0.4512
Episode: 44461/50100 (88.7445%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5598s / 3360.2826 s
agent0:                 episode reward: 0.2224,                 loss: nan
agent1:                 episode reward: -0.2224,                 loss: 0.4515
Episode: 44481/50100 (88.7844%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5539s / 3361.8365 s
agent0:                 episode reward: -0.3929,                 loss: nan
agent1:                 episode reward: 0.3929,                 loss: 0.4516
Episode: 44501/50100 (88.8244%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5739s / 3363.4104 s
agent0:                 episode reward: -0.3413,                 loss: nan
agent1:                 episode reward: 0.3413,                 loss: 0.4514
Episode: 44521/50100 (88.8643%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0023s / 3365.4128 s
agent0:                 episode reward: 0.3533,                 loss: 0.3522
agent1:                 episode reward: -0.3533,                 loss: 0.4529
Score delta: 2.082533891054616, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/44290_1.
Episode: 44541/50100 (88.9042%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0056s / 3367.4184 s
agent0:                 episode reward: 0.1629,                 loss: 0.3465
agent1:                 episode reward: -0.1629,                 loss: nan
Episode: 44561/50100 (88.9441%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0009s / 3369.4193 s
agent0:                 episode reward: 0.0355,                 loss: 0.3441
agent1:                 episode reward: -0.0355,                 loss: nan
Episode: 44581/50100 (88.9840%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0090s / 3371.4283 s
agent0:                 episode reward: 0.1241,                 loss: 0.3456
agent1:                 episode reward: -0.1241,                 loss: nan
Episode: 44601/50100 (89.0240%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0249s / 3373.4532 s
agent0:                 episode reward: -0.2427,                 loss: 0.3457
agent1:                 episode reward: 0.2427,                 loss: nan
Episode: 44621/50100 (89.0639%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0218s / 3375.4750 s
agent0:                 episode reward: 0.7066,                 loss: 0.3425
agent1:                 episode reward: -0.7066,                 loss: nan
Episode: 44641/50100 (89.1038%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6165s / 3377.0915 s
agent0:                 episode reward: -0.6709,                 loss: 0.3414
agent1:                 episode reward: 0.6709,                 loss: 0.4558
Score delta: 2.0951963553483326, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/44411_0.
Episode: 44661/50100 (89.1437%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5538s / 3378.6453 s
agent0:                 episode reward: -0.4269,                 loss: nan
agent1:                 episode reward: 0.4269,                 loss: 0.4557
Episode: 44681/50100 (89.1836%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5677s / 3380.2130 s
agent0:                 episode reward: -0.4470,                 loss: nan
agent1:                 episode reward: 0.4470,                 loss: 0.4545
Episode: 44701/50100 (89.2236%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5584s / 3381.7714 s
agent0:                 episode reward: -0.6772,                 loss: nan
agent1:                 episode reward: 0.6772,                 loss: 0.4528
Episode: 44721/50100 (89.2635%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5639s / 3383.3353 s
agent0:                 episode reward: -0.2362,                 loss: nan
agent1:                 episode reward: 0.2362,                 loss: 0.4534
Episode: 44741/50100 (89.3034%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5680s / 3384.9033 s
agent0:                 episode reward: -0.5072,                 loss: nan
agent1:                 episode reward: 0.5072,                 loss: 0.4533
Episode: 44761/50100 (89.3433%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5749s / 3386.4782 s
agent0:                 episode reward: -0.3462,                 loss: nan
agent1:                 episode reward: 0.3462,                 loss: 0.4525
Episode: 44781/50100 (89.3832%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5755s / 3388.0538 s
agent0:                 episode reward: 0.0521,                 loss: nan
agent1:                 episode reward: -0.0521,                 loss: 0.4519
Episode: 44801/50100 (89.4232%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5890s / 3389.6428 s
agent0:                 episode reward: -0.3179,                 loss: nan
agent1:                 episode reward: 0.3179,                 loss: 0.4523
Episode: 44821/50100 (89.4631%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6848s / 3391.3276 s
agent0:                 episode reward: -0.4200,                 loss: nan
agent1:                 episode reward: 0.4200,                 loss: 0.4521
Episode: 44841/50100 (89.5030%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5717s / 3392.8993 s
agent0:                 episode reward: -0.0084,                 loss: nan
agent1:                 episode reward: 0.0084,                 loss: 0.4519
Episode: 44861/50100 (89.5429%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7768s / 3394.6762 s
agent0:                 episode reward: -0.5629,                 loss: 0.3974
agent1:                 episode reward: 0.5629,                 loss: 0.4512
Score delta: 2.0614163347718195, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/44642_1.
Episode: 44881/50100 (89.5828%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0304s / 3396.7065 s
agent0:                 episode reward: -1.0231,                 loss: 0.4015
agent1:                 episode reward: 1.0231,                 loss: nan
Episode: 44901/50100 (89.6228%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0145s / 3398.7211 s
agent0:                 episode reward: -0.3479,                 loss: 0.4276
agent1:                 episode reward: 0.3479,                 loss: nan
Episode: 44921/50100 (89.6627%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0042s / 3400.7253 s
agent0:                 episode reward: 0.0653,                 loss: 0.4247
agent1:                 episode reward: -0.0653,                 loss: nan
Episode: 44941/50100 (89.7026%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0269s / 3402.7522 s
agent0:                 episode reward: -0.2913,                 loss: 0.4233
agent1:                 episode reward: 0.2913,                 loss: nan
Episode: 44961/50100 (89.7425%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0848s / 3404.8370 s
agent0:                 episode reward: -0.6818,                 loss: 0.4226
agent1:                 episode reward: 0.6818,                 loss: nan
Episode: 44981/50100 (89.7824%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0164s / 3406.8534 s
agent0:                 episode reward: -0.2088,                 loss: 0.4237
agent1:                 episode reward: 0.2088,                 loss: nan
Episode: 45001/50100 (89.8224%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0154s / 3408.8688 s
agent0:                 episode reward: -0.3381,                 loss: 0.4226
agent1:                 episode reward: 0.3381,                 loss: nan
Episode: 45021/50100 (89.8623%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0113s / 3410.8801 s
agent0:                 episode reward: -0.1021,                 loss: 0.4218
agent1:                 episode reward: 0.1021,                 loss: nan
Episode: 45041/50100 (89.9022%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0152s / 3412.8953 s
agent0:                 episode reward: 0.1286,                 loss: 0.4232
agent1:                 episode reward: -0.1286,                 loss: nan
Episode: 45061/50100 (89.9421%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0372s / 3414.9325 s
agent0:                 episode reward: -0.2342,                 loss: 0.3981
agent1:                 episode reward: 0.2342,                 loss: nan
Episode: 45081/50100 (89.9820%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0175s / 3416.9500 s
agent0:                 episode reward: 0.0486,                 loss: 0.3872
agent1:                 episode reward: -0.0486,                 loss: nan
Episode: 45101/50100 (90.0220%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0335s / 3418.9834 s
agent0:                 episode reward: 0.4648,                 loss: 0.3848
agent1:                 episode reward: -0.4648,                 loss: nan
Episode: 45121/50100 (90.0619%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0011s / 3420.9845 s
agent0:                 episode reward: -0.0595,                 loss: 0.3860
agent1:                 episode reward: 0.0595,                 loss: nan
Episode: 45141/50100 (90.1018%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0571s / 3423.0415 s
agent0:                 episode reward: 0.0140,                 loss: 0.3844
agent1:                 episode reward: -0.0140,                 loss: nan
Episode: 45161/50100 (90.1417%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0402s / 3425.0817 s
agent0:                 episode reward: 0.1997,                 loss: 0.3833
agent1:                 episode reward: -0.1997,                 loss: nan
Episode: 45181/50100 (90.1816%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0080s / 3427.0897 s
agent0:                 episode reward: 0.2423,                 loss: 0.3847
agent1:                 episode reward: -0.2423,                 loss: nan
Episode: 45201/50100 (90.2216%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0397s / 3429.1294 s
agent0:                 episode reward: -0.4343,                 loss: 0.3856
agent1:                 episode reward: 0.4343,                 loss: nan
Episode: 45221/50100 (90.2615%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0314s / 3431.1608 s
agent0:                 episode reward: 0.0505,                 loss: 0.3884
agent1:                 episode reward: -0.0505,                 loss: nan
Episode: 45241/50100 (90.3014%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0133s / 3433.1741 s
agent0:                 episode reward: 0.0900,                 loss: 0.3813
agent1:                 episode reward: -0.0900,                 loss: nan
Episode: 45261/50100 (90.3413%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0397s / 3435.2138 s
agent0:                 episode reward: -0.0618,                 loss: 0.3819
agent1:                 episode reward: 0.0618,                 loss: nan
Episode: 45281/50100 (90.3812%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0284s / 3437.2422 s
agent0:                 episode reward: -0.1851,                 loss: 0.3811
agent1:                 episode reward: 0.1851,                 loss: nan
Episode: 45301/50100 (90.4212%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0196s / 3439.2618 s
agent0:                 episode reward: -0.2212,                 loss: 0.3795
agent1:                 episode reward: 0.2212,                 loss: nan
Episode: 45321/50100 (90.4611%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1226s / 3441.3844 s
agent0:                 episode reward: 0.3737,                 loss: 0.3777
agent1:                 episode reward: -0.3737,                 loss: nan
Episode: 45341/50100 (90.5010%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0375s / 3443.4219 s
agent0:                 episode reward: 0.0473,                 loss: 0.3791
agent1:                 episode reward: -0.0473,                 loss: nan
Episode: 45361/50100 (90.5409%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0317s / 3445.4536 s
agent0:                 episode reward: -0.2932,                 loss: 0.3813
agent1:                 episode reward: 0.2932,                 loss: nan
Episode: 45381/50100 (90.5808%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1360s / 3447.5895 s
agent0:                 episode reward: 0.2870,                 loss: 0.3802
agent1:                 episode reward: -0.2870,                 loss: nan
Episode: 45401/50100 (90.6208%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0465s / 3449.6360 s
agent0:                 episode reward: -0.6811,                 loss: 0.3343
agent1:                 episode reward: 0.6811,                 loss: nan
Episode: 45421/50100 (90.6607%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0243s / 3451.6603 s
agent0:                 episode reward: -0.4478,                 loss: 0.3305
agent1:                 episode reward: 0.4478,                 loss: nan
Episode: 45441/50100 (90.7006%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0409s / 3453.7012 s
agent0:                 episode reward: -0.2900,                 loss: 0.3275
agent1:                 episode reward: 0.2900,                 loss: nan
Episode: 45461/50100 (90.7405%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0214s / 3455.7226 s
agent0:                 episode reward: -0.1723,                 loss: 0.3253
agent1:                 episode reward: 0.1723,                 loss: nan
Episode: 45481/50100 (90.7804%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0402s / 3457.7628 s
agent0:                 episode reward: 0.1165,                 loss: 0.3264
agent1:                 episode reward: -0.1165,                 loss: nan
Episode: 45501/50100 (90.8204%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0386s / 3459.8015 s
agent0:                 episode reward: 0.0113,                 loss: 0.3231
agent1:                 episode reward: -0.0113,                 loss: nan
Episode: 45521/50100 (90.8603%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0664s / 3461.8678 s
agent0:                 episode reward: 0.1294,                 loss: 0.3236
agent1:                 episode reward: -0.1294,                 loss: nan
Episode: 45541/50100 (90.9002%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1054s / 3463.9732 s
agent0:                 episode reward: -0.1736,                 loss: 0.3249
agent1:                 episode reward: 0.1736,                 loss: nan
Episode: 45561/50100 (90.9401%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8340s / 3465.8072 s
agent0:                 episode reward: 0.2801,                 loss: 0.3325
agent1:                 episode reward: -0.2801,                 loss: 0.4601
Score delta: 2.0921352177424124, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/45340_0.
Episode: 45581/50100 (90.9800%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6009s / 3467.4081 s
agent0:                 episode reward: -0.6119,                 loss: nan
agent1:                 episode reward: 0.6119,                 loss: 0.4570
Episode: 45601/50100 (91.0200%),                 avg. length: 5.0,                last time consumption/overall running time: 1.5887s / 3468.9968 s
agent0:                 episode reward: -0.5341,                 loss: nan
agent1:                 episode reward: 0.5341,                 loss: 0.4555
Episode: 45621/50100 (91.0599%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6187s / 3470.6155 s
agent0:                 episode reward: -0.1641,                 loss: nan
agent1:                 episode reward: 0.1641,                 loss: 0.4558
Episode: 45641/50100 (91.0998%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6070s / 3472.2225 s
agent0:                 episode reward: -0.3659,                 loss: nan
agent1:                 episode reward: 0.3659,                 loss: 0.4541
Episode: 45661/50100 (91.1397%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6036s / 3473.8261 s
agent0:                 episode reward: -0.5556,                 loss: nan
agent1:                 episode reward: 0.5556,                 loss: 0.4541
Episode: 45681/50100 (91.1796%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9883s / 3475.8144 s
agent0:                 episode reward: 0.0942,                 loss: 0.3825
agent1:                 episode reward: -0.0942,                 loss: 0.4545
Score delta: 2.192442984660114, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/45452_1.
Episode: 45701/50100 (91.2196%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1018s / 3477.9162 s
agent0:                 episode reward: -0.1357,                 loss: 0.3800
agent1:                 episode reward: 0.1357,                 loss: nan
Episode: 45721/50100 (91.2595%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0570s / 3479.9732 s
agent0:                 episode reward: -0.3668,                 loss: 0.3776
agent1:                 episode reward: 0.3668,                 loss: nan
Episode: 45741/50100 (91.2994%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1380s / 3482.1112 s
agent0:                 episode reward: -0.2954,                 loss: 0.3795
agent1:                 episode reward: 0.2954,                 loss: nan
Episode: 45761/50100 (91.3393%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0356s / 3484.1468 s
agent0:                 episode reward: -0.1062,                 loss: 0.3790
agent1:                 episode reward: 0.1062,                 loss: nan
Episode: 45781/50100 (91.3792%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0539s / 3486.2007 s
agent0:                 episode reward: 0.0680,                 loss: 0.3782
agent1:                 episode reward: -0.0680,                 loss: nan
Episode: 45801/50100 (91.4192%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0201s / 3488.2208 s
agent0:                 episode reward: -0.2790,                 loss: 0.3792
agent1:                 episode reward: 0.2790,                 loss: nan
Episode: 45821/50100 (91.4591%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0657s / 3490.2864 s
agent0:                 episode reward: -0.4967,                 loss: 0.3783
agent1:                 episode reward: 0.4967,                 loss: nan
Episode: 45841/50100 (91.4990%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0256s / 3492.3120 s
agent0:                 episode reward: -0.5933,                 loss: 0.3596
agent1:                 episode reward: 0.5933,                 loss: nan
Episode: 45861/50100 (91.5389%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0330s / 3494.3450 s
agent0:                 episode reward: 0.2240,                 loss: 0.3447
agent1:                 episode reward: -0.2240,                 loss: nan
Episode: 45881/50100 (91.5788%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0223s / 3496.3672 s
agent0:                 episode reward: -0.0561,                 loss: 0.3417
agent1:                 episode reward: 0.0561,                 loss: nan
Episode: 45901/50100 (91.6188%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0298s / 3498.3970 s
agent0:                 episode reward: -0.1755,                 loss: 0.3428
agent1:                 episode reward: 0.1755,                 loss: nan
Episode: 45921/50100 (91.6587%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1084s / 3500.5055 s
agent0:                 episode reward: -0.1114,                 loss: 0.3427
agent1:                 episode reward: 0.1114,                 loss: nan
Episode: 45941/50100 (91.6986%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7748s / 3502.2803 s
agent0:                 episode reward: 0.4817,                 loss: 0.3413
agent1:                 episode reward: -0.4817,                 loss: 0.4509
Score delta: 2.163584120726089, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/45715_0.
Episode: 45961/50100 (91.7385%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6293s / 3503.9096 s
agent0:                 episode reward: -0.1724,                 loss: nan
agent1:                 episode reward: 0.1724,                 loss: 0.4510
Episode: 45981/50100 (91.7784%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7089s / 3505.6185 s
agent0:                 episode reward: -0.1090,                 loss: nan
agent1:                 episode reward: 0.1090,                 loss: 0.4508
Episode: 46001/50100 (91.8184%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6144s / 3507.2329 s
agent0:                 episode reward: -0.2074,                 loss: nan
agent1:                 episode reward: 0.2074,                 loss: 0.4505
Episode: 46021/50100 (91.8583%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6423s / 3508.8752 s
agent0:                 episode reward: -0.3348,                 loss: nan
agent1:                 episode reward: 0.3348,                 loss: 0.4490
Episode: 46041/50100 (91.8982%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8841s / 3510.7593 s
agent0:                 episode reward: -0.4954,                 loss: 0.4296
agent1:                 episode reward: 0.4954,                 loss: 0.4498
Score delta: 2.105845755647545, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/45817_1.
Episode: 46061/50100 (91.9381%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0307s / 3512.7899 s
agent0:                 episode reward: 0.1838,                 loss: 0.4296
agent1:                 episode reward: -0.1838,                 loss: nan
Episode: 46081/50100 (91.9780%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0491s / 3514.8391 s
agent0:                 episode reward: 0.5040,                 loss: 0.4291
agent1:                 episode reward: -0.5040,                 loss: nan
Episode: 46101/50100 (92.0180%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0256s / 3516.8647 s
agent0:                 episode reward: -0.1309,                 loss: 0.4251
agent1:                 episode reward: 0.1309,                 loss: nan
Episode: 46121/50100 (92.0579%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0489s / 3518.9136 s
agent0:                 episode reward: 0.2206,                 loss: 0.4106
agent1:                 episode reward: -0.2206,                 loss: nan
Episode: 46141/50100 (92.0978%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0408s / 3520.9544 s
agent0:                 episode reward: -0.0538,                 loss: 0.4110
agent1:                 episode reward: 0.0538,                 loss: nan
Episode: 46161/50100 (92.1377%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0717s / 3523.0261 s
agent0:                 episode reward: 0.4666,                 loss: 0.4110
agent1:                 episode reward: -0.4666,                 loss: nan
Episode: 46181/50100 (92.1776%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0161s / 3525.0422 s
agent0:                 episode reward: -0.3502,                 loss: 0.4108
agent1:                 episode reward: 0.3502,                 loss: nan
Episode: 46201/50100 (92.2176%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0463s / 3527.0885 s
agent0:                 episode reward: 0.3462,                 loss: 0.4113
agent1:                 episode reward: -0.3462,                 loss: nan
Episode: 46221/50100 (92.2575%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0364s / 3529.1248 s
agent0:                 episode reward: 0.6400,                 loss: 0.4101
agent1:                 episode reward: -0.6400,                 loss: nan
Episode: 46241/50100 (92.2974%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0146s / 3531.1395 s
agent0:                 episode reward: 0.1784,                 loss: 0.4121
agent1:                 episode reward: -0.1784,                 loss: nan
Episode: 46261/50100 (92.3373%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0446s / 3533.1841 s
agent0:                 episode reward: -0.3630,                 loss: 0.4094
agent1:                 episode reward: 0.3630,                 loss: nan
Episode: 46281/50100 (92.3772%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0263s / 3535.2104 s
agent0:                 episode reward: 0.0685,                 loss: 0.3742
agent1:                 episode reward: -0.0685,                 loss: nan
Episode: 46301/50100 (92.4172%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0367s / 3537.2471 s
agent0:                 episode reward: 0.1293,                 loss: 0.3730
agent1:                 episode reward: -0.1293,                 loss: nan
Episode: 46321/50100 (92.4571%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0389s / 3539.2860 s
agent0:                 episode reward: -0.1855,                 loss: 0.3723
agent1:                 episode reward: 0.1855,                 loss: nan
Episode: 46341/50100 (92.4970%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0388s / 3541.3249 s
agent0:                 episode reward: 0.1965,                 loss: 0.3714
agent1:                 episode reward: -0.1965,                 loss: nan
Episode: 46361/50100 (92.5369%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0729s / 3543.3978 s
agent0:                 episode reward: 0.1317,                 loss: 0.3709
agent1:                 episode reward: -0.1317,                 loss: nan
Episode: 46381/50100 (92.5768%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1129s / 3545.5107 s
agent0:                 episode reward: 0.4590,                 loss: 0.3698
agent1:                 episode reward: -0.4590,                 loss: nan
Episode: 46401/50100 (92.6168%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0462s / 3547.5569 s
agent0:                 episode reward: -0.1610,                 loss: 0.3703
agent1:                 episode reward: 0.1610,                 loss: nan
Episode: 46421/50100 (92.6567%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0216s / 3549.5785 s
agent0:                 episode reward: -0.1809,                 loss: 0.3686
agent1:                 episode reward: 0.1809,                 loss: nan
Episode: 46441/50100 (92.6966%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0362s / 3551.6147 s
agent0:                 episode reward: 0.5453,                 loss: 0.3803
agent1:                 episode reward: -0.5453,                 loss: nan
Episode: 46461/50100 (92.7365%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0569s / 3553.6716 s
agent0:                 episode reward: -0.0515,                 loss: 0.3807
agent1:                 episode reward: 0.0515,                 loss: nan
Episode: 46481/50100 (92.7764%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0646s / 3555.7363 s
agent0:                 episode reward: -0.3453,                 loss: 0.3824
agent1:                 episode reward: 0.3453,                 loss: nan
Episode: 46501/50100 (92.8164%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0268s / 3557.7631 s
agent0:                 episode reward: -0.1564,                 loss: 0.3823
agent1:                 episode reward: 0.1564,                 loss: nan
Episode: 46521/50100 (92.8563%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0262s / 3559.7893 s
agent0:                 episode reward: -0.3528,                 loss: 0.3802
agent1:                 episode reward: 0.3528,                 loss: nan
Episode: 46541/50100 (92.8962%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0139s / 3561.8032 s
agent0:                 episode reward: 0.1026,                 loss: 0.3798
agent1:                 episode reward: -0.1026,                 loss: nan
Episode: 46561/50100 (92.9361%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0289s / 3563.8320 s
agent0:                 episode reward: -0.1657,                 loss: 0.3822
agent1:                 episode reward: 0.1657,                 loss: nan
Episode: 46581/50100 (92.9760%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0324s / 3565.8644 s
agent0:                 episode reward: 0.0606,                 loss: 0.3802
agent1:                 episode reward: -0.0606,                 loss: nan
Episode: 46601/50100 (93.0160%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0277s / 3567.8921 s
agent0:                 episode reward: 0.5639,                 loss: 0.3959
agent1:                 episode reward: -0.5639,                 loss: nan
Episode: 46621/50100 (93.0559%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0504s / 3569.9425 s
agent0:                 episode reward: -0.1020,                 loss: 0.4128
agent1:                 episode reward: 0.1020,                 loss: nan
Episode: 46641/50100 (93.0958%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0390s / 3571.9815 s
agent0:                 episode reward: -0.2099,                 loss: 0.4132
agent1:                 episode reward: 0.2099,                 loss: nan
Episode: 46661/50100 (93.1357%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1310s / 3574.1125 s
agent0:                 episode reward: 0.1366,                 loss: 0.4113
agent1:                 episode reward: -0.1366,                 loss: nan
Episode: 46681/50100 (93.1756%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0370s / 3576.1495 s
agent0:                 episode reward: -0.7470,                 loss: 0.4098
agent1:                 episode reward: 0.7470,                 loss: nan
Episode: 46701/50100 (93.2156%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0455s / 3578.1949 s
agent0:                 episode reward: -0.0311,                 loss: 0.4125
agent1:                 episode reward: 0.0311,                 loss: nan
Episode: 46721/50100 (93.2555%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0305s / 3580.2254 s
agent0:                 episode reward: -0.2323,                 loss: 0.4137
agent1:                 episode reward: 0.2323,                 loss: nan
Episode: 46741/50100 (93.2954%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0448s / 3582.2702 s
agent0:                 episode reward: -0.3864,                 loss: 0.4117
agent1:                 episode reward: 0.3864,                 loss: nan
Episode: 46761/50100 (93.3353%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0400s / 3584.3102 s
agent0:                 episode reward: 0.1216,                 loss: 0.4142
agent1:                 episode reward: -0.1216,                 loss: nan
Episode: 46781/50100 (93.3752%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0365s / 3586.3467 s
agent0:                 episode reward: 0.1010,                 loss: 0.4168
agent1:                 episode reward: -0.1010,                 loss: nan
Episode: 46801/50100 (93.4152%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9640s / 3588.3108 s
agent0:                 episode reward: 0.5712,                 loss: 0.4147
agent1:                 episode reward: -0.5712,                 loss: 0.4444
Score delta: 2.019244297372162, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/46585_0.
Episode: 46821/50100 (93.4551%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6183s / 3589.9290 s
agent0:                 episode reward: -0.6465,                 loss: nan
agent1:                 episode reward: 0.6465,                 loss: 0.4442
Episode: 46841/50100 (93.4950%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6115s / 3591.5405 s
agent0:                 episode reward: -0.5327,                 loss: nan
agent1:                 episode reward: 0.5327,                 loss: 0.4456
Episode: 46861/50100 (93.5349%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6079s / 3593.1484 s
agent0:                 episode reward: -0.4079,                 loss: nan
agent1:                 episode reward: 0.4079,                 loss: 0.4450
Episode: 46881/50100 (93.5749%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6128s / 3594.7612 s
agent0:                 episode reward: 0.0077,                 loss: nan
agent1:                 episode reward: -0.0077,                 loss: 0.4459
Episode: 46901/50100 (93.6148%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6041s / 3596.3653 s
agent0:                 episode reward: -0.8566,                 loss: nan
agent1:                 episode reward: 0.8566,                 loss: 0.4450
Episode: 46921/50100 (93.6547%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0229s / 3598.3882 s
agent0:                 episode reward: -0.2367,                 loss: 0.4298
agent1:                 episode reward: 0.2367,                 loss: 0.4442
Score delta: 2.0820908359096277, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/46690_1.
Episode: 46941/50100 (93.6946%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0395s / 3600.4277 s
agent0:                 episode reward: -0.2233,                 loss: 0.4295
agent1:                 episode reward: 0.2233,                 loss: nan
Episode: 46961/50100 (93.7345%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0330s / 3602.4607 s
agent0:                 episode reward: -0.0479,                 loss: 0.4297
agent1:                 episode reward: 0.0479,                 loss: nan
Episode: 46981/50100 (93.7745%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0388s / 3604.4995 s
agent0:                 episode reward: -0.3380,                 loss: 0.4294
agent1:                 episode reward: 0.3380,                 loss: nan
Episode: 47001/50100 (93.8144%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1213s / 3606.6209 s
agent0:                 episode reward: 0.1100,                 loss: 0.4296
agent1:                 episode reward: -0.1100,                 loss: nan
Episode: 47021/50100 (93.8543%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0589s / 3608.6798 s
agent0:                 episode reward: -0.0808,                 loss: 0.4307
agent1:                 episode reward: 0.0808,                 loss: nan
Episode: 47041/50100 (93.8942%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0373s / 3610.7171 s
agent0:                 episode reward: -0.0341,                 loss: 0.4387
agent1:                 episode reward: 0.0341,                 loss: nan
Episode: 47061/50100 (93.9341%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0861s / 3612.8032 s
agent0:                 episode reward: -0.2252,                 loss: 0.4418
agent1:                 episode reward: 0.2252,                 loss: nan
Episode: 47081/50100 (93.9741%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0836s / 3614.8867 s
agent0:                 episode reward: 0.3124,                 loss: 0.4414
agent1:                 episode reward: -0.3124,                 loss: nan
Episode: 47101/50100 (94.0140%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0877s / 3616.9745 s
agent0:                 episode reward: 0.1586,                 loss: 0.4424
agent1:                 episode reward: -0.1586,                 loss: nan
Episode: 47121/50100 (94.0539%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9983s / 3618.9727 s
agent0:                 episode reward: -0.1289,                 loss: 0.4415
agent1:                 episode reward: 0.1289,                 loss: 0.4479
Score delta: 2.060843186816458, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/46906_0.
Episode: 47141/50100 (94.0938%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6222s / 3620.5949 s
agent0:                 episode reward: -0.1243,                 loss: nan
agent1:                 episode reward: 0.1243,                 loss: 0.4479
Episode: 47161/50100 (94.1337%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6167s / 3622.2116 s
agent0:                 episode reward: -0.0073,                 loss: nan
agent1:                 episode reward: 0.0073,                 loss: 0.4473
Episode: 47181/50100 (94.1737%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6281s / 3623.8397 s
agent0:                 episode reward: -0.1523,                 loss: nan
agent1:                 episode reward: 0.1523,                 loss: 0.4472
Episode: 47201/50100 (94.2136%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6333s / 3625.4730 s
agent0:                 episode reward: -0.6678,                 loss: nan
agent1:                 episode reward: 0.6678,                 loss: 0.4472
Episode: 47221/50100 (94.2535%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6346s / 3627.1075 s
agent0:                 episode reward: -0.2671,                 loss: nan
agent1:                 episode reward: 0.2671,                 loss: 0.4458
Episode: 47241/50100 (94.2934%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6197s / 3628.7272 s
agent0:                 episode reward: -0.2353,                 loss: nan
agent1:                 episode reward: 0.2353,                 loss: 0.4456
Episode: 47261/50100 (94.3333%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6330s / 3630.3603 s
agent0:                 episode reward: 0.2147,                 loss: nan
agent1:                 episode reward: -0.2147,                 loss: 0.4459
Episode: 47281/50100 (94.3733%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6239s / 3631.9841 s
agent0:                 episode reward: -0.3464,                 loss: nan
agent1:                 episode reward: 0.3464,                 loss: 0.4460
Episode: 47301/50100 (94.4132%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9693s / 3633.9535 s
agent0:                 episode reward: -1.0999,                 loss: 0.3471
agent1:                 episode reward: 1.0999,                 loss: 0.4454
Score delta: 2.479013559021293, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/47076_1.
Episode: 47321/50100 (94.4531%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0790s / 3636.0324 s
agent0:                 episode reward: -0.3817,                 loss: 0.3481
agent1:                 episode reward: 0.3817,                 loss: nan
Episode: 47341/50100 (94.4930%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0372s / 3638.0696 s
agent0:                 episode reward: 0.3618,                 loss: 0.3486
agent1:                 episode reward: -0.3618,                 loss: nan
Episode: 47361/50100 (94.5329%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0639s / 3640.1336 s
agent0:                 episode reward: -0.0680,                 loss: 0.3474
agent1:                 episode reward: 0.0680,                 loss: nan
Episode: 47381/50100 (94.5729%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0914s / 3642.2250 s
agent0:                 episode reward: -0.3848,                 loss: 0.3906
agent1:                 episode reward: 0.3848,                 loss: nan
Episode: 47401/50100 (94.6128%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1693s / 3644.3942 s
agent0:                 episode reward: -0.3799,                 loss: 0.4138
agent1:                 episode reward: 0.3799,                 loss: nan
Episode: 47421/50100 (94.6527%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1280s / 3646.5222 s
agent0:                 episode reward: -0.3407,                 loss: 0.4131
agent1:                 episode reward: 0.3407,                 loss: nan
Episode: 47441/50100 (94.6926%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9378s / 3648.4600 s
agent0:                 episode reward: 0.4804,                 loss: 0.4136
agent1:                 episode reward: -0.4804,                 loss: 0.4488
Score delta: 2.0963021866979545, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/47224_0.
Episode: 47461/50100 (94.7325%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6477s / 3650.1077 s
agent0:                 episode reward: -0.2465,                 loss: nan
agent1:                 episode reward: 0.2465,                 loss: 0.4432
Episode: 47481/50100 (94.7725%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6412s / 3651.7489 s
agent0:                 episode reward: -0.0225,                 loss: nan
agent1:                 episode reward: 0.0225,                 loss: 0.4400
Episode: 47501/50100 (94.8124%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6383s / 3653.3872 s
agent0:                 episode reward: -0.1373,                 loss: nan
agent1:                 episode reward: 0.1373,                 loss: 0.4383
Episode: 47521/50100 (94.8523%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6359s / 3655.0230 s
agent0:                 episode reward: 0.7291,                 loss: nan
agent1:                 episode reward: -0.7291,                 loss: 0.4376
Episode: 47541/50100 (94.8922%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6443s / 3656.6673 s
agent0:                 episode reward: -0.2531,                 loss: nan
agent1:                 episode reward: 0.2531,                 loss: 0.4379
Episode: 47561/50100 (94.9321%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6475s / 3658.3148 s
agent0:                 episode reward: -0.5209,                 loss: nan
agent1:                 episode reward: 0.5209,                 loss: 0.4384
Episode: 47581/50100 (94.9721%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8910s / 3660.2058 s
agent0:                 episode reward: -0.0694,                 loss: 0.3445
agent1:                 episode reward: 0.0694,                 loss: 0.4385
Score delta: 2.1612930571845808, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/47357_1.
Episode: 47601/50100 (95.0120%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0473s / 3662.2531 s
agent0:                 episode reward: -0.3600,                 loss: 0.3442
agent1:                 episode reward: 0.3600,                 loss: nan
Episode: 47621/50100 (95.0519%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0621s / 3664.3152 s
agent0:                 episode reward: 0.3331,                 loss: 0.3432
agent1:                 episode reward: -0.3331,                 loss: nan
Episode: 47641/50100 (95.0918%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0489s / 3666.3641 s
agent0:                 episode reward: -0.0889,                 loss: 0.3448
agent1:                 episode reward: 0.0889,                 loss: nan
Episode: 47661/50100 (95.1317%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0367s / 3668.4008 s
agent0:                 episode reward: -0.5162,                 loss: 0.3443
agent1:                 episode reward: 0.5162,                 loss: nan
Episode: 47681/50100 (95.1717%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0510s / 3670.4518 s
agent0:                 episode reward: 0.4885,                 loss: 0.3849
agent1:                 episode reward: -0.4885,                 loss: nan
Episode: 47701/50100 (95.2116%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0440s / 3672.4958 s
agent0:                 episode reward: 0.1156,                 loss: 0.3996
agent1:                 episode reward: -0.1156,                 loss: nan
Episode: 47721/50100 (95.2515%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0404s / 3674.5362 s
agent0:                 episode reward: -0.2659,                 loss: 0.3999
agent1:                 episode reward: 0.2659,                 loss: nan
Episode: 47741/50100 (95.2914%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0337s / 3676.5699 s
agent0:                 episode reward: -0.0295,                 loss: 0.3999
agent1:                 episode reward: 0.0295,                 loss: nan
Episode: 47761/50100 (95.3313%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0546s / 3678.6245 s
agent0:                 episode reward: 0.5315,                 loss: 0.4007
agent1:                 episode reward: -0.5315,                 loss: nan
Episode: 47781/50100 (95.3713%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0279s / 3680.6525 s
agent0:                 episode reward: -0.4678,                 loss: 0.3977
agent1:                 episode reward: 0.4678,                 loss: nan
Episode: 47801/50100 (95.4112%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0378s / 3682.6902 s
agent0:                 episode reward: -0.0490,                 loss: 0.4004
agent1:                 episode reward: 0.0490,                 loss: nan
Episode: 47821/50100 (95.4511%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0412s / 3684.7314 s
agent0:                 episode reward: 0.2249,                 loss: 0.3999
agent1:                 episode reward: -0.2249,                 loss: nan
Episode: 47841/50100 (95.4910%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0526s / 3686.7841 s
agent0:                 episode reward: 0.5012,                 loss: 0.4129
agent1:                 episode reward: -0.5012,                 loss: nan
Episode: 47861/50100 (95.5309%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0349s / 3688.8190 s
agent0:                 episode reward: -0.3115,                 loss: 0.4315
agent1:                 episode reward: 0.3115,                 loss: nan
Episode: 47881/50100 (95.5709%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0401s / 3690.8591 s
agent0:                 episode reward: -0.1430,                 loss: 0.4273
agent1:                 episode reward: 0.1430,                 loss: nan
Episode: 47901/50100 (95.6108%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0580s / 3692.9170 s
agent0:                 episode reward: 0.0038,                 loss: 0.4273
agent1:                 episode reward: -0.0038,                 loss: nan
Episode: 47921/50100 (95.6507%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0559s / 3694.9729 s
agent0:                 episode reward: 0.1050,                 loss: 0.4262
agent1:                 episode reward: -0.1050,                 loss: nan
Episode: 47941/50100 (95.6906%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0775s / 3697.0504 s
agent0:                 episode reward: 0.0558,                 loss: 0.4284
agent1:                 episode reward: -0.0558,                 loss: nan
Episode: 47961/50100 (95.7305%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0972s / 3699.1476 s
agent0:                 episode reward: 0.1619,                 loss: 0.4264
agent1:                 episode reward: -0.1619,                 loss: nan
Episode: 47981/50100 (95.7705%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0636s / 3701.2112 s
agent0:                 episode reward: 0.0128,                 loss: 0.4260
agent1:                 episode reward: -0.0128,                 loss: nan
Episode: 48001/50100 (95.8104%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0839s / 3703.2951 s
agent0:                 episode reward: 0.0565,                 loss: 0.4252
agent1:                 episode reward: -0.0565,                 loss: nan
Episode: 48021/50100 (95.8503%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1291s / 3705.4243 s
agent0:                 episode reward: -0.0935,                 loss: 0.4124
agent1:                 episode reward: 0.0935,                 loss: nan
Episode: 48041/50100 (95.8902%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1369s / 3707.5611 s
agent0:                 episode reward: 0.1789,                 loss: 0.4048
agent1:                 episode reward: -0.1789,                 loss: nan
Episode: 48061/50100 (95.9301%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8537s / 3709.4149 s
agent0:                 episode reward: 0.0249,                 loss: 0.4068
agent1:                 episode reward: -0.0249,                 loss: 0.4525
Score delta: 2.1236905991702733, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/47835_0.
Episode: 48081/50100 (95.9701%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6747s / 3711.0895 s
agent0:                 episode reward: -0.5074,                 loss: nan
agent1:                 episode reward: 0.5074,                 loss: 0.4506
Episode: 48101/50100 (96.0100%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6865s / 3712.7760 s
agent0:                 episode reward: 0.2927,                 loss: nan
agent1:                 episode reward: -0.2927,                 loss: 0.4515
Episode: 48121/50100 (96.0499%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6849s / 3714.4609 s
agent0:                 episode reward: -0.3442,                 loss: nan
agent1:                 episode reward: 0.3442,                 loss: 0.4506
Episode: 48141/50100 (96.0898%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6734s / 3716.1343 s
agent0:                 episode reward: -0.2823,                 loss: nan
agent1:                 episode reward: 0.2823,                 loss: 0.4496
Episode: 48161/50100 (96.1297%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6776s / 3717.8119 s
agent0:                 episode reward: -0.5249,                 loss: nan
agent1:                 episode reward: 0.5249,                 loss: 0.4490
Score delta: 2.1746593039978857, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/47949_1.
Episode: 48181/50100 (96.1697%),                 avg. length: 5.0,                last time consumption/overall running time: 2.1625s / 3719.9744 s
agent0:                 episode reward: 0.1175,                 loss: 0.4264
agent1:                 episode reward: -0.1175,                 loss: nan
Episode: 48201/50100 (96.2096%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0629s / 3722.0373 s
agent0:                 episode reward: -0.5355,                 loss: 0.4240
agent1:                 episode reward: 0.5355,                 loss: nan
Episode: 48221/50100 (96.2495%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0405s / 3724.0778 s
agent0:                 episode reward: 0.1245,                 loss: 0.4234
agent1:                 episode reward: -0.1245,                 loss: nan
Episode: 48241/50100 (96.2894%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0150s / 3726.0928 s
agent0:                 episode reward: 0.1272,                 loss: 0.4221
agent1:                 episode reward: -0.1272,                 loss: nan
Episode: 48261/50100 (96.3293%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0079s / 3728.1007 s
agent0:                 episode reward: 0.1861,                 loss: 0.4226
agent1:                 episode reward: -0.1861,                 loss: nan
Episode: 48281/50100 (96.3693%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0070s / 3730.1077 s
agent0:                 episode reward: -0.0543,                 loss: 0.4220
agent1:                 episode reward: 0.0543,                 loss: nan
Episode: 48301/50100 (96.4092%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8596s / 3731.9673 s
agent0:                 episode reward: 0.5415,                 loss: 0.4049
agent1:                 episode reward: -0.5415,                 loss: 0.4545
Score delta: 2.2176133457567024, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/48081_0.
Episode: 48321/50100 (96.4491%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6330s / 3733.6003 s
agent0:                 episode reward: -0.3861,                 loss: nan
agent1:                 episode reward: 0.3861,                 loss: 0.4536
Episode: 48341/50100 (96.4890%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6155s / 3735.2158 s
agent0:                 episode reward: -0.6925,                 loss: nan
agent1:                 episode reward: 0.6925,                 loss: 0.4525
Episode: 48361/50100 (96.5289%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6715s / 3736.8873 s
agent0:                 episode reward: -0.5311,                 loss: nan
agent1:                 episode reward: 0.5311,                 loss: 0.4518
Episode: 48381/50100 (96.5689%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6378s / 3738.5251 s
agent0:                 episode reward: 0.3596,                 loss: nan
agent1:                 episode reward: -0.3596,                 loss: 0.4520
Episode: 48401/50100 (96.6088%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6399s / 3740.1650 s
agent0:                 episode reward: 0.0215,                 loss: nan
agent1:                 episode reward: -0.0215,                 loss: 0.4533
Episode: 48421/50100 (96.6487%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6406s / 3741.8056 s
agent0:                 episode reward: -0.2912,                 loss: nan
agent1:                 episode reward: 0.2912,                 loss: 0.4530
Episode: 48441/50100 (96.6886%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6433s / 3743.4489 s
agent0:                 episode reward: -0.4731,                 loss: nan
agent1:                 episode reward: 0.4731,                 loss: 0.4534
Episode: 48461/50100 (96.7285%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6336s / 3745.0825 s
agent0:                 episode reward: 0.0216,                 loss: nan
agent1:                 episode reward: -0.0216,                 loss: 0.4523
Episode: 48481/50100 (96.7685%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6415s / 3746.7239 s
agent0:                 episode reward: -0.0685,                 loss: nan
agent1:                 episode reward: 0.0685,                 loss: 0.4522
Episode: 48501/50100 (96.8084%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6462s / 3748.3701 s
agent0:                 episode reward: 0.1367,                 loss: nan
agent1:                 episode reward: -0.1367,                 loss: 0.4522
Episode: 48521/50100 (96.8483%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6458s / 3750.0159 s
agent0:                 episode reward: 0.1162,                 loss: nan
agent1:                 episode reward: -0.1162,                 loss: 0.4510
Episode: 48541/50100 (96.8882%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8508s / 3751.8666 s
agent0:                 episode reward: -0.8380,                 loss: 0.3108
agent1:                 episode reward: 0.8380,                 loss: 0.4504
Score delta: 2.280578632770972, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/48320_1.
Episode: 48561/50100 (96.9281%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0170s / 3753.8836 s
agent0:                 episode reward: -0.3260,                 loss: 0.3050
agent1:                 episode reward: 0.3260,                 loss: nan
Episode: 48581/50100 (96.9681%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0115s / 3755.8951 s
agent0:                 episode reward: -0.0675,                 loss: 0.3028
agent1:                 episode reward: 0.0675,                 loss: nan
Episode: 48601/50100 (97.0080%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0265s / 3757.9216 s
agent0:                 episode reward: 0.0690,                 loss: 0.3059
agent1:                 episode reward: -0.0690,                 loss: nan
Episode: 48621/50100 (97.0479%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0216s / 3759.9432 s
agent0:                 episode reward: 0.4051,                 loss: 0.3007
agent1:                 episode reward: -0.4051,                 loss: nan
Episode: 48641/50100 (97.0878%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0221s / 3761.9653 s
agent0:                 episode reward: -0.6410,                 loss: 0.3033
agent1:                 episode reward: 0.6410,                 loss: nan
Episode: 48661/50100 (97.1277%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0147s / 3763.9800 s
agent0:                 episode reward: 0.2654,                 loss: 0.3020
agent1:                 episode reward: -0.2654,                 loss: nan
Episode: 48681/50100 (97.1677%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0206s / 3766.0006 s
agent0:                 episode reward: 0.0723,                 loss: 0.3022
agent1:                 episode reward: -0.0723,                 loss: nan
Episode: 48701/50100 (97.2076%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0236s / 3768.0242 s
agent0:                 episode reward: -0.5762,                 loss: 0.3442
agent1:                 episode reward: 0.5762,                 loss: nan
Episode: 48721/50100 (97.2475%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0254s / 3770.0496 s
agent0:                 episode reward: 0.4551,                 loss: 0.3638
agent1:                 episode reward: -0.4551,                 loss: nan
Episode: 48741/50100 (97.2874%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0197s / 3772.0694 s
agent0:                 episode reward: -0.5396,                 loss: 0.3671
agent1:                 episode reward: 0.5396,                 loss: nan
Episode: 48761/50100 (97.3273%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0141s / 3774.0834 s
agent0:                 episode reward: 0.2528,                 loss: 0.3649
agent1:                 episode reward: -0.2528,                 loss: nan
Episode: 48781/50100 (97.3673%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0108s / 3776.0942 s
agent0:                 episode reward: -0.3069,                 loss: 0.3661
agent1:                 episode reward: 0.3069,                 loss: nan
Episode: 48801/50100 (97.4072%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0273s / 3778.1215 s
agent0:                 episode reward: -0.2332,                 loss: 0.3653
agent1:                 episode reward: 0.2332,                 loss: nan
Episode: 48821/50100 (97.4471%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0133s / 3780.1347 s
agent0:                 episode reward: 0.2938,                 loss: 0.3658
agent1:                 episode reward: -0.2938,                 loss: nan
Episode: 48841/50100 (97.4870%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0064s / 3782.1412 s
agent0:                 episode reward: 0.4307,                 loss: 0.3640
agent1:                 episode reward: -0.4307,                 loss: nan
Episode: 48861/50100 (97.5269%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0025s / 3784.1436 s
agent0:                 episode reward: -0.0600,                 loss: 0.3889
agent1:                 episode reward: 0.0600,                 loss: nan
Episode: 48881/50100 (97.5669%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0239s / 3786.1675 s
agent0:                 episode reward: -0.1454,                 loss: 0.4288
agent1:                 episode reward: 0.1454,                 loss: nan
Episode: 48901/50100 (97.6068%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0078s / 3788.1752 s
agent0:                 episode reward: 0.1423,                 loss: 0.4289
agent1:                 episode reward: -0.1423,                 loss: nan
Episode: 48921/50100 (97.6467%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0017s / 3790.1769 s
agent0:                 episode reward: 0.0078,                 loss: 0.4265
agent1:                 episode reward: -0.0078,                 loss: nan
Episode: 48941/50100 (97.6866%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0029s / 3792.1798 s
agent0:                 episode reward: -0.1217,                 loss: 0.4275
agent1:                 episode reward: 0.1217,                 loss: nan
Episode: 48961/50100 (97.7265%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0212s / 3794.2010 s
agent0:                 episode reward: 0.2544,                 loss: 0.4272
agent1:                 episode reward: -0.2544,                 loss: nan
Episode: 48981/50100 (97.7665%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0109s / 3796.2119 s
agent0:                 episode reward: 0.2915,                 loss: 0.4272
agent1:                 episode reward: -0.2915,                 loss: nan
Episode: 49001/50100 (97.8064%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9928s / 3798.2048 s
agent0:                 episode reward: 0.3469,                 loss: 0.4284
agent1:                 episode reward: -0.3469,                 loss: nan
Episode: 49021/50100 (97.8463%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0008s / 3800.2056 s
agent0:                 episode reward: 0.0344,                 loss: 0.4292
agent1:                 episode reward: -0.0344,                 loss: nan
Episode: 49041/50100 (97.8862%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0081s / 3802.2136 s
agent0:                 episode reward: 0.0182,                 loss: 0.4325
agent1:                 episode reward: -0.0182,                 loss: nan
Episode: 49061/50100 (97.9261%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0056s / 3804.2193 s
agent0:                 episode reward: -0.3796,                 loss: 0.4285
agent1:                 episode reward: 0.3796,                 loss: nan
Episode: 49081/50100 (97.9661%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0121s / 3806.2314 s
agent0:                 episode reward: 0.0982,                 loss: 0.4287
agent1:                 episode reward: -0.0982,                 loss: nan
Episode: 49101/50100 (98.0060%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0052s / 3808.2366 s
agent0:                 episode reward: -0.1774,                 loss: 0.4295
agent1:                 episode reward: 0.1774,                 loss: nan
Episode: 49121/50100 (98.0459%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9939s / 3810.2305 s
agent0:                 episode reward: 0.2242,                 loss: 0.4284
agent1:                 episode reward: -0.2242,                 loss: nan
Episode: 49141/50100 (98.0858%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9946s / 3812.2251 s
agent0:                 episode reward: -0.0643,                 loss: 0.4279
agent1:                 episode reward: 0.0643,                 loss: nan
Episode: 49161/50100 (98.1257%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9956s / 3814.2207 s
agent0:                 episode reward: -0.5376,                 loss: 0.4277
agent1:                 episode reward: 0.5376,                 loss: nan
Episode: 49181/50100 (98.1657%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0042s / 3816.2249 s
agent0:                 episode reward: 0.4468,                 loss: 0.4260
agent1:                 episode reward: -0.4468,                 loss: nan
Episode: 49201/50100 (98.2056%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9968s / 3818.2217 s
agent0:                 episode reward: -0.2533,                 loss: 0.4229
agent1:                 episode reward: 0.2533,                 loss: nan
Episode: 49221/50100 (98.2455%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0024s / 3820.2241 s
agent0:                 episode reward: 0.0875,                 loss: 0.4192
agent1:                 episode reward: -0.0875,                 loss: nan
Episode: 49241/50100 (98.2854%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0001s / 3822.2242 s
agent0:                 episode reward: -0.1888,                 loss: 0.4177
agent1:                 episode reward: 0.1888,                 loss: nan
Episode: 49261/50100 (98.3253%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0029s / 3824.2271 s
agent0:                 episode reward: -0.1992,                 loss: 0.4178
agent1:                 episode reward: 0.1992,                 loss: nan
Episode: 49281/50100 (98.3653%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0153s / 3826.2423 s
agent0:                 episode reward: 0.1738,                 loss: 0.4177
agent1:                 episode reward: -0.1738,                 loss: nan
Episode: 49301/50100 (98.4052%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0073s / 3828.2496 s
agent0:                 episode reward: -0.2631,                 loss: 0.4193
agent1:                 episode reward: 0.2631,                 loss: nan
Episode: 49321/50100 (98.4451%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9412s / 3830.1908 s
agent0:                 episode reward: 0.4153,                 loss: 0.4181
agent1:                 episode reward: -0.4153,                 loss: 0.4518
Score delta: 2.242094341320102, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/49100_0.
Episode: 49341/50100 (98.4850%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6450s / 3831.8358 s
agent0:                 episode reward: -0.4450,                 loss: nan
agent1:                 episode reward: 0.4450,                 loss: 0.4511
Episode: 49361/50100 (98.5250%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6576s / 3833.4934 s
agent0:                 episode reward: -0.1985,                 loss: nan
agent1:                 episode reward: 0.1985,                 loss: 0.4505
Episode: 49381/50100 (98.5649%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6416s / 3835.1350 s
agent0:                 episode reward: -0.0475,                 loss: nan
agent1:                 episode reward: 0.0475,                 loss: 0.4493
Episode: 49401/50100 (98.6048%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6524s / 3836.7874 s
agent0:                 episode reward: -1.0764,                 loss: nan
agent1:                 episode reward: 1.0764,                 loss: 0.4479
Episode: 49421/50100 (98.6447%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6549s / 3838.4424 s
agent0:                 episode reward: -0.7316,                 loss: nan
agent1:                 episode reward: 0.7316,                 loss: 0.4492
Episode: 49441/50100 (98.6846%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8220s / 3840.2644 s
agent0:                 episode reward: -0.8734,                 loss: 0.4374
agent1:                 episode reward: 0.8734,                 loss: 0.4483
Score delta: 2.4609921994080124, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/49220_1.
Episode: 49461/50100 (98.7246%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9906s / 3842.2549 s
agent0:                 episode reward: 0.0263,                 loss: 0.4371
agent1:                 episode reward: -0.0263,                 loss: nan
Episode: 49481/50100 (98.7645%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0068s / 3844.2617 s
agent0:                 episode reward: 0.1558,                 loss: 0.4334
agent1:                 episode reward: -0.1558,                 loss: nan
Episode: 49501/50100 (98.8044%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0012s / 3846.2630 s
agent0:                 episode reward: 0.1450,                 loss: 0.4241
agent1:                 episode reward: -0.1450,                 loss: nan
Episode: 49521/50100 (98.8443%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0080s / 3848.2709 s
agent0:                 episode reward: 0.5409,                 loss: 0.4221
agent1:                 episode reward: -0.5409,                 loss: nan
Episode: 49541/50100 (98.8842%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9989s / 3850.2699 s
agent0:                 episode reward: 0.0016,                 loss: 0.4224
agent1:                 episode reward: -0.0016,                 loss: nan
Episode: 49561/50100 (98.9242%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0043s / 3852.2741 s
agent0:                 episode reward: -0.4610,                 loss: 0.4226
agent1:                 episode reward: 0.4610,                 loss: nan
Episode: 49581/50100 (98.9641%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9691s / 3854.2432 s
agent0:                 episode reward: 0.4323,                 loss: 0.4222
agent1:                 episode reward: -0.4323,                 loss: 0.4533
Score delta: 2.1748050954050036, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/49366_0.
Episode: 49601/50100 (99.0040%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6684s / 3855.9116 s
agent0:                 episode reward: -0.1981,                 loss: nan
agent1:                 episode reward: 0.1981,                 loss: 0.4537
Episode: 49621/50100 (99.0439%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6850s / 3857.5967 s
agent0:                 episode reward: -0.3306,                 loss: nan
agent1:                 episode reward: 0.3306,                 loss: 0.4533
Episode: 49641/50100 (99.0838%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6680s / 3859.2646 s
agent0:                 episode reward: -0.6041,                 loss: nan
agent1:                 episode reward: 0.6041,                 loss: 0.4527
Episode: 49661/50100 (99.1238%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6682s / 3860.9329 s
agent0:                 episode reward: -0.4107,                 loss: nan
agent1:                 episode reward: 0.4107,                 loss: 0.4514
Episode: 49681/50100 (99.1637%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6784s / 3862.6112 s
agent0:                 episode reward: -0.2720,                 loss: nan
agent1:                 episode reward: 0.2720,                 loss: 0.4503
Episode: 49701/50100 (99.2036%),                 avg. length: 5.0,                last time consumption/overall running time: 1.8548s / 3864.4660 s
agent0:                 episode reward: -0.9993,                 loss: 0.4121
agent1:                 episode reward: 0.9993,                 loss: 0.4498
Score delta: 2.304155541840303, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/49478_1.
Episode: 49721/50100 (99.2435%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0101s / 3866.4761 s
agent0:                 episode reward: 0.4933,                 loss: 0.4111
agent1:                 episode reward: -0.4933,                 loss: nan
Episode: 49741/50100 (99.2834%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9929s / 3868.4691 s
agent0:                 episode reward: -0.2159,                 loss: 0.4106
agent1:                 episode reward: 0.2159,                 loss: nan
Episode: 49761/50100 (99.3234%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0062s / 3870.4753 s
agent0:                 episode reward: -0.0104,                 loss: 0.4017/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr = np.asanyarray(arr)

agent1:                 episode reward: 0.0104,                 loss: nan
Episode: 49781/50100 (99.3633%),                 avg. length: 5.0,                last time consumption/overall running time: 1.9967s / 3872.4719 s
agent0:                 episode reward: 0.0772,                 loss: 0.3836
agent1:                 episode reward: -0.0772,                 loss: nan
Episode: 49801/50100 (99.4032%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0016s / 3874.4735 s
agent0:                 episode reward: -0.2281,                 loss: 0.3830
agent1:                 episode reward: 0.2281,                 loss: nan
Episode: 49821/50100 (99.4431%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0037s / 3876.4772 s
agent0:                 episode reward: -0.0768,                 loss: 0.3820
agent1:                 episode reward: 0.0768,                 loss: nan
Episode: 49841/50100 (99.4830%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0062s / 3878.4833 s
agent0:                 episode reward: 0.0940,                 loss: 0.3809
agent1:                 episode reward: -0.0940,                 loss: nan
Episode: 49861/50100 (99.5230%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0092s / 3880.4925 s
agent0:                 episode reward: 0.1397,                 loss: 0.3818
agent1:                 episode reward: -0.1397,                 loss: nan
Episode: 49881/50100 (99.5629%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0094s / 3882.5019 s
agent0:                 episode reward: -0.0234,                 loss: 0.3821
agent1:                 episode reward: 0.0234,                 loss: nan
Episode: 49901/50100 (99.6028%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0009s / 3884.5028 s
agent0:                 episode reward: -0.3140,                 loss: 0.3827
agent1:                 episode reward: 0.3140,                 loss: nan
Episode: 49921/50100 (99.6427%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0037s / 3886.5064 s
agent0:                 episode reward: -0.7877,                 loss: 0.3848
agent1:                 episode reward: 0.7877,                 loss: nan
Episode: 49941/50100 (99.6826%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0040s / 3888.5105 s
agent0:                 episode reward: 0.4203,                 loss: 0.4216
agent1:                 episode reward: -0.4203,                 loss: nan
Episode: 49961/50100 (99.7226%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0050s / 3890.5154 s
agent0:                 episode reward: 0.0632,                 loss: 0.4182
agent1:                 episode reward: -0.0632,                 loss: nan
Episode: 49981/50100 (99.7625%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0140s / 3892.5294 s
agent0:                 episode reward: 0.2060,                 loss: 0.4194
agent1:                 episode reward: -0.2060,                 loss: nan
Episode: 50001/50100 (99.8024%),                 avg. length: 5.0,                last time consumption/overall running time: 2.0056s / 3894.5350 s
agent0:                 episode reward: 0.0705,                 loss: 0.4180
agent1:                 episode reward: -0.0705,                 loss: nan
Episode: 50021/50100 (99.8423%),                 avg. length: 5.0,                last time consumption/overall running time: 1.7607s / 3896.2957 s
agent0:                 episode reward: -0.1403,                 loss: 0.4137
agent1:                 episode reward: 0.1403,                 loss: 0.4538
Score delta: 2.03192106561126, save the model to .//data/model/20220516172129/mdp_arbitrary_mdp_fictitious_selfplay2/49793_0.
Episode: 50041/50100 (99.8822%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6857s / 3897.9813 s
agent0:                 episode reward: -0.4936,                 loss: nan
agent1:                 episode reward: 0.4936,                 loss: 0.4520
Episode: 50061/50100 (99.9222%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6859s / 3899.6672 s
agent0:                 episode reward: 0.0258,                 loss: nan
agent1:                 episode reward: -0.0258,                 loss: 0.4524
Episode: 50081/50100 (99.9621%),                 avg. length: 5.0,                last time consumption/overall running time: 1.6870s / 3901.3543 s
agent0:                 episode reward: -0.6880,                 loss: nan
agent1:                 episode reward: 0.6880,                 loss: 0.4521
