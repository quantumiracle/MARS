pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220128_0331/pettingzoo_double_dunk_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220128_0331/pettingzoo_double_dunk_v2_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 4891.0,                last time consumption/overall running time: 28.2104s / 28.2104 s
env0_first_0:                 episode reward: -28.0000,                 loss: -0.0812
env0_second_0:                 episode reward: 28.0000,                 loss: -0.0913
env1_first_0:                 episode reward: -45.0000,                 loss: nan
env1_second_0:                 episode reward: 45.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 3766.15,                last time consumption/overall running time: 415.5274s / 443.7378 s
env0_first_0:                 episode reward: -24.4000,                 loss: -0.0395
env0_second_0:                 episode reward: 24.4000,                 loss: -0.0397
env1_first_0:                 episode reward: -24.4000,                 loss: nan
env1_second_0:                 episode reward: 24.4000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 4061.0,                last time consumption/overall running time: 452.6507s / 896.3886 s
env0_first_0:                 episode reward: -27.7000,                 loss: 0.0166
env0_second_0:                 episode reward: 27.7000,                 loss: 0.0187
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 3582.0,                last time consumption/overall running time: 398.1596s / 1294.5481 s
env0_first_0:                 episode reward: -21.7000,                 loss: 0.0570
env0_second_0:                 episode reward: 21.7000,                 loss: 0.0588
env1_first_0:                 episode reward: -22.6500,                 loss: nan
env1_second_0:                 episode reward: 22.6500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 3659.1,                last time consumption/overall running time: 407.1260s / 1701.6741 s
env0_first_0:                 episode reward: -24.8500,                 loss: 0.0670
env0_second_0:                 episode reward: 24.8500,                 loss: 0.0661
env1_first_0:                 episode reward: -22.1000,                 loss: nan
env1_second_0:                 episode reward: 22.1000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 3915.45,                last time consumption/overall running time: 433.8784s / 2135.5525 s
env0_first_0:                 episode reward: -22.5000,                 loss: 0.0684
env0_second_0:                 episode reward: 22.5000,                 loss: 0.0699
env1_first_0:                 episode reward: -24.9500,                 loss: nan
env1_second_0:                 episode reward: 24.9500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 3862.8,                last time consumption/overall running time: 432.3434s / 2567.8959 s
env0_first_0:                 episode reward: -25.3500,                 loss: 0.0668
env0_second_0:                 episode reward: 25.3500,                 loss: 0.0678
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 3865.4,                last time consumption/overall running time: 432.3551s / 3000.2510 s
env0_first_0:                 episode reward: -23.4500,                 loss: 0.0728
env0_second_0:                 episode reward: 23.4500,                 loss: 0.0711
env1_first_0:                 episode reward: -21.1000,                 loss: nan
env1_second_0:                 episode reward: 21.1000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 4022.2,                last time consumption/overall running time: 451.1518s / 3451.4028 s
env0_first_0:                 episode reward: -22.9500,                 loss: 0.0685
env0_second_0:                 episode reward: 22.9500,                 loss: 0.0683
env1_first_0:                 episode reward: -23.3500,                 loss: nan
env1_second_0:                 episode reward: 23.3500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 4321.2,                last time consumption/overall running time: 493.8800s / 3945.2827 s
env0_first_0:                 episode reward: -30.0500,                 loss: 0.0831
env0_second_0:                 episode reward: 30.0500,                 loss: 0.0856
env1_first_0:                 episode reward: -28.7000,                 loss: nan
env1_second_0:                 episode reward: 28.7000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 4068.7,                last time consumption/overall running time: 465.7876s / 4411.0703 s
env0_first_0:                 episode reward: -26.5000,                 loss: 0.0779
env0_second_0:                 episode reward: 26.5000,                 loss: 0.0830
env1_first_0:                 episode reward: -25.6500,                 loss: nan
env1_second_0:                 episode reward: 25.6500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 4308.25,                last time consumption/overall running time: 500.2219s / 4911.2922 s
env0_first_0:                 episode reward: -24.8000,                 loss: 0.0551
env0_second_0:                 episode reward: 24.8000,                 loss: 0.0551
env1_first_0:                 episode reward: -27.9500,                 loss: nan
env1_second_0:                 episode reward: 27.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 4055.2,                last time consumption/overall running time: 461.7491s / 5373.0413 s
env0_first_0:                 episode reward: -23.6000,                 loss: 0.0505
env0_second_0:                 episode reward: 23.6000,                 loss: 0.0522
env1_first_0:                 episode reward: -26.2500,                 loss: nan
env1_second_0:                 episode reward: 26.2500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 4141.4,                last time consumption/overall running time: 471.5023s / 5844.5436 s
env0_first_0:                 episode reward: -31.3000,                 loss: 0.0316
env0_second_0:                 episode reward: 31.3000,                 loss: 0.0355
env1_first_0:                 episode reward: -28.2000,                 loss: nan
env1_second_0:                 episode reward: 28.2000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 4399.85,                last time consumption/overall running time: 499.9002s / 6344.4438 s
env0_first_0:                 episode reward: -31.7000,                 loss: 0.0301
env0_second_0:                 episode reward: 31.7000,                 loss: 0.0378
env1_first_0:                 episode reward: -30.0500,                 loss: nan
env1_second_0:                 episode reward: 30.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 4314.85,                last time consumption/overall running time: 483.9838s / 6828.4276 s
env0_first_0:                 episode reward: -32.1500,                 loss: -0.0006
env0_second_0:                 episode reward: 32.1500,                 loss: 0.0097
env1_first_0:                 episode reward: -30.3500,                 loss: nan
env1_second_0:                 episode reward: 30.3500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 4143.2,                last time consumption/overall running time: 464.5084s / 7292.9360 s
env0_first_0:                 episode reward: -22.9000,                 loss: -0.1519
env0_second_0:                 episode reward: 22.9000,                 loss: -0.1437
env1_first_0:                 episode reward: -26.8500,                 loss: nan
env1_second_0:                 episode reward: 26.8500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 3962.2,                last time consumption/overall running time: 446.7142s / 7739.6502 s
env0_first_0:                 episode reward: -17.6000,                 loss: -0.1929
env0_second_0:                 episode reward: 17.6000,                 loss: -0.1855
env1_first_0:                 episode reward: -21.0500,                 loss: nan
env1_second_0:                 episode reward: 21.0500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 3771.65,                last time consumption/overall running time: 425.9576s / 8165.6078 s
env0_first_0:                 episode reward: -13.9000,                 loss: -0.1769
env0_second_0:                 episode reward: 13.9000,                 loss: -0.1638
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 3798.85,                last time consumption/overall running time: 431.5957s / 8597.2035 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.1658
env0_second_0:                 episode reward: 8.8500,                 loss: -0.1561
env1_first_0:                 episode reward: -10.3500,                 loss: nan
env1_second_0:                 episode reward: 10.3500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 3849.9,                last time consumption/overall running time: 431.0847s / 9028.2882 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.1735
env0_second_0:                 episode reward: 5.1000,                 loss: -0.1656
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 3974.5,                last time consumption/overall running time: 449.5125s / 9477.8007 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.1839
env0_second_0:                 episode reward: 2.9500,                 loss: -0.1778
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 3687.8,                last time consumption/overall running time: 415.4224s / 9893.2231 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1809
env0_second_0:                 episode reward: -2.5500,                 loss: -0.1682
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 3681.95,                last time consumption/overall running time: 410.3102s / 10303.5333 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1869
env0_second_0:                 episode reward: -3.5500,                 loss: -0.1765
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 3331.75,                last time consumption/overall running time: 377.7088s / 10681.2422 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1996
env0_second_0:                 episode reward: -7.0000,                 loss: -0.1963
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 3318.1,                last time consumption/overall running time: 369.7314s / 11050.9736 s
env0_first_0:                 episode reward: 10.8500,                 loss: -0.1894
env0_second_0:                 episode reward: -10.8500,                 loss: -0.1870
env1_first_0:                 episode reward: 11.8500,                 loss: nan
env1_second_0:                 episode reward: -11.8500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2831.35,                last time consumption/overall running time: 316.4966s / 11367.4702 s
env0_first_0:                 episode reward: 15.1500,                 loss: -0.2052
env0_second_0:                 episode reward: -15.1500,                 loss: -0.2040
env1_first_0:                 episode reward: 14.4500,                 loss: nan
env1_second_0:                 episode reward: -14.4500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2714.95,                last time consumption/overall running time: 308.1218s / 11675.5920 s
env0_first_0:                 episode reward: 14.8000,                 loss: -0.2139
env0_second_0:                 episode reward: -14.8000,                 loss: -0.2053
env1_first_0:                 episode reward: 14.7000,                 loss: nan
env1_second_0:                 episode reward: -14.7000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2836.4,                last time consumption/overall running time: 315.2130s / 11990.8050 s
env0_first_0:                 episode reward: 12.4000,                 loss: -0.2007
env0_second_0:                 episode reward: -12.4000,                 loss: -0.1864
env1_first_0:                 episode reward: 14.8000,                 loss: nan
env1_second_0:                 episode reward: -14.8000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2719.75,                last time consumption/overall running time: 303.5652s / 12294.3702 s
env0_first_0:                 episode reward: 12.9000,                 loss: -0.2127
env0_second_0:                 episode reward: -12.9000,                 loss: -0.2026
env1_first_0:                 episode reward: 14.7000,                 loss: nan
env1_second_0:                 episode reward: -14.7000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2713.95,                last time consumption/overall running time: 309.0791s / 12603.4493 s
env0_first_0:                 episode reward: 15.2500,                 loss: -0.2271
env0_second_0:                 episode reward: -15.2500,                 loss: -0.2221
env1_first_0:                 episode reward: 16.0500,                 loss: nan
env1_second_0:                 episode reward: -16.0500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2780.85,                last time consumption/overall running time: 312.8008s / 12916.2501 s
env0_first_0:                 episode reward: 16.2500,                 loss: -0.2306
env0_second_0:                 episode reward: -16.2500,                 loss: -0.2223
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2562.45,                last time consumption/overall running time: 286.6370s / 13202.8871 s
env0_first_0:                 episode reward: 17.0000,                 loss: -0.2347
env0_second_0:                 episode reward: -17.0000,                 loss: -0.2295
env1_first_0:                 episode reward: 15.5500,                 loss: nan
env1_second_0:                 episode reward: -15.5500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2504.7,                last time consumption/overall running time: 280.8793s / 13483.7664 s
env0_first_0:                 episode reward: 14.9500,                 loss: -0.2279
env0_second_0:                 episode reward: -14.9500,                 loss: -0.2158
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2630.2,                last time consumption/overall running time: 298.2534s / 13782.0198 s
env0_first_0:                 episode reward: 17.1000,                 loss: -0.2299
env0_second_0:                 episode reward: -17.1000,                 loss: -0.2210
env1_first_0:                 episode reward: 16.1000,                 loss: nan
env1_second_0:                 episode reward: -16.1000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2505.55,                last time consumption/overall running time: 284.1560s / 14066.1758 s
env0_first_0:                 episode reward: 17.1500,                 loss: -0.2436
env0_second_0:                 episode reward: -17.1500,                 loss: -0.2317
env1_first_0:                 episode reward: 17.6500,                 loss: nan
env1_second_0:                 episode reward: -17.6500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2464.9,                last time consumption/overall running time: 274.1024s / 14340.2783 s
env0_first_0:                 episode reward: 15.4000,                 loss: -0.2248
env0_second_0:                 episode reward: -15.4000,                 loss: -0.2114
env1_first_0:                 episode reward: 16.5000,                 loss: nan
env1_second_0:                 episode reward: -16.5000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2435.0,                last time consumption/overall running time: 270.6555s / 14610.9338 s
env0_first_0:                 episode reward: 16.7000,                 loss: -0.2157
env0_second_0:                 episode reward: -16.7000,                 loss: -0.2018
env1_first_0:                 episode reward: 16.6500,                 loss: nan
env1_second_0:                 episode reward: -16.6500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2539.5,                last time consumption/overall running time: 284.9351s / 14895.8689 s
env0_first_0:                 episode reward: 16.9000,                 loss: -0.2054
env0_second_0:                 episode reward: -16.9000,                 loss: -0.1935
env1_first_0:                 episode reward: 14.0500,                 loss: nan
env1_second_0:                 episode reward: -14.0500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2350.05,                last time consumption/overall running time: 270.3857s / 15166.2546 s
env0_first_0:                 episode reward: 16.0500,                 loss: -0.2299
env0_second_0:                 episode reward: -16.0500,                 loss: -0.2201
env1_first_0:                 episode reward: 13.6500,                 loss: nan
env1_second_0:                 episode reward: -13.6500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2444.85,                last time consumption/overall running time: 271.0599s / 15437.3146 s
env0_first_0:                 episode reward: 16.7000,                 loss: -0.2407
env0_second_0:                 episode reward: -16.7000,                 loss: -0.2311
env1_first_0:                 episode reward: 13.3500,                 loss: nan
env1_second_0:                 episode reward: -13.3500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2483.3,                last time consumption/overall running time: 278.1222s / 15715.4368 s
env0_first_0:                 episode reward: 15.1000,                 loss: -0.2354
env0_second_0:                 episode reward: -15.1000,                 loss: -0.2204
env1_first_0:                 episode reward: 16.4000,                 loss: nan
env1_second_0:                 episode reward: -16.4000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2682.1,                last time consumption/overall running time: 303.8306s / 16019.2674 s
env0_first_0:                 episode reward: 11.2000,                 loss: -0.1893
env0_second_0:                 episode reward: -11.2000,                 loss: -0.1759
env1_first_0:                 episode reward: 8.3000,                 loss: nan
env1_second_0:                 episode reward: -8.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2322.4,                last time consumption/overall running time: 264.1066s / 16283.3740 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.1580
env0_second_0:                 episode reward: -9.9000,                 loss: -0.1439
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2346.85,                last time consumption/overall running time: 261.9969s / 16545.3709 s
env0_first_0:                 episode reward: 13.3500,                 loss: -0.2128
env0_second_0:                 episode reward: -13.3500,                 loss: -0.2057
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2386.8,                last time consumption/overall running time: 269.2804s / 16814.6513 s
env0_first_0:                 episode reward: 15.6000,                 loss: -0.2321
env0_second_0:                 episode reward: -15.6000,                 loss: -0.2143
env1_first_0:                 episode reward: 17.1500,                 loss: nan
env1_second_0:                 episode reward: -17.1500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2532.3,                last time consumption/overall running time: 282.0070s / 17096.6583 s
env0_first_0:                 episode reward: 17.2500,                 loss: -0.2345
env0_second_0:                 episode reward: -17.2500,                 loss: -0.2167
env1_first_0:                 episode reward: 16.5500,                 loss: nan
env1_second_0:                 episode reward: -16.5500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2511.75,                last time consumption/overall running time: 279.5507s / 17376.2091 s
env0_first_0:                 episode reward: 17.9000,                 loss: -0.2483
env0_second_0:                 episode reward: -17.9000,                 loss: -0.2376
env1_first_0:                 episode reward: 16.4000,                 loss: nan
env1_second_0:                 episode reward: -16.4000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2526.3,                last time consumption/overall running time: 281.9466s / 17658.1556 s
env0_first_0:                 episode reward: 19.2500,                 loss: -0.2248
env0_second_0:                 episode reward: -19.2500,                 loss: -0.2108
env1_first_0:                 episode reward: 18.1000,                 loss: nan
env1_second_0:                 episode reward: -18.1000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2315.8,                last time consumption/overall running time: 258.5791s / 17916.7347 s
env0_first_0:                 episode reward: 16.6000,                 loss: -0.2135
env0_second_0:                 episode reward: -16.6000,                 loss: -0.1999
env1_first_0:                 episode reward: 17.9000,                 loss: nan
env1_second_0:                 episode reward: -17.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2350.3,                last time consumption/overall running time: 261.8660s / 18178.6008 s
env0_first_0:                 episode reward: 13.9500,                 loss: -0.2196
env0_second_0:                 episode reward: -13.9500,                 loss: -0.2014
env1_first_0:                 episode reward: 17.4000,                 loss: nan
env1_second_0:                 episode reward: -17.4000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2378.2,                last time consumption/overall running time: 263.9506s / 18442.5513 s
env0_first_0:                 episode reward: 18.2000,                 loss: -0.2291
env0_second_0:                 episode reward: -18.2000,                 loss: -0.2108
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2618.1,                last time consumption/overall running time: 304.1437s / 18746.6950 s
env0_first_0:                 episode reward: 16.4500,                 loss: -0.2464
env0_second_0:                 episode reward: -16.4500,                 loss: -0.2257
env1_first_0:                 episode reward: 16.4500,                 loss: nan
env1_second_0:                 episode reward: -16.4500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2386.8,                last time consumption/overall running time: 267.0137s / 19013.7087 s
env0_first_0:                 episode reward: 16.8000,                 loss: -0.2312
env0_second_0:                 episode reward: -16.8000,                 loss: -0.2074
env1_first_0:                 episode reward: 17.3000,                 loss: nan
env1_second_0:                 episode reward: -17.3000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2673.15,                last time consumption/overall running time: 298.2270s / 19311.9358 s
env0_first_0:                 episode reward: 16.1000,                 loss: -0.2022
env0_second_0:                 episode reward: -16.1000,                 loss: -0.1828
env1_first_0:                 episode reward: 17.3000,                 loss: nan
env1_second_0:                 episode reward: -17.3000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2640.2,                last time consumption/overall running time: 287.4802s / 19599.4160 s
env0_first_0:                 episode reward: 17.1000,                 loss: -0.2237
env0_second_0:                 episode reward: -17.1000,                 loss: -0.2133
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2567.65,                last time consumption/overall running time: 285.0087s / 19884.4247 s
env0_first_0:                 episode reward: 16.5000,                 loss: -0.2129
env0_second_0:                 episode reward: -16.5000,                 loss: -0.1912
env1_first_0:                 episode reward: 16.5500,                 loss: nan
env1_second_0:                 episode reward: -16.5500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2509.55,                last time consumption/overall running time: 281.9007s / 20166.3254 s
env0_first_0:                 episode reward: 16.0000,                 loss: -0.2128
env0_second_0:                 episode reward: -16.0000,                 loss: -0.1832
env1_first_0:                 episode reward: 16.3000,                 loss: nan
env1_second_0:                 episode reward: -16.3000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2481.75,                last time consumption/overall running time: 276.4658s / 20442.7912 s
env0_first_0:                 episode reward: 16.8500,                 loss: -0.1726
env0_second_0:                 episode reward: -16.8500,                 loss: -0.1477
env1_first_0:                 episode reward: 15.0000,                 loss: nan
env1_second_0:                 episode reward: -15.0000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2454.4,                last time consumption/overall running time: 272.2127s / 20715.0039 s
env0_first_0:                 episode reward: 13.6500,                 loss: -0.1690
env0_second_0:                 episode reward: -13.6500,                 loss: -0.1507
env1_first_0:                 episode reward: 15.5500,                 loss: nan
env1_second_0:                 episode reward: -15.5500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2427.35,                last time consumption/overall running time: 270.9875s / 20985.9914 s
env0_first_0:                 episode reward: 13.1000,                 loss: -0.1779
env0_second_0:                 episode reward: -13.1000,                 loss: -0.1559
env1_first_0:                 episode reward: 13.5500,                 loss: nan
env1_second_0:                 episode reward: -13.5500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2450.85,                last time consumption/overall running time: 274.4021s / 21260.3935 s
env0_first_0:                 episode reward: 16.8500,                 loss: -0.1846
env0_second_0:                 episode reward: -16.8500,                 loss: -0.1636
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2484.2,                last time consumption/overall running time: 280.5025s / 21540.8960 s
env0_first_0:                 episode reward: 16.9000,                 loss: -0.1789
env0_second_0:                 episode reward: -16.9000,                 loss: -0.1481
env1_first_0:                 episode reward: 14.8500,                 loss: nan
env1_second_0:                 episode reward: -14.8500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2507.1,                last time consumption/overall running time: 280.7328s / 21821.6288 s
env0_first_0:                 episode reward: 14.3000,                 loss: -0.1786
env0_second_0:                 episode reward: -14.3000,                 loss: -0.1494
env1_first_0:                 episode reward: 13.5000,                 loss: nan
env1_second_0:                 episode reward: -13.5000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 2398.8,                last time consumption/overall running time: 269.5734s / 22091.2021 s
env0_first_0:                 episode reward: 13.6000,                 loss: -0.2050
env0_second_0:                 episode reward: -13.6000,                 loss: -0.1798
env1_first_0:                 episode reward: 12.5500,                 loss: nan
env1_second_0:                 episode reward: -12.5500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2039.2,                last time consumption/overall running time: 230.4320s / 22321.6341 s
env0_first_0:                 episode reward: 11.8500,                 loss: -0.1887
env0_second_0:                 episode reward: -11.8500,                 loss: -0.1701
env1_first_0:                 episode reward: 10.6000,                 loss: nan
env1_second_0:                 episode reward: -10.6000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 2051.75,                last time consumption/overall running time: 233.2829s / 22554.9170 s
env0_first_0:                 episode reward: 12.2500,                 loss: -0.1950
env0_second_0:                 episode reward: -12.2500,                 loss: -0.1572
env1_first_0:                 episode reward: 10.4000,                 loss: nan
env1_second_0:                 episode reward: -10.4000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 2062.5,                last time consumption/overall running time: 233.4490s / 22788.3660 s
env0_first_0:                 episode reward: 13.6000,                 loss: -0.1886
env0_second_0:                 episode reward: -13.6000,                 loss: -0.1533
env1_first_0:                 episode reward: 14.6500,                 loss: nan
env1_second_0:                 episode reward: -14.6500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 2026.9,                last time consumption/overall running time: 229.3180s / 23017.6840 s
env0_first_0:                 episode reward: 12.2000,                 loss: -0.1841
env0_second_0:                 episode reward: -12.2000,                 loss: -0.1499
env1_first_0:                 episode reward: 12.2000,                 loss: nan
env1_second_0:                 episode reward: -12.2000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2073.85,                last time consumption/overall running time: 230.3214s / 23248.0054 s
env0_first_0:                 episode reward: 11.2000,                 loss: -0.1746
env0_second_0:                 episode reward: -11.2000,                 loss: -0.1357
env1_first_0:                 episode reward: 13.3000,                 loss: nan
env1_second_0:                 episode reward: -13.3000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2106.0,                last time consumption/overall running time: 234.7651s / 23482.7705 s
env0_first_0:                 episode reward: 11.1500,                 loss: -0.1719
env0_second_0:                 episode reward: -11.1500,                 loss: -0.1332
env1_first_0:                 episode reward: 12.3500,                 loss: nan
env1_second_0:                 episode reward: -12.3500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2079.95,                last time consumption/overall running time: 231.6641s / 23714.4346 s
env0_first_0:                 episode reward: 11.5500,                 loss: -0.1977
env0_second_0:                 episode reward: -11.5500,                 loss: -0.1653
env1_first_0:                 episode reward: 11.3500,                 loss: nan
env1_second_0:                 episode reward: -11.3500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2005.45,                last time consumption/overall running time: 227.0385s / 23941.4731 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1806
env0_second_0:                 episode reward: -10.0000,                 loss: -0.1397
env1_first_0:                 episode reward: 11.9500,                 loss: nan
env1_second_0:                 episode reward: -11.9500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1983.55,                last time consumption/overall running time: 227.2385s / 24168.7115 s
env0_first_0:                 episode reward: 12.9500,                 loss: -0.1835
env0_second_0:                 episode reward: -12.9500,                 loss: -0.1430
env1_first_0:                 episode reward: 11.2500,                 loss: nan
env1_second_0:                 episode reward: -11.2500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1892.2,                last time consumption/overall running time: 211.5007s / 24380.2123 s
env0_first_0:                 episode reward: 12.8500,                 loss: -0.1762
env0_second_0:                 episode reward: -12.8500,                 loss: -0.1311
env1_first_0:                 episode reward: 12.0500,                 loss: nan
env1_second_0:                 episode reward: -12.0500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1761.85,                last time consumption/overall running time: 194.8154s / 24575.0277 s
env0_first_0:                 episode reward: 10.5000,                 loss: -0.1713
env0_second_0:                 episode reward: -10.5000,                 loss: -0.1309
env1_first_0:                 episode reward: 11.8000,                 loss: nan
env1_second_0:                 episode reward: -11.8000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1742.75,                last time consumption/overall running time: 195.0497s / 24770.0774 s
env0_first_0:                 episode reward: 6.2000,                 loss: -0.1964
env0_second_0:                 episode reward: -6.2000,                 loss: -0.1466
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1834.35,                last time consumption/overall running time: 205.8365s / 24975.9138 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.2017
env0_second_0:                 episode reward: -6.6000,                 loss: -0.1434
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1806.05,                last time consumption/overall running time: 204.7068s / 25180.6206 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1778
env0_second_0:                 episode reward: -5.1500,                 loss: -0.1258
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1963.5,                last time consumption/overall running time: 226.0652s / 25406.6859 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1563
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0958
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1910.25,                last time consumption/overall running time: 219.1477s / 25625.8336 s
env0_first_0:                 episode reward: 12.6000,                 loss: -0.1133
env0_second_0:                 episode reward: -12.6000,                 loss: -0.0335
env1_first_0:                 episode reward: 11.2500,                 loss: nan
env1_second_0:                 episode reward: -11.2500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1825.5,                last time consumption/overall running time: 211.4935s / 25837.3270 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1640
env0_second_0:                 episode reward: -3.4000,                 loss: -0.1048
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1792.4,                last time consumption/overall running time: 205.8567s / 26043.1838 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1660
env0_second_0:                 episode reward: -3.8500,                 loss: -0.1215
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1606.85,                last time consumption/overall running time: 185.4839s / 26228.6677 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1676
env0_second_0:                 episode reward: -3.2500,                 loss: -0.1148
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1748.05,                last time consumption/overall running time: 196.4275s / 26425.0953 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1527
env0_second_0:                 episode reward: -3.0500,                 loss: -0.1094
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1712.25,                last time consumption/overall running time: 196.6289s / 26621.7242 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1812
env0_second_0:                 episode reward: -3.2000,                 loss: -0.0476
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1772.05,                last time consumption/overall running time: 203.8500s / 26825.5742 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.1440
env0_second_0:                 episode reward: -4.7500,                 loss: -0.0899
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1811.05,                last time consumption/overall running time: 207.0890s / 27032.6632 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1824
env0_second_0:                 episode reward: -3.5500,                 loss: -0.1491
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1695.7,                last time consumption/overall running time: 193.4368s / 27226.1000 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1595
env0_second_0:                 episode reward: -2.8000,                 loss: -0.0816
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1722.05,                last time consumption/overall running time: 196.7003s / 27422.8004 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1819
env0_second_0:                 episode reward: -2.0500,                 loss: -0.1201
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1606.5,                last time consumption/overall running time: 179.6057s / 27602.4061 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1412
env0_second_0:                 episode reward: -1.9500,                 loss: -0.0708
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1618.85,                last time consumption/overall running time: 183.0871s / 27785.4931 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1470
env0_second_0:                 episode reward: -2.4000,                 loss: -0.0529
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1651.85,                last time consumption/overall running time: 187.0636s / 27972.5567 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1405
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0633
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1622.25,                last time consumption/overall running time: 188.7895s / 28161.3462 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1868
env0_second_0:                 episode reward: -1.5000,                 loss: -0.1358
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1613.65,                last time consumption/overall running time: 185.2134s / 28346.5597 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1727
env0_second_0:                 episode reward: -1.2000,                 loss: -0.0918
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1653.3,                last time consumption/overall running time: 187.5593s / 28534.1189 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1551
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1103
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1823.6,                last time consumption/overall running time: 201.3984s / 28735.5173 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1481
env0_second_0:                 episode reward: -5.0500,                 loss: -0.0699
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2208.4,                last time consumption/overall running time: 247.0130s / 28982.5302 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.1379
env0_second_0:                 episode reward: 7.4000,                 loss: -0.0031
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 5817.85,                last time consumption/overall running time: 631.0572s / 29613.5875 s
env0_first_0:                 episode reward: -23.9500,                 loss: -0.2241
env0_second_0:                 episode reward: 23.9500,                 loss: -0.1767
env1_first_0:                 episode reward: -22.6500,                 loss: nan
env1_second_0:                 episode reward: 22.6500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 6591.5,                last time consumption/overall running time: 707.4276s / 30321.0151 s
env0_first_0:                 episode reward: -19.1500,                 loss: -0.2390
env0_second_0:                 episode reward: 19.1500,                 loss: -0.2136
env1_first_0:                 episode reward: -23.0000,                 loss: nan
env1_second_0:                 episode reward: 23.0000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 9459.15,                last time consumption/overall running time: 1020.1170s / 31341.1320 s
env0_first_0:                 episode reward: -20.7000,                 loss: -0.2447
env0_second_0:                 episode reward: 20.7000,                 loss: -0.2210
env1_first_0:                 episode reward: -15.9500,                 loss: nan
env1_second_0:                 episode reward: 15.9500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 8766.35,                last time consumption/overall running time: 946.9045s / 32288.0366 s
env0_first_0:                 episode reward: -17.5500,                 loss: -0.2239
env0_second_0:                 episode reward: 17.5500,                 loss: -0.2139
env1_first_0:                 episode reward: -31.3000,                 loss: nan
env1_second_0:                 episode reward: 31.3000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 6460.1,                last time consumption/overall running time: 702.9479s / 32990.9845 s
env0_first_0:                 episode reward: -27.9500,                 loss: -0.2084
env0_second_0:                 episode reward: 27.9500,                 loss: -0.1781
env1_first_0:                 episode reward: -26.0000,                 loss: nan
env1_second_0:                 episode reward: 26.0000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 9355.2,                last time consumption/overall running time: 1011.5295s / 34002.5139 s
env0_first_0:                 episode reward: -41.9000,                 loss: -0.2003
env0_second_0:                 episode reward: 41.9000,                 loss: -0.1967
env1_first_0:                 episode reward: -35.1000,                 loss: nan
env1_second_0:                 episode reward: 35.1000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 9662.95,                last time consumption/overall running time: 1055.1861s / 35057.7000 s
env0_first_0:                 episode reward: -34.8000,                 loss: -0.2438
env0_second_0:                 episode reward: 34.8000,                 loss: -0.2379
env1_first_0:                 episode reward: -28.2500,                 loss: nan
env1_second_0:                 episode reward: 28.2500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 6250.4,                last time consumption/overall running time: 674.6454s / 35732.3455 s
env0_first_0:                 episode reward: -14.5000,                 loss: -0.2230
env0_second_0:                 episode reward: 14.5000,                 loss: -0.1860
env1_first_0:                 episode reward: -15.2000,                 loss: nan
env1_second_0:                 episode reward: 15.2000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1810.7,                last time consumption/overall running time: 197.6106s / 35929.9561 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.2210
env0_second_0:                 episode reward: -4.8000,                 loss: -0.1733
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1709.05,                last time consumption/overall running time: 191.3427s / 36121.2987 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.1938
env0_second_0:                 episode reward: -4.9000,                 loss: -0.1673
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1673.6,                last time consumption/overall running time: 188.1654s / 36309.4641 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.2248
env0_second_0:                 episode reward: -5.2000,                 loss: -0.1904
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1708.9,                last time consumption/overall running time: 189.6108s / 36499.0749 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.2042
env0_second_0:                 episode reward: -5.1500,                 loss: -0.1483
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1532.05,                last time consumption/overall running time: 171.7290s / 36670.8039 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2142
env0_second_0:                 episode reward: -2.3500,                 loss: -0.1518
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1561.3,                last time consumption/overall running time: 174.4617s / 36845.2656 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1950
env0_second_0:                 episode reward: -2.2000,                 loss: -0.1462
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1715.5,                last time consumption/overall running time: 188.8128s / 37034.0784 s
env0_first_0:                 episode reward: 7.5000,                 loss: -0.1965
env0_second_0:                 episode reward: -7.5000,                 loss: -0.1478
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1619.3,                last time consumption/overall running time: 181.4460s / 37215.5244 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.2024
env0_second_0:                 episode reward: -5.3000,                 loss: -0.1531
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1625.05,                last time consumption/overall running time: 183.1767s / 37398.7011 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.1873
env0_second_0:                 episode reward: -6.7500,                 loss: -0.1691
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1505.55,                last time consumption/overall running time: 168.5227s / 37567.2238 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2183
env0_second_0:                 episode reward: -1.9500,                 loss: -0.1366
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1502.65,                last time consumption/overall running time: 168.7295s / 37735.9533 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2108
env0_second_0:                 episode reward: -1.8000,                 loss: -0.1700
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1529.7,                last time consumption/overall running time: 173.9762s / 37909.9295 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1814
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1327
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1497.4,                last time consumption/overall running time: 165.8271s / 38075.7566 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2101
env0_second_0:                 episode reward: -1.8000,                 loss: -0.1833
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1554.75,                last time consumption/overall running time: 177.0159s / 38252.7726 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2078
env0_second_0:                 episode reward: -3.3500,                 loss: -0.1153
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1530.55,                last time consumption/overall running time: 172.3463s / 38425.1189 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2143
env0_second_0:                 episode reward: -1.8500,                 loss: -0.1785
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1555.4,                last time consumption/overall running time: 175.7440s / 38600.8629 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.2064
env0_second_0:                 episode reward: -2.9500,                 loss: -0.1819
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1524.1,                last time consumption/overall running time: 171.9947s / 38772.8576 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2296
env0_second_0:                 episode reward: -2.3000,                 loss: -0.1921
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1574.1,                last time consumption/overall running time: 176.6627s / 38949.5203 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.2125
env0_second_0:                 episode reward: -3.0000,                 loss: -0.1862
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1507.45,                last time consumption/overall running time: 175.7852s / 39125.3055 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1821
env0_second_0:                 episode reward: -1.5000,                 loss: -0.1347
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1571.9,                last time consumption/overall running time: 176.6367s / 39301.9422 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1864
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1074
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1543.15,                last time consumption/overall running time: 171.0888s / 39473.0310 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1327
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0396
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1471.3,                last time consumption/overall running time: 165.0772s / 39638.1082 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1862
env0_second_0:                 episode reward: -0.9000,                 loss: 0.1853
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1515.25,                last time consumption/overall running time: 168.9746s / 39807.0829 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1654
env0_second_0:                 episode reward: -1.1500,                 loss: 0.1641
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1503.65,                last time consumption/overall running time: 175.9214s / 39983.0042 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1837
env0_second_0:                 episode reward: -1.0500,                 loss: 1.2417
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1480.95,                last time consumption/overall running time: 167.0231s / 40150.0274 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1769
env0_second_0:                 episode reward: -1.5500,                 loss: 0.3743
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1538.35,                last time consumption/overall running time: 169.7754s / 40319.8027 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1498
env0_second_0:                 episode reward: -1.1000,                 loss: 0.1518
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1481.75,                last time consumption/overall running time: 168.4833s / 40488.2860 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1313
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0703
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1578.95,                last time consumption/overall running time: 173.8919s / 40662.1779 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1307
env0_second_0:                 episode reward: -1.9000,                 loss: 0.2816
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1541.55,                last time consumption/overall running time: 170.4254s / 40832.6033 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1834
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0655
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1513.15,                last time consumption/overall running time: 170.5598s / 41003.1631 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1893
env0_second_0:                 episode reward: -1.5000,                 loss: 0.1213
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1506.85,                last time consumption/overall running time: 171.1074s / 41174.2705 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1769
env0_second_0:                 episode reward: -2.5000,                 loss: 0.1453
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1785.2,                last time consumption/overall running time: 203.7911s / 41378.0616 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1106
env0_second_0:                 episode reward: -0.6000,                 loss: 0.1579
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1536.05,                last time consumption/overall running time: 173.3639s / 41551.4254 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1782
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0826
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1508.35,                last time consumption/overall running time: 172.8736s / 41724.2990 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1356
env0_second_0:                 episode reward: -0.8500,                 loss: 0.1318
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1501.2,                last time consumption/overall running time: 168.0818s / 41892.3808 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1368
env0_second_0:                 episode reward: -1.4500,                 loss: 0.1935
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1499.95,                last time consumption/overall running time: 171.1625s / 42063.5432 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1611
env0_second_0:                 episode reward: -2.3500,                 loss: 0.2210
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1478.0,                last time consumption/overall running time: 167.1101s / 42230.6533 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1637
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0604
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 2019.1,                last time consumption/overall running time: 228.6623s / 42459.3157 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1465
env0_second_0:                 episode reward: -0.7500,                 loss: 0.1334
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1487.25,                last time consumption/overall running time: 167.5684s / 42626.8840 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1582
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0637
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1487.8,                last time consumption/overall running time: 169.2931s / 42796.1771 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1780
env0_second_0:                 episode reward: -1.7000,                 loss: 0.2230
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1493.55,                last time consumption/overall running time: 165.8831s / 42962.0602 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1741
env0_second_0:                 episode reward: -1.0500,                 loss: 0.1371
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1517.55,                last time consumption/overall running time: 168.1991s / 43130.2593 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1885
env0_second_0:                 episode reward: -1.7000,                 loss: 0.1091
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1488.25,                last time consumption/overall running time: 171.8209s / 43302.0802 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1930
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0408
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1485.5,                last time consumption/overall running time: 167.0492s / 43469.1295 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2034
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0116
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1488.6,                last time consumption/overall running time: 169.2489s / 43638.3784 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1972
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0008
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1518.5,                last time consumption/overall running time: 165.3636s / 43803.7419 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1800
env0_second_0:                 episode reward: -1.2500,                 loss: 0.1039
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1493.15,                last time consumption/overall running time: 171.9740s / 43975.7160 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1987
env0_second_0:                 episode reward: -1.6000,                 loss: 0.1605
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1473.95,                last time consumption/overall running time: 165.4723s / 44141.1883 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1864
env0_second_0:                 episode reward: -2.1500,                 loss: 0.1229
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1509.8,                last time consumption/overall running time: 171.5445s / 44312.7328 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1999
env0_second_0:                 episode reward: -2.0000,                 loss: 0.7806
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1493.4,                last time consumption/overall running time: 165.7967s / 44478.5294 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1745
env0_second_0:                 episode reward: -1.8000,                 loss: 0.6282
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1506.4,                last time consumption/overall running time: 171.3937s / 44649.9231 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1835
env0_second_0:                 episode reward: -1.4500,                 loss: 0.1183
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1501.25,                last time consumption/overall running time: 177.7938s / 44827.7169 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1915
env0_second_0:                 episode reward: -1.9000,                 loss: 1.1483
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1485.05,                last time consumption/overall running time: 174.6521s / 45002.3690 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2089
env0_second_0:                 episode reward: -2.3000,                 loss: 1.0017
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1491.2,                last time consumption/overall running time: 171.3077s / 45173.6766 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2059
env0_second_0:                 episode reward: -1.7000,                 loss: 0.1764
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1469.15,                last time consumption/overall running time: 169.0131s / 45342.6897 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1928
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0385
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1551.7,                last time consumption/overall running time: 178.4150s / 45521.1047 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1769
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0900
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1531.3,                last time consumption/overall running time: 172.9241s / 45694.0288 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1934
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0936
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1479.45,                last time consumption/overall running time: 167.1117s / 45861.1405 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1724
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0156
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1458.55,                last time consumption/overall running time: 162.8391s / 46023.9796 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1752
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0000
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1657.35,                last time consumption/overall running time: 189.7626s / 46213.7422 s
env0_first_0:                 episode reward: 8.1000,                 loss: -0.1132
env0_second_0:                 episode reward: -8.1000,                 loss: 0.2202
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1658.9,                last time consumption/overall running time: 187.1715s / 46400.9137 s
env0_first_0:                 episode reward: 9.0000,                 loss: -0.0711
env0_second_0:                 episode reward: -9.0000,                 loss: 0.1597
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1495.85,                last time consumption/overall running time: 165.3846s / 46566.2983 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1745
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0565
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1490.6,                last time consumption/overall running time: 167.8995s / 46734.1978 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1677
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0576
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1476.25,                last time consumption/overall running time: 167.4374s / 46901.6352 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1850
env0_second_0:                 episode reward: -1.9500,                 loss: 0.2726
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1506.15,                last time consumption/overall running time: 173.2031s / 47074.8383 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1722
env0_second_0:                 episode reward: -1.7500,                 loss: 0.3229
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1487.95,                last time consumption/overall running time: 175.3201s / 47250.1584 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1759
env0_second_0:                 episode reward: -0.8500,                 loss: 0.1164
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1496.2,                last time consumption/overall running time: 169.7478s / 47419.9061 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1677
env0_second_0:                 episode reward: -1.8000,                 loss: 0.3220
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1478.9,                last time consumption/overall running time: 166.3879s / 47586.2941 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1621
env0_second_0:                 episode reward: -2.0500,                 loss: 0.3104
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1498.8,                last time consumption/overall running time: 169.3960s / 47755.6900 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1921
env0_second_0:                 episode reward: -2.5500,                 loss: 0.3946
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1476.1,                last time consumption/overall running time: 169.3529s / 47925.0429 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1820
env0_second_0:                 episode reward: -1.8000,                 loss: 0.4139
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1473.9,                last time consumption/overall running time: 166.3005s / 48091.3434 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1371
env0_second_0:                 episode reward: -3.6500,                 loss: 0.7056
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1452.0,                last time consumption/overall running time: 168.4333s / 48259.7766 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1666
env0_second_0:                 episode reward: -1.0500,                 loss: 1.0242
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1490.2,                last time consumption/overall running time: 167.7310s / 48427.5076 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1739
env0_second_0:                 episode reward: -1.6000,                 loss: 0.5019
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1490.05,                last time consumption/overall running time: 166.7747s / 48594.2823 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1584
env0_second_0:                 episode reward: -1.2000,                 loss: 0.7203
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1513.25,                last time consumption/overall running time: 170.6415s / 48764.9238 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1632
env0_second_0:                 episode reward: -2.0000,                 loss: 0.7253
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1506.0,                last time consumption/overall running time: 172.9719s / 48937.8957 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1804
env0_second_0:                 episode reward: -1.1000,                 loss: 0.8891
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1511.25,                last time consumption/overall running time: 176.5857s / 49114.4814 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1629
env0_second_0:                 episode reward: -2.1500,                 loss: 1.2851
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1596.3,                last time consumption/overall running time: 179.5088s / 49293.9903 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1375
env0_second_0:                 episode reward: 0.3000,                 loss: 1.9010
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1454.25,                last time consumption/overall running time: 158.4766s / 49452.4669 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1738
env0_second_0:                 episode reward: -1.7500,                 loss: 0.6660
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1483.7,                last time consumption/overall running time: 170.9498s / 49623.4167 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1394
env0_second_0:                 episode reward: -1.7000,                 loss: 0.3015
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1464.75,                last time consumption/overall running time: 169.5418s / 49792.9586 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1771
env0_second_0:                 episode reward: -1.7500,                 loss: 0.2232
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1494.8,                last time consumption/overall running time: 169.9081s / 49962.8667 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1841
env0_second_0:                 episode reward: -1.9500,                 loss: 0.4971
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1471.05,                last time consumption/overall running time: 179.1401s / 50142.0068 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1574
env0_second_0:                 episode reward: -1.7000,                 loss: 0.3229
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1483.35,                last time consumption/overall running time: 172.7055s / 50314.7123 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1716
env0_second_0:                 episode reward: -1.9500,                 loss: 0.4104
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1525.0,                last time consumption/overall running time: 165.5997s / 50480.3120 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0983
env0_second_0:                 episode reward: -2.6000,                 loss: 0.3880
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1464.75,                last time consumption/overall running time: 163.6220s / 50643.9341 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1774
env0_second_0:                 episode reward: -0.8000,                 loss: 0.1944
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1497.15,                last time consumption/overall running time: 165.5461s / 50809.4802 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1679
env0_second_0:                 episode reward: -1.2500,                 loss: 0.2652
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1515.7,                last time consumption/overall running time: 168.8017s / 50978.2819 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1614
env0_second_0:                 episode reward: -1.4500,                 loss: 0.4025
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1474.95,                last time consumption/overall running time: 174.4552s / 51152.7371 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1658
env0_second_0:                 episode reward: -1.5000,                 loss: 0.3223
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1535.25,                last time consumption/overall running time: 174.0076s / 51326.7447 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.0906
env0_second_0:                 episode reward: -2.9000,                 loss: 0.2666
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1516.3,                last time consumption/overall running time: 173.8194s / 51500.5641 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1696
env0_second_0:                 episode reward: -2.0500,                 loss: 0.1430
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1941.95,                last time consumption/overall running time: 215.4050s / 51715.9691 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1569
env0_second_0:                 episode reward: -3.2500,                 loss: 0.2324
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1924.35,                last time consumption/overall running time: 209.3736s / 51925.3426 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1386
env0_second_0:                 episode reward: -3.0000,                 loss: 0.3140
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1665.3,                last time consumption/overall running time: 190.2013s / 52115.5439 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1483
env0_second_0:                 episode reward: -2.5000,                 loss: 0.1876
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1625.1,                last time consumption/overall running time: 183.1664s / 52298.7103 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1536
env0_second_0:                 episode reward: -2.5000,                 loss: 0.2281
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1614.3,                last time consumption/overall running time: 185.6777s / 52484.3880 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1631
env0_second_0:                 episode reward: -1.0000,                 loss: 0.2874
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1931.25,                last time consumption/overall running time: 218.5504s / 52702.9385 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.0942
env0_second_0:                 episode reward: -2.2000,                 loss: 0.5056
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 2180.8,                last time consumption/overall running time: 241.3540s / 52944.2925 s
env0_first_0:                 episode reward: 16.2000,                 loss: -0.0126
env0_second_0:                 episode reward: -16.2000,                 loss: 0.7446
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 2074.85,                last time consumption/overall running time: 232.9647s / 53177.2572 s
env0_first_0:                 episode reward: 16.7000,                 loss: -0.0910
env0_second_0:                 episode reward: -16.7000,                 loss: 0.5636
env1_first_0:                 episode reward: 17.5500,                 loss: nan
env1_second_0:                 episode reward: -17.5500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 2004.35,                last time consumption/overall running time: 222.5782s / 53399.8355 s
env0_first_0:                 episode reward: 17.3500,                 loss: -0.1044
env0_second_0:                 episode reward: -17.3500,                 loss: 0.6356
env1_first_0:                 episode reward: 17.2500,                 loss: nan
env1_second_0:                 episode reward: -17.2500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 2016.25,                last time consumption/overall running time: 224.5872s / 53624.4226 s
env0_first_0:                 episode reward: 16.3500,                 loss: -0.1187
env0_second_0:                 episode reward: -16.3500,                 loss: 0.4333
env1_first_0:                 episode reward: 17.5500,                 loss: nan
env1_second_0:                 episode reward: -17.5500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 2134.35,                last time consumption/overall running time: 237.2250s / 53861.6477 s
env0_first_0:                 episode reward: 17.9500,                 loss: -0.1247
env0_second_0:                 episode reward: -17.9500,                 loss: 0.6618
env1_first_0:                 episode reward: 17.9000,                 loss: nan
env1_second_0:                 episode reward: -17.9000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1954.5,                last time consumption/overall running time: 210.9732s / 54072.6208 s
env0_first_0:                 episode reward: 16.6500,                 loss: -0.1009
env0_second_0:                 episode reward: -16.6500,                 loss: 0.5513
env1_first_0:                 episode reward: 16.2500,                 loss: nan
env1_second_0:                 episode reward: -16.2500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1977.25,                last time consumption/overall running time: 217.6357s / 54290.2565 s
env0_first_0:                 episode reward: 17.4500,                 loss: -0.1032
env0_second_0:                 episode reward: -17.4500,                 loss: 0.3984
env1_first_0:                 episode reward: 16.0000,                 loss: nan
env1_second_0:                 episode reward: -16.0000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1920.95,                last time consumption/overall running time: 210.5111s / 54500.7676 s
env0_first_0:                 episode reward: 11.8000,                 loss: -0.0771
env0_second_0:                 episode reward: -11.8000,                 loss: 0.6940
env1_first_0:                 episode reward: 11.9000,                 loss: nan
env1_second_0:                 episode reward: -11.9000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1939.45,                last time consumption/overall running time: 211.5203s / 54712.2880 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.0068
env0_second_0:                 episode reward: -4.1000,                 loss: 0.6054
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 2072.15,                last time consumption/overall running time: 224.4385s / 54936.7265 s
env0_first_0:                 episode reward: 7.6000,                 loss: -0.0554
env0_second_0:                 episode reward: -7.6000,                 loss: 0.5525
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 2025.6,                last time consumption/overall running time: 222.1675s / 55158.8940 s
env0_first_0:                 episode reward: 8.3000,                 loss: -0.0823
env0_second_0:                 episode reward: -8.3000,                 loss: 1.4506
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2032.35,                last time consumption/overall running time: 216.4608s / 55375.3548 s
env0_first_0:                 episode reward: 7.7000,                 loss: -0.0667
env0_second_0:                 episode reward: -7.7000,                 loss: 1.3951
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1994.05,                last time consumption/overall running time: 212.8949s / 55588.2497 s
env0_first_0:                 episode reward: 7.4500,                 loss: -0.0522
env0_second_0:                 episode reward: -7.4500,                 loss: 0.7136
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1914.85,                last time consumption/overall running time: 208.2161s / 55796.4658 s
env0_first_0:                 episode reward: 9.8500,                 loss: -0.0717
env0_second_0:                 episode reward: -9.8500,                 loss: 0.8600
env1_first_0:                 episode reward: 8.7500,                 loss: nan
env1_second_0:                 episode reward: -8.7500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1709.35,                last time consumption/overall running time: 186.9221s / 55983.3879 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.0627
env0_second_0:                 episode reward: -4.0500,                 loss: 0.7317
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1875.55,                last time consumption/overall running time: 204.9860s / 56188.3739 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.0499
env0_second_0:                 episode reward: -6.8000,                 loss: 0.4959
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1887.15,                last time consumption/overall running time: 205.7659s / 56394.1398 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.0582
env0_second_0:                 episode reward: -6.6000,                 loss: 0.5117
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1679.6,                last time consumption/overall running time: 182.7013s / 56576.8411 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.0561
env0_second_0:                 episode reward: -6.6000,                 loss: 0.5942
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 2041.65,                last time consumption/overall running time: 223.5697s / 56800.4109 s
env0_first_0:                 episode reward: 10.4000,                 loss: -0.0510
env0_second_0:                 episode reward: -10.4000,                 loss: 0.6593
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1740.6,                last time consumption/overall running time: 192.7465s / 56993.1573 s
env0_first_0:                 episode reward: 7.4000,                 loss: -0.0655
env0_second_0:                 episode reward: -7.4000,                 loss: 0.5027
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1629.7,                last time consumption/overall running time: 182.7658s / 57175.9232 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.0859
env0_second_0:                 episode reward: -6.2500,                 loss: 0.5468
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1561.2,                last time consumption/overall running time: 173.4665s / 57349.3897 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1100
env0_second_0:                 episode reward: -3.7000,                 loss: 0.4263
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1591.4,                last time consumption/overall running time: 179.7369s / 57529.1266 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.0893
env0_second_0:                 episode reward: -7.3000,                 loss: 0.4212
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1492.4,                last time consumption/overall running time: 169.1712s / 57698.2978 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1236
env0_second_0:                 episode reward: -3.2000,                 loss: 0.6624
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1504.6,                last time consumption/overall running time: 169.6928s / 57867.9906 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.1197
env0_second_0:                 episode reward: -4.0500,                 loss: 0.8851
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1589.55,                last time consumption/overall running time: 178.4563s / 58046.4469 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1047
env0_second_0:                 episode reward: -3.8000,                 loss: 0.5912
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1671.6,                last time consumption/overall running time: 191.0157s / 58237.4626 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1094
env0_second_0:                 episode reward: -2.6500,                 loss: 0.7006
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1663.65,                last time consumption/overall running time: 185.5216s / 58422.9842 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1266
env0_second_0:                 episode reward: -2.9500,                 loss: 0.4937
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1844.4,                last time consumption/overall running time: 203.9974s / 58626.9816 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.0897
env0_second_0:                 episode reward: -6.7500,                 loss: 0.6168
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1699.7,                last time consumption/overall running time: 189.2444s / 58816.2260 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.1211
env0_second_0:                 episode reward: -4.0000,                 loss: 0.7937
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 2362.05,                last time consumption/overall running time: 261.6469s / 59077.8729 s
env0_first_0:                 episode reward: 8.3500,                 loss: -0.0602
env0_second_0:                 episode reward: -8.3500,                 loss: 0.6237
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1844.05,                last time consumption/overall running time: 209.0903s / 59286.9632 s
env0_first_0:                 episode reward: 9.0000,                 loss: -0.0618
env0_second_0:                 episode reward: -9.0000,                 loss: 0.8910
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1936.45,                last time consumption/overall running time: 217.1412s / 59504.1044 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.0612
env0_second_0:                 episode reward: -6.6500,                 loss: 0.7960
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1957.65,                last time consumption/overall running time: 219.6118s / 59723.7162 s
env0_first_0:                 episode reward: 8.3500,                 loss: -0.0925
env0_second_0:                 episode reward: -8.3500,                 loss: 0.5504
env1_first_0:                 episode reward: 7.3500,                 loss: nan
env1_second_0:                 episode reward: -7.3500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1540.85,                last time consumption/overall running time: 173.6676s / 59897.3838 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1702
env0_second_0:                 episode reward: -3.0000,                 loss: 0.5463
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1505.6,                last time consumption/overall running time: 168.6686s / 60066.0524 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1902
env0_second_0:                 episode reward: -1.5000,                 loss: 1.8724
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1488.95,                last time consumption/overall running time: 164.0090s / 60230.0614 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1697
env0_second_0:                 episode reward: -1.4500,                 loss: 1.4926
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1507.45,                last time consumption/overall running time: 170.1380s / 60400.1995 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1601
env0_second_0:                 episode reward: -2.4000,                 loss: 1.5364
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1470.15,                last time consumption/overall running time: 163.2163s / 60563.4158 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1816
env0_second_0:                 episode reward: -3.0000,                 loss: 1.5661
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1534.5,                last time consumption/overall running time: 170.5787s / 60733.9945 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1355
env0_second_0:                 episode reward: -2.2000,                 loss: 1.3868
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1480.1,                last time consumption/overall running time: 164.4950s / 60898.4896 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.1538
env0_second_0:                 episode reward: -0.2500,                 loss: 1.3279
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1540.7,                last time consumption/overall running time: 170.5201s / 61069.0096 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1422
env0_second_0:                 episode reward: -2.1000,                 loss: 0.8686
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1497.95,                last time consumption/overall running time: 166.5176s / 61235.5272 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1686
env0_second_0:                 episode reward: -2.5000,                 loss: 1.2485
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1546.15,                last time consumption/overall running time: 175.1737s / 61410.7009 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1484
env0_second_0:                 episode reward: -1.7500,                 loss: 1.0195
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1539.0,                last time consumption/overall running time: 174.0330s / 61584.7338 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1446
env0_second_0:                 episode reward: -2.8500,                 loss: 0.7060
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1528.75,                last time consumption/overall running time: 170.1664s / 61754.9002 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1561
env0_second_0:                 episode reward: -2.5500,                 loss: 0.7164
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1736.35,                last time consumption/overall running time: 185.4739s / 61940.3741 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1528
env0_second_0:                 episode reward: -3.8500,                 loss: 0.5942
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1811.95,                last time consumption/overall running time: 199.0079s / 62139.3820 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0458
env0_second_0:                 episode reward: -3.8500,                 loss: 0.7581
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1599.4,                last time consumption/overall running time: 177.4384s / 62316.8204 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1359
env0_second_0:                 episode reward: -2.0500,                 loss: 0.4938
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1512.55,                last time consumption/overall running time: 167.2301s / 62484.0505 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1325
env0_second_0:                 episode reward: -3.4000,                 loss: 0.7515
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1541.9,                last time consumption/overall running time: 169.1263s / 62653.1768 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1426
env0_second_0:                 episode reward: -1.4500,                 loss: 0.5040
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1590.9,                last time consumption/overall running time: 182.5342s / 62835.7111 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1402
env0_second_0:                 episode reward: -3.0500,                 loss: 0.5619
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1514.2,                last time consumption/overall running time: 166.9762s / 63002.6873 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1726
env0_second_0:                 episode reward: -3.0000,                 loss: 0.3876
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1569.3,                last time consumption/overall running time: 175.8005s / 63178.4878 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1830
env0_second_0:                 episode reward: -2.7500,                 loss: 0.3622
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1587.4,                last time consumption/overall running time: 177.6707s / 63356.1585 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1025
env0_second_0:                 episode reward: -2.5000,                 loss: 0.6588
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1560.35,                last time consumption/overall running time: 177.9759s / 63534.1344 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.1450
env0_second_0:                 episode reward: -3.9500,                 loss: 1.7799
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1539.55,                last time consumption/overall running time: 172.8719s / 63707.0063 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1263
env0_second_0:                 episode reward: -3.8000,                 loss: 2.0821
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1598.05,                last time consumption/overall running time: 174.7273s / 63881.7336 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1408
env0_second_0:                 episode reward: -3.3500,                 loss: 2.2853
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 2101.2,                last time consumption/overall running time: 223.1616s / 64104.8952 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.0205
env0_second_0:                 episode reward: 2.5000,                 loss: 1.9871
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1806.55,                last time consumption/overall running time: 206.9900s / 64311.8852 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.0823
env0_second_0:                 episode reward: 3.2500,                 loss: 1.7782
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1637.45,                last time consumption/overall running time: 183.3901s / 64495.2753 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1202
env0_second_0:                 episode reward: 0.0000,                 loss: 1.7631
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1680.35,                last time consumption/overall running time: 197.5925s / 64692.8678 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1623
env0_second_0:                 episode reward: -0.7000,                 loss: 2.5513
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1815.5,                last time consumption/overall running time: 202.1075s / 64894.9753 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1628
env0_second_0:                 episode reward: -3.1500,                 loss: 2.1370
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1704.6,                last time consumption/overall running time: 193.4013s / 65088.3766 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1648
env0_second_0:                 episode reward: -2.7000,                 loss: 1.8104
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1629.4,                last time consumption/overall running time: 176.2067s / 65264.5833 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1511
env0_second_0:                 episode reward: -1.9000,                 loss: 1.8862
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1785.0,                last time consumption/overall running time: 196.9990s / 65461.5823 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1439
env0_second_0:                 episode reward: -3.8000,                 loss: 1.4726
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1551.85,                last time consumption/overall running time: 171.6245s / 65633.2068 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1476
env0_second_0:                 episode reward: -2.9500,                 loss: 1.2102
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1556.3,                last time consumption/overall running time: 172.0257s / 65805.2325 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1643
env0_second_0:                 episode reward: -2.8500,                 loss: 0.8944
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1733.95,                last time consumption/overall running time: 192.7894s / 65998.0218 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1506
env0_second_0:                 episode reward: -2.6500,                 loss: 0.9872
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1689.95,                last time consumption/overall running time: 190.3563s / 66188.3781 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1595
env0_second_0:                 episode reward: -1.5500,                 loss: 0.7605
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1701.5,                last time consumption/overall running time: 189.1609s / 66377.5391 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1475
env0_second_0:                 episode reward: -3.2500,                 loss: 1.3234
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1800.75,                last time consumption/overall running time: 198.4771s / 66576.0162 s
env0_first_0:                 episode reward: 5.3500,                 loss: -0.1390
env0_second_0:                 episode reward: -5.3500,                 loss: 2.8655
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 2015.35,                last time consumption/overall running time: 219.9298s / 66795.9460 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1152
env0_second_0:                 episode reward: -4.6000,                 loss: 1.8843
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1674.5,                last time consumption/overall running time: 184.3039s / 66980.2500 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1803
env0_second_0:                 episode reward: -3.4500,                 loss: 3.0758
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1766.2,                last time consumption/overall running time: 196.3714s / 67176.6214 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1454
env0_second_0:                 episode reward: -4.8000,                 loss: 1.2120
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 2089.6,                last time consumption/overall running time: 226.0180s / 67402.6393 s
env0_first_0:                 episode reward: 11.0500,                 loss: -0.1654
env0_second_0:                 episode reward: -11.0500,                 loss: 1.2600
env1_first_0:                 episode reward: 10.7000,                 loss: nan
env1_second_0:                 episode reward: -10.7000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1957.8,                last time consumption/overall running time: 211.9677s / 67614.6070 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.1359
env0_second_0:                 episode reward: -5.0000,                 loss: 1.5454
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 2155.35,                last time consumption/overall running time: 232.7111s / 67847.3182 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0999
env0_second_0:                 episode reward: -4.9000,                 loss: 1.2710
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1949.6,                last time consumption/overall running time: 212.8591s / 68060.1772 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0993
env0_second_0:                 episode reward: -6.0500,                 loss: 1.1546
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1530.95,                last time consumption/overall running time: 168.9670s / 68229.1442 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1071
env0_second_0:                 episode reward: -4.6000,                 loss: 1.1029
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1745.5,                last time consumption/overall running time: 188.4819s / 68417.6261 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.1164
env0_second_0:                 episode reward: -4.4500,                 loss: 0.9830
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1753.3,                last time consumption/overall running time: 191.6886s / 68609.3147 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.1012
env0_second_0:                 episode reward: -5.7500,                 loss: 0.9877
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1763.8,                last time consumption/overall running time: 194.4782s / 68803.7929 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1019
env0_second_0:                 episode reward: -6.6000,                 loss: 0.7551
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1560.85,                last time consumption/overall running time: 167.3183s / 68971.1112 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0910
env0_second_0:                 episode reward: -3.6500,                 loss: 0.8260
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1718.8,                last time consumption/overall running time: 189.4742s / 69160.5854 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0921
env0_second_0:                 episode reward: -1.0500,                 loss: 0.7631
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1563.6,                last time consumption/overall running time: 174.3517s / 69334.9371 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1482
env0_second_0:                 episode reward: -3.1000,                 loss: 0.7973
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1667.5,                last time consumption/overall running time: 186.8700s / 69521.8072 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1307
env0_second_0:                 episode reward: -3.1500,                 loss: 0.7484
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1623.4,                last time consumption/overall running time: 174.3814s / 69696.1885 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1367
env0_second_0:                 episode reward: -2.7000,                 loss: 0.5364
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1587.9,                last time consumption/overall running time: 170.4279s / 69866.6164 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1330
env0_second_0:                 episode reward: -2.2000,                 loss: 0.5405
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1546.15,                last time consumption/overall running time: 171.8020s / 70038.4184 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1593
env0_second_0:                 episode reward: -2.2500,                 loss: 0.5218
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1547.55,                last time consumption/overall running time: 173.1333s / 70211.5517 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1477
env0_second_0:                 episode reward: -2.5000,                 loss: 0.5129
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1607.15,                last time consumption/overall running time: 179.9590s / 70391.5107 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1173
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7589
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1565.2,                last time consumption/overall running time: 172.6814s / 70564.1921 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1500
env0_second_0:                 episode reward: -2.9000,                 loss: 0.7070
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1567.7,                last time consumption/overall running time: 172.7771s / 70736.9692 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1350
env0_second_0:                 episode reward: -1.4500,                 loss: 0.6035
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1800.5,                last time consumption/overall running time: 194.6857s / 70931.6550 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.1147
env0_second_0:                 episode reward: -4.5000,                 loss: 0.6506
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1563.5,                last time consumption/overall running time: 175.7981s / 71107.4530 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1323
env0_second_0:                 episode reward: -3.2000,                 loss: 0.7536
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1828.65,                last time consumption/overall running time: 195.2971s / 71302.7502 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1056
env0_second_0:                 episode reward: -3.5500,                 loss: 0.7296
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 1707.65,                last time consumption/overall running time: 181.6625s / 71484.4126 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1106
env0_second_0:                 episode reward: -3.7000,                 loss: 0.5896
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1650.4,                last time consumption/overall running time: 175.9637s / 71660.3764 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1262
env0_second_0:                 episode reward: -3.1500,                 loss: 0.5883
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1651.05,                last time consumption/overall running time: 178.1487s / 71838.5251 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1008
env0_second_0:                 episode reward: -2.8000,                 loss: 0.5511
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 1535.0,                last time consumption/overall running time: 171.9176s / 72010.4427 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1199
env0_second_0:                 episode reward: -2.6500,                 loss: 0.6457
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1609.2,                last time consumption/overall running time: 180.8763s / 72191.3189 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0857
env0_second_0:                 episode reward: -4.8500,                 loss: 0.7915
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1551.9,                last time consumption/overall running time: 169.0073s / 72360.3263 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.1503
env0_second_0:                 episode reward: -6.8000,                 loss: 0.6361
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1666.15,                last time consumption/overall running time: 183.9945s / 72544.3208 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.1141
env0_second_0:                 episode reward: -5.8000,                 loss: 0.6326
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1687.15,                last time consumption/overall running time: 189.3725s / 72733.6933 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1240
env0_second_0:                 episode reward: -3.8000,                 loss: 0.7806
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1672.8,                last time consumption/overall running time: 179.2875s / 72912.9808 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.1193
env0_second_0:                 episode reward: -5.6000,                 loss: 0.6157
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1803.0,                last time consumption/overall running time: 195.9377s / 73108.9185 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.1194
env0_second_0:                 episode reward: -6.0500,                 loss: 0.5288
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1817.15,                last time consumption/overall running time: 200.7848s / 73309.7033 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0977
env0_second_0:                 episode reward: -5.8000,                 loss: 0.5668
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1959.0,                last time consumption/overall running time: 213.1306s / 73522.8339 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0357
env0_second_0:                 episode reward: -4.9500,                 loss: 0.6161
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1774.55,                last time consumption/overall running time: 198.2156s / 73721.0495 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.0772
env0_second_0:                 episode reward: -5.6000,                 loss: 0.5773
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1982.5,                last time consumption/overall running time: 218.5679s / 73939.6174 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0678
env0_second_0:                 episode reward: -4.8000,                 loss: 1.3349
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1814.35,                last time consumption/overall running time: 202.0232s / 74141.6406 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.0617
env0_second_0:                 episode reward: -3.1000,                 loss: 1.2864
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1772.8,                last time consumption/overall running time: 195.1229s / 74336.7635 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.1095
env0_second_0:                 episode reward: -4.0000,                 loss: 0.9296
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1795.65,                last time consumption/overall running time: 197.5430s / 74534.3065 s
env0_first_0:                 episode reward: 8.4500,                 loss: -0.0376
env0_second_0:                 episode reward: -8.4500,                 loss: 1.4066
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 1636.15,                last time consumption/overall running time: 177.8635s / 74712.1700 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1565
env0_second_0:                 episode reward: -3.8500,                 loss: 1.1590
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1565.1,                last time consumption/overall running time: 174.6655s / 74886.8355 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1404
env0_second_0:                 episode reward: -2.9500,                 loss: 0.9542
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1901.45,                last time consumption/overall running time: 212.6912s / 75099.5267 s
env0_first_0:                 episode reward: 6.0500,                 loss: -0.0909
env0_second_0:                 episode reward: -6.0500,                 loss: 1.0762
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 2081.0,                last time consumption/overall running time: 225.8049s / 75325.3316 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.0817
env0_second_0:                 episode reward: -6.8000,                 loss: 1.0413
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 2047.25,                last time consumption/overall running time: 219.5104s / 75544.8420 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0905
env0_second_0:                 episode reward: -7.1000,                 loss: 0.8391
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 2026.55,                last time consumption/overall running time: 218.1402s / 75762.9822 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.0981
env0_second_0:                 episode reward: -7.3000,                 loss: 1.0601
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 2069.85,                last time consumption/overall running time: 224.3289s / 75987.3112 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.0544
env0_second_0:                 episode reward: -7.3000,                 loss: 1.3075
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 2149.55,                last time consumption/overall running time: 235.5189s / 76222.8300 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0330
env0_second_0:                 episode reward: -7.1500,                 loss: 1.1023
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1980.7,                last time consumption/overall running time: 217.9968s / 76440.8268 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.0867
env0_second_0:                 episode reward: -6.2500,                 loss: 0.7547
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1772.1,                last time consumption/overall running time: 192.5779s / 76633.4047 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1064
env0_second_0:                 episode reward: -2.9500,                 loss: 1.3307
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1914.0,                last time consumption/overall running time: 213.2231s / 76846.6278 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.0879
env0_second_0:                 episode reward: -3.5500,                 loss: 0.7563
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1948.8,                last time consumption/overall running time: 213.1103s / 77059.7381 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.0297
env0_second_0:                 episode reward: -1.2000,                 loss: 0.6347
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 2243.2,                last time consumption/overall running time: 242.0106s / 77301.7487 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1072
env0_second_0:                 episode reward: -2.1000,                 loss: 0.7131
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1943.2,                last time consumption/overall running time: 212.3320s / 77514.0807 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.0795
env0_second_0:                 episode reward: -1.9000,                 loss: 1.0086
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1764.35,                last time consumption/overall running time: 194.3235s / 77708.4042 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1333
env0_second_0:                 episode reward: -2.0000,                 loss: 0.8158
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1664.1,                last time consumption/overall running time: 190.9383s / 77899.3425 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0760
env0_second_0:                 episode reward: -3.8500,                 loss: 0.7128
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1554.6,                last time consumption/overall running time: 168.1282s / 78067.4707 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1566
env0_second_0:                 episode reward: -2.2500,                 loss: 0.6245
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 1538.8,                last time consumption/overall running time: 168.2101s / 78235.6808 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1258
env0_second_0:                 episode reward: -2.3500,                 loss: 1.1475
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1543.45,                last time consumption/overall running time: 172.4449s / 78408.1257 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1219
env0_second_0:                 episode reward: -1.6500,                 loss: 1.0610
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1572.35,                last time consumption/overall running time: 175.9353s / 78584.0609 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1405
env0_second_0:                 episode reward: -2.9500,                 loss: 1.1608
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1580.4,                last time consumption/overall running time: 172.1034s / 78756.1644 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1221
env0_second_0:                 episode reward: -2.6500,                 loss: 1.1680
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1639.7,                last time consumption/overall running time: 181.8934s / 78938.0578 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1371
env0_second_0:                 episode reward: -3.6500,                 loss: 1.3449
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1510.7,                last time consumption/overall running time: 165.1668s / 79103.2245 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1643
env0_second_0:                 episode reward: -2.8000,                 loss: 1.3410
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1481.6,                last time consumption/overall running time: 167.9780s / 79271.2025 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1696
env0_second_0:                 episode reward: -1.7500,                 loss: 0.9179
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1506.7,                last time consumption/overall running time: 165.7568s / 79436.9594 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1711
env0_second_0:                 episode reward: -2.4500,                 loss: 0.6785
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1498.9,                last time consumption/overall running time: 162.4564s / 79599.4158 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1441
env0_second_0:                 episode reward: -2.1500,                 loss: 0.6722
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1494.6,                last time consumption/overall running time: 164.4683s / 79763.8841 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.1675
env0_second_0:                 episode reward: -2.6000,                 loss: 0.6928
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1527.15,                last time consumption/overall running time: 167.1078s / 79930.9919 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1222
env0_second_0:                 episode reward: -1.2000,                 loss: 0.5841
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1506.15,                last time consumption/overall running time: 166.1002s / 80097.0921 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1530
env0_second_0:                 episode reward: -1.9500,                 loss: 0.5331
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 1489.65,                last time consumption/overall running time: 171.2937s / 80268.3858 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1351
env0_second_0:                 episode reward: -2.3500,                 loss: 0.5458
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1507.8,                last time consumption/overall running time: 162.5805s / 80430.9664 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1552
env0_second_0:                 episode reward: -1.2000,                 loss: 0.5763
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1535.25,                last time consumption/overall running time: 171.4478s / 80602.4142 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.1586
env0_second_0:                 episode reward: -2.6000,                 loss: 0.7708
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1510.75,                last time consumption/overall running time: 169.2747s / 80771.6888 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1680
env0_second_0:                 episode reward: -1.9000,                 loss: 0.5938
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1513.95,                last time consumption/overall running time: 162.7059s / 80934.3947 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1719
env0_second_0:                 episode reward: -1.6500,                 loss: 0.7910
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 1482.7,                last time consumption/overall running time: 162.5039s / 81096.8986 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1755
env0_second_0:                 episode reward: -1.7000,                 loss: 0.9687
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1515.25,                last time consumption/overall running time: 173.3192s / 81270.2178 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1731
env0_second_0:                 episode reward: -1.6500,                 loss: 0.7913
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1561.1,                last time consumption/overall running time: 175.1072s / 81445.3250 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1622
env0_second_0:                 episode reward: -2.4500,                 loss: 0.7499
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1559.65,                last time consumption/overall running time: 172.9039s / 81618.2289 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1202
env0_second_0:                 episode reward: -0.8000,                 loss: 0.6306
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1518.85,                last time consumption/overall running time: 165.8538s / 81784.0827 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1478
env0_second_0:                 episode reward: -1.2000,                 loss: 0.4923
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 1509.7,                last time consumption/overall running time: 168.2733s / 81952.3560 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1473
env0_second_0:                 episode reward: -2.4000,                 loss: 0.5724
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 1509.35,                last time consumption/overall running time: 166.1425s / 82118.4984 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1646
env0_second_0:                 episode reward: -1.3000,                 loss: 0.5435
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1519.35,                last time consumption/overall running time: 173.9017s / 82292.4001 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1256
env0_second_0:                 episode reward: -0.9000,                 loss: 0.4714
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 1597.4,                last time consumption/overall running time: 181.5530s / 82473.9530 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1090
env0_second_0:                 episode reward: -1.4000,                 loss: 0.5469
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1548.95,                last time consumption/overall running time: 168.8916s / 82642.8447 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.0801
env0_second_0:                 episode reward: 0.3000,                 loss: 0.8389
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1707.6,                last time consumption/overall running time: 190.4527s / 82833.2974 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0375
env0_second_0:                 episode reward: 0.5000,                 loss: 0.9413
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1584.25,                last time consumption/overall running time: 172.8596s / 83006.1570 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.0370
env0_second_0:                 episode reward: 1.4500,                 loss: 0.8894
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1576.45,                last time consumption/overall running time: 174.4494s / 83180.6064 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1327
env0_second_0:                 episode reward: -2.0000,                 loss: 0.8615
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1705.6,                last time consumption/overall running time: 192.5871s / 83373.1934 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1345
env0_second_0:                 episode reward: -2.3000,                 loss: 0.6972
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1704.9,                last time consumption/overall running time: 186.9750s / 83560.1684 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1518
env0_second_0:                 episode reward: -3.4500,                 loss: 0.7385
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1603.65,                last time consumption/overall running time: 176.1961s / 83736.3645 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1439
env0_second_0:                 episode reward: -2.9500,                 loss: 0.6026
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1653.05,                last time consumption/overall running time: 183.1592s / 83919.5237 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1524
env0_second_0:                 episode reward: -2.4000,                 loss: 0.5089
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1608.75,                last time consumption/overall running time: 177.7628s / 84097.2865 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1553
env0_second_0:                 episode reward: -2.1500,                 loss: 0.3496
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1680.15,                last time consumption/overall running time: 179.9270s / 84277.2134 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1376
env0_second_0:                 episode reward: -2.2000,                 loss: 0.4229
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1534.6,                last time consumption/overall running time: 168.8893s / 84446.1027 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1783
env0_second_0:                 episode reward: -2.3500,                 loss: 0.4037
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1535.3,                last time consumption/overall running time: 167.1328s / 84613.2355 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1827
env0_second_0:                 episode reward: -1.7500,                 loss: 1.3099
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1720.25,                last time consumption/overall running time: 196.7171s / 84809.9525 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1824
env0_second_0:                 episode reward: -2.6500,                 loss: 1.0467
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1670.35,                last time consumption/overall running time: 185.4285s / 84995.3810 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1674
env0_second_0:                 episode reward: -3.2000,                 loss: 0.7341
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1626.35,                last time consumption/overall running time: 179.8474s / 85175.2284 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1633
env0_second_0:                 episode reward: -3.0500,                 loss: 0.6333
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 1589.6,                last time consumption/overall running time: 177.7502s / 85352.9786 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1339
env0_second_0:                 episode reward: -2.8500,                 loss: 0.4940
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1770.45,                last time consumption/overall running time: 193.9348s / 85546.9134 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1171
env0_second_0:                 episode reward: -1.2500,                 loss: 0.8902
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1622.45,                last time consumption/overall running time: 178.4404s / 85725.3539 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1647
env0_second_0:                 episode reward: -2.2500,                 loss: 0.6540
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1854.55,                last time consumption/overall running time: 203.1497s / 85928.5036 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1279
env0_second_0:                 episode reward: -2.0000,                 loss: 0.7455
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1812.7,                last time consumption/overall running time: 197.3529s / 86125.8565 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1556
env0_second_0:                 episode reward: -2.4000,                 loss: 0.7407
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1748.15,                last time consumption/overall running time: 197.8215s / 86323.6779 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1690
env0_second_0:                 episode reward: -3.7500,                 loss: 0.9938
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1825.25,                last time consumption/overall running time: 203.7418s / 86527.4198 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1413
env0_second_0:                 episode reward: -2.9000,                 loss: 0.7542
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1650.1,                last time consumption/overall running time: 181.7046s / 86709.1244 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1596
env0_second_0:                 episode reward: -3.7500,                 loss: 0.3936
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1713.85,                last time consumption/overall running time: 191.8360s / 86900.9604 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1730
env0_second_0:                 episode reward: -2.2500,                 loss: 0.4076
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 1889.65,                last time consumption/overall running time: 205.0472s / 87106.0076 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.1520
env0_second_0:                 episode reward: -4.3500,                 loss: 0.5187
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 2057.9,                last time consumption/overall running time: 228.8299s / 87334.8374 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.1422
env0_second_0:                 episode reward: -5.4500,                 loss: 0.5305
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1875.35,                last time consumption/overall running time: 204.6032s / 87539.4407 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1495
env0_second_0:                 episode reward: -2.5000,                 loss: 0.4197
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1806.1,                last time consumption/overall running time: 199.2764s / 87738.7171 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1565
env0_second_0:                 episode reward: -2.8500,                 loss: 0.2955
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1638.6,                last time consumption/overall running time: 176.8401s / 87915.5571 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.1879
env0_second_0:                 episode reward: -3.3000,                 loss: 0.2556
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 1604.1,                last time consumption/overall running time: 173.9880s / 88089.5451 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1650
env0_second_0:                 episode reward: -1.4500,                 loss: 0.2505
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 1574.3,                last time consumption/overall running time: 173.6312s / 88263.1763 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1988
env0_second_0:                 episode reward: -1.5500,                 loss: 0.1852
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 3260.95,                last time consumption/overall running time: 347.0060s / 88610.1823 s
env0_first_0:                 episode reward: -17.9000,                 loss: -0.0362
env0_second_0:                 episode reward: 17.9000,                 loss: 0.3545
env1_first_0:                 episode reward: -17.4500,                 loss: nan
env1_second_0:                 episode reward: 17.4500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 3887.9,                last time consumption/overall running time: 418.3438s / 89028.5261 s
env0_first_0:                 episode reward: -17.3000,                 loss: -0.1154
env0_second_0:                 episode reward: 17.3000,                 loss: 0.2367
env1_first_0:                 episode reward: -19.9500,                 loss: nan
env1_second_0:                 episode reward: 19.9500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 2764.1,                last time consumption/overall running time: 295.9497s / 89324.4758 s
env0_first_0:                 episode reward: -15.1500,                 loss: -0.1233
env0_second_0:                 episode reward: 15.1500,                 loss: 0.2578
env1_first_0:                 episode reward: -12.0000,                 loss: nan
env1_second_0:                 episode reward: 12.0000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 2426.05,                last time consumption/overall running time: 262.8272s / 89587.3030 s
env0_first_0:                 episode reward: -7.6500,                 loss: -0.1181
env0_second_0:                 episode reward: 7.6500,                 loss: 0.3494
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 1893.0,                last time consumption/overall running time: 208.0832s / 89795.3863 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1388
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3965
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 1875.2,                last time consumption/overall running time: 208.0000s / 90003.3862 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1484
env0_second_0:                 episode reward: -1.6000,                 loss: 0.3183
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1960.65,                last time consumption/overall running time: 221.7461s / 90225.1323 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1175
env0_second_0:                 episode reward: -2.2000,                 loss: 0.3525
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1841.5,                last time consumption/overall running time: 209.7243s / 90434.8566 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1235
env0_second_0:                 episode reward: -0.6500,                 loss: 0.3138
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 1877.55,                last time consumption/overall running time: 211.4729s / 90646.3296 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1103
env0_second_0:                 episode reward: -1.5000,                 loss: 0.5555
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 1712.75,                last time consumption/overall running time: 188.8183s / 90835.1479 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1515
env0_second_0:                 episode reward: -3.6500,                 loss: 0.3597
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 1530.6,                last time consumption/overall running time: 166.3148s / 91001.4627 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1546
env0_second_0:                 episode reward: -2.5500,                 loss: 0.2891
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 1625.6,                last time consumption/overall running time: 180.3669s / 91181.8295 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1168
env0_second_0:                 episode reward: -2.3500,                 loss: 0.3258
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 1754.05,                last time consumption/overall running time: 196.2109s / 91378.0404 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.0954
env0_second_0:                 episode reward: -2.1000,                 loss: 0.3654
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1758.3,                last time consumption/overall running time: 195.0451s / 91573.0855 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0883
env0_second_0:                 episode reward: -0.9500,                 loss: 0.3000
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1703.15,                last time consumption/overall running time: 189.0485s / 91762.1340 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.0973
env0_second_0:                 episode reward: 0.0500,                 loss: 0.3079
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 1556.6,                last time consumption/overall running time: 174.2008s / 91936.3348 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0714
env0_second_0:                 episode reward: 0.8500,                 loss: 0.2448
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 1548.25,                last time consumption/overall running time: 168.5448s / 92104.8796 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1158
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2338
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1765.8,                last time consumption/overall running time: 208.2676s / 92313.1473 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1211
env0_second_0:                 episode reward: -2.4000,                 loss: 0.3133
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1601.5,                last time consumption/overall running time: 172.9390s / 92486.0863 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1191
env0_second_0:                 episode reward: -2.9000,                 loss: 0.4400
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 1723.0,                last time consumption/overall running time: 189.0112s / 92675.0975 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.0898
env0_second_0:                 episode reward: -2.3500,                 loss: 0.3052
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1543.1,                last time consumption/overall running time: 170.4720s / 92845.5695 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.0832
env0_second_0:                 episode reward: -3.4000,                 loss: 0.7395
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 1515.8,                last time consumption/overall running time: 169.3178s / 93014.8873 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.0804
env0_second_0:                 episode reward: -0.5000,                 loss: 1.2404
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1615.15,                last time consumption/overall running time: 178.6403s / 93193.5277 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1030
env0_second_0:                 episode reward: -1.9500,                 loss: 1.1999
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1529.65,                last time consumption/overall running time: 170.9191s / 93364.4467 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1173
env0_second_0:                 episode reward: -3.1000,                 loss: 0.5662
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 1557.4,                last time consumption/overall running time: 172.5655s / 93537.0123 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.0941
env0_second_0:                 episode reward: -2.6500,                 loss: 0.7215
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 1535.8,                last time consumption/overall running time: 171.4690s / 93708.4813 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1383
env0_second_0:                 episode reward: -1.3500,                 loss: 0.3626
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 1610.35,                last time consumption/overall running time: 178.2936s / 93886.7749 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.0986
env0_second_0:                 episode reward: -2.3000,                 loss: 0.4703
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 1567.65,                last time consumption/overall running time: 175.8947s / 94062.6696 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1620
env0_second_0:                 episode reward: -1.4500,                 loss: 0.2491
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 1555.65,                last time consumption/overall running time: 171.0477s / 94233.7173 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1157
env0_second_0:                 episode reward: -2.2000,                 loss: 0.5774
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1555.2,                last time consumption/overall running time: 173.0674s / 94406.7846 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1371
env0_second_0:                 episode reward: -2.3500,                 loss: 0.5494
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 1546.1,                last time consumption/overall running time: 174.3073s / 94581.0919 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1593
env0_second_0:                 episode reward: -2.1500,                 loss: 0.5119
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1522.1,                last time consumption/overall running time: 167.2368s / 94748.3287 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1300
env0_second_0:                 episode reward: -1.2000,                 loss: 0.5361
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1565.0,                last time consumption/overall running time: 174.1397s / 94922.4684 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.1513
env0_second_0:                 episode reward: -2.6000,                 loss: 0.6103
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1530.65,                last time consumption/overall running time: 170.2337s / 95092.7021 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.1125
env0_second_0:                 episode reward: -4.3500,                 loss: 0.4077
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 1485.05,                last time consumption/overall running time: 166.4825s / 95259.1846 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1492
env0_second_0:                 episode reward: -2.0000,                 loss: 0.3197
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 1494.7,                last time consumption/overall running time: 167.8006s / 95426.9852 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1190
env0_second_0:                 episode reward: -0.8500,                 loss: 0.5565
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1535.2,                last time consumption/overall running time: 168.6666s / 95595.6519 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0999
env0_second_0:                 episode reward: -0.8000,                 loss: 0.3442
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 1522.7,                last time consumption/overall running time: 169.7373s / 95765.3891 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1429
env0_second_0:                 episode reward: -1.7000,                 loss: 0.3471
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 1646.95,                last time consumption/overall running time: 187.0935s / 95952.4826 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1310
env0_second_0:                 episode reward: -1.9000,                 loss: 0.3147
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 1529.5,                last time consumption/overall running time: 171.8879s / 96124.3705 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1407
env0_second_0:                 episode reward: -0.8500,                 loss: 0.3613
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 1666.9,                last time consumption/overall running time: 178.6413s / 96303.0118 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1182
env0_second_0:                 episode reward: -0.9000,                 loss: 0.4369
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1614.25,                last time consumption/overall running time: 173.8801s / 96476.8920 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1257
env0_second_0:                 episode reward: -2.7000,                 loss: 0.8800
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1630.05,                last time consumption/overall running time: 177.4071s / 96654.2991 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1035
env0_second_0:                 episode reward: -1.4500,                 loss: 0.5883
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 1508.65,                last time consumption/overall running time: 165.1932s / 96819.4923 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1432
env0_second_0:                 episode reward: -1.9500,                 loss: 0.3886
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1598.55,                last time consumption/overall running time: 179.0917s / 96998.5839 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1507
env0_second_0:                 episode reward: -2.1500,                 loss: 0.4527
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1583.0,                last time consumption/overall running time: 173.5452s / 97172.1292 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1443
env0_second_0:                 episode reward: -2.4000,                 loss: 0.5561
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1504.15,                last time consumption/overall running time: 169.5142s / 97341.6434 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1599
env0_second_0:                 episode reward: -3.6000,                 loss: 0.5293
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1511.65,                last time consumption/overall running time: 166.5329s / 97508.1763 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.0953
env0_second_0:                 episode reward: -3.6000,                 loss: 0.6855
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1602.9,                last time consumption/overall running time: 176.6795s / 97684.8558 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1190
env0_second_0:                 episode reward: -6.6000,                 loss: 0.6920
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1529.35,                last time consumption/overall running time: 168.6796s / 97853.5354 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1486
env0_second_0:                 episode reward: -3.1000,                 loss: 0.5206
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1424.0,                last time consumption/overall running time: 157.7118s / 98011.2472 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1552
env0_second_0:                 episode reward: -2.8000,                 loss: 0.5058
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1433.15,                last time consumption/overall running time: 156.6225s / 98167.8697 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1182
env0_second_0:                 episode reward: -2.1000,                 loss: 0.9056
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 1459.1,                last time consumption/overall running time: 159.9715s / 98327.8412 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1484
env0_second_0:                 episode reward: -1.9500,                 loss: 0.7651
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1450.4,                last time consumption/overall running time: 174.9100s / 98502.7512 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1707
env0_second_0:                 episode reward: -2.3000,                 loss: 0.7167
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 1435.6,                last time consumption/overall running time: 158.0295s / 98660.7807 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1181
env0_second_0:                 episode reward: -2.1000,                 loss: 0.6106
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1443.2,                last time consumption/overall running time: 161.3981s / 98822.1788 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1420
env0_second_0:                 episode reward: -2.1000,                 loss: 0.7964
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 1451.6,                last time consumption/overall running time: 162.1678s / 98984.3466 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1184
env0_second_0:                 episode reward: -2.6500,                 loss: 1.0085
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1479.8,                last time consumption/overall running time: 166.4325s / 99150.7791 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1571
env0_second_0:                 episode reward: -1.8000,                 loss: 1.2803
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1507.35,                last time consumption/overall running time: 172.0198s / 99322.7989 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1649
env0_second_0:                 episode reward: -3.7000,                 loss: 0.9555
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1412.3,                last time consumption/overall running time: 159.3740s / 99482.1729 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1678
env0_second_0:                 episode reward: -3.2000,                 loss: 0.6192
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1459.55,                last time consumption/overall running time: 168.0386s / 99650.2115 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1411
env0_second_0:                 episode reward: -2.4500,                 loss: 0.7029
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1568.15,                last time consumption/overall running time: 175.3759s / 99825.5874 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.1460
env0_second_0:                 episode reward: -2.6000,                 loss: 0.5349
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 1505.5,                last time consumption/overall running time: 169.3890s / 99994.9764 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1568
env0_second_0:                 episode reward: -1.3500,                 loss: 0.5018
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 1679.35,                last time consumption/overall running time: 196.2716s / 100191.2480 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1425
env0_second_0:                 episode reward: -1.3000,                 loss: 0.4657
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1582.45,                last time consumption/overall running time: 182.6022s / 100373.8502 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1182
env0_second_0:                 episode reward: -0.9000,                 loss: 1.0167
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 1684.1,                last time consumption/overall running time: 196.3137s / 100570.1639 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1329
env0_second_0:                 episode reward: -2.0500,                 loss: 0.6655
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 1839.8,                last time consumption/overall running time: 201.2516s / 100771.4155 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1176
env0_second_0:                 episode reward: -2.4500,                 loss: 0.7260
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 1636.25,                last time consumption/overall running time: 178.9860s / 100950.4015 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.1387
env0_second_0:                 episode reward: -3.9500,                 loss: 0.6640
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1784.4,                last time consumption/overall running time: 194.4386s / 101144.8401 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1100
env0_second_0:                 episode reward: -3.0500,                 loss: 0.6958
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1788.15,                last time consumption/overall running time: 196.2018s / 101341.0419 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1324
env0_second_0:                 episode reward: -2.4000,                 loss: 0.7949
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 1654.5,                last time consumption/overall running time: 182.1534s / 101523.1953 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1274
env0_second_0:                 episode reward: -3.6500,                 loss: 0.6879
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 1559.85,                last time consumption/overall running time: 175.7842s / 101698.9795 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1660
env0_second_0:                 episode reward: -1.6500,                 loss: 0.4013
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1584.4,                last time consumption/overall running time: 171.4273s / 101870.4068 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1266
env0_second_0:                 episode reward: -0.9500,                 loss: 0.7992
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1683.75,                last time consumption/overall running time: 197.3745s / 102067.7814 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.0925
env0_second_0:                 episode reward: -3.7000,                 loss: 0.7157
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 1653.2,                last time consumption/overall running time: 183.2682s / 102251.0496 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1016
env0_second_0:                 episode reward: -2.2500,                 loss: 0.8028
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 1507.2,                last time consumption/overall running time: 165.5429s / 102416.5925 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1187
env0_second_0:                 episode reward: -1.3000,                 loss: 1.1037
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 1493.15,                last time consumption/overall running time: 170.1712s / 102586.7637 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1570
env0_second_0:                 episode reward: -1.7000,                 loss: 0.6097
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1511.1,                last time consumption/overall running time: 170.0556s / 102756.8193 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1581
env0_second_0:                 episode reward: -1.1000,                 loss: 0.3422
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1639.8,                last time consumption/overall running time: 179.1406s / 102935.9599 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1183
env0_second_0:                 episode reward: 0.3500,                 loss: 1.5032
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1671.85,                last time consumption/overall running time: 187.8166s / 103123.7765 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1152
env0_second_0:                 episode reward: -2.2500,                 loss: 1.3877
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1605.35,                last time consumption/overall running time: 184.1570s / 103307.9335 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1107
env0_second_0:                 episode reward: -1.4000,                 loss: 1.3029
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 1679.6,                last time consumption/overall running time: 187.1831s / 103495.1166 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1325
env0_second_0:                 episode reward: -2.1000,                 loss: 1.1979
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 1681.4,                last time consumption/overall running time: 188.0888s / 103683.2054 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.0807
env0_second_0:                 episode reward: -1.1500,                 loss: 1.0991
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 1618.1,                last time consumption/overall running time: 184.0092s / 103867.2146 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1410
env0_second_0:                 episode reward: -1.6000,                 loss: 0.7587
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 1522.05,                last time consumption/overall running time: 171.5384s / 104038.7531 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1453
env0_second_0:                 episode reward: -1.0000,                 loss: 0.7209
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1464.65,                last time consumption/overall running time: 160.3689s / 104199.1220 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1216
env0_second_0:                 episode reward: -1.6000,                 loss: 0.5490
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 1488.75,                last time consumption/overall running time: 162.0776s / 104361.1996 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1514
env0_second_0:                 episode reward: -1.7000,                 loss: 0.4573
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 1483.5,                last time consumption/overall running time: 166.3155s / 104527.5151 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1792
env0_second_0:                 episode reward: -2.4000,                 loss: 0.9359
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1490.05,                last time consumption/overall running time: 166.0150s / 104693.5301 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1792
env0_second_0:                 episode reward: -1.3500,                 loss: 0.5964
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 1551.65,                last time consumption/overall running time: 168.3805s / 104861.9105 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1477
env0_second_0:                 episode reward: -2.0500,                 loss: 0.6484
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1573.4,                last time consumption/overall running time: 169.4770s / 105031.3876 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1576
env0_second_0:                 episode reward: -1.7500,                 loss: 0.8035
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 1485.75,                last time consumption/overall running time: 172.0528s / 105203.4404 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1657
env0_second_0:                 episode reward: -1.4000,                 loss: 0.4543
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 1497.7,                last time consumption/overall running time: 177.7016s / 105381.1420 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1636
env0_second_0:                 episode reward: -1.7000,                 loss: 0.5864
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1544.75,                last time consumption/overall running time: 178.9610s / 105560.1029 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1454
env0_second_0:                 episode reward: -1.5000,                 loss: 0.6241
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 1515.6,                last time consumption/overall running time: 169.9095s / 105730.0125 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1514
env0_second_0:                 episode reward: -0.6500,                 loss: 0.5275
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1516.1,                last time consumption/overall running time: 171.7610s / 105901.7735 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1481
env0_second_0:                 episode reward: -0.5000,                 loss: 0.8076
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 1687.85,                last time consumption/overall running time: 193.2056s / 106094.9791 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.1572
env0_second_0:                 episode reward: -2.6000,                 loss: 0.5886
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 1617.5,                last time consumption/overall running time: 182.2772s / 106277.2562 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1609
env0_second_0:                 episode reward: -2.4000,                 loss: 0.5236
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1495.6,                last time consumption/overall running time: 182.1369s / 106459.3931 sLoad double_dunk_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load double_dunk_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rl/agents/nash_ppo.py:181: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  s,a,r,s_prime,prob_a,done_mask =    torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst).to(self.device), \
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: 2.3500,                 loss: -0.1543
env0_second_0:                 episode reward: -2.3500,                 loss: 0.7465
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 1562.7,                last time consumption/overall running time: 176.4464s / 106635.8395 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1918
env0_second_0:                 episode reward: -3.0000,                 loss: 0.5456
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1509.9,                last time consumption/overall running time: 174.1916s / 106810.0311 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.2059
env0_second_0:                 episode reward: -2.8000,                 loss: 0.4962
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1515.5,                last time consumption/overall running time: 167.6690s / 106977.7001 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1991
env0_second_0:                 episode reward: -2.9000,                 loss: 0.7176
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 1492.2,                last time consumption/overall running time: 176.5224s / 107154.2225 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2256
env0_second_0:                 episode reward: -2.1000,                 loss: 1.1847
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 1527.05,                last time consumption/overall running time: 174.3501s / 107328.5726 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1831
env0_second_0:                 episode reward: -3.1500,                 loss: 1.1232
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 1607.9,                last time consumption/overall running time: 180.1260s / 107508.6986 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.1777
env0_second_0:                 episode reward: -3.3000,                 loss: 1.3526
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1525.65,                last time consumption/overall running time: 174.7716s / 107683.4702 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2135
env0_second_0:                 episode reward: -2.3000,                 loss: 1.2148
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1709.6,                last time consumption/overall running time: 197.3102s / 107880.7804 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1144
env0_second_0:                 episode reward: -3.9000,                 loss: 1.7241
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1503.35,                last time consumption/overall running time: 164.7688s / 108045.5491 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1854
env0_second_0:                 episode reward: -1.3000,                 loss: 1.2373
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1533.6,                last time consumption/overall running time: 173.6309s / 108219.1800 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1825
env0_second_0:                 episode reward: -1.5000,                 loss: 1.1107
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
