Cannot import pettingzoo env:  combat_jet_v1
pong_v3 pettingzoo
type:  pettingzoo
pong_v3
Load pong_v3 environment in type pettingzoo.
Env observation space: <bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f08bd8d19e8>> action space: <bound method aec_to_parallel_wrapper.action_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f08bd8d19e8>>
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f08bd8cb4e0>
random seed: [710, 326, 778, 123, 815, 567, 546, 975, 234, 335, 182, 118, 916, 847, 346, 364]
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f08bd8cb4e0>
discrete_policy 6 Discrete(6)
NashDQNBase(
  (net): CNN(
    (features): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=3136, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=36, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 6 Discrete(6)
NashDQNBase(
  (net): CNN(
    (features): Sequential(
      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))
      (1): ReLU()
      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))
      (3): ReLU()
      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))
      (5): ReLU()
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=3136, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=36, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 6 Discrete(6)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'pong_v3', 'env_type': 'pettingzoo', 'num_envs': 16, 'ram': False, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 5000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [512, 512], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax', 'channel_list': [32, 64, 64], 'kernel_size_list': [8, 4, 3], 'stride_list': [4, 2, 1]}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220622_0025/pettingzoo_pong_v3_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220622_0025/pettingzoo_pong_v3_nash_dqn.
Episode: 1/50000 (0.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 28.94s / 28.94 s
first_0:                 episode reward: 3.0000,                 loss: 0.0189
second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 792.85s / 821.79 s
first_0:                 episode reward: 2.8000,                 loss: 0.0217
second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 791.49s / 1613.28 s
first_0:                 episode reward: 2.5000,                 loss: 0.0222
second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 800.91s / 2414.19 s
first_0:                 episode reward: 1.9000,                 loss: 0.0223
second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 807.59s / 3221.78 s
first_0:                 episode reward: 1.8000,                 loss: 0.0226
second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 816.54s / 4038.31 s
first_0:                 episode reward: 1.2000,                 loss: 0.0225
second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 821.18s / 4859.50 s
first_0:                 episode reward: 2.8500,                 loss: 0.0223
second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 835.18s / 5694.68 s
first_0:                 episode reward: 2.2500,                 loss: 0.0221
second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 841.42s / 6536.10 s
first_0:                 episode reward: 1.8000,                 loss: 0.0223
second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 852.90s / 7389.01 s
first_0:                 episode reward: 0.7000,                 loss: 0.0224
second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 858.92s / 8247.92 s
first_0:                 episode reward: 1.5000,                 loss: 0.0223
second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 886.15s / 9134.07 s
first_0:                 episode reward: 1.6500,                 loss: 0.0224
second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 904.16s / 10038.22 s
first_0:                 episode reward: 1.4000,                 loss: 0.0222
second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 919.20s / 10957.42 s
first_0:                 episode reward: 1.3500,                 loss: 0.0223
second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 929.59s / 11887.02 s
first_0:                 episode reward: 1.4500,                 loss: 0.0225
second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 950.64s / 12837.66 s
first_0:                 episode reward: 1.6000,                 loss: 0.0223
second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 958.14s / 13795.80 s
first_0:                 episode reward: 2.1000,                 loss: 0.0228
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 962.54s / 14758.33 s
first_0:                 episode reward: 0.8500,                 loss: 0.0223
second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 947.76s / 15706.09 s
first_0:                 episode reward: 1.4000,                 loss: 0.0223
second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 954.26s / 16660.35 s
first_0:                 episode reward: 1.5000,                 loss: 0.0221
second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 960.51s / 17620.87 s
first_0:                 episode reward: 3.3500,                 loss: 0.0219
second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 946.95s / 18567.82 s
first_0:                 episode reward: 3.1000,                 loss: 0.0218
second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 947.04s / 19514.86 s
first_0:                 episode reward: 2.1000,                 loss: 0.0223
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 952.31s / 20467.17 s
first_0:                 episode reward: 2.5500,                 loss: 0.0223
second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 942.59s / 21409.75 s
first_0:                 episode reward: 0.3500,                 loss: 0.0221
second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 951.98s / 22361.73 s
first_0:                 episode reward: 3.1500,                 loss: 0.0222
second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 957.13s / 23318.86 s
first_0:                 episode reward: 3.8000,                 loss: 0.0220
second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 948.72s / 24267.58 s
first_0:                 episode reward: 1.9500,                 loss: 0.0223
second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 964.42s / 25232.00 s
first_0:                 episode reward: 2.5500,                 loss: 0.0219
second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 966.01s / 26198.01 s
first_0:                 episode reward: 3.2500,                 loss: 0.0222
second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 969.80s / 27167.81 s
first_0:                 episode reward: 0.6500,                 loss: 0.0222
second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 970.20s / 28138.00 s
first_0:                 episode reward: 4.4500,                 loss: 0.0222
second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 968.06s / 29106.06 s
first_0:                 episode reward: 4.0000,                 loss: 0.0220
second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 969.28s / 30075.34 s
first_0:                 episode reward: 3.0500,                 loss: 0.0220
second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 957.57s / 31032.91 s
first_0:                 episode reward: 4.5000,                 loss: 0.0220
second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 953.82s / 31986.73 s
first_0:                 episode reward: 2.8000,                 loss: 0.0224
second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 965.06s / 32951.80 s
first_0:                 episode reward: 0.9500,                 loss: 0.0222
second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 950.39s / 33902.19 s
first_0:                 episode reward: 2.1500,                 loss: 0.0220
second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 959.42s / 34861.61 s
first_0:                 episode reward: 1.5500,                 loss: 0.0223
second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 965.60s / 35827.21 s
first_0:                 episode reward: 3.4500,                 loss: 0.0219
second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 961.81s / 36789.02 s
first_0:                 episode reward: 2.1000,                 loss: 0.0218
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 954.49s / 37743.51 s
first_0:                 episode reward: 2.6000,                 loss: 0.0222
second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 968.87s / 38712.38 s
first_0:                 episode reward: 1.7000,                 loss: 0.0221
second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 967.92s / 39680.29 s
first_0:                 episode reward: 4.0500,                 loss: 0.0223