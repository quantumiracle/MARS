pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [23, 24]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=18, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 1000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 100000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220411_2111/pettingzoo_boxing_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220411_2111/pettingzoo_boxing_v1_nash_dqn.
Episode: 1/100000 (0.0010%),                 avg. length: 299.0,                last time consumption/overall running time: 19.5491s / 19.5491 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0075
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/100000 (0.0210%),                 avg. length: 299.0,                last time consumption/overall running time: 664.2355s / 683.7846 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0057
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 41/100000 (0.0410%),                 avg. length: 299.0,                last time consumption/overall running time: 668.9388s / 1352.7234 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0055
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 61/100000 (0.0610%),                 avg. length: 299.0,                last time consumption/overall running time: 666.2164s / 2018.9398 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0053
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 81/100000 (0.0810%),                 avg. length: 299.0,                last time consumption/overall running time: 668.2162s / 2687.1559 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0051
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 101/100000 (0.1010%),                 avg. length: 299.0,                last time consumption/overall running time: 671.1365s / 3358.2925 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0049
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 121/100000 (0.1210%),                 avg. length: 299.0,                last time consumption/overall running time: 673.2477s / 4031.5402 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0056
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 141/100000 (0.1410%),                 avg. length: 299.0,                last time consumption/overall running time: 673.1387s / 4704.6788 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0051
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 161/100000 (0.1610%),                 avg. length: 299.0,                last time consumption/overall running time: 675.8291s / 5380.5079 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0047
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 181/100000 (0.1810%),                 avg. length: 299.0,                last time consumption/overall running time: 677.6502s / 6058.1581 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0045
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 201/100000 (0.2010%),                 avg. length: 299.0,                last time consumption/overall running time: 682.5734s / 6740.7315 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0043
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 221/100000 (0.2210%),                 avg. length: 299.0,                last time consumption/overall running time: 681.2989s / 7422.0304 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0044
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 241/100000 (0.2410%),                 avg. length: 299.0,                last time consumption/overall running time: 683.2412s / 8105.2716 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0046
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 261/100000 (0.2610%),                 avg. length: 299.0,                last time consumption/overall running time: 686.7988s / 8792.0704 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0040
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 281/100000 (0.2810%),                 avg. length: 299.0,                last time consumption/overall running time: 687.8127s / 9479.8831 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0043
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 301/100000 (0.3010%),                 avg. length: 299.0,                last time consumption/overall running time: 689.8788s / 10169.7619 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 321/100000 (0.3210%),                 avg. length: 299.0,                last time consumption/overall running time: 692.5312s / 10862.2931 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0041
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 341/100000 (0.3410%),                 avg. length: 299.0,                last time consumption/overall running time: 693.9103s / 11556.2034 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0039
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 361/100000 (0.3610%),                 avg. length: 299.0,                last time consumption/overall running time: 697.8578s / 12254.0612 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0038
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 381/100000 (0.3810%),                 avg. length: 299.0,                last time consumption/overall running time: 699.5941s / 12953.6552 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0038
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 401/100000 (0.4010%),                 avg. length: 299.0,                last time consumption/overall running time: 699.8019s / 13653.4571 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0037
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 421/100000 (0.4210%),                 avg. length: 299.0,                last time consumption/overall running time: 696.3536s / 14349.8107 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0041
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 441/100000 (0.4410%),                 avg. length: 299.0,                last time consumption/overall running time: 696.1461s / 15045.9568 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0043
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 461/100000 (0.4610%),                 avg. length: 299.0,                last time consumption/overall running time: 694.9717s / 15740.9285 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0045
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 481/100000 (0.4810%),                 avg. length: 299.0,                last time consumption/overall running time: 697.0671s / 16437.9956 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0045
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 501/100000 (0.5010%),                 avg. length: 299.0,                last time consumption/overall running time: 695.4407s / 17133.4363 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0047
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 521/100000 (0.5210%),                 avg. length: 299.0,                last time consumption/overall running time: 696.5337s / 17829.9700 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0045
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 541/100000 (0.5410%),                 avg. length: 299.0,                last time consumption/overall running time: 714.8529s / 18544.8229 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0045
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 561/100000 (0.5610%),                 avg. length: 299.0,                last time consumption/overall running time: 698.1284s / 19242.9513 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0047
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 581/100000 (0.5810%),                 avg. length: 299.0,                last time consumption/overall running time: 696.1163s / 19939.0676 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0048
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 601/100000 (0.6010%),                 avg. length: 299.0,                last time consumption/overall running time: 698.4276s / 20637.4952 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0046
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 621/100000 (0.6210%),                 avg. length: 299.0,                last time consumption/overall running time: 699.2305s / 21336.7257 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0047
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 641/100000 (0.6410%),                 avg. length: 299.0,                last time consumption/overall running time: 702.6499s / 22039.3756 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0049
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 661/100000 (0.6610%),                 avg. length: 299.0,                last time consumption/overall running time: 699.8647s / 22739.2402 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0052
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 681/100000 (0.6810%),                 avg. length: 299.0,                last time consumption/overall running time: 697.8328s / 23437.0731 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0056
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 701/100000 (0.7010%),                 avg. length: 299.0,                last time consumption/overall running time: 699.4274s / 24136.5005 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0057
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 721/100000 (0.7210%),                 avg. length: 299.0,                last time consumption/overall running time: 699.5434s / 24836.0438 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0057
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 741/100000 (0.7410%),                 avg. length: 299.0,                last time consumption/overall running time: 696.5922s / 25532.6360 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 761/100000 (0.7610%),                 avg. length: 299.0,                last time consumption/overall running time: 696.1098s / 26228.7458 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 781/100000 (0.7810%),                 avg. length: 299.0,                last time consumption/overall running time: 694.1297s / 26922.8754 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0081
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 801/100000 (0.8010%),                 avg. length: 299.0,                last time consumption/overall running time: 693.8324s / 27616.7079 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0084
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 821/100000 (0.8210%),                 avg. length: 299.0,                last time consumption/overall running time: 691.2126s / 28307.9205 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0089
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 841/100000 (0.8410%),                 avg. length: 299.0,                last time consumption/overall running time: 688.3198s / 28996.2402 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0091
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 861/100000 (0.8610%),                 avg. length: 299.0,                last time consumption/overall running time: 687.7439s / 29683.9841 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0101
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 881/100000 (0.8810%),                 avg. length: 299.0,                last time consumption/overall running time: 687.7280s / 30371.7122 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0101
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 901/100000 (0.9010%),                 avg. length: 299.0,                last time consumption/overall running time: 682.3066s / 31054.0188 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0097
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 921/100000 (0.9210%),                 avg. length: 299.0,                last time consumption/overall running time: 685.8487s / 31739.8675 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0114
env0_second_0:                 episode reward: 3.2000,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 941/100000 (0.9410%),                 avg. length: 299.0,                last time consumption/overall running time: 684.5558s / 32424.4233 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0125
env0_second_0:                 episode reward: 2.9000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 961/100000 (0.9610%),                 avg. length: 299.0,                last time consumption/overall running time: 681.9032s / 33106.3265 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0116
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 981/100000 (0.9810%),                 avg. length: 299.0,                last time consumption/overall running time: 681.0539s / 33787.3805 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0126
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 1001/100000 (1.0010%),                 avg. length: 299.0,                last time consumption/overall running time: 681.3923s / 34468.7728 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0139
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 1021/100000 (1.0210%),                 avg. length: 299.0,                last time consumption/overall running time: 682.9087s / 35151.6815 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0153
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 1041/100000 (1.0410%),                 avg. length: 299.0,                last time consumption/overall running time: 683.2740s / 35834.9555 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0178
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 1061/100000 (1.0610%),                 avg. length: 299.0,                last time consumption/overall running time: 680.4343s / 36515.3898 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0180
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 1081/100000 (1.0810%),                 avg. length: 299.0,                last time consumption/overall running time: 682.1625s / 37197.5522 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0178
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 1101/100000 (1.1010%),                 avg. length: 299.0,                last time consumption/overall running time: 678.9066s / 37876.4588 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0218
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 1121/100000 (1.1210%),                 avg. length: 299.0,                last time consumption/overall running time: 677.8870s / 38554.3458 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0210
env0_second_0:                 episode reward: 2.9000,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 1141/100000 (1.1410%),                 avg. length: 299.0,                last time consumption/overall running time: 678.0190s / 39232.3648 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0231
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 1161/100000 (1.1610%),                 avg. length: 299.0,                last time consumption/overall running time: 677.3238s / 39909.6886 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0250
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 1181/100000 (1.1810%),                 avg. length: 299.0,                last time consumption/overall running time: 677.1001s / 40586.7887 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0232
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1201/100000 (1.2010%),                 avg. length: 299.0,                last time consumption/overall running time: 678.6645s / 41265.4532 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0237
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1221/100000 (1.2210%),                 avg. length: 299.0,                last time consumption/overall running time: 679.4741s / 41944.9273 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0278
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1241/100000 (1.2410%),                 avg. length: 299.0,                last time consumption/overall running time: 677.0388s / 42621.9661 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0275
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 1261/100000 (1.2610%),                 avg. length: 299.0,                last time consumption/overall running time: 678.3770s / 43300.3431 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0265
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 1281/100000 (1.2810%),                 avg. length: 299.0,                last time consumption/overall running time: 681.9331s / 43982.2762 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0247
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 1301/100000 (1.3010%),                 avg. length: 299.0,                last time consumption/overall running time: 679.4388s / 44661.7150 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.0307
env0_second_0:                 episode reward: 4.3000,                 loss: nan
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 1321/100000 (1.3210%),                 avg. length: 299.0,                last time consumption/overall running time: 673.6735s / 45335.3884 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0356
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1341/100000 (1.3410%),                 avg. length: 299.0,                last time consumption/overall running time: 679.8260s / 46015.2144 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0299
env0_second_0:                 episode reward: 3.1000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 1361/100000 (1.3610%),                 avg. length: 299.0,                last time consumption/overall running time: 676.2269s / 46691.4413 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0343
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1381/100000 (1.3810%),                 avg. length: 299.0,                last time consumption/overall running time: 673.3968s / 47364.8381 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0391
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1401/100000 (1.4010%),                 avg. length: 299.0,                last time consumption/overall running time: 675.9460s / 48040.7840 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0359
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 1421/100000 (1.4210%),                 avg. length: 299.0,                last time consumption/overall running time: 674.3285s / 48715.1126 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0414
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1441/100000 (1.4410%),                 avg. length: 299.0,                last time consumption/overall running time: 675.3829s / 49390.4955 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0420
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1461/100000 (1.4610%),                 avg. length: 299.0,                last time consumption/overall running time: 673.4917s / 50063.9872 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0445
env0_second_0:                 episode reward: 2.8500,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1481/100000 (1.4810%),                 avg. length: 299.0,                last time consumption/overall running time: 671.6285s / 50735.6156 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0499
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 1501/100000 (1.5010%),                 avg. length: 299.0,                last time consumption/overall running time: 670.0056s / 51405.6213 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0504
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 1521/100000 (1.5210%),                 avg. length: 299.0,                last time consumption/overall running time: 672.2063s / 52077.8276 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0518
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1541/100000 (1.5410%),                 avg. length: 299.0,                last time consumption/overall running time: 672.4571s / 52750.2847 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0532
env0_second_0:                 episode reward: 5.0500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1561/100000 (1.5610%),                 avg. length: 299.0,                last time consumption/overall running time: 672.6415s / 53422.9262 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0516
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1581/100000 (1.5810%),                 avg. length: 299.0,                last time consumption/overall running time: 670.7987s / 54093.7249 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0555
env0_second_0:                 episode reward: 2.6000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1601/100000 (1.6010%),                 avg. length: 299.0,                last time consumption/overall running time: 669.5958s / 54763.3206 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0596
env0_second_0:                 episode reward: 4.4500,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 1621/100000 (1.6210%),                 avg. length: 299.0,                last time consumption/overall running time: 669.1553s / 55432.4759 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0572
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1641/100000 (1.6410%),                 avg. length: 299.0,                last time consumption/overall running time: 672.2682s / 56104.7441 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0552
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 1661/100000 (1.6610%),                 avg. length: 299.0,                last time consumption/overall running time: 669.6773s / 56774.4215 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0626
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 1681/100000 (1.6810%),                 avg. length: 299.0,                last time consumption/overall running time: 670.3733s / 57444.7948 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0658
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1701/100000 (1.7010%),                 avg. length: 299.0,                last time consumption/overall running time: 671.6862s / 58116.4810 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0625
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 1721/100000 (1.7210%),                 avg. length: 299.0,                last time consumption/overall running time: 675.0577s / 58791.5386 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0644
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1741/100000 (1.7410%),                 avg. length: 299.0,                last time consumption/overall running time: 666.7153s / 59458.2540 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0755
env0_second_0:                 episode reward: 4.0500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 1761/100000 (1.7610%),                 avg. length: 299.0,                last time consumption/overall running time: 665.4814s / 60123.7353 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0778
env0_second_0:                 episode reward: 4.0000,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 1781/100000 (1.7810%),                 avg. length: 299.0,                last time consumption/overall running time: 668.9350s / 60792.6703 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0749
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 1801/100000 (1.8010%),                 avg. length: 299.0,                last time consumption/overall running time: 665.4969s / 61458.1672 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0834
env0_second_0:                 episode reward: 1.7000,                 loss: nan
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 1821/100000 (1.8210%),                 avg. length: 299.0,                last time consumption/overall running time: 665.9020s / 62124.0692 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0914
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 1841/100000 (1.8410%),                 avg. length: 299.0,                last time consumption/overall running time: 665.6133s / 62789.6825 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0948
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 1861/100000 (1.8610%),                 avg. length: 299.0,                last time consumption/overall running time: 669.2458s / 63458.9283 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0931
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 1881/100000 (1.8810%),                 avg. length: 299.0,                last time consumption/overall running time: 666.9719s / 64125.9002 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0945
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1901/100000 (1.9010%),                 avg. length: 299.0,                last time consumption/overall running time: 667.6629s / 64793.5632 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0992
env0_second_0:                 episode reward: 5.3000,                 loss: nan
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1921/100000 (1.9210%),                 avg. length: 299.0,                last time consumption/overall running time: 668.5033s / 65462.0665 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0888
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 1941/100000 (1.9410%),                 avg. length: 299.0,                last time consumption/overall running time: 664.1730s / 66126.2395 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0984
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 1961/100000 (1.9610%),                 avg. length: 299.0,                last time consumption/overall running time: 665.3827s / 66791.6222 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.1005
env0_second_0:                 episode reward: 2.0000,                 loss: nan