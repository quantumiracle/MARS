pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
surround_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): ReLU()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): ReLU()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): ReLU()
      (6): Linear(in_features=256, out_features=256, bias=True)
      (7): ReLU()
      (8): Linear(in_features=256, out_features=5, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'surround_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [256, 256, 256, 256], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0526/pettingzoo_surround_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0526/pettingzoo_surround_v1_nash_dqn.
Episode: 1/10000 (0.0100%),                 avg. length: 1628.0,                last time consumption/overall running time: 31.5620s / 31.5620 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0096
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0103
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1380.95,                last time consumption/overall running time: 711.6318s / 743.1939 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0097
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0096
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1496.25,                last time consumption/overall running time: 1010.9210s / 1754.1149 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0096
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0096
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1466.55,                last time consumption/overall running time: 1080.1875s / 2834.3024 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0097
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0096
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1471.85,                last time consumption/overall running time: 1115.7816s / 3950.0840 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0097
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0098
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1462.0,                last time consumption/overall running time: 1121.6968s / 5071.7809 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0099
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0094
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1440.55,                last time consumption/overall running time: 1116.3192s / 6188.1001 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0097
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0095
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1491.8,                last time consumption/overall running time: 1153.1843s / 7341.2844 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0092
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0094
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1510.05,                last time consumption/overall running time: 1176.2614s / 8517.5459 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0094
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0093
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1422.35,                last time consumption/overall running time: 1111.1471s / 9628.6929 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0093
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0093
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1506.2,                last time consumption/overall running time: 1165.6915s / 10794.3844 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0096
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0095
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1516.95,                last time consumption/overall running time: 1180.9547s / 11975.3391 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0097
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0097
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1484.8,                last time consumption/overall running time: 1150.0231s / 13125.3622 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0092
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0091
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1542.05,                last time consumption/overall running time: 1201.4636s / 14326.8258 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0093
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0095
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1439.75,                last time consumption/overall running time: 1120.4265s / 15447.2524 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0091
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0093
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1451.9,                last time consumption/overall running time: 1133.7016s / 16580.9539 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0095
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0091
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1435.3,                last time consumption/overall running time: 1115.8210s / 17696.7749 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0094
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0092
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1448.15,                last time consumption/overall running time: 1125.6817s / 18822.4566 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0094
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0094
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1446.8,                last time consumption/overall running time: 1129.2678s / 19951.7244 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0093
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0092
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1473.85,                last time consumption/overall running time: 1143.8414s / 21095.5659 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0092
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0094
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1520.25,                last time consumption/overall running time: 1171.5014s / 22267.0672 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0091
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0095
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1391.05,                last time consumption/overall running time: 1076.1722s / 23343.2395 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0094
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0089
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1500.8,                last time consumption/overall running time: 1165.8616s / 24509.1011 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0093
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0092
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1540.1,                last time consumption/overall running time: 1199.2578s / 25708.3589 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0091
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0092
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1441.8,                last time consumption/overall running time: 1118.9750s / 26827.3338 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0090
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0092
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1469.9,                last time consumption/overall running time: 1140.3578s / 27967.6917 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0090
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0088
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1479.5,                last time consumption/overall running time: 1152.0716s / 29119.7632 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0086
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0087
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1448.6,                last time consumption/overall running time: 1127.7916s / 30247.5549 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0086
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0090
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1510.2,                last time consumption/overall running time: 1180.6451s / 31428.2000 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0091
env0_second_0:                 episode reward: -2.0500,                 loss: 0.0089
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1491.45,                last time consumption/overall running time: 1161.7932s / 32589.9932 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0089
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0090
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1491.7,                last time consumption/overall running time: 1163.0425s / 33753.0357 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0091
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0089
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1473.5,                last time consumption/overall running time: 1144.5886s / 34897.6243 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0090
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0086
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1426.85,                last time consumption/overall running time: 1097.1541s / 35994.7784 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0088
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0084
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1493.25,                last time consumption/overall running time: 1174.5254s / 37169.3039 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0088
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0085
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1560.4,                last time consumption/overall running time: 1400.8403s / 38570.1442 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0089
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0090
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1591.45,                last time consumption/overall running time: 1437.7441s / 40007.8883 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0086
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0086
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1466.25,                last time consumption/overall running time: 1326.5852s / 41334.4735 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0089
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0088
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1520.95,                last time consumption/overall running time: 1374.5294s / 42709.0029 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0085
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0087
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1471.45,                last time consumption/overall running time: 1327.6888s / 44036.6917 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0086
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0087
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1480.8,                last time consumption/overall running time: 1326.3768s / 45363.0685 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0089
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0086
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1454.35,                last time consumption/overall running time: 1304.6721s / 46667.7406 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0087
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0085
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1390.2,                last time consumption/overall running time: 1247.2845s / 47915.0251 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0087
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0085
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1494.45,                last time consumption/overall running time: 1339.7540s / 49254.7790 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0084
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0084
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1536.15,                last time consumption/overall running time: 1374.4118s / 50629.1908 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0084
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0084
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1449.95,                last time consumption/overall running time: 1295.7967s / 51924.9875 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0087
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0086
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1521.25,                last time consumption/overall running time: 1366.2929s / 53291.2804 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0081
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0083
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1495.1,                last time consumption/overall running time: 1340.8420s / 54632.1224 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0081
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0083
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1520.25,                last time consumption/overall running time: 1359.1222s / 55991.2446 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0083
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0079
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1548.85,                last time consumption/overall running time: 1394.4968s / 57385.7414 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0082
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0082
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1507.8,                last time consumption/overall running time: 1349.9504s / 58735.6917 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0079
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0082
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1510.4,                last time consumption/overall running time: 1361.2723s / 60096.9640 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0081
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0083
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1583.2,                last time consumption/overall running time: 1420.3971s / 61517.3611 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0079
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0082
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1582.9,                last time consumption/overall running time: 1418.3281s / 62935.6892 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0076
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0078
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1666.2,                last time consumption/overall running time: 1499.1639s / 64434.8532 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0074
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0076
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1665.65,                last time consumption/overall running time: 1497.6870s / 65932.5402 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0075
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0075
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1629.15,                last time consumption/overall running time: 1462.5754s / 67395.1156 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0074
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0073
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1618.15,                last time consumption/overall running time: 1450.0183s / 68845.1339 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0075
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0075
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1710.0,                last time consumption/overall running time: 1533.0121s / 70378.1461 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0075
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0071
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1685.05,                last time consumption/overall running time: 1509.8413s / 71887.9874 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0072
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0072
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1694.7,                last time consumption/overall running time: 1515.9156s / 73403.9030 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0072
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0074
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1677.3,                last time consumption/overall running time: 1503.0096s / 74906.9126 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0074
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0076
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1528.75,                last time consumption/overall running time: 1372.5378s / 76279.4504 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0072
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0071
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1651.55,                last time consumption/overall running time: 1490.1186s / 77769.5690 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0071
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0070
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1609.2,                last time consumption/overall running time: 1448.7971s / 79218.3661 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0070
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0069
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1675.25,                last time consumption/overall running time: 1505.1163s / 80723.4824 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0073
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0068
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1751.0,                last time consumption/overall running time: 1573.3479s / 82296.8303 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0072
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0070
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1720.7,                last time consumption/overall running time: 1552.6187s / 83849.4490 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0075
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0072
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1736.7,                last time consumption/overall running time: 1558.7136s / 85408.1626 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0072
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0073
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1765.1,                last time consumption/overall running time: 1584.8431s / 86993.0058 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0071
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0071
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1669.7,                last time consumption/overall running time: 1507.7831s / 88500.7888 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0070
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0068
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1721.1,                last time consumption/overall running time: 1539.1639s / 90039.9528 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0072
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0070
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1729.6,                last time consumption/overall running time: 1554.2805s / 91594.2332 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0068
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1654.15,                last time consumption/overall running time: 1481.6549s / 93075.8882 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0071
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0069
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1696.4,                last time consumption/overall running time: 1515.5315s / 94591.4197 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0068
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1767.0,                last time consumption/overall running time: 1579.3673s / 96170.7869 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0070
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0071
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1700.9,                last time consumption/overall running time: 1509.1530s / 97679.9399 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0071
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0072
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1663.1,                last time consumption/overall running time: 1462.1649s / 99142.1048 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0071
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0070
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1605.8,                last time consumption/overall running time: 1420.7842s / 100562.8891 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0072
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0070
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1590.35,                last time consumption/overall running time: 1403.2938s / 101966.1828 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0072
env0_second_0:                 episode reward: -3.0000,                 loss: 0.0071
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1688.55,                last time consumption/overall running time: 1494.1243s / 103460.3071 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0070
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0071
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1700.15,                last time consumption/overall running time: 1498.0929s / 104958.3999 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0069
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0071
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1655.1,                last time consumption/overall running time: 1466.0330s / 106424.4329 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0068
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0069
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1584.15,                last time consumption/overall running time: 1407.1670s / 107831.5999 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0068
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0070
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1632.0,                last time consumption/overall running time: 1440.9987s / 109272.5986 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0068
env0_second_0:                 episode reward: -2.0500,                 loss: 0.0068
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1673.4,                last time consumption/overall running time: 1479.4689s / 110752.0675 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0070
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1715.25,                last time consumption/overall running time: 1512.6417s / 112264.7092 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0073
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0075
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1639.75,                last time consumption/overall running time: 1443.3452s / 113708.0544 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0071
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0070
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1634.45,                last time consumption/overall running time: 1447.8194s / 115155.8738 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0070
env0_second_0:                 episode reward: -2.2500,                 loss: 0.0072
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1799.6,                last time consumption/overall running time: 1589.1688s / 116745.0425 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0070
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0070
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1680.85,                last time consumption/overall running time: 1475.8805s / 118220.9231 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0068
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0068
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1728.25,                last time consumption/overall running time: 1531.3450s / 119752.2681 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0068
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1644.45,                last time consumption/overall running time: 1458.7795s / 121211.0475 s
env0_first_0:                 episode reward: 3.0500,                 loss: 0.0069
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0070
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1746.35,                last time consumption/overall running time: 1541.3742s / 122752.4217 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0067
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0067
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1695.0,                last time consumption/overall running time: 1502.7470s / 124255.1688 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0065
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0070
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1670.5,                last time consumption/overall running time: 1474.8172s / 125729.9859 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0066
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0068
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1625.3,                last time consumption/overall running time: 1435.7737s / 127165.7596 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0065
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0069
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1664.2,                last time consumption/overall running time: 1468.0057s / 128633.7653 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0067
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0069
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1660.35,                last time consumption/overall running time: 1467.5172s / 130101.2825 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0068
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0068
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1665.4,                last time consumption/overall running time: 1465.5217s / 131566.8042 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0068
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0068
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1588.3,                last time consumption/overall running time: 1397.3884s / 132964.1926 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0069
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0068
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1643.35,                last time consumption/overall running time: 1455.7378s / 134419.9304 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0069
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0069
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1747.0,                last time consumption/overall running time: 1538.0220s / 135957.9524 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0069
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1790.15,                last time consumption/overall running time: 1591.4732s / 137549.4256 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0068
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0069
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1709.0,                last time consumption/overall running time: 1516.4840s / 139065.9096 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0069
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1755.45,                last time consumption/overall running time: 1558.2493s / 140624.1589 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0073
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0069
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1769.75,                last time consumption/overall running time: 1564.4381s / 142188.5970 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0068
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1739.6,                last time consumption/overall running time: 1549.5676s / 143738.1646 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1691.35,                last time consumption/overall running time: 1484.1446s / 145222.3092 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1747.6,                last time consumption/overall running time: 1517.8662s / 146740.1754 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0066
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0068
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1777.85,                last time consumption/overall running time: 1551.0758s / 148291.2512 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0070
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1798.1,                last time consumption/overall running time: 1555.7581s / 149847.0093 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0069
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1708.15,                last time consumption/overall running time: 1483.7120s / 151330.7213 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0067
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0068
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1686.2,                last time consumption/overall running time: 1460.2123s / 152790.9337 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0070
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0068
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1734.65,                last time consumption/overall running time: 1492.9570s / 154283.8907 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0067
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1679.15,                last time consumption/overall running time: 1458.8407s / 155742.7314 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0066
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1775.3,                last time consumption/overall running time: 1529.3012s / 157272.0326 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0068
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1754.35,                last time consumption/overall running time: 1523.7996s / 158795.8322 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0067
env0_second_0:                 episode reward: -1.2000,                 loss: 0.0070
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1774.75,                last time consumption/overall running time: 1522.1447s / 160317.9769 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0068
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1839.85,                last time consumption/overall running time: 1580.0389s / 161898.0158 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0067
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0068
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1765.75,                last time consumption/overall running time: 1505.8710s / 163403.8868 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1722.05,                last time consumption/overall running time: 1474.5496s / 164878.4364 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0065
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0068
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1687.45,                last time consumption/overall running time: 1439.1263s / 166317.5626 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0067
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1669.7,                last time consumption/overall running time: 1424.7639s / 167742.3265 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0066
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1749.5,                last time consumption/overall running time: 1492.7477s / 169235.0743 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1789.55,                last time consumption/overall running time: 1522.8671s / 170757.9413 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0066
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0068
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1765.3,                last time consumption/overall running time: 1492.7467s / 172250.6881 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0065
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0069
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1822.8,                last time consumption/overall running time: 1555.1092s / 173805.7973 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0068
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1743.9,                last time consumption/overall running time: 1485.2703s / 175291.0675 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1845.05,                last time consumption/overall running time: 1577.5264s / 176868.5939 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0068
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0068
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1763.7,                last time consumption/overall running time: 1504.0044s / 178372.5983 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0069
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1726.05,                last time consumption/overall running time: 1470.5648s / 179843.1631 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0067
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0068
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1772.05,                last time consumption/overall running time: 1506.1395s / 181349.3025 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1769.75,                last time consumption/overall running time: 1497.0561s / 182846.3586 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0069
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1744.15,                last time consumption/overall running time: 1482.5123s / 184328.8709 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1632.4,                last time consumption/overall running time: 1390.6045s / 185719.4755 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0067
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0062
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1773.8,                last time consumption/overall running time: 1486.3965s / 187205.8720 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0066
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1835.6,                last time consumption/overall running time: 1548.4647s / 188754.3366 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1668.9,                last time consumption/overall running time: 1398.5304s / 190152.8670 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0066
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0067
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1735.3,                last time consumption/overall running time: 1449.6716s / 191602.5386 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0068
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0068
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1737.35,                last time consumption/overall running time: 1444.7972s / 193047.3358 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0069
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0069
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1774.0,                last time consumption/overall running time: 1475.4304s / 194522.7662 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0067
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0068
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1821.3,                last time consumption/overall running time: 1501.7778s / 196024.5440 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0064
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1765.0,                last time consumption/overall running time: 1446.9316s / 197471.4757 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1810.4,                last time consumption/overall running time: 1484.0491s / 198955.5248 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1690.9,                last time consumption/overall running time: 1367.6929s / 200323.2176 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0067
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0068
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1738.05,                last time consumption/overall running time: 1400.6041s / 201723.8218 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0068
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0068
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1679.05,                last time consumption/overall running time: 1348.1771s / 203071.9988 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0067
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1592.05,                last time consumption/overall running time: 1279.4808s / 204351.4797 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0068
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0068
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1698.1,                last time consumption/overall running time: 1373.5226s / 205725.0023 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0065
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0069
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1727.15,                last time consumption/overall running time: 1400.6073s / 207125.6095 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0065
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1759.45,                last time consumption/overall running time: 1420.9599s / 208546.5694 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1668.2,                last time consumption/overall running time: 1342.7003s / 209889.2697 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0064
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1769.45,                last time consumption/overall running time: 1431.5359s / 211320.8056 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0068
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0068
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1828.4,                last time consumption/overall running time: 1478.9214s / 212799.7270 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0067
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0069
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1772.85,                last time consumption/overall running time: 1423.6368s / 214223.3638 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0063
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0064
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1778.65,                last time consumption/overall running time: 1418.7217s / 215642.0855 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0067
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1830.35,                last time consumption/overall running time: 1459.9935s / 217102.0790 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0067
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1772.95,                last time consumption/overall running time: 1400.0926s / 218502.1716 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0068
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1790.65,                last time consumption/overall running time: 1423.6867s / 219925.8583 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1758.35,                last time consumption/overall running time: 1393.0173s / 221318.8755 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1779.3,                last time consumption/overall running time: 1409.4754s / 222728.3510 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0065
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0064
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1906.3,                last time consumption/overall running time: 1515.4008s / 224243.7518 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0064
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1715.8,                last time consumption/overall running time: 1350.7020s / 225594.4538 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0062
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1794.6,                last time consumption/overall running time: 1419.2455s / 227013.6992 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1803.7,                last time consumption/overall running time: 1422.1196s / 228435.8188 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0062
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1706.95,                last time consumption/overall running time: 1335.9690s / 229771.7878 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0066
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1758.65,                last time consumption/overall running time: 1373.5382s / 231145.3260 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0065
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0069
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1675.5,                last time consumption/overall running time: 1301.5652s / 232446.8913 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0065
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1783.85,                last time consumption/overall running time: 1385.6641s / 233832.5554 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0068
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1780.75,                last time consumption/overall running time: 1381.9494s / 235214.5047 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0067
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1697.7,                last time consumption/overall running time: 1329.7616s / 236544.2663 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1837.35,                last time consumption/overall running time: 1423.9258s / 237968.1921 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0068
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1806.35,                last time consumption/overall running time: 1415.4302s / 239383.6223 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1817.55,                last time consumption/overall running time: 1414.8791s / 240798.5014 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0068
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1782.1,                last time consumption/overall running time: 1389.7121s / 242188.2136 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1769.4,                last time consumption/overall running time: 1383.2976s / 243571.5112 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0063
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1698.8,                last time consumption/overall running time: 1316.2869s / 244887.7980 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1694.7,                last time consumption/overall running time: 1323.7440s / 246211.5420 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0069
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1850.75,                last time consumption/overall running time: 1441.1686s / 247652.7106 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0068
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1798.55,                last time consumption/overall running time: 1386.3885s / 249039.0991 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1775.65,                last time consumption/overall running time: 1375.4620s / 250414.5611 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0063
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0062
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1723.35,                last time consumption/overall running time: 1339.5081s / 251754.0693 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0066
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1690.6,                last time consumption/overall running time: 1306.3786s / 253060.4479 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0067
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0067
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1777.3,                last time consumption/overall running time: 1371.0132s / 254431.4611 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0067
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1759.05,                last time consumption/overall running time: 1359.5993s / 255791.0604 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0068
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1740.0,                last time consumption/overall running time: 1347.6242s / 257138.6846 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0067
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0064
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1778.6,                last time consumption/overall running time: 1358.9236s / 258497.6082 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0068
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1824.95,                last time consumption/overall running time: 1387.9562s / 259885.5644 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0066
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1687.95,                last time consumption/overall running time: 1288.5558s / 261174.1202 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0067
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1803.75,                last time consumption/overall running time: 1383.7400s / 262557.8602 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1802.35,                last time consumption/overall running time: 1372.2730s / 263930.1332 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0066
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1747.4,                last time consumption/overall running time: 1328.5182s / 265258.6514 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0064
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1801.8,                last time consumption/overall running time: 1374.4126s / 266633.0639 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1676.2,                last time consumption/overall running time: 1282.1807s / 267915.2446 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0063
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0067
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1729.05,                last time consumption/overall running time: 1326.0174s / 269241.2620 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1815.8,                last time consumption/overall running time: 1389.1831s / 270630.4451 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0067
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1795.35,                last time consumption/overall running time: 1371.4005s / 272001.8455 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0067
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0064
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1773.95,                last time consumption/overall running time: 1355.7879s / 273357.6334 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0065
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1730.6,                last time consumption/overall running time: 1326.4707s / 274684.1041 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1755.55,                last time consumption/overall running time: 1339.4499s / 276023.5541 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0068
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0068
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1781.05,                last time consumption/overall running time: 1363.3808s / 277386.9349 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1797.95,                last time consumption/overall running time: 1374.6473s / 278761.5822 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0067
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0066
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1766.1,                last time consumption/overall running time: 1335.3142s / 280096.8964 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1883.6,                last time consumption/overall running time: 1426.2834s / 281523.1798 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0065
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1810.15,                last time consumption/overall running time: 1367.3500s / 282890.5298 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0063
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1828.0,                last time consumption/overall running time: 1387.3337s / 284277.8635 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1804.5,                last time consumption/overall running time: 1362.9689s / 285640.8324 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0063
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1749.85,                last time consumption/overall running time: 1323.3653s / 286964.1977 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0064
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0066
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1777.15,                last time consumption/overall running time: 1343.4771s / 288307.6748 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0066
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1824.95,                last time consumption/overall running time: 1381.5353s / 289689.2101 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0068
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1787.45,                last time consumption/overall running time: 1346.3760s / 291035.5861 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0068
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0068
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1856.0,                last time consumption/overall running time: 1388.1889s / 292423.7750 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0068
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1861.55,                last time consumption/overall running time: 1382.9170s / 293806.6920 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0066
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0068
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1743.15,                last time consumption/overall running time: 1283.4548s / 295090.1468 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0065
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1799.2,                last time consumption/overall running time: 1322.2232s / 296412.3699 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0067
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1813.05,                last time consumption/overall running time: 1335.1879s / 297747.5579 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0062
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1788.95,                last time consumption/overall running time: 1311.3774s / 299058.9353 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1847.15,                last time consumption/overall running time: 1352.3532s / 300411.2885 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1770.2,                last time consumption/overall running time: 1288.4661s / 301699.7546 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1761.4,                last time consumption/overall running time: 1277.3940s / 302977.1486 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0063
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1769.15,                last time consumption/overall running time: 1286.5432s / 304263.6918 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0063
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0067
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1823.2,                last time consumption/overall running time: 1359.2046s / 305622.8964 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0067
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1857.25,                last time consumption/overall running time: 1342.8528s / 306965.7492 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0063
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1726.65,                last time consumption/overall running time: 1244.9606s / 308210.7098 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0066
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1716.3,                last time consumption/overall running time: 1219.9872s / 309430.6970 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0064
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0067
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1810.3,                last time consumption/overall running time: 1290.1200s / 310720.8170 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1819.6,                last time consumption/overall running time: 1308.8434s / 312029.6604 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0064
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1672.5,                last time consumption/overall running time: 1188.2017s / 313217.8620 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1755.35,                last time consumption/overall running time: 1245.3172s / 314463.1792 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1773.5,                last time consumption/overall running time: 1256.4534s / 315719.6326 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0063
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1732.7,                last time consumption/overall running time: 1215.7102s / 316935.3429 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1665.55,                last time consumption/overall running time: 1159.6133s / 318094.9562 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0062
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1791.75,                last time consumption/overall running time: 1249.2394s / 319344.1956 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1670.5,                last time consumption/overall running time: 1164.9364s / 320509.1320 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0065
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1687.65,                last time consumption/overall running time: 1166.2317s / 321675.3637 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0069
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0069
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1792.0,                last time consumption/overall running time: 1232.5088s / 322907.8725 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0066
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1793.75,                last time consumption/overall running time: 1235.4018s / 324143.2744 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0066
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1840.3,                last time consumption/overall running time: 1262.5986s / 325405.8729 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1799.85,                last time consumption/overall running time: 1231.9051s / 326637.7781 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1765.55,                last time consumption/overall running time: 1206.0070s / 327843.7850 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1790.9,                last time consumption/overall running time: 1224.7803s / 329068.5654 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0065
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1707.55,                last time consumption/overall running time: 1162.8754s / 330231.4408 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0063
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1816.1,                last time consumption/overall running time: 1240.4184s / 331471.8592 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1776.0,                last time consumption/overall running time: 1207.2358s / 332679.0950 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0066
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1831.1,                last time consumption/overall running time: 1248.7426s / 333927.8376 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1795.7,                last time consumption/overall running time: 1222.1366s / 335149.9742 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0062
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1736.4,                last time consumption/overall running time: 1170.4634s / 336320.4375 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1732.05,                last time consumption/overall running time: 1176.9591s / 337497.3967 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0064
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1792.15,                last time consumption/overall running time: 1207.8343s / 338705.2310 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1825.95,                last time consumption/overall running time: 1230.1463s / 339935.3773 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1834.55,                last time consumption/overall running time: 1229.1255s / 341164.5027 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0066
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0062
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1817.95,                last time consumption/overall running time: 1212.3856s / 342376.8883 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0062
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1777.75,                last time consumption/overall running time: 1190.9180s / 343567.8063 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0063
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1829.4,                last time consumption/overall running time: 1220.5172s / 344788.3235 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1762.55,                last time consumption/overall running time: 1181.3656s / 345969.6891 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0066
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1773.4,                last time consumption/overall running time: 1179.8726s / 347149.5617 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1712.8,                last time consumption/overall running time: 1134.5735s / 348284.1351 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0066
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1732.95,                last time consumption/overall running time: 1151.0067s / 349435.1418 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0064
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1750.35,                last time consumption/overall running time: 1163.9045s / 350599.0463 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1774.45,                last time consumption/overall running time: 1175.8375s / 351774.8838 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0066
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1717.3,                last time consumption/overall running time: 1139.6089s / 352914.4927 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1820.7,                last time consumption/overall running time: 1206.9299s / 354121.4226 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1666.65,                last time consumption/overall running time: 1103.5024s / 355224.9250 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1707.9,                last time consumption/overall running time: 1141.5292s / 356366.4542 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0068
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1667.95,                last time consumption/overall running time: 1095.7541s / 357462.2083 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0069
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0065
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1630.45,                last time consumption/overall running time: 1075.0761s / 358537.2843 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0065
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1738.15,                last time consumption/overall running time: 1133.7497s / 359671.0340 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0062
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1764.3,                last time consumption/overall running time: 1159.7124s / 360830.7464 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0067
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1783.1,                last time consumption/overall running time: 1176.2936s / 362007.0400 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1795.95,                last time consumption/overall running time: 1178.9847s / 363186.0248 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0065
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1710.1,                last time consumption/overall running time: 1118.5196s / 364304.5444 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0062
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1762.6,                last time consumption/overall running time: 1143.1954s / 365447.7397 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0062
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0063
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1754.5,                last time consumption/overall running time: 1141.2186s / 366588.9584 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0061
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1786.95,                last time consumption/overall running time: 1167.0318s / 367755.9902 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0063
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1812.85,                last time consumption/overall running time: 1174.5439s / 368930.5341 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0064
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1802.35,                last time consumption/overall running time: 1160.2461s / 370090.7802 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0060
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1747.5,                last time consumption/overall running time: 1150.1085s / 371240.8887 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0063
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1790.9,                last time consumption/overall running time: 1160.0473s / 372400.9360 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0061
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0062
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1813.75,                last time consumption/overall running time: 1172.9657s / 373573.9017 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0063
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0063
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1799.55,                last time consumption/overall running time: 1152.0327s / 374725.9344 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1747.6,                last time consumption/overall running time: 1114.0879s / 375840.0224 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1773.35,                last time consumption/overall running time: 1130.8356s / 376970.8580 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0063
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1714.35,                last time consumption/overall running time: 1099.7516s / 378070.6097 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0061
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1700.25,                last time consumption/overall running time: 1091.6400s / 379162.2497 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0063
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1801.25,                last time consumption/overall running time: 1162.9802s / 380325.2298 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0063
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1770.55,                last time consumption/overall running time: 1133.2064s / 381458.4363 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1729.65,                last time consumption/overall running time: 1108.7905s / 382567.2268 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0067
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0066
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1739.15,                last time consumption/overall running time: 1112.3020s / 383679.5288 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0066
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0065
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1748.1,                last time consumption/overall running time: 1124.1740s / 384803.7028 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0064
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0066
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1848.65,                last time consumption/overall running time: 1312.5097s / 386116.2126 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1732.05,                last time consumption/overall running time: 1267.2326s / 387383.4452 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1720.4,                last time consumption/overall running time: 1254.1161s / 388637.5613 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0063
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1762.9,                last time consumption/overall running time: 1293.4876s / 389931.0489 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0062
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1652.25,                last time consumption/overall running time: 1223.9217s / 391154.9706 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0064
env0_second_0:                 episode reward: -2.0500,                 loss: 0.0066
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1795.3,                last time consumption/overall running time: 1297.3577s / 392452.3283 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0064
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1771.0,                last time consumption/overall running time: 1275.2784s / 393727.6066 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0059
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0062
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1795.25,                last time consumption/overall running time: 1290.7585s / 395018.3652 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0064
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1783.2,                last time consumption/overall running time: 1289.2114s / 396307.5765 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0062
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1805.7,                last time consumption/overall running time: 1298.8415s / 397606.4180 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0062
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0061
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1875.25,                last time consumption/overall running time: 1331.7620s / 398938.1800 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0062
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 1706.95,                last time consumption/overall running time: 1228.3747s / 400166.5548 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0063
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0066
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1676.05,                last time consumption/overall running time: 1206.7858s / 401373.3406 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0061
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1733.55,                last time consumption/overall running time: 1237.5059s / 402610.8465 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0062
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0063
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 1855.55,                last time consumption/overall running time: 1321.9782s / 403932.8248 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0061
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0062
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1817.55,                last time consumption/overall running time: 1299.1976s / 405232.0223 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1812.4,                last time consumption/overall running time: 1292.2940s / 406524.3164 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1721.95,                last time consumption/overall running time: 1218.0856s / 407742.4019 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0062
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0062
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1832.05,                last time consumption/overall running time: 1302.1361s / 409044.5380 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0061
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0060
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1805.8,                last time consumption/overall running time: 1282.0158s / 410326.5537 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1796.55,                last time consumption/overall running time: 1268.0828s / 411594.6365 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0063
env0_second_0:                 episode reward: -1.7000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1812.15,                last time consumption/overall running time: 1279.4853s / 412874.1218 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1798.25,                last time consumption/overall running time: 1261.5105s / 414135.6324 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1708.9,                last time consumption/overall running time: 1200.9713s / 415336.6037 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0066
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1853.15,                last time consumption/overall running time: 1308.8113s / 416645.4150 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1831.45,                last time consumption/overall running time: 1287.3132s / 417932.7282 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1753.2,                last time consumption/overall running time: 1242.7838s / 419175.5119 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1772.8,                last time consumption/overall running time: 1246.0346s / 420421.5465 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0062
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 1876.55,                last time consumption/overall running time: 1316.4894s / 421738.0359 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0068
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1700.95,                last time consumption/overall running time: 1180.4555s / 422918.4914 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0067
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0068
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1880.65,                last time consumption/overall running time: 1303.7765s / 424222.2679 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 1737.55,                last time consumption/overall running time: 1210.6281s / 425432.8959 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0067
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1878.2,                last time consumption/overall running time: 1304.9929s / 426737.8888 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0067
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1797.75,                last time consumption/overall running time: 1238.7140s / 427976.6028 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1739.8,                last time consumption/overall running time: 1196.2458s / 429172.8487 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0064
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0064
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1894.4,                last time consumption/overall running time: 1316.2413s / 430489.0899 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1828.75,                last time consumption/overall running time: 1266.8274s / 431755.9173 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0061
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1785.6,                last time consumption/overall running time: 1243.2823s / 432999.1997 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0067
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1736.3,                last time consumption/overall running time: 1213.8893s / 434213.0890 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0066
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1740.75,                last time consumption/overall running time: 1206.2799s / 435419.3689 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0064
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0063
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1818.95,                last time consumption/overall running time: 1259.4585s / 436678.8274 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0067
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1688.45,                last time consumption/overall running time: 1164.0600s / 437842.8873 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1739.75,                last time consumption/overall running time: 1189.9213s / 439032.8086 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0065
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1813.9,                last time consumption/overall running time: 1241.2042s / 440274.0128 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0062
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1774.7,                last time consumption/overall running time: 1221.8504s / 441495.8632 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 1804.35,                last time consumption/overall running time: 1286.9004s / 442782.7636 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0063
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1848.0,                last time consumption/overall running time: 1255.1735s / 444037.9371 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1750.45,                last time consumption/overall running time: 1194.4780s / 445232.4151 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0065
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0067
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1854.05,                last time consumption/overall running time: 1233.9800s / 446466.3951 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0067
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1735.75,                last time consumption/overall running time: 1144.8505s / 447611.2456 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0063
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1851.4,                last time consumption/overall running time: 1190.1180s / 448801.3636 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1795.6,                last time consumption/overall running time: 1143.5556s / 449944.9192 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0061
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1804.65,                last time consumption/overall running time: 1141.9512s / 451086.8705 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0063
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1807.85,                last time consumption/overall running time: 1141.2187s / 452228.0892 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1722.05,                last time consumption/overall running time: 1081.8161s / 453309.9053 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0064
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0069
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1761.15,                last time consumption/overall running time: 1106.9689s / 454416.8742 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1768.35,                last time consumption/overall running time: 1110.5336s / 455527.4078 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 1819.85,                last time consumption/overall running time: 1131.8858s / 456659.2936 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1773.85,                last time consumption/overall running time: 1101.9565s / 457761.2501 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0061
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0062
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1686.5,                last time consumption/overall running time: 1058.4363s / 458819.6864 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0063
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1788.05,                last time consumption/overall running time: 1117.5959s / 459937.2823 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1735.7,                last time consumption/overall running time: 1085.9417s / 461023.2240 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 1778.45,                last time consumption/overall running time: 1109.7879s / 462133.0119 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0063
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1752.35,                last time consumption/overall running time: 1093.3270s / 463226.3388 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1728.0,                last time consumption/overall running time: 1074.2614s / 464300.6003 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0062
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0063
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1771.05,                last time consumption/overall running time: 1108.8878s / 465409.4881 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0061
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1900.75,                last time consumption/overall running time: 1228.8979s / 466638.3860 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0064
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 1807.7,                last time consumption/overall running time: 1277.7053s / 467916.0913 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0063
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 1725.8,                last time consumption/overall running time: 1225.5342s / 469141.6255 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1780.15,                last time consumption/overall running time: 1265.7096s / 470407.3351 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0061
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 1827.5,                last time consumption/overall running time: 1286.3999s / 471693.7350 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1761.65,                last time consumption/overall running time: 1253.4716s / 472947.2066 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0063
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0066
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1885.25,                last time consumption/overall running time: 1330.3885s / 474277.5952 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1816.85,                last time consumption/overall running time: 1272.1227s / 475549.7179 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1735.2,                last time consumption/overall running time: 1230.1885s / 476779.9064 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0063
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1843.45,                last time consumption/overall running time: 1297.2566s / 478077.1629 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1893.05,                last time consumption/overall running time: 1336.1438s / 479413.3067 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1830.05,                last time consumption/overall running time: 1287.5155s / 480700.8222 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0061
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1765.45,                last time consumption/overall running time: 1250.7863s / 481951.6085 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0062
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0061
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1726.85,                last time consumption/overall running time: 1233.4394s / 483185.0479 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0068
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1783.95,                last time consumption/overall running time: 1267.5812s / 484452.6291 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0065
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0072
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1843.95,                last time consumption/overall running time: 1305.3325s / 485757.9617 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0065
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1851.85,                last time consumption/overall running time: 1313.2637s / 487071.2253 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0064
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0067
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1826.05,                last time consumption/overall running time: 1292.5779s / 488363.8033 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0062
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0067
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1857.0,                last time consumption/overall running time: 1313.3691s / 489677.1724 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1809.35,                last time consumption/overall running time: 1284.6146s / 490961.7869 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 1824.05,                last time consumption/overall running time: 1289.8347s / 492251.6216 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0063
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1775.1,                last time consumption/overall running time: 1250.7300s / 493502.3516 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1816.35,                last time consumption/overall running time: 1282.1134s / 494784.4650 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0062
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1784.05,                last time consumption/overall running time: 1259.0018s / 496043.4668 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0066
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1876.95,                last time consumption/overall running time: 1319.9392s / 497363.4060 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1848.15,                last time consumption/overall running time: 1298.5287s / 498661.9348 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0064
env0_second_0:                 episode reward: -1.0000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1790.5,                last time consumption/overall running time: 1264.9083s / 499926.8431 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0062
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1718.25,                last time consumption/overall running time: 1215.1300s / 501141.9731 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0060
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1795.2,                last time consumption/overall running time: 1264.6904s / 502406.6635 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0061
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 1776.8,                last time consumption/overall running time: 1244.2755s / 503650.9390 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0066
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1845.4,                last time consumption/overall running time: 1309.5329s / 504960.4719 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1850.05,                last time consumption/overall running time: 1303.5408s / 506264.0127 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0063
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1824.05,                last time consumption/overall running time: 1299.8161s / 507563.8288 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0062
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0067
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1775.4,                last time consumption/overall running time: 1326.7334s / 508890.5622 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 1783.75,                last time consumption/overall running time: 1685.5157s / 510576.0779 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 1749.7,                last time consumption/overall running time: 1653.8833s / 512229.9612 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0064
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 1925.1,                last time consumption/overall running time: 1820.1812s / 514050.1424 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 1813.6,                last time consumption/overall running time: 1702.4770s / 515752.6193 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 1723.45,                last time consumption/overall running time: 1617.5961s / 517370.2154 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 1811.95,                last time consumption/overall running time: 1686.8189s / 519057.0344 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0061
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 1838.3,                last time consumption/overall running time: 1712.6867s / 520769.7211 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0063
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 1710.7,                last time consumption/overall running time: 1610.8117s / 522380.5328 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0060
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1856.0,                last time consumption/overall running time: 1719.3027s / 524099.8355 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0060
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1746.0,                last time consumption/overall running time: 1616.0951s / 525715.9305 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0062
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0063
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 1895.95,                last time consumption/overall running time: 1762.2391s / 527478.1696 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 1816.2,                last time consumption/overall running time: 1672.8772s / 529151.0468 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0063
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 1848.45,                last time consumption/overall running time: 1720.0303s / 530871.0771 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0064
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 1837.2,                last time consumption/overall running time: 1705.4474s / 532576.5246 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0066
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0063
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 1926.8,                last time consumption/overall running time: 1786.7791s / 534363.3037 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0063
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1849.15,                last time consumption/overall running time: 1711.5035s / 536074.8072 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0062
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1781.0,                last time consumption/overall running time: 1644.6745s / 537719.4817 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0062
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 1857.9,                last time consumption/overall running time: 1711.2176s / 539430.6993 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0062
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0063
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 1814.75,                last time consumption/overall running time: 1670.6223s / 541101.3215 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1777.15,                last time consumption/overall running time: 1626.4336s / 542727.7551 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1815.65,                last time consumption/overall running time: 1650.0445s / 544377.7996 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0063
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 1825.8,                last time consumption/overall running time: 1661.4326s / 546039.2323 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0063
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0064
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1660.85,                last time consumption/overall running time: 1486.9443s / 547526.1766 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0066
env0_second_0:                 episode reward: -1.2000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 1700.7,                last time consumption/overall running time: 1546.5982s / 549072.7748 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0063
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1742.45,                last time consumption/overall running time: 1561.4086s / 550634.1834 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1779.7,                last time consumption/overall running time: 1611.4826s / 552245.6661 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 1781.2,                last time consumption/overall running time: 1617.2139s / 553862.8799 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0061
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 1800.5,                last time consumption/overall running time: 1622.1998s / 555485.0798 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0061
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0061
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 1777.05,                last time consumption/overall running time: 1606.5527s / 557091.6325 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0062
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 1789.0,                last time consumption/overall running time: 1608.4834s / 558700.1158 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0063
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 1815.6,                last time consumption/overall running time: 1619.6023s / 560319.7181 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0066
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1849.45,                last time consumption/overall running time: 1627.8287s / 561947.5469 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0064
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 1803.85,                last time consumption/overall running time: 1566.2222s / 563513.7691 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0060
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0064
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1841.3,                last time consumption/overall running time: 1613.5385s / 565127.3075 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1721.4,                last time consumption/overall running time: 1515.9715s / 566643.2790 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1867.4,                last time consumption/overall running time: 1636.3059s / 568279.5849 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0059
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0063
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 1807.9,                last time consumption/overall running time: 1576.4860s / 569856.0709 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0061
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0063
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 1869.65,                last time consumption/overall running time: 1640.9688s / 571497.0397 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0064
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0062
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1679.45,                last time consumption/overall running time: 1476.1331s / 572973.1728 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0063
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 1794.85,                last time consumption/overall running time: 1577.0198s / 574550.1926 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0062
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 1740.0,                last time consumption/overall running time: 1507.8452s / 576058.0378 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0066
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0068
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 1830.4,                last time consumption/overall running time: 1596.5203s / 577654.5580 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0065
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0067
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 1811.9,                last time consumption/overall running time: 1569.9552s / 579224.5132 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0063
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1770.4,                last time consumption/overall running time: 1554.7870s / 580779.3002 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1814.6,                last time consumption/overall running time: 1565.3938s / 582344.6939 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 1792.8,                last time consumption/overall running time: 1542.0423s / 583886.7362 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1815.7,                last time consumption/overall running time: 1521.6847s / 585408.4209 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1828.05,                last time consumption/overall running time: 1536.6543s / 586945.0751 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0065
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1752.65,                last time consumption/overall running time: 1468.3170s / 588413.3922 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0064
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1764.25,                last time consumption/overall running time: 1460.9884s / 589874.3806 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0065
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1759.5,                last time consumption/overall running time: 1468.8589s / 591343.2394 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0064
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1747.6,                last time consumption/overall running time: 1455.6677s / 592798.9071 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1812.0,                last time consumption/overall running time: 1508.9971s / 594307.9043 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0063
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0063
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1736.25,                last time consumption/overall running time: 1438.6833s / 595746.5875 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0066
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 1738.35,                last time consumption/overall running time: 1439.9286s / 597186.5161 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1790.85,                last time consumption/overall running time: 1470.2602s / 598656.7764 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0063
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 1769.4,                last time consumption/overall running time: 1475.7815s / 600132.5579 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0065
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0065
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1675.6,                last time consumption/overall running time: 1363.1983s / 601495.7562 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0064
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 1624.45,                last time consumption/overall running time: 1323.4101s / 602819.1663 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0066
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0069
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1818.0,                last time consumption/overall running time: 1483.5815s / 604302.7478 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0065
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1813.9,                last time consumption/overall running time: 1475.4759s / 605778.2237 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0066
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1850.95,                last time consumption/overall running time: 1504.4058s / 607282.6294 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0065
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0064
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1913.65,                last time consumption/overall running time: 1565.4587s / 608848.0881 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0065
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1828.7,                last time consumption/overall running time: 1485.8993s / 610333.9873 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0066
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 1802.7,                last time consumption/overall running time: 1462.3073s / 611796.2946 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 1832.85,                last time consumption/overall running time: 1489.0197s / 613285.3143 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0062
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1849.05,                last time consumption/overall running time: 1496.8489s / 614782.1633 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0065
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0062
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 1793.0,                last time consumption/overall running time: 1435.0857s / 616217.2490 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0066
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 1754.2,                last time consumption/overall running time: 1397.0974s / 617614.3464 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0064
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 1727.45,                last time consumption/overall running time: 1361.6916s / 618976.0380 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0064
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0064
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1786.4,                last time consumption/overall running time: 1404.2119s / 620380.2500 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0064
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0062
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1954.6,                last time consumption/overall running time: 1544.3226s / 621924.5726 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 1850.1,                last time consumption/overall running time: 1456.7679s / 623381.3405 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0063
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0063
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 1690.75,                last time consumption/overall running time: 1310.4959s / 624691.8364 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0061
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1876.2,                last time consumption/overall running time: 1458.7205s / 626150.5569 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0066
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0064
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1747.45,                last time consumption/overall running time: 1369.5089s / 627520.0657 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0061
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0064
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 1721.65,                last time consumption/overall running time: 1344.3369s / 628864.4026 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0063
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0063
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 1808.8,                last time consumption/overall running time: 1397.2724s / 630261.6750 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0066
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0063
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 1827.65,                last time consumption/overall running time: 1391.9681s / 631653.6431 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0063
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1843.65,                last time consumption/overall running time: 1394.0637s / 633047.7068 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1751.25,                last time consumption/overall running time: 1300.4803s / 634348.1871 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0064
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0062
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1856.55,                last time consumption/overall running time: 1374.2838s / 635722.4709 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1862.05,                last time consumption/overall running time: 1374.9988s / 637097.4697 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0066
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 1754.05,                last time consumption/overall running time: 1312.4438s / 638409.9135 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0064
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0064
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 1716.75,                last time consumption/overall running time: 1276.2665s / 639686.1800 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0063
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0066
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 1764.35,                last time consumption/overall running time: 1316.1701s / 641002.3501 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0066
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 1825.4,                last time consumption/overall running time: 1340.0793s / 642342.4294 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0062
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1813.05,                last time consumption/overall running time: 1316.2395s / 643658.6690 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0065
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 1795.85,                last time consumption/overall running time: 1303.3077s / 644961.9766 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 1756.9,                last time consumption/overall running time: 1263.8189s / 646225.7955 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0065
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1811.4,                last time consumption/overall running time: 1290.2549s / 647516.0504 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 1778.7,                last time consumption/overall running time: 1265.4068s / 648781.4573 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0067
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0068
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1866.45,                last time consumption/overall running time: 1327.8973s / 650109.3546 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0066
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0065
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 1813.55,                last time consumption/overall running time: 1284.6178s / 651393.9724 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0065
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0065
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 1829.2,                last time consumption/overall running time: 1297.1673s / 652691.1397 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0062
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0066
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1863.6,                last time consumption/overall running time: 1324.2696s / 654015.4093 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0067
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 1752.0,                last time consumption/overall running time: 1230.8946s / 655246.3039 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0063
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0065
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1809.1,                last time consumption/overall running time: 1281.5262s / 656527.8302 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0063
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0063
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 1750.25,                last time consumption/overall running time: 1251.2911s / 657779.1213 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0064
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0063
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 1806.55,                last time consumption/overall running time: 1279.9018s / 659059.0231 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0063
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0064Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
Load surround_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(5)
/home/zihan/research/MARS/mars/rl/agents/nash_dqn.py:293: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  action = torch.FloatTensor(action).to(self.device)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1810.1,                last time consumption/overall running time: 1279.1210s / 660338.1441 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0062
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0062
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 1876.9,                last time consumption/overall running time: 1320.2414s / 661658.3854 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0063
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1810.05,                last time consumption/overall running time: 1269.7606s / 662928.1460 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0065
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0066
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1851.85,                last time consumption/overall running time: 1297.4475s / 664225.5936 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0067
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 1762.35,                last time consumption/overall running time: 1230.2444s / 665455.8380 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0064
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0064
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 1765.35,                last time consumption/overall running time: 1233.4891s / 666689.3270 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0064
env0_second_0:                 episode reward: -0.3000,                 loss: 0.0068
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 1826.4,                last time consumption/overall running time: 1266.8063s / 667956.1333 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0065
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0067
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1782.55,                last time consumption/overall running time: 1242.6470s / 669198.7803 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0066
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0064
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1788.2,                last time consumption/overall running time: 1238.1828s / 670436.9631 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0064
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0063
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1882.6,                last time consumption/overall running time: 1282.7583s / 671719.7214 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0067
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0065
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1727.15,                last time consumption/overall running time: 1184.9281s / 672904.6495 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0068
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0068
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
