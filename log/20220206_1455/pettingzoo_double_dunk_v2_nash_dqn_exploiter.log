pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_1455/pettingzoo_double_dunk_v2_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_1455/pettingzoo_double_dunk_v2_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 2654.0,                last time consumption/overall running time: 186.1456s / 186.1456 s
env0_first_0:                 episode reward: -11.0000,                 loss: 0.0457
env0_second_0:                 episode reward: 11.0000,                 loss: 0.0454
env1_first_0:                 episode reward: -27.0000,                 loss: nan
env1_second_0:                 episode reward: 27.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 3993.45,                last time consumption/overall running time: 6540.7562s / 6726.9018 s
env0_first_0:                 episode reward: -19.6000,                 loss: 0.0745
env0_second_0:                 episode reward: 19.6000,                 loss: 0.0719
env1_first_0:                 episode reward: -17.8500,                 loss: nan
env1_second_0:                 episode reward: 17.8500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 4076.0,                last time consumption/overall running time: 7705.3872s / 14432.2890 s
env0_first_0:                 episode reward: -12.1000,                 loss: 0.0653
env0_second_0:                 episode reward: 12.1000,                 loss: 0.0647
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 4163.0,                last time consumption/overall running time: 7875.2822s / 22307.5712 s
env0_first_0:                 episode reward: -26.3000,                 loss: 0.0607
env0_second_0:                 episode reward: 26.3000,                 loss: 0.0580
env1_first_0:                 episode reward: -28.7000,                 loss: nan
env1_second_0:                 episode reward: 28.7000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 3779.6,                last time consumption/overall running time: 7190.1435s / 29497.7147 s
env0_first_0:                 episode reward: -30.2500,                 loss: 0.0523
env0_second_0:                 episode reward: 30.2500,                 loss: 0.0498
env1_first_0:                 episode reward: -29.9000,                 loss: nan
env1_second_0:                 episode reward: 29.9000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 3470.5,                last time consumption/overall running time: 6601.9462s / 36099.6609 s
env0_first_0:                 episode reward: -27.2500,                 loss: 0.0486
env0_second_0:                 episode reward: 27.2500,                 loss: 0.0469
env1_first_0:                 episode reward: -25.8500,                 loss: nan
env1_second_0:                 episode reward: 25.8500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 3223.05,                last time consumption/overall running time: 6126.4783s / 42226.1392 s
env0_first_0:                 episode reward: -25.4500,                 loss: 0.0483
env0_second_0:                 episode reward: 25.4500,                 loss: 0.0475
env1_first_0:                 episode reward: -26.3000,                 loss: nan
env1_second_0:                 episode reward: 26.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 3220.3,                last time consumption/overall running time: 6116.9757s / 48343.1149 s
env0_first_0:                 episode reward: -27.3500,                 loss: 0.0475
env0_second_0:                 episode reward: 27.3500,                 loss: 0.0462
env1_first_0:                 episode reward: -27.0500,                 loss: nan
env1_second_0:                 episode reward: 27.0500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 3394.05,                last time consumption/overall running time: 6480.9764s / 54824.0913 s
env0_first_0:                 episode reward: -31.4500,                 loss: 0.0485
env0_second_0:                 episode reward: 31.4500,                 loss: 0.0471
env1_first_0:                 episode reward: -32.6500,                 loss: nan
env1_second_0:                 episode reward: 32.6500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 3307.3,                last time consumption/overall running time: 6325.7588s / 61149.8500 s
env0_first_0:                 episode reward: -27.7000,                 loss: 0.0483
env0_second_0:                 episode reward: 27.7000,                 loss: 0.0467
env1_first_0:                 episode reward: -26.8500,                 loss: nan
env1_second_0:                 episode reward: 26.8500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 3635.55,                last time consumption/overall running time: 6935.4369s / 68085.2869 s
env0_first_0:                 episode reward: -30.9500,                 loss: 0.0472
env0_second_0:                 episode reward: 30.9500,                 loss: 0.0451
env1_first_0:                 episode reward: -31.8500,                 loss: nan
env1_second_0:                 episode reward: 31.8500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 3698.8,                last time consumption/overall running time: 7021.7306s / 75107.0175 s
env0_first_0:                 episode reward: -29.8000,                 loss: 0.0467
env0_second_0:                 episode reward: 29.8000,                 loss: 0.0459
env1_first_0:                 episode reward: -30.9500,                 loss: nan
env1_second_0:                 episode reward: 30.9500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 3416.45,                last time consumption/overall running time: 6519.4968s / 81626.5143 s
env0_first_0:                 episode reward: -29.0500,                 loss: 0.0433
env0_second_0:                 episode reward: 29.0500,                 loss: 0.0415
env1_first_0:                 episode reward: -29.3500,                 loss: nan
env1_second_0:                 episode reward: 29.3500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 3063.95,                last time consumption/overall running time: 5828.8672s / 87455.3814 s
env0_first_0:                 episode reward: -27.3000,                 loss: 0.0421
env0_second_0:                 episode reward: 27.3000,                 loss: 0.0403
env1_first_0:                 episode reward: -28.3500,                 loss: nan
env1_second_0:                 episode reward: 28.3500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2901.25,                last time consumption/overall running time: 5514.8649s / 92970.2463 s
env0_first_0:                 episode reward: -24.1000,                 loss: 0.0409
env0_second_0:                 episode reward: 24.1000,                 loss: 0.0406
env1_first_0:                 episode reward: -24.8500,                 loss: nan
env1_second_0:                 episode reward: 24.8500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 3305.4,                last time consumption/overall running time: 6301.0877s / 99271.3340 s
env0_first_0:                 episode reward: -26.8500,                 loss: 0.0394
env0_second_0:                 episode reward: 26.8500,                 loss: 0.0394
env1_first_0:                 episode reward: -26.3500,                 loss: nan
env1_second_0:                 episode reward: 26.3500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 2886.25,                last time consumption/overall running time: 5514.7794s / 104786.1134 s
env0_first_0:                 episode reward: -25.8500,                 loss: 0.0386
env0_second_0:                 episode reward: 25.8500,                 loss: 0.0387
env1_first_0:                 episode reward: -24.2500,                 loss: nan
env1_second_0:                 episode reward: 24.2500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2948.4,                last time consumption/overall running time: 5582.9364s / 110369.0498 s
env0_first_0:                 episode reward: -23.3500,                 loss: 0.0387
env0_second_0:                 episode reward: 23.3500,                 loss: 0.0383
env1_first_0:                 episode reward: -26.5500,                 loss: nan
env1_second_0:                 episode reward: 26.5500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2919.4,                last time consumption/overall running time: 5531.7052s / 115900.7550 s
env0_first_0:                 episode reward: -26.4500,                 loss: 0.0356
env0_second_0:                 episode reward: 26.4500,                 loss: 0.0360
env1_first_0:                 episode reward: -26.2500,                 loss: nan
env1_second_0:                 episode reward: 26.2500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 3009.35,                last time consumption/overall running time: 5715.8966s / 121616.6516 s
env0_first_0:                 episode reward: -26.2000,                 loss: 0.0337
env0_second_0:                 episode reward: 26.2000,                 loss: 0.0353
env1_first_0:                 episode reward: -26.2500,                 loss: nan
env1_second_0:                 episode reward: 26.2500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 2978.05,                last time consumption/overall running time: 5603.0315s / 127219.6831 s
env0_first_0:                 episode reward: -25.9000,                 loss: 0.0319
env0_second_0:                 episode reward: 25.9000,                 loss: 0.0333
env1_first_0:                 episode reward: -25.9000,                 loss: nan
env1_second_0:                 episode reward: 25.9000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 3099.9,                last time consumption/overall running time: 5811.5994s / 133031.2825 s
env0_first_0:                 episode reward: -30.4000,                 loss: 0.0322
env0_second_0:                 episode reward: 30.4000,                 loss: 0.0328
env1_first_0:                 episode reward: -27.9500,                 loss: nan
env1_second_0:                 episode reward: 27.9500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2852.05,                last time consumption/overall running time: 5354.7362s / 138386.0187 s
env0_first_0:                 episode reward: -25.0500,                 loss: 0.0315
env0_second_0:                 episode reward: 25.0500,                 loss: 0.0317
env1_first_0:                 episode reward: -26.3500,                 loss: nan
env1_second_0:                 episode reward: 26.3500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 3091.5,                last time consumption/overall running time: 5818.8277s / 144204.8464 s
env0_first_0:                 episode reward: -27.0000,                 loss: 0.0305
env0_second_0:                 episode reward: 27.0000,                 loss: 0.0303
env1_first_0:                 episode reward: -26.8000,                 loss: nan
env1_second_0:                 episode reward: 26.8000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2821.75,                last time consumption/overall running time: 5314.9016s / 149519.7480 s
env0_first_0:                 episode reward: -22.1000,                 loss: 0.0296
env0_second_0:                 episode reward: 22.1000,                 loss: 0.0302
env1_first_0:                 episode reward: -23.4500,                 loss: nan
env1_second_0:                 episode reward: 23.4500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2976.4,                last time consumption/overall running time: 5603.7441s / 155123.4921 s
env0_first_0:                 episode reward: -23.4000,                 loss: 0.0302
env0_second_0:                 episode reward: 23.4000,                 loss: 0.0299
env1_first_0:                 episode reward: -24.2500,                 loss: nan
env1_second_0:                 episode reward: 24.2500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2930.65,                last time consumption/overall running time: 5468.9318s / 160592.4239 s
env0_first_0:                 episode reward: -24.4000,                 loss: 0.0287
env0_second_0:                 episode reward: 24.4000,                 loss: 0.0295
env1_first_0:                 episode reward: -21.7500,                 loss: nan
env1_second_0:                 episode reward: 21.7500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2825.85,                last time consumption/overall running time: 5306.8344s / 165899.2583 s
env0_first_0:                 episode reward: -22.9500,                 loss: 0.0276
env0_second_0:                 episode reward: 22.9500,                 loss: 0.0285
env1_first_0:                 episode reward: -22.0000,                 loss: nan
env1_second_0:                 episode reward: 22.0000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2692.9,                last time consumption/overall running time: 4986.6542s / 170885.9126 s
env0_first_0:                 episode reward: -21.7000,                 loss: 0.0273
env0_second_0:                 episode reward: 21.7000,                 loss: 0.0285
env1_first_0:                 episode reward: -24.1000,                 loss: nan
env1_second_0:                 episode reward: 24.1000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2566.95,                last time consumption/overall running time: 4751.3969s / 175637.3095 s
env0_first_0:                 episode reward: -21.9500,                 loss: 0.0265
env0_second_0:                 episode reward: 21.9500,                 loss: 0.0277
env1_first_0:                 episode reward: -23.8500,                 loss: nan
env1_second_0:                 episode reward: 23.8500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2448.5,                last time consumption/overall running time: 4507.1899s / 180144.4994 s
env0_first_0:                 episode reward: -22.5000,                 loss: 0.0258
env0_second_0:                 episode reward: 22.5000,                 loss: 0.0281
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2308.0,                last time consumption/overall running time: 4247.5718s / 184392.0712 s
env0_first_0:                 episode reward: -20.4000,                 loss: 0.0262
env0_second_0:                 episode reward: 20.4000,                 loss: 0.0290
env1_first_0:                 episode reward: -18.5500,                 loss: nan
env1_second_0:                 episode reward: 18.5500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2545.85,                last time consumption/overall running time: 4676.8744s / 189068.9456 s
env0_first_0:                 episode reward: -19.4000,                 loss: 0.0267
env0_second_0:                 episode reward: 19.4000,                 loss: 0.0293
env1_first_0:                 episode reward: -18.9500,                 loss: nan
env1_second_0:                 episode reward: 18.9500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2754.75,                last time consumption/overall running time: 5054.8933s / 194123.8389 s
env0_first_0:                 episode reward: -21.2500,                 loss: 0.0269
env0_second_0:                 episode reward: 21.2500,                 loss: 0.0285
env1_first_0:                 episode reward: -21.2500,                 loss: nan
env1_second_0:                 episode reward: 21.2500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2706.45,                last time consumption/overall running time: 5000.4138s / 199124.2527 s
env0_first_0:                 episode reward: -17.6500,                 loss: 0.0262
env0_second_0:                 episode reward: 17.6500,                 loss: 0.0274
env1_first_0:                 episode reward: -18.9000,                 loss: nan
env1_second_0:                 episode reward: 18.9000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2325.2,                last time consumption/overall running time: 4289.2882s / 203413.5409 s
env0_first_0:                 episode reward: -19.3000,                 loss: 0.0263
env0_second_0:                 episode reward: 19.3000,                 loss: 0.0269
env1_first_0:                 episode reward: -19.1500,                 loss: nan
env1_second_0:                 episode reward: 19.1500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2414.2,                last time consumption/overall running time: 4440.0774s / 207853.6184 s
env0_first_0:                 episode reward: -18.9000,                 loss: 0.0259
env0_second_0:                 episode reward: 18.9000,                 loss: 0.0268
env1_first_0:                 episode reward: -19.4000,                 loss: nan
env1_second_0:                 episode reward: 19.4000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2816.8,                last time consumption/overall running time: 5187.5861s / 213041.2044 s
env0_first_0:                 episode reward: -20.0000,                 loss: 0.0248
env0_second_0:                 episode reward: 20.0000,                 loss: 0.0266
env1_first_0:                 episode reward: -21.9500,                 loss: nan
env1_second_0:                 episode reward: 21.9500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2560.85,                last time consumption/overall running time: 4736.0227s / 217777.2271 s
env0_first_0:                 episode reward: -20.6500,                 loss: 0.0250
env0_second_0:                 episode reward: 20.6500,                 loss: 0.0262
env1_first_0:                 episode reward: -19.4500,                 loss: nan
env1_second_0:                 episode reward: 19.4500,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2704.55,                last time consumption/overall running time: 4960.1879s / 222737.4150 s
env0_first_0:                 episode reward: -22.0000,                 loss: 0.0248
env0_second_0:                 episode reward: 22.0000,                 loss: 0.0257
env1_first_0:                 episode reward: -21.5000,                 loss: nan
env1_second_0:                 episode reward: 21.5000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2763.7,                last time consumption/overall running time: 5071.9166s / 227809.3317 s
env0_first_0:                 episode reward: -22.7000,                 loss: 0.0262
env0_second_0:                 episode reward: 22.7000,                 loss: 0.0270
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2475.5,                last time consumption/overall running time: 4530.4979s / 232339.8295 s
env0_first_0:                 episode reward: -22.3500,                 loss: 0.0266
env0_second_0:                 episode reward: 22.3500,                 loss: 0.0279
env1_first_0:                 episode reward: -19.7000,                 loss: nan
env1_second_0:                 episode reward: 19.7000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2841.3,                last time consumption/overall running time: 5196.4481s / 237536.2776 s
env0_first_0:                 episode reward: -21.8500,                 loss: 0.0261
env0_second_0:                 episode reward: 21.8500,                 loss: 0.0273
env1_first_0:                 episode reward: -24.1000,                 loss: nan
env1_second_0:                 episode reward: 24.1000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 3352.9,                last time consumption/overall running time: 6162.0886s / 243698.3662 s
env0_first_0:                 episode reward: -22.3500,                 loss: 0.0254
env0_second_0:                 episode reward: 22.3500,                 loss: 0.0268
env1_first_0:                 episode reward: -24.9500,                 loss: nan
env1_second_0:                 episode reward: 24.9500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2971.35,                last time consumption/overall running time: 5463.5619s / 249161.9281 s
env0_first_0:                 episode reward: -19.7500,                 loss: 0.0278
env0_second_0:                 episode reward: 19.7500,                 loss: 0.0288
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2522.4,                last time consumption/overall running time: 4615.8042s / 253777.7323 s
env0_first_0:                 episode reward: -18.9000,                 loss: 0.0262
env0_second_0:                 episode reward: 18.9000,                 loss: 0.0282
env1_first_0:                 episode reward: -17.9000,                 loss: nan
env1_second_0:                 episode reward: 17.9000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2656.55,                last time consumption/overall running time: 4840.6715s / 258618.4038 s
env0_first_0:                 episode reward: -18.9500,                 loss: 0.0256
env0_second_0:                 episode reward: 18.9500,                 loss: 0.0274
env1_first_0:                 episode reward: -20.5000,                 loss: nan
env1_second_0:                 episode reward: 20.5000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2830.4,                last time consumption/overall running time: 5206.1510s / 263824.5549 s
env0_first_0:                 episode reward: -21.6000,                 loss: 0.0260
env0_second_0:                 episode reward: 21.6000,                 loss: 0.0275
env1_first_0:                 episode reward: -20.1500,                 loss: nan
env1_second_0:                 episode reward: 20.1500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2497.4,                last time consumption/overall running time: 4638.1644s / 268462.7192 s
env0_first_0:                 episode reward: -17.7000,                 loss: 0.0253
env0_second_0:                 episode reward: 17.7000,                 loss: 0.0274
env1_first_0:                 episode reward: -19.8500,                 loss: nan
env1_second_0:                 episode reward: 19.8500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2878.95,                last time consumption/overall running time: 5295.2063s / 273757.9255 s
env0_first_0:                 episode reward: -22.5500,                 loss: 0.0249
env0_second_0:                 episode reward: 22.5500,                 loss: 0.0270
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2610.4,                last time consumption/overall running time: 4840.3614s / 278598.2870 s
env0_first_0:                 episode reward: -19.3000,                 loss: 0.0251
env0_second_0:                 episode reward: 19.3000,                 loss: 0.0260
env1_first_0:                 episode reward: -22.0500,                 loss: nan
env1_second_0:                 episode reward: 22.0500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2590.95,                last time consumption/overall running time: 4735.2799s / 283333.5669 s
env0_first_0:                 episode reward: -21.2500,                 loss: 0.0269
env0_second_0:                 episode reward: 21.2500,                 loss: 0.0276
env1_first_0:                 episode reward: -20.2500,                 loss: nan
env1_second_0:                 episode reward: 20.2500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2370.5,                last time consumption/overall running time: 4331.4182s / 287664.9851 s
env0_first_0:                 episode reward: -20.5500,                 loss: 0.0259
env0_second_0:                 episode reward: 20.5500,                 loss: 0.0275
env1_first_0:                 episode reward: -20.9500,                 loss: nan
env1_second_0:                 episode reward: 20.9500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2426.05,                last time consumption/overall running time: 4503.6520s / 292168.6371 s
env0_first_0:                 episode reward: -20.7500,                 loss: 0.0244
env0_second_0:                 episode reward: 20.7500,                 loss: 0.0254
env1_first_0:                 episode reward: -21.3000,                 loss: nan
env1_second_0:                 episode reward: 21.3000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2361.7,                last time consumption/overall running time: 4325.5301s / 296494.1672 s
env0_first_0:                 episode reward: -18.9500,                 loss: 0.0240
env0_second_0:                 episode reward: 18.9500,                 loss: 0.0247
env1_first_0:                 episode reward: -21.5000,                 loss: nan
env1_second_0:                 episode reward: 21.5000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2359.2,                last time consumption/overall running time: 4406.6845s / 300900.8517 s
env0_first_0:                 episode reward: -19.1500,                 loss: 0.0234
env0_second_0:                 episode reward: 19.1500,                 loss: 0.0244
env1_first_0:                 episode reward: -19.5000,                 loss: nan
env1_second_0:                 episode reward: 19.5000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2290.8,                last time consumption/overall running time: 4261.3281s / 305162.1799 s
env0_first_0:                 episode reward: -19.3000,                 loss: 0.0233
env0_second_0:                 episode reward: 19.3000,                 loss: 0.0241
env1_first_0:                 episode reward: -18.4000,                 loss: nan
env1_second_0:                 episode reward: 18.4000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2379.15,                last time consumption/overall running time: 4450.2364s / 309612.4162 s
env0_first_0:                 episode reward: -18.4000,                 loss: 0.0234
env0_second_0:                 episode reward: 18.4000,                 loss: 0.0249
env1_first_0:                 episode reward: -17.2000,                 loss: nan
env1_second_0:                 episode reward: 17.2000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2592.4,                last time consumption/overall running time: 4794.4977s / 314406.9140 s
env0_first_0:                 episode reward: -21.8000,                 loss: 0.0258
env0_second_0:                 episode reward: 21.8000,                 loss: 0.0269
env1_first_0:                 episode reward: -22.0000,                 loss: nan
env1_second_0:                 episode reward: 22.0000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2495.1,                last time consumption/overall running time: 4608.7078s / 319015.6217 s
env0_first_0:                 episode reward: -21.8000,                 loss: 0.0271
env0_second_0:                 episode reward: 21.8000,                 loss: 0.0282
env1_first_0:                 episode reward: -24.1500,                 loss: nan
env1_second_0:                 episode reward: 24.1500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2633.85,                last time consumption/overall running time: 4838.4469s / 323854.0686 s
env0_first_0:                 episode reward: -19.4500,                 loss: 0.0284
env0_second_0:                 episode reward: 19.4500,                 loss: 0.0292
env1_first_0:                 episode reward: -22.0000,                 loss: nan
env1_second_0:                 episode reward: 22.0000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2514.05,                last time consumption/overall running time: 4621.2590s / 328475.3276 s
env0_first_0:                 episode reward: -18.0500,                 loss: 0.0278
env0_second_0:                 episode reward: 18.0500,                 loss: 0.0294
env1_first_0:                 episode reward: -17.6500,                 loss: nan
env1_second_0:                 episode reward: 17.6500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2478.05,                last time consumption/overall running time: 4576.4268s / 333051.7544 s
env0_first_0:                 episode reward: -20.4000,                 loss: 0.0265
env0_second_0:                 episode reward: 20.4000,                 loss: 0.0276
env1_first_0:                 episode reward: -20.5000,                 loss: nan
env1_second_0:                 episode reward: 20.5000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2441.2,                last time consumption/overall running time: 4502.5303s / 337554.2847 s
env0_first_0:                 episode reward: -17.1000,                 loss: 0.0264
env0_second_0:                 episode reward: 17.1000,                 loss: 0.0264
env1_first_0:                 episode reward: -21.9500,                 loss: nan
env1_second_0:                 episode reward: 21.9500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 2430.3,                last time consumption/overall running time: 4504.2657s / 342058.5503 s
env0_first_0:                 episode reward: -19.0000,                 loss: 0.0276
env0_second_0:                 episode reward: 19.0000,                 loss: 0.0280
env1_first_0:                 episode reward: -20.8000,                 loss: nan
env1_second_0:                 episode reward: 20.8000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2750.6,                last time consumption/overall running time: 5119.6149s / 347178.1652 s
env0_first_0:                 episode reward: -18.7500,                 loss: 0.0298
env0_second_0:                 episode reward: 18.7500,                 loss: 0.0298
env1_first_0:                 episode reward: -17.7500,                 loss: nan
env1_second_0:                 episode reward: 17.7500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 2813.5,                last time consumption/overall running time: 5197.2936s / 352375.4588 s
env0_first_0:                 episode reward: -18.0000,                 loss: 0.0303
env0_second_0:                 episode reward: 18.0000,                 loss: 0.0313
env1_first_0:                 episode reward: -19.5000,                 loss: nan
env1_second_0:                 episode reward: 19.5000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 2761.35,                last time consumption/overall running time: 5229.0974s / 357604.5562 s
env0_first_0:                 episode reward: -17.0500,                 loss: 0.0294
env0_second_0:                 episode reward: 17.0500,                 loss: 0.0315
env1_first_0:                 episode reward: -20.3500,                 loss: nan
env1_second_0:                 episode reward: 20.3500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 2667.6,                last time consumption/overall running time: 4896.4765s / 362501.0328 s
env0_first_0:                 episode reward: -18.4500,                 loss: 0.0279
env0_second_0:                 episode reward: 18.4500,                 loss: 0.0295
env1_first_0:                 episode reward: -20.4500,                 loss: nan
env1_second_0:                 episode reward: 20.4500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2706.6,                last time consumption/overall running time: 4949.7091s / 367450.7418 s
env0_first_0:                 episode reward: -15.9500,                 loss: 0.0278
env0_second_0:                 episode reward: 15.9500,                 loss: 0.0282
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2901.2,                last time consumption/overall running time: 5318.6364s / 372769.3783 s
env0_first_0:                 episode reward: -19.7500,                 loss: 0.0288
env0_second_0:                 episode reward: 19.7500,                 loss: 0.0292
env1_first_0:                 episode reward: -19.8500,                 loss: nan
env1_second_0:                 episode reward: 19.8500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2699.5,                last time consumption/overall running time: 4975.6469s / 377745.0251 s
env0_first_0:                 episode reward: -20.9000,                 loss: 0.0290
env0_second_0:                 episode reward: 20.9000,                 loss: 0.0294
env1_first_0:                 episode reward: -20.8500,                 loss: nan
env1_second_0:                 episode reward: 20.8500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2596.5,                last time consumption/overall running time: 4779.8701s / 382524.8952 s
env0_first_0:                 episode reward: -17.7500,                 loss: 0.0284
env0_second_0:                 episode reward: 17.7500,                 loss: 0.0287
env1_first_0:                 episode reward: -17.6500,                 loss: nan
env1_second_0:                 episode reward: 17.6500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2764.85,                last time consumption/overall running time: 5049.9304s / 387574.8256 s
env0_first_0:                 episode reward: -22.5000,                 loss: 0.0274
env0_second_0:                 episode reward: 22.5000,                 loss: 0.0275
env1_first_0:                 episode reward: -23.9500,                 loss: nan
env1_second_0:                 episode reward: 23.9500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2334.35,                last time consumption/overall running time: 4271.3440s / 391846.1696 s
env0_first_0:                 episode reward: -18.8500,                 loss: 0.0259
env0_second_0:                 episode reward: 18.8500,                 loss: 0.0256
env1_first_0:                 episode reward: -20.1500,                 loss: nan
env1_second_0:                 episode reward: 20.1500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2423.4,                last time consumption/overall running time: 4453.5356s / 396299.7052 s
env0_first_0:                 episode reward: -22.3000,                 loss: 0.0257
env0_second_0:                 episode reward: 22.3000,                 loss: 0.0253
env1_first_0:                 episode reward: -20.4000,                 loss: nan
env1_second_0:                 episode reward: 20.4000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2268.2,                last time consumption/overall running time: 4143.8796s / 400443.5849 s
env0_first_0:                 episode reward: -20.5000,                 loss: 0.0261
env0_second_0:                 episode reward: 20.5000,                 loss: 0.0257
env1_first_0:                 episode reward: -21.2000,                 loss: nan
env1_second_0:                 episode reward: 21.2000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2354.7,                last time consumption/overall running time: 4339.2755s / 404782.8604 s
env0_first_0:                 episode reward: -22.7000,                 loss: 0.0255
env0_second_0:                 episode reward: 22.7000,                 loss: 0.0258
env1_first_0:                 episode reward: -20.7000,                 loss: nan
env1_second_0:                 episode reward: 20.7000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 2296.9,                last time consumption/overall running time: 4193.2775s / 408976.1379 s
env0_first_0:                 episode reward: -22.0500,                 loss: 0.0262
env0_second_0:                 episode reward: 22.0500,                 loss: 0.0269
env1_first_0:                 episode reward: -21.8500,                 loss: nan
env1_second_0:                 episode reward: 21.8500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 2456.35,                last time consumption/overall running time: 4511.3246s / 413487.4625 s
env0_first_0:                 episode reward: -23.2500,                 loss: 0.0278
env0_second_0:                 episode reward: 23.2500,                 loss: 0.0286
env1_first_0:                 episode reward: -19.2000,                 loss: nan
env1_second_0:                 episode reward: 19.2000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 2344.7,                last time consumption/overall running time: 4300.9143s / 417788.3769 s
env0_first_0:                 episode reward: -19.5500,                 loss: 0.0292
env0_second_0:                 episode reward: 19.5500,                 loss: 0.0300
env1_first_0:                 episode reward: -21.2000,                 loss: nan
env1_second_0:                 episode reward: 21.2000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 2316.8,                last time consumption/overall running time: 4228.1262s / 422016.5030 s
env0_first_0:                 episode reward: -22.4000,                 loss: 0.0296
env0_second_0:                 episode reward: 22.4000,                 loss: 0.0306
env1_first_0:                 episode reward: -17.4500,                 loss: nan
env1_second_0:                 episode reward: 17.4500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 2505.05,                last time consumption/overall running time: 4561.2420s / 426577.7451 s
env0_first_0:                 episode reward: -21.0000,                 loss: 0.0303
env0_second_0:                 episode reward: 21.0000,                 loss: 0.0304
env1_first_0:                 episode reward: -18.5000,                 loss: nan
env1_second_0:                 episode reward: 18.5000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2590.8,                last time consumption/overall running time: 4710.5230s / 431288.2680 s
env0_first_0:                 episode reward: -22.1500,                 loss: 0.0301
env0_second_0:                 episode reward: 22.1500,                 loss: 0.0307
env1_first_0:                 episode reward: -22.3000,                 loss: nan
env1_second_0:                 episode reward: 22.3000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 2645.25,                last time consumption/overall running time: 4799.3309s / 436087.5989 s
env0_first_0:                 episode reward: -20.9500,                 loss: 0.0288
env0_second_0:                 episode reward: 20.9500,                 loss: 0.0303
env1_first_0:                 episode reward: -20.9000,                 loss: nan
env1_second_0:                 episode reward: 20.9000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 2612.95,                last time consumption/overall running time: 4747.0490s / 440834.6479 s
env0_first_0:                 episode reward: -20.4500,                 loss: 0.0291
env0_second_0:                 episode reward: 20.4500,                 loss: 0.0301
env1_first_0:                 episode reward: -23.9000,                 loss: nan
env1_second_0:                 episode reward: 23.9000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2403.65,                last time consumption/overall running time: 4359.3094s / 445193.9573 s
env0_first_0:                 episode reward: -20.9000,                 loss: 0.0286
env0_second_0:                 episode reward: 20.9000,                 loss: 0.0291
env1_first_0:                 episode reward: -20.5500,                 loss: nan
env1_second_0:                 episode reward: 20.5500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 2654.5,                last time consumption/overall running time: 4823.3509s / 450017.3082 s
env0_first_0:                 episode reward: -20.2000,                 loss: 0.0277
env0_second_0:                 episode reward: 20.2000,                 loss: 0.0277
env1_first_0:                 episode reward: -20.4500,                 loss: nan
env1_second_0:                 episode reward: 20.4500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 2313.6,                last time consumption/overall running time: 4213.4635s / 454230.7717 s
env0_first_0:                 episode reward: -20.8500,                 loss: 0.0276
env0_second_0:                 episode reward: 20.8500,                 loss: 0.0283
env1_first_0:                 episode reward: -16.2500,                 loss: nan
env1_second_0:                 episode reward: 16.2500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 2478.5,                last time consumption/overall running time: 4526.4254s / 458757.1971 s
env0_first_0:                 episode reward: -15.6500,                 loss: 0.0282
env0_second_0:                 episode reward: 15.6500,                 loss: 0.0279
env1_first_0:                 episode reward: -19.4000,                 loss: nan
env1_second_0:                 episode reward: 19.4000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 2600.5,                last time consumption/overall running time: 4734.1757s / 463491.3728 s
env0_first_0:                 episode reward: -23.1000,                 loss: 0.0308
env0_second_0:                 episode reward: 23.1000,                 loss: 0.0305
env1_first_0:                 episode reward: -17.9500,                 loss: nan
env1_second_0:                 episode reward: 17.9500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2640.3,                last time consumption/overall running time: 4810.1631s / 468301.5358 s
env0_first_0:                 episode reward: -20.1500,                 loss: 0.0313
env0_second_0:                 episode reward: 20.1500,                 loss: 0.0306
env1_first_0:                 episode reward: -24.7500,                 loss: nan
env1_second_0:                 episode reward: 24.7500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 2551.7,                last time consumption/overall running time: 4666.6608s / 472968.1966 s
env0_first_0:                 episode reward: -16.9500,                 loss: 0.0280
env0_second_0:                 episode reward: 16.9500,                 loss: 0.0274
env1_first_0:                 episode reward: -18.7500,                 loss: nan
env1_second_0:                 episode reward: 18.7500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 2579.5,                last time consumption/overall running time: 4704.9441s / 477673.1407 s
env0_first_0:                 episode reward: -18.9000,                 loss: 0.0258
env0_second_0:                 episode reward: 18.9000,                 loss: 0.0258
env1_first_0:                 episode reward: -20.8500,                 loss: nan
env1_second_0:                 episode reward: 20.8500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 2433.15,                last time consumption/overall running time: 4439.2219s / 482112.3626 s
env0_first_0:                 episode reward: -17.4500,                 loss: 0.0246
env0_second_0:                 episode reward: 17.4500,                 loss: 0.0255
env1_first_0:                 episode reward: -18.0500,                 loss: nan
env1_second_0:                 episode reward: 18.0500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 2555.3,                last time consumption/overall running time: 4673.4309s / 486785.7936 s
env0_first_0:                 episode reward: -20.9500,                 loss: 0.0228
env0_second_0:                 episode reward: 20.9500,                 loss: 0.0243
env1_first_0:                 episode reward: -21.5500,                 loss: nan
env1_second_0:                 episode reward: 21.5500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 2278.2,                last time consumption/overall running time: 4151.2863s / 490937.0798 s
env0_first_0:                 episode reward: -17.7500,                 loss: 0.0221
env0_second_0:                 episode reward: 17.7500,                 loss: 0.0236
env1_first_0:                 episode reward: -17.3000,                 loss: nan
env1_second_0:                 episode reward: 17.3000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2233.7,                last time consumption/overall running time: 4061.9551s / 494999.0350 s
env0_first_0:                 episode reward: -18.5000,                 loss: 0.0228
env0_second_0:                 episode reward: 18.5000,                 loss: 0.0237
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 2162.3,                last time consumption/overall running time: 3908.7637s / 498907.7986 s
env0_first_0:                 episode reward: -17.9500,                 loss: 0.0219
env0_second_0:                 episode reward: 17.9500,                 loss: 0.0226
env1_first_0:                 episode reward: -19.6500,                 loss: nan
env1_second_0:                 episode reward: 19.6500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 2292.6,                last time consumption/overall running time: 4166.1098s / 503073.9085 s
env0_first_0:                 episode reward: -18.3000,                 loss: 0.0217
env0_second_0:                 episode reward: 18.3000,                 loss: 0.0223
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 2312.45,                last time consumption/overall running time: 4218.6777s / 507292.5862 s
env0_first_0:                 episode reward: -15.9500,                 loss: 0.0221
env0_second_0:                 episode reward: 15.9500,                 loss: 0.0233
env1_first_0:                 episode reward: -17.3000,                 loss: nan
env1_second_0:                 episode reward: 17.3000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 2187.7,                last time consumption/overall running time: 3963.4662s / 511256.0523 s
env0_first_0:                 episode reward: -17.9000,                 loss: 0.0217
env0_second_0:                 episode reward: 17.9000,                 loss: 0.0231
env1_first_0:                 episode reward: -19.2000,                 loss: nan
env1_second_0:                 episode reward: 19.2000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 2281.55,                last time consumption/overall running time: 4132.5131s / 515388.5654 s
env0_first_0:                 episode reward: -17.4500,                 loss: 0.0207
env0_second_0:                 episode reward: 17.4500,                 loss: 0.0220
env1_first_0:                 episode reward: -17.8500,                 loss: nan
env1_second_0:                 episode reward: 17.8500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 2312.05,                last time consumption/overall running time: 4195.4759s / 519584.0414 s
env0_first_0:                 episode reward: -18.6000,                 loss: 0.0229
env0_second_0:                 episode reward: 18.6000,                 loss: 0.0229
env1_first_0:                 episode reward: -17.3500,                 loss: nan
env1_second_0:                 episode reward: 17.3500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 2333.05,                last time consumption/overall running time: 4245.4850s / 523829.5264 s
env0_first_0:                 episode reward: -20.6000,                 loss: 0.0218
env0_second_0:                 episode reward: 20.6000,                 loss: 0.0234
env1_first_0:                 episode reward: -19.9000,                 loss: nan
env1_second_0:                 episode reward: 19.9000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 2354.95,                last time consumption/overall running time: 4314.5355s / 528144.0618 s
env0_first_0:                 episode reward: -17.1500,                 loss: 0.0207
env0_second_0:                 episode reward: 17.1500,                 loss: 0.0218
env1_first_0:                 episode reward: -19.4000,                 loss: nan
env1_second_0:                 episode reward: 19.4000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 2613.35,                last time consumption/overall running time: 4738.1964s / 532882.2582 s
env0_first_0:                 episode reward: -18.4500,                 loss: 0.0214
env0_second_0:                 episode reward: 18.4500,                 loss: 0.0214
env1_first_0:                 episode reward: -18.7500,                 loss: nan
env1_second_0:                 episode reward: 18.7500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 2591.9,                last time consumption/overall running time: 4729.1131s / 537611.3713 s
env0_first_0:                 episode reward: -16.9000,                 loss: 0.0213
env0_second_0:                 episode reward: 16.9000,                 loss: 0.0210
env1_first_0:                 episode reward: -18.8500,                 loss: nan
env1_second_0:                 episode reward: 18.8500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 2496.8,                last time consumption/overall running time: 4563.5229s / 542174.8942 s
env0_first_0:                 episode reward: -17.0500,                 loss: 0.0220
env0_second_0:                 episode reward: 17.0500,                 loss: 0.0226
env1_first_0:                 episode reward: -16.9000,                 loss: nan
env1_second_0:                 episode reward: 16.9000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 2122.95,                last time consumption/overall running time: 3881.2001s / 546056.0943 s
env0_first_0:                 episode reward: -20.2500,                 loss: 0.0220
env0_second_0:                 episode reward: 20.2500,                 loss: 0.0232
env1_first_0:                 episode reward: -19.9000,                 loss: nan
env1_second_0:                 episode reward: 19.9000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 2326.8,                last time consumption/overall running time: 4241.4404s / 550297.5347 s
env0_first_0:                 episode reward: -19.3500,                 loss: 0.0208
env0_second_0:                 episode reward: 19.3500,                 loss: 0.0220
env1_first_0:                 episode reward: -20.6500,                 loss: nan
env1_second_0:                 episode reward: 20.6500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 2280.4,                last time consumption/overall running time: 4165.8455s / 554463.3803 s
env0_first_0:                 episode reward: -19.8500,                 loss: 0.0203
env0_second_0:                 episode reward: 19.8500,                 loss: 0.0212
env1_first_0:                 episode reward: -18.2500,                 loss: nan
env1_second_0:                 episode reward: 18.2500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 2779.95,                last time consumption/overall running time: 5078.2705s / 559541.6507 s
env0_first_0:                 episode reward: -18.5500,                 loss: 0.0210
env0_second_0:                 episode reward: 18.5500,                 loss: 0.0222
env1_first_0:                 episode reward: -20.5000,                 loss: nan
env1_second_0:                 episode reward: 20.5000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 2686.35,                last time consumption/overall running time: 4909.5035s / 564451.1542 s
env0_first_0:                 episode reward: -17.9000,                 loss: 0.0220
env0_second_0:                 episode reward: 17.9000,                 loss: 0.0227
env1_first_0:                 episode reward: -16.8500,                 loss: nan
env1_second_0:                 episode reward: 16.8500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 2541.25,                last time consumption/overall running time: 4636.1952s / 569087.3495 s