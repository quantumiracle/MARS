{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Space Response Oracle (PSRO)\n",
    "This tutorial demonstrates Policy Space Response Oracle (PSRO) on a zero-sum Markov game, which is more general than extensive-from game (more general than normal-form game). \n",
    "\n",
    "For the payoff matrix, row player is maximizer, coloumn player is minimizer.\n",
    "\n",
    "Reference: A unified game-theoretic approach to multiagent reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 6.0, (1,), float32) Discrete(2)\n",
      "[[1], [1]]\n",
      "[[3], [3]] [0.8893378340991678, -0.8893378340991678] [False, False]\n",
      "[[4], [4]] [-0.06737845428738742, 0.06737845428738742] [True, True]\n",
      "Average initial state value of oracle Nash equilibrium for the first player:  -0.03296559176754302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.\n",
      "  warn(\"Converting G to a CSC matrix; may take a while.\")\n",
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.\n",
      "  warn(\"Converting A to a CSC matrix; may take a while.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from mars.env.mdp import ArbitraryMDP, MDPWrapper\n",
    "import numpy as np\n",
    "\n",
    "num_states = 2\n",
    "num_actions_per_player = 2\n",
    "num_trans = 2\n",
    "\n",
    "env = MDPWrapper(ArbitraryMDP(num_states=num_states, num_actions_per_player=num_actions_per_player, num_trans=num_trans))\n",
    "trans_matrices = env.env.trans_prob_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "reward_matrices = env.env.reward_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "\n",
    "oracle_nash_v, oracle_nash_q, oracle_nash_strategies = env.NEsolver(verbose=False)\n",
    "oracle_v_star = oracle_nash_v[0]\n",
    "\n",
    "oracle_v_star = np.mean(oracle_v_star, axis=0)\n",
    "print(env.observation_space, env.action_space)\n",
    "# env.render()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "while not np.any(done):\n",
    "    obs, r, done, _ = env.step([1,0])\n",
    "    print(obs, r, done)\n",
    "print('Average initial state value of oracle Nash equilibrium for the first player: ', oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 2, 2)\n",
      "(2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(oracle_nash_strategies).shape)\n",
    "oracle_max_policy = np.array(oracle_nash_strategies)[:, :, 0, :]\n",
    "print(oracle_max_policy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy shape: \n",
      "(2, 2)\n",
      "(2, 2, 2, 2, 2)\n",
      "Q table shape: \n",
      "(2, 2, 2)\n",
      "(2, 2, 2, 2, 2, 2)\n",
      "itr: 0, exploitability: 0.38378150889065277\n",
      "itr: 10, exploitability: 0.03296559179128712\n",
      "itr: 20, exploitability: 0.03663431541850807\n",
      "itr: 30, exploitability: 0.03712659903955932\n",
      "itr: 40, exploitability: 0.03578783783536544\n",
      "itr: 50, exploitability: 0.037863470835449206\n",
      "itr: 60, exploitability: 0.03824793001425987\n",
      "itr: 70, exploitability: 0.037085568333705424\n",
      "itr: 80, exploitability: 0.037237588215922185\n",
      "itr: 90, exploitability: 0.037662069177131496\n",
      "itr: 100, exploitability: 0.037923994549463116\n",
      "itr: 110, exploitability: 0.03845542295142959\n",
      "itr: 120, exploitability: 0.03781491827905098\n",
      "itr: 130, exploitability: 0.03865755401865325\n",
      "itr: 140, exploitability: 0.03867604898676211\n",
      "itr: 150, exploitability: 0.036953597082822996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/cvxpy/problems/problem.py:1279: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 160, exploitability: 0.03710436838207318\n",
      "itr: 170, exploitability: 0.03753433869582343\n",
      "itr: 180, exploitability: 0.037391029182109466\n",
      "itr: 190, exploitability: 0.03674820881990515\n",
      "itr: 200, exploitability: 0.036881548487680604\n",
      "itr: 210, exploitability: 0.03662069265625629\n",
      "itr: 220, exploitability: 0.03668900211643682\n",
      "itr: 230, exploitability: 0.0368309361455072\n",
      "itr: 240, exploitability: 0.03684183073478001\n",
      "itr: 250, exploitability: 0.03754073463922197\n",
      "itr: 260, exploitability: 0.03708021870828947\n",
      "itr: 270, exploitability: 0.03681097035504124\n",
      "itr: 280, exploitability: 0.03703491631717687\n",
      "itr: 290, exploitability: 0.037348739438874366\n",
      "itr: 300, exploitability: 0.03781313332175607\n",
      "itr: 310, exploitability: 0.0375733431307565\n",
      "itr: 320, exploitability: 0.03713683464032657\n",
      "itr: 330, exploitability: 0.037144830782565835\n",
      "itr: 340, exploitability: 0.037590589580454165\n",
      "itr: 350, exploitability: 0.03782574014893946\n",
      "itr: 360, exploitability: 0.038007359528823094\n",
      "itr: 370, exploitability: 0.03835517037547103\n",
      "itr: 380, exploitability: 0.0384660688309335\n",
      "itr: 390, exploitability: 0.03840602056382485\n",
      "itr: 400, exploitability: 0.037679260336393315\n",
      "itr: 410, exploitability: 0.03783343312950347\n",
      "itr: 420, exploitability: 0.03793350655800057\n",
      "itr: 430, exploitability: 0.0376996790313813\n",
      "itr: 440, exploitability: 0.037957380324126544\n",
      "itr: 450, exploitability: 0.038126170627822406\n",
      "itr: 460, exploitability: 0.03787344654013483\n",
      "itr: 470, exploitability: 0.037682839229067566\n",
      "itr: 480, exploitability: 0.037768847622044646\n",
      "itr: 490, exploitability: 0.03749795116006918\n",
      "itr: 500, exploitability: 0.03763896815004054\n",
      "itr: 510, exploitability: 0.037583272972102545\n",
      "itr: 520, exploitability: 0.03745524234347882\n",
      "itr: 530, exploitability: 0.03767078248769061\n",
      "itr: 540, exploitability: 0.037458626682298005\n",
      "itr: 550, exploitability: 0.03757203056856545\n",
      "itr: 560, exploitability: 0.03772643162089914\n",
      "itr: 570, exploitability: 0.03745813696195778\n",
      "itr: 580, exploitability: 0.03757225345759394\n",
      "itr: 590, exploitability: 0.03764504971329517\n",
      "itr: 600, exploitability: 0.037559138760271754\n",
      "itr: 610, exploitability: 0.037563095578984004\n",
      "itr: 620, exploitability: 0.03737740375806094\n",
      "itr: 630, exploitability: 0.03736582005073165\n",
      "itr: 640, exploitability: 0.03749106595706324\n",
      "itr: 650, exploitability: 0.03750184159896885\n",
      "itr: 660, exploitability: 0.03758083962602604\n",
      "itr: 670, exploitability: 0.037586076990199585\n",
      "itr: 680, exploitability: 0.03749056220365453\n",
      "itr: 690, exploitability: 0.03749463721926866\n",
      "itr: 700, exploitability: 0.03744834010890154\n",
      "itr: 710, exploitability: 0.037298602272619046\n",
      "itr: 720, exploitability: 0.037219246960405156\n",
      "itr: 730, exploitability: 0.03760523410770534\n",
      "itr: 740, exploitability: 0.03731429585293565\n",
      "itr: 750, exploitability: 0.03706327217262884\n",
      "itr: 760, exploitability: 0.03711458699931845\n",
      "itr: 770, exploitability: 0.037037604538423696\n",
      "itr: 780, exploitability: 0.03739644115145363\n",
      "itr: 790, exploitability: 0.037193388628207466\n",
      "itr: 800, exploitability: 0.03708122741591016\n",
      "itr: 810, exploitability: 0.03706104668911594\n",
      "itr: 820, exploitability: 0.036971557118411415\n",
      "itr: 830, exploitability: 0.03709209870329636\n",
      "itr: 840, exploitability: 0.03689502513290942\n",
      "itr: 850, exploitability: 0.03678075093111469\n",
      "itr: 860, exploitability: 0.03679247625745623\n",
      "itr: 870, exploitability: 0.03711599207132162\n",
      "itr: 880, exploitability: 0.0373641208481076\n",
      "itr: 890, exploitability: 0.036899240136605824\n",
      "itr: 900, exploitability: 0.037625892133720557\n",
      "itr: 910, exploitability: 0.03730472192619277\n",
      "itr: 920, exploitability: 0.037452190746398\n",
      "itr: 930, exploitability: 0.03746055903921497\n",
      "itr: 940, exploitability: 0.03722565961284601\n",
      "itr: 950, exploitability: 0.037388081468664985\n",
      "itr: 960, exploitability: 0.03714055122501443\n",
      "itr: 970, exploitability: 0.03719519980530356\n",
      "itr: 980, exploitability: 0.03716995601102628\n",
      "itr: 990, exploitability: 0.03790061313885402\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from mars.equilibrium_solver import NashEquilibriumECOSSolver, NashEquilibriumCVXPYSolver\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    sample_id = np.argmax(np.random.multinomial(1, dist))\n",
    "    sample_prob = dist[sample_id]\n",
    "    return sample_id, sample_prob\n",
    "\n",
    "def unified_state(s):\n",
    "    unified_s = s[0]%num_states\n",
    "    return unified_s\n",
    "\n",
    "def create_expand_policy(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    policies = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            policy =  (1./num_actions_per_player) * np.zeros((*intial_dim, num_actions_per_player))\n",
    "        else:\n",
    "            policy =  (1./num_actions_per_player) * np.ones((*intial_dim, num_actions_per_player))\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        policies.append(policy)\n",
    "\n",
    "    return policies\n",
    "\n",
    "def create_expand_value():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim],\n",
    "        ...\n",
    "    ]    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def create_expand_Q():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states, num_actions_per_player, num_actions_per_player]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_states, num_actions_per_player, num_actions_per_player])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_posterior_policy(policy_set, meta_prior, likelihood, con_seq, only_update_observed=True):\n",
    "    posterior_policy = create_expand_policy(zero_ini=True)  # zero initiate to sum later\n",
    "    num_policies = len(policy_set)\n",
    "    denom = meta_prior @ likelihood  # [#policy] * [#policy, H] -> [H]\n",
    "    if only_update_observed:  # only update observed sequences, others use prior\n",
    "        for pi_i, rho_i in zip(policy_set, meta_prior):  # loop over policy\n",
    "            for i, (p,) in enumerate(zip(pi_i)): # loop over transition \n",
    "                posterior_policy[i] += np.array(p*rho_i)  # all entries using prior\n",
    "\n",
    "        for pi_i in policy_set:  # loop over policy\n",
    "            for i, s in enumerate(con_seq): # loop over transition \n",
    "                posterior_policy[i][tuple(s)] = 0  # clear the observed entries\n",
    "\n",
    "        for pi_i, rho_i, likelihood_per_policy in zip(policy_set, meta_prior, likelihood):  # loop over policy\n",
    "            for i, (p, d, l, s) in enumerate(zip(pi_i, denom, likelihood_per_policy, con_seq)): # loop over transition \n",
    "                posterior_policy[i][tuple(s)] += np.array(p[tuple(s)]*rho_i*l/d)  # only update observed\n",
    "\n",
    "    else:  # update all entries\n",
    "        for pi_i, rho_i, likelihood_per_policy in zip(policy_set, meta_prior, likelihood):  # loop over policy\n",
    "            for i, (p, d, l) in enumerate(zip(pi_i, denom, likelihood_per_policy)): # loop over transition\n",
    "                posterior_policy[i] += np.array(p*rho_i*l/d)  # sum over policies in mixture\n",
    "\n",
    "    return posterior_policy\n",
    "\n",
    "def get_prior_policy(policy_set, meta_prior,):\n",
    "    prior_policy = create_expand_policy(zero_ini=True)  # zero initiate to sum later\n",
    "    num_policies = len(policy_set)\n",
    "    for pi_i, rho_i in zip(policy_set, meta_prior):  # loop over policy\n",
    "        for i, (p,) in enumerate(zip(pi_i)): # loop over transition \n",
    "            prior_policy[i] += np.array(p*rho_i)  # all entries using prior\n",
    "    return prior_policy\n",
    "\n",
    "def broadcast_shape(input, output_shape):\n",
    "    \"\"\" broadcast input to have the same shape as output_to_be \"\"\"\n",
    "    len_input_shape = len(np.array(input).shape)\n",
    "    incrs_shape = output_shape[:-len_input_shape]+len_input_shape*(1,)\n",
    "    output = np.tile(input, incrs_shape)\n",
    "    return output\n",
    "\n",
    "def get_best_response_policy(player, prob_seq, con_seq):\n",
    "    given_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], prob_seq, con_seq)\n",
    "    br_policy = create_expand_policy()  # best response policy\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))  # broadcast the shape of reward matrix to be expand Q shape plus additional state dimension\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if player['side'] == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', given_policy[i], br_q[i])\n",
    "            arg_id = np.argmin(mu_dot_q, axis=-1)  # min player takes minimum as best response against max player's policy\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], given_policy[i])\n",
    "            arg_id = np.argmax(q_dot_nu, axis=-1)   # vice versa    \n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)     \n",
    "        \n",
    "        br_policy[i] = (np.arange(num_actions_per_player) == arg_id[...,None]).astype(int)  # from extreme (min/max) idx to one-hot simplex\n",
    "        # print(br_policy[i].shape, br_q[i].shape)\n",
    "\n",
    "    return br_policy\n",
    "\n",
    "def best_response_value(trans_prob_matrices, reward_matrices, policy, num_actions, side='max'):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if side == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', policy[i], br_q[i])\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], policy[i])\n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)         \n",
    "\n",
    "    avg_init_br_v = -np.mean(br_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_br_v\n",
    "\n",
    "def get_best_response_value(player):\n",
    "    # wrong! this is the weighted average (by meta strategy) of exploitability for each policy in policy set\n",
    "    # per_policy_exploits = []\n",
    "    # for p in policy_set:\n",
    "    #     per_policy_exploits.append(best_response_value(trans_matrices, reward_matrices, p, num_actions_per_player, player['side']))\n",
    "    # per_policy_exploits = np.array(per_policy_exploits)\n",
    "    # print(per_policy_exploits)\n",
    "    # exploitability = per_policy_exploits @ meta_strategy  # average over policy set weighted by meta strategy\n",
    "\n",
    "    # should still get the posterior policy as the mixture policy to exploit!\n",
    "    prior_mixture_policy = get_prior_policy(player['policy_set'], player['meta_strategy'])\n",
    "    exploitability = best_response_value(trans_matrices, reward_matrices, prior_mixture_policy, num_actions_per_player, player['side'])\n",
    "\n",
    "    return exploitability\n",
    "\n",
    "def policy_against_policy_value(trans_prob_matrices, reward_matrices, max_policy, min_policy, num_actions):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    eval_v = create_expand_value()\n",
    "    eval_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, eval_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            eval_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = eval_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            eval_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        mu_dot_q = np.einsum('...i, ...ij->...j', max_policy[i], eval_q[i])\n",
    "        mu_dot_q_dot_nu = np.einsum('...kj, ...kj->...k', mu_dot_q, min_policy[i])\n",
    "        eval_v[i] = mu_dot_q_dot_nu    \n",
    "\n",
    "    avg_init_eval_v = np.mean(eval_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_eval_v\n",
    "\n",
    "def update_matrix(matrix, side, row):\n",
    "    \"\"\"\n",
    "    Adding a new policy to the league will add a row or column to the present evaluation matrix, after it has\n",
    "    been evaluated against all policies in the opponent's league. Whether adding a row or column depends on whether\n",
    "    it is the row player (idx=0) or column player (idx=1). \n",
    "    For example, if current evaluation matrix is:\n",
    "    2  1\n",
    "    -1 3\n",
    "    After adding a new policy for row player with evaluated average episode reward (a,b) against current two policies in \n",
    "    the league of the column player, it gives: \n",
    "    it gives:\n",
    "    2   1\n",
    "    -1  3\n",
    "    a   b    \n",
    "    \"\"\" \n",
    "    if side == 'max': # for row player\n",
    "        if matrix.shape[0] == 0: \n",
    "            matrix = np.array([row])\n",
    "        else:  # add row\n",
    "            matrix = np.vstack([matrix, row])\n",
    "    else: # for column player\n",
    "        if matrix.shape[0] == 0:  # the first update\n",
    "            matrix = np.array([row]) \n",
    "        else:  # add column\n",
    "            matrix=np.hstack([matrix, np.array([row]).T])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sample_policy(policy_set, dist):\n",
    "    policy_id, _ = sample_from_categorical(dist)\n",
    "    policy = policy_set[policy_id]   \n",
    "    return policy \n",
    "\n",
    "# test_policy = create_expand_policy() \n",
    "# get_best_response(test_policy)\n",
    "\n",
    "def psro(env, save_path, solve_episodes = 1000):\n",
    "    evaluation_matrix = np.array([[0]])\n",
    "    ini_max_policy = create_expand_policy()\n",
    "    ini_min_policy = create_expand_policy()\n",
    "    max_player = {\n",
    "        'side': 'max',\n",
    "        'policy_set': [ini_max_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "    min_player = {\n",
    "        'side': 'min',\n",
    "        'policy_set': [ini_min_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "\n",
    "    max_side_q_table = create_expand_Q()\n",
    "    min_side_q_table = create_expand_Q()\n",
    " \n",
    "    print('policy shape: ')\n",
    "    for p in ini_max_policy:\n",
    "        print(p.shape)\n",
    "    print('Q table shape: ')\n",
    "    for q in max_side_q_table:\n",
    "        print(q.shape)\n",
    "\n",
    "    exploitability_records = []\n",
    "\n",
    "    for i in range(solve_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        max_player_observed_sequence = []\n",
    "        min_player_observed_sequence = []\n",
    "        max_player_conditional_sequence = []\n",
    "        min_player_conditional_sequence = []\n",
    "        max_player_action_prob_sequence = [[] for _ in max_player['policy_set']]\n",
    "        min_player_action_prob_sequence = [[] for _ in min_player['policy_set']]\n",
    "        shared_sequence_before_actions = []\n",
    "        step = 0\n",
    "\n",
    "        # sample policy from meta strategy for the current episode\n",
    "        max_policy = sample_policy(max_player['policy_set'], max_player['meta_strategy'])\n",
    "        min_policy = sample_policy(min_player['policy_set'], min_player['meta_strategy'])\n",
    "\n",
    "        while not np.any(done):    \n",
    "            # get observed sequence\n",
    "            max_player_observed_sequence.extend([unified_state(s[0])])\n",
    "            min_player_observed_sequence.extend([unified_state(s[1])])\n",
    "            shared_sequence_before_actions = copy.deepcopy(max_player_observed_sequence)\n",
    "\n",
    "            # get action distribution given specific observed history\n",
    "            max_policy_to_choose = max_policy[step][tuple(max_player_observed_sequence)]  # a=np.ones([1,2]), b=(0,0), a[b] -> 1.\n",
    "            min_policy_to_choose = min_policy[step][tuple(min_player_observed_sequence)]\n",
    "\n",
    "            # choose action\n",
    "            max_action, _ = sample_from_categorical(max_policy_to_choose)\n",
    "            min_action, _ = sample_from_categorical(min_policy_to_choose)\n",
    "\n",
    "            # roullout info for mixture policy\n",
    "            if i % 2 == 0:  # update min player side\n",
    "                for p_id, p in enumerate(max_player['policy_set']):  # get trajectory probabilities for each policy in policy set\n",
    "                    max_p = p[step][tuple(max_player_observed_sequence)]\n",
    "                    # _, max_a_prob = sample_from_categorical(max_p) # this is wrong\n",
    "                    max_a_prob = max_p[max_action]  # get the probability of real action (rollout) under each policy in policy set\n",
    "                    if step == 0:\n",
    "                        max_player_action_prob_sequence[p_id].append(max_a_prob)\n",
    "                    else:\n",
    "                        max_player_action_prob_sequence[p_id].append(max_player_action_prob_sequence[p_id][-1]*max_a_prob)  # [p1, p1p2, p1p2p3, ...]\n",
    "                max_player_conditional_sequence.append(copy.deepcopy(max_player_observed_sequence))\n",
    "\n",
    "            else:\n",
    "                for p_id, p in enumerate(min_player['policy_set']):\n",
    "                    min_p = p[step][tuple(min_player_observed_sequence)]\n",
    "                    # _, min_a_prob = sample_from_categorical(min_p)  # this is wrong\n",
    "                    min_a_prob = min_p[min_action]  # get the probability of real action (rollout) under each policy in policy set\n",
    "                    if step == 0:\n",
    "                        min_player_action_prob_sequence[p_id].append(min_a_prob)\n",
    "                    else:\n",
    "                        min_player_action_prob_sequence[p_id].append(min_player_action_prob_sequence[p_id][-1]*min_a_prob)  # [p1, p1p2, p1p2p3, ...]\n",
    "                min_player_conditional_sequence.append(copy.deepcopy(min_player_observed_sequence))\n",
    "\n",
    "            action = [max_action, min_action]\n",
    "            max_player_observed_sequence.extend(action)\n",
    "            min_player_observed_sequence.extend(action)\n",
    "\n",
    "            s_, r, done, _  = env.step(action)  \n",
    "            s = s_\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "        if i % 2 == 0:  # update min player side\n",
    "            new_policy = get_best_response_policy(max_player, np.array(max_player_action_prob_sequence), max_player_conditional_sequence)  # get best response against the max player, max_player_action_prob_sequence: [#policies, H]\n",
    "            min_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in max_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=p, min_policy=new_policy, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'min', eval_scores)\n",
    "\n",
    "        else: \n",
    "            new_policy = get_best_response_policy(min_player, np.array(min_player_action_prob_sequence), min_player_conditional_sequence) # get best response against the min player\n",
    "            max_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in min_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=new_policy, min_policy=p, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'max', eval_scores)\n",
    "\n",
    "        if len(max_player['policy_set']) * len(min_player['policy_set']) >= 2:  # enough policies to get Nash\n",
    "            (max_player['meta_strategy'], min_player['meta_strategy']), _ = NashEquilibriumCVXPYSolver(evaluation_matrix)\n",
    "        else: # uniform for initialization only\n",
    "            max_policies = len(max_player['policy_set'])\n",
    "            max_player['meta_strategy'] = 1./max_policies*np.ones(max_policies)\n",
    "            min_policies = len(min_player['policy_set'])\n",
    "            min_player['meta_strategy'] = 1./min_policies*np.ones(min_policies)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            exploitability = get_best_response_value(max_player)  # best response of the max player\n",
    "            print(f'itr: {i}, exploitability: {exploitability}', )\n",
    "            exploitability_records.append(exploitability)\n",
    "            np.save(save_path, exploitability_records)\n",
    "\n",
    "save_path = 'psro_exp_stochastic.npy'\n",
    "psro(env, save_path, solve_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2e2f7bb470>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "exp_data = np.load(save_path)\n",
    "plt.title('PSRO')\n",
    "plt.xlabel('iter(x10)')\n",
    "plt.ylabel('exploitability')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot(exp_data+oracle_v_star)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('x': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
