Cannot import pettingzoo env:  combat_jet_v1
pong_v3 pettingzoo
type:  pettingzoo
pong_v3
Load pong_v3 environment in type pettingzoo.
Env observation space: <bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f4031c92940>> action space: <bound method aec_to_parallel_wrapper.action_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f4031c92940>>
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f4031c8f128>
random seed: [439, 75, 403, 910, 117]
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f4031c8f128>
discrete_policy 6 Discrete(6)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=36, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 6 Discrete(6)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=36, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 6 Discrete(6)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'pong_v3', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 5000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax', 'channel_list': [32, 16], 'kernel_size_list': [4, 4], 'stride_list': [1, 1]}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220615_2120/pettingzoo_pong_v3_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220615_2120/pettingzoo_pong_v3_nash_dqn.
Episode: 1/50000 (0.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 21.5487s / 21.5487 s
first_0:                 episode reward: 6.0000,                 loss: 0.0181
second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 608.2614s / 629.8100 s
first_0:                 episode reward: 2.4500,                 loss: 0.0228
second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 612.4956s / 1242.3057 s
first_0:                 episode reward: 2.9000,                 loss: 0.0219
second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 617.2722s / 1859.5779 s
first_0:                 episode reward: 1.0000,                 loss: 0.0210
second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 623.4124s / 2482.9902 s
first_0:                 episode reward: 2.1000,                 loss: 0.0190
second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 631.1724s / 3114.1626 s