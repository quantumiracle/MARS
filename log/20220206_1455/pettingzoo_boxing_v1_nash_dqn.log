pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_1455/pettingzoo_boxing_v1_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_1455/pettingzoo_boxing_v1_nash_dqn.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 105.7524s / 105.7524 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0694
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0674
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2408.5531s / 2514.3055 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0761
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0761
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2670.8771s / 5185.1826 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0734
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0735
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2953.7236s / 8138.9062 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0737
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0734
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3014.5117s / 11153.4179 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0752
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0742
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2990.4541s / 14143.8720 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0799
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0769
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3001.9688s / 17145.8408 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0769
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0726
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3006.3784s / 20152.2192 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0768
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0754
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2986.7208s / 23138.9400 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0767
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0749
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3023.4716s / 26162.4117 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0787
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0776
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3037.1179s / 29199.5296 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0808
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0806
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3050.2730s / 32249.8026 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0884
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0913
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3008.8901s / 35258.6927 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0942
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0974
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3040.2040s / 38298.8967 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.1009
env0_second_0:                 episode reward: -1.9500,                 loss: 0.1012
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3041.6987s / 41340.5954 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.1078
env0_second_0:                 episode reward: -1.3500,                 loss: 0.1110
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3033.7662s / 44374.3616 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.1318
env0_second_0:                 episode reward: -1.3000,                 loss: 0.1285
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3040.9826s / 47415.3443 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.1406
env0_second_0:                 episode reward: -3.9500,                 loss: 0.1435
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3047.1812s / 50462.5254 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1628
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1578
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3026.6916s / 53489.2170 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.1754
env0_second_0:                 episode reward: -8.3500,                 loss: 0.1688
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3019.0322s / 56508.2492 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.1906
env0_second_0:                 episode reward: 5.0000,                 loss: 0.1822
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3045.7669s / 59554.0161 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.2071
env0_second_0:                 episode reward: -2.1000,                 loss: 0.1969
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3040.3521s / 62594.3682 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.1908
env0_second_0:                 episode reward: 1.3000,                 loss: 0.1933
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3045.5856s / 65639.9538 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1762
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1793
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3039.4751s / 68679.4289 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1555
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1521
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 3003.7272s / 71683.1561 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.1584
env0_second_0:                 episode reward: -0.5000,                 loss: 0.1558
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2948.9649s / 74632.1210 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.1651
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1562
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2954.3047s / 77586.4257 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.1850
env0_second_0:                 episode reward: 1.5500,                 loss: 0.1784
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2952.9252s / 80539.3509 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.1763
env0_second_0:                 episode reward: 4.3500,                 loss: 0.1749
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2950.5702s / 83489.9212 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.1720
env0_second_0:                 episode reward: 4.3000,                 loss: 0.1694
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2940.3871s / 86430.3082 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.1589
env0_second_0:                 episode reward: 5.9500,                 loss: 0.1587
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2927.9752s / 89358.2834 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.1557
env0_second_0:                 episode reward: -0.0500,                 loss: 0.1537
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2954.1460s / 92312.4294 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1621
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1553
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2972.8869s / 95285.3163 s
env0_first_0:                 episode reward: -4.8500,                 loss: 0.1939
env0_second_0:                 episode reward: 4.8500,                 loss: 0.1777
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2964.7161s / 98250.0324 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.1956
env0_second_0:                 episode reward: 1.3500,                 loss: 0.1866
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2960.9722s / 101211.0046 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.1906
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1781
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2952.5314s / 104163.5360 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.1780
env0_second_0:                 episode reward: 5.8000,                 loss: 0.1712
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2892.5970s / 107056.1329 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1711
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1681
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2904.3705s / 109960.5034 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1841
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1748
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1778.25,                last time consumption/overall running time: 2883.9103s / 112844.4137 s
env0_first_0:                 episode reward: -9.7500,                 loss: 0.2118
env0_second_0:                 episode reward: 9.7500,                 loss: 0.1955
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2895.6025s / 115740.0162 s
env0_first_0:                 episode reward: -9.5500,                 loss: 0.2413
env0_second_0:                 episode reward: 9.5500,                 loss: 0.2201
env1_first_0:                 episode reward: -16.5000,                 loss: nan
env1_second_0:                 episode reward: 16.5000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2901.6026s / 118641.6188 s
env0_first_0:                 episode reward: -10.4000,                 loss: 0.2313
env0_second_0:                 episode reward: 10.4000,                 loss: 0.2274
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2906.1978s / 121547.8165 s
env0_first_0:                 episode reward: -10.7500,                 loss: 0.2258
env0_second_0:                 episode reward: 10.7500,                 loss: 0.2323
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2892.9375s / 124440.7540 s
env0_first_0:                 episode reward: -10.0000,                 loss: 0.2252
env0_second_0:                 episode reward: 10.0000,                 loss: 0.2278
env1_first_0:                 episode reward: -11.1000,                 loss: nan
env1_second_0:                 episode reward: 11.1000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2865.8923s / 127306.6464 s
env0_first_0:                 episode reward: -10.4000,                 loss: 0.2315
env0_second_0:                 episode reward: 10.4000,                 loss: 0.2312
env1_first_0:                 episode reward: -14.3000,                 loss: nan
env1_second_0:                 episode reward: 14.3000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1777.3,                last time consumption/overall running time: 2846.9257s / 130153.5721 s
env0_first_0:                 episode reward: -14.4000,                 loss: 0.2571
env0_second_0:                 episode reward: 14.4000,                 loss: 0.2619
env1_first_0:                 episode reward: -19.6000,                 loss: nan
env1_second_0:                 episode reward: 19.6000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2860.6441s / 133014.2162 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.3140
env0_second_0:                 episode reward: 8.5000,                 loss: 0.3039
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2871.0431s / 135885.2593 s
env0_first_0:                 episode reward: -10.6000,                 loss: 0.3401
env0_second_0:                 episode reward: 10.6000,                 loss: 0.3175
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1764.1,                last time consumption/overall running time: 2829.8917s / 138715.1511 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.3413
env0_second_0:                 episode reward: 7.6500,                 loss: 0.3212
env1_first_0:                 episode reward: -14.3500,                 loss: nan
env1_second_0:                 episode reward: 14.3500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1774.6,                last time consumption/overall running time: 2855.3153s / 141570.4664 s
env0_first_0:                 episode reward: -12.7000,                 loss: 0.3607
env0_second_0:                 episode reward: 12.7000,                 loss: 0.3360
env1_first_0:                 episode reward: -17.3500,                 loss: nan
env1_second_0:                 episode reward: 17.3500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1778.5,                last time consumption/overall running time: 2856.9029s / 144427.3693 s
env0_first_0:                 episode reward: -9.8000,                 loss: 0.3345
env0_second_0:                 episode reward: 9.8000,                 loss: 0.3230
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1742.9,                last time consumption/overall running time: 2791.1798s / 147218.5491 s
env0_first_0:                 episode reward: -19.1000,                 loss: 0.3422
env0_second_0:                 episode reward: 19.1000,                 loss: 0.3185
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1780.85,                last time consumption/overall running time: 2860.8950s / 150079.4441 s
env0_first_0:                 episode reward: -28.6500,                 loss: 0.3644
env0_second_0:                 episode reward: 28.6500,                 loss: 0.3348
env1_first_0:                 episode reward: -17.1000,                 loss: nan
env1_second_0:                 episode reward: 17.1000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1778.7,                last time consumption/overall running time: 2861.7310s / 152941.1750 s
env0_first_0:                 episode reward: -20.0500,                 loss: 0.3969
env0_second_0:                 episode reward: 20.0500,                 loss: 0.3815
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1714.25,                last time consumption/overall running time: 2742.3245s / 155683.4995 s
env0_first_0:                 episode reward: -27.6500,                 loss: 0.4185
env0_second_0:                 episode reward: 27.6500,                 loss: 0.3930
env1_first_0:                 episode reward: -20.4000,                 loss: nan
env1_second_0:                 episode reward: 20.4000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1740.0,                last time consumption/overall running time: 2786.6488s / 158470.1483 s
env0_first_0:                 episode reward: -33.6000,                 loss: 0.4777
env0_second_0:                 episode reward: 33.6000,                 loss: 0.4289
env1_first_0:                 episode reward: -19.2500,                 loss: nan
env1_second_0:                 episode reward: 19.2500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1771.15,                last time consumption/overall running time: 2844.9708s / 161315.1192 s
env0_first_0:                 episode reward: -28.7000,                 loss: 0.4862
env0_second_0:                 episode reward: 28.7000,                 loss: 0.4533
env1_first_0:                 episode reward: -15.4500,                 loss: nan
env1_second_0:                 episode reward: 15.4500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1732.95,                last time consumption/overall running time: 2772.7827s / 164087.9018 s
env0_first_0:                 episode reward: -14.2000,                 loss: 0.5313
env0_second_0:                 episode reward: 14.2000,                 loss: 0.4618
env1_first_0:                 episode reward: -19.1000,                 loss: nan
env1_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1721.05,                last time consumption/overall running time: 2751.0500s / 166838.9519 s
env0_first_0:                 episode reward: -11.0000,                 loss: 0.4599
env0_second_0:                 episode reward: 11.0000,                 loss: 0.4371
env1_first_0:                 episode reward: -21.9500,                 loss: nan
env1_second_0:                 episode reward: 21.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1640.95,                last time consumption/overall running time: 2632.1525s / 169471.1044 s
env0_first_0:                 episode reward: -28.9000,                 loss: 0.4771
env0_second_0:                 episode reward: 28.9000,                 loss: 0.4321
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1720.95,                last time consumption/overall running time: 2756.2250s / 172227.3294 s
env0_first_0:                 episode reward: -14.0500,                 loss: 0.4835
env0_second_0:                 episode reward: 14.0500,                 loss: 0.4416
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1772.1,                last time consumption/overall running time: 2829.6953s / 175057.0247 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.4814
env0_second_0:                 episode reward: 11.4500,                 loss: 0.4285
env1_first_0:                 episode reward: -18.0000,                 loss: nan
env1_second_0:                 episode reward: 18.0000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1709.5,                last time consumption/overall running time: 2745.3912s / 177802.4158 s
env0_first_0:                 episode reward: -13.7500,                 loss: 0.4914
env0_second_0:                 episode reward: 13.7500,                 loss: 0.4624
env1_first_0:                 episode reward: -24.3500,                 loss: nan
env1_second_0:                 episode reward: 24.3500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1688.75,                last time consumption/overall running time: 2719.2216s / 180521.6375 s
env0_first_0:                 episode reward: -22.3500,                 loss: 0.4966
env0_second_0:                 episode reward: 22.3500,                 loss: 0.4684
env1_first_0:                 episode reward: -17.9500,                 loss: nan
env1_second_0:                 episode reward: 17.9500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1706.7,                last time consumption/overall running time: 2735.7448s / 183257.3822 s
env0_first_0:                 episode reward: -16.6000,                 loss: 0.5354
env0_second_0:                 episode reward: 16.6000,                 loss: 0.5102
env1_first_0:                 episode reward: -18.8500,                 loss: nan
env1_second_0:                 episode reward: 18.8500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1644.6,                last time consumption/overall running time: 2626.8267s / 185884.2089 s
env0_first_0:                 episode reward: -19.1500,                 loss: 0.6313
env0_second_0:                 episode reward: 19.1500,                 loss: 0.5561
env1_first_0:                 episode reward: -19.2000,                 loss: nan
env1_second_0:                 episode reward: 19.2000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1706.5,                last time consumption/overall running time: 2728.2505s / 188612.4595 s
env0_first_0:                 episode reward: -18.1500,                 loss: 0.6813
env0_second_0:                 episode reward: 18.1500,                 loss: 0.5901
env1_first_0:                 episode reward: -15.4000,                 loss: nan
env1_second_0:                 episode reward: 15.4000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1681.4,                last time consumption/overall running time: 2710.3626s / 191322.8221 s
env0_first_0:                 episode reward: -17.5500,                 loss: 0.7034
env0_second_0:                 episode reward: 17.5500,                 loss: 0.6279
env1_first_0:                 episode reward: -13.6500,                 loss: nan
env1_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1753.5,                last time consumption/overall running time: 2830.4169s / 194153.2390 s
env0_first_0:                 episode reward: -18.9000,                 loss: 0.6500
env0_second_0:                 episode reward: 18.9000,                 loss: 0.5863
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1689.2,                last time consumption/overall running time: 2827.2480s / 196980.4870 s
env0_first_0:                 episode reward: -15.9000,                 loss: 0.6634
env0_second_0:                 episode reward: 15.9000,                 loss: 0.6040
env1_first_0:                 episode reward: -21.3500,                 loss: nan
env1_second_0:                 episode reward: 21.3500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1702.75,                last time consumption/overall running time: 2830.3763s / 199810.8633 s
env0_first_0:                 episode reward: -22.7500,                 loss: 0.6341
env0_second_0:                 episode reward: 22.7500,                 loss: 0.5811
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1708.05,                last time consumption/overall running time: 2844.2102s / 202655.0734 s
env0_first_0:                 episode reward: -13.8000,                 loss: 0.5971
env0_second_0:                 episode reward: 13.8000,                 loss: 0.5528
env1_first_0:                 episode reward: -17.7500,                 loss: nan
env1_second_0:                 episode reward: 17.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1678.4,                last time consumption/overall running time: 2790.5583s / 205445.6317 s
env0_first_0:                 episode reward: -18.8000,                 loss: 0.6118
env0_second_0:                 episode reward: 18.8000,                 loss: 0.5467
env1_first_0:                 episode reward: -17.8500,                 loss: nan
env1_second_0:                 episode reward: 17.8500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1678.3,                last time consumption/overall running time: 2757.6207s / 208203.2524 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.6366
env0_second_0:                 episode reward: 10.9000,                 loss: 0.5557
env1_first_0:                 episode reward: -22.6000,                 loss: nan
env1_second_0:                 episode reward: 22.6000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1700.15,                last time consumption/overall running time: 2703.1969s / 210906.4494 s
env0_first_0:                 episode reward: -21.8500,                 loss: 0.6858
env0_second_0:                 episode reward: 21.8500,                 loss: 0.5978
env1_first_0:                 episode reward: -22.3000,                 loss: nan
env1_second_0:                 episode reward: 22.3000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1718.0,                last time consumption/overall running time: 2737.3901s / 213643.8395 s
env0_first_0:                 episode reward: -14.3000,                 loss: 0.6736
env0_second_0:                 episode reward: 14.3000,                 loss: 0.5963
env1_first_0:                 episode reward: -21.9000,                 loss: nan
env1_second_0:                 episode reward: 21.9000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1711.5,                last time consumption/overall running time: 2718.6272s / 216362.4667 s
env0_first_0:                 episode reward: -20.2500,                 loss: 0.6251
env0_second_0:                 episode reward: 20.2500,                 loss: 0.5688
env1_first_0:                 episode reward: -13.1500,                 loss: nan
env1_second_0:                 episode reward: 13.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1765.85,                last time consumption/overall running time: 2825.3564s / 219187.8231 s
env0_first_0:                 episode reward: -13.9500,                 loss: 0.5419
env0_second_0:                 episode reward: 13.9500,                 loss: 0.5072
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1667.4,                last time consumption/overall running time: 2672.0496s / 221859.8727 s
env0_first_0:                 episode reward: -24.0000,                 loss: 0.5593
env0_second_0:                 episode reward: 24.0000,                 loss: 0.5275
env1_first_0:                 episode reward: -24.2500,                 loss: nan
env1_second_0:                 episode reward: 24.2500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1663.0,                last time consumption/overall running time: 2648.9579s / 224508.8306 s
env0_first_0:                 episode reward: -24.8000,                 loss: 0.5724
env0_second_0:                 episode reward: 24.8000,                 loss: 0.5536
env1_first_0:                 episode reward: -20.2000,                 loss: nan
env1_second_0:                 episode reward: 20.2000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1745.65,                last time consumption/overall running time: 2791.7613s / 227300.5919 s
env0_first_0:                 episode reward: -14.6500,                 loss: 0.6119
env0_second_0:                 episode reward: 14.6500,                 loss: 0.5609
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1692.4,                last time consumption/overall running time: 2702.9986s / 230003.5904 s
env0_first_0:                 episode reward: -21.0500,                 loss: 0.5801
env0_second_0:                 episode reward: 21.0500,                 loss: 0.5075
env1_first_0:                 episode reward: -13.6500,                 loss: nan
env1_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1764.7,                last time consumption/overall running time: 2807.7948s / 232811.3852 s
env0_first_0:                 episode reward: -25.1500,                 loss: 0.5580
env0_second_0:                 episode reward: 25.1500,                 loss: 0.4879
env1_first_0:                 episode reward: -18.3000,                 loss: nan
env1_second_0:                 episode reward: 18.3000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1683.65,                last time consumption/overall running time: 2673.3131s / 235484.6984 s
env0_first_0:                 episode reward: -8.1000,                 loss: 0.5291
env0_second_0:                 episode reward: 8.1000,                 loss: 0.4739
env1_first_0:                 episode reward: -27.7500,                 loss: nan
env1_second_0:                 episode reward: 27.7500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1755.65,                last time consumption/overall running time: 2792.2315s / 238276.9299 s
env0_first_0:                 episode reward: -14.7500,                 loss: 0.5056
env0_second_0:                 episode reward: 14.7500,                 loss: 0.4479
env1_first_0:                 episode reward: -20.6500,                 loss: nan
env1_second_0:                 episode reward: 20.6500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1688.1,                last time consumption/overall running time: 2679.4572s / 240956.3871 s
env0_first_0:                 episode reward: -19.7000,                 loss: 0.5006
env0_second_0:                 episode reward: 19.7000,                 loss: 0.4414
env1_first_0:                 episode reward: -19.2000,                 loss: nan
env1_second_0:                 episode reward: 19.2000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1737.1,                last time consumption/overall running time: 2770.6257s / 243727.0128 s
env0_first_0:                 episode reward: -9.2500,                 loss: 0.4235
env0_second_0:                 episode reward: 9.2500,                 loss: 0.3740
env1_first_0:                 episode reward: -18.1000,                 loss: nan
env1_second_0:                 episode reward: 18.1000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1661.35,                last time consumption/overall running time: 2640.9247s / 246367.9375 s
env0_first_0:                 episode reward: -22.4500,                 loss: 0.4381
env0_second_0:                 episode reward: 22.4500,                 loss: 0.3985
env1_first_0:                 episode reward: -21.9500,                 loss: nan
env1_second_0:                 episode reward: 21.9500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1695.05,                last time consumption/overall running time: 2699.5666s / 249067.5040 s
env0_first_0:                 episode reward: -21.8000,                 loss: 0.5043
env0_second_0:                 episode reward: 21.8000,                 loss: 0.4648
env1_first_0:                 episode reward: -19.8500,                 loss: nan
env1_second_0:                 episode reward: 19.8500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1772.7,                last time consumption/overall running time: 2805.4067s / 251872.9108 s
env0_first_0:                 episode reward: -9.2500,                 loss: 0.5546
env0_second_0:                 episode reward: 9.2500,                 loss: 0.5122
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1666.5,                last time consumption/overall running time: 2644.3879s / 254517.2987 s
env0_first_0:                 episode reward: -29.4000,                 loss: 0.5955
env0_second_0:                 episode reward: 29.4000,                 loss: 0.5281
env1_first_0:                 episode reward: -15.4000,                 loss: nan
env1_second_0:                 episode reward: 15.4000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1759.35,                last time consumption/overall running time: 2791.2269s / 257308.5255 s
env0_first_0:                 episode reward: -15.6000,                 loss: 0.5968
env0_second_0:                 episode reward: 15.6000,                 loss: 0.5467
env1_first_0:                 episode reward: -19.9500,                 loss: nan
env1_second_0:                 episode reward: 19.9500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1651.45,                last time consumption/overall running time: 2628.3238s / 259936.8493 s
env0_first_0:                 episode reward: -24.2000,                 loss: 0.5848
env0_second_0:                 episode reward: 24.2000,                 loss: 0.5340
env1_first_0:                 episode reward: -17.6500,                 loss: nan
env1_second_0:                 episode reward: 17.6500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1568.45,                last time consumption/overall running time: 2511.6156s / 262448.4649 s
env0_first_0:                 episode reward: -35.0500,                 loss: 0.6262
env0_second_0:                 episode reward: 35.0500,                 loss: 0.5874
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1570.6,                last time consumption/overall running time: 2495.4332s / 264943.8981 s
env0_first_0:                 episode reward: -23.0000,                 loss: 0.7038
env0_second_0:                 episode reward: 23.0000,                 loss: 0.6328
env1_first_0:                 episode reward: -31.8500,                 loss: nan
env1_second_0:                 episode reward: 31.8500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1451.75,                last time consumption/overall running time: 2313.3555s / 267257.2536 s
env0_first_0:                 episode reward: -22.2500,                 loss: 0.8500
env0_second_0:                 episode reward: 22.2500,                 loss: 0.7444
env1_first_0:                 episode reward: -34.5500,                 loss: nan
env1_second_0:                 episode reward: 34.5500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 1683.05,                last time consumption/overall running time: 2681.8625s / 269939.1160 s
env0_first_0:                 episode reward: -11.5500,                 loss: 0.8377
env0_second_0:                 episode reward: 11.5500,                 loss: 0.7593
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1671.15,                last time consumption/overall running time: 2661.5469s / 272600.6630 s
env0_first_0:                 episode reward: -16.2500,                 loss: 0.7654
env0_second_0:                 episode reward: 16.2500,                 loss: 0.6736
env1_first_0:                 episode reward: -20.6000,                 loss: nan
env1_second_0:                 episode reward: 20.6000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1731.85,                last time consumption/overall running time: 2915.9105s / 275516.5735 s
env0_first_0:                 episode reward: -8.3500,                 loss: 0.6337
env0_second_0:                 episode reward: 8.3500,                 loss: 0.5716
env1_first_0:                 episode reward: -14.2000,                 loss: nan
env1_second_0:                 episode reward: 14.2000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1763.0,                last time consumption/overall running time: 2809.6151s / 278326.1885 s
env0_first_0:                 episode reward: -13.7500,                 loss: 0.5555
env0_second_0:                 episode reward: 13.7500,                 loss: 0.5211
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1772.1,                last time consumption/overall running time: 2850.1815s / 281176.3700 s
env0_first_0:                 episode reward: -10.9500,                 loss: 0.5292
env0_second_0:                 episode reward: 10.9500,                 loss: 0.4964
env1_first_0:                 episode reward: -14.2000,                 loss: nan
env1_second_0:                 episode reward: 14.2000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1734.75,                last time consumption/overall running time: 2750.7480s / 283927.1181 s
env0_first_0:                 episode reward: -13.7500,                 loss: 0.5059
env0_second_0:                 episode reward: 13.7500,                 loss: 0.4601
env1_first_0:                 episode reward: -19.3000,                 loss: nan
env1_second_0:                 episode reward: 19.3000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1626.5,                last time consumption/overall running time: 2583.3386s / 286510.4567 s
env0_first_0:                 episode reward: -19.1000,                 loss: 0.5007
env0_second_0:                 episode reward: 19.1000,                 loss: 0.4610
env1_first_0:                 episode reward: -13.9500,                 loss: nan
env1_second_0:                 episode reward: 13.9500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1781.6,                last time consumption/overall running time: 2833.7614s / 289344.2180 s
env0_first_0:                 episode reward: -16.7500,                 loss: 0.5081
env0_second_0:                 episode reward: 16.7500,                 loss: 0.5005
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1768.75,                last time consumption/overall running time: 2820.5510s / 292164.7690 s
env0_first_0:                 episode reward: -16.8000,                 loss: 0.5000
env0_second_0:                 episode reward: 16.8000,                 loss: 0.4711
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1748.05,                last time consumption/overall running time: 2716.0858s / 294880.8548 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.5416
env0_second_0:                 episode reward: 12.2000,                 loss: 0.5151
env1_first_0:                 episode reward: -25.8000,                 loss: nan
env1_second_0:                 episode reward: 25.8000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1680.25,                last time consumption/overall running time: 2613.5173s / 297494.3721 s
env0_first_0:                 episode reward: -25.9500,                 loss: 0.5681
env0_second_0:                 episode reward: 25.9500,                 loss: 0.5431
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1711.2,                last time consumption/overall running time: 2672.1918s / 300166.5639 s
env0_first_0:                 episode reward: -9.1000,                 loss: 0.5801
env0_second_0:                 episode reward: 9.1000,                 loss: 0.5749
env1_first_0:                 episode reward: -21.0000,                 loss: nan
env1_second_0:                 episode reward: 21.0000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1727.75,                last time consumption/overall running time: 2695.6786s / 302862.2425 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.5876
env0_second_0:                 episode reward: 4.7000,                 loss: 0.5835
env1_first_0:                 episode reward: -16.0000,                 loss: nan
env1_second_0:                 episode reward: 16.0000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1759.45,                last time consumption/overall running time: 2733.2506s / 305595.4932 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.5055
env0_second_0:                 episode reward: 7.3000,                 loss: 0.4999
env1_first_0:                 episode reward: -17.0500,                 loss: nan
env1_second_0:                 episode reward: 17.0500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1638.65,                last time consumption/overall running time: 2558.5377s / 308154.0308 s
env0_first_0:                 episode reward: -34.4000,                 loss: 0.5934
env0_second_0:                 episode reward: 34.4000,                 loss: 0.5828
env1_first_0:                 episode reward: -25.3000,                 loss: nan
env1_second_0:                 episode reward: 25.3000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1723.6,                last time consumption/overall running time: 2678.5572s / 310832.5880 s
env0_first_0:                 episode reward: -22.4500,                 loss: 0.6357
env0_second_0:                 episode reward: 22.4500,                 loss: 0.6194
env1_first_0:                 episode reward: -17.6000,                 loss: nan
env1_second_0:                 episode reward: 17.6000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2777.1808s / 313609.7688 s
env0_first_0:                 episode reward: -15.2000,                 loss: 0.6215
env0_second_0:                 episode reward: 15.2000,                 loss: 0.5956
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1718.95,                last time consumption/overall running time: 2692.3732s / 316302.1419 s
env0_first_0:                 episode reward: -12.5500,                 loss: 0.5904
env0_second_0:                 episode reward: 12.5500,                 loss: 0.5488
env1_first_0:                 episode reward: -21.6000,                 loss: nan
env1_second_0:                 episode reward: 21.6000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2773.1432s / 319075.2852 s
env0_first_0:                 episode reward: -11.9500,                 loss: 0.4896
env0_second_0:                 episode reward: 11.9500,                 loss: 0.4580
env1_first_0:                 episode reward: -11.4000,                 loss: nan
env1_second_0:                 episode reward: 11.4000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1697.0,                last time consumption/overall running time: 2636.8835s / 321712.1687 s
env0_first_0:                 episode reward: -15.8500,                 loss: 0.4515
env0_second_0:                 episode reward: 15.8500,                 loss: 0.4312
env1_first_0:                 episode reward: -18.0000,                 loss: nan
env1_second_0:                 episode reward: 18.0000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1762.35,                last time consumption/overall running time: 2744.6714s / 324456.8401 s
env0_first_0:                 episode reward: -13.1500,                 loss: 0.4001
env0_second_0:                 episode reward: 13.1500,                 loss: 0.3871
env1_first_0:                 episode reward: -15.5500,                 loss: nan
env1_second_0:                 episode reward: 15.5500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1767.85,                last time consumption/overall running time: 2754.5503s / 327211.3903 s
env0_first_0:                 episode reward: -6.5000,                 loss: 0.3922
env0_second_0:                 episode reward: 6.5000,                 loss: 0.3938
env1_first_0:                 episode reward: -20.9500,                 loss: nan
env1_second_0:                 episode reward: 20.9500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2777.3145s / 329988.7048 s
env0_first_0:                 episode reward: -18.1500,                 loss: 0.3865
env0_second_0:                 episode reward: 18.1500,                 loss: 0.3812
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1757.2,                last time consumption/overall running time: 2753.9992s / 332742.7040 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.4157
env0_second_0:                 episode reward: 18.2000,                 loss: 0.3984
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1775.85,                last time consumption/overall running time: 2761.0277s / 335503.7317 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.3818
env0_second_0:                 episode reward: 5.7500,                 loss: 0.3629
env1_first_0:                 episode reward: -14.1500,                 loss: nan
env1_second_0:                 episode reward: 14.1500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 1771.15,                last time consumption/overall running time: 2765.7155s / 338269.4473 s
env0_first_0:                 episode reward: -9.5000,                 loss: 0.3331
env0_second_0:                 episode reward: 9.5000,                 loss: 0.3345
env1_first_0:                 episode reward: -10.2000,                 loss: nan
env1_second_0:                 episode reward: 10.2000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1741.55,                last time consumption/overall running time: 2716.8702s / 340986.3174 s
env0_first_0:                 episode reward: -7.4500,                 loss: 0.3160
env0_second_0:                 episode reward: 7.4500,                 loss: 0.3121
env1_first_0:                 episode reward: -9.5500,                 loss: nan
env1_second_0:                 episode reward: 9.5500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1744.85,                last time consumption/overall running time: 2725.9491s / 343712.2665 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.3494
env0_second_0:                 episode reward: 5.1000,                 loss: 0.3452
env1_first_0:                 episode reward: -16.8500,                 loss: nan
env1_second_0:                 episode reward: 16.8500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1776.9,                last time consumption/overall running time: 2760.3018s / 346472.5684 s
env0_first_0:                 episode reward: -11.8000,                 loss: 0.3949
env0_second_0:                 episode reward: 11.8000,                 loss: 0.3976
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1763.2,                last time consumption/overall running time: 2748.9651s / 349221.5334 s
env0_first_0:                 episode reward: -9.6000,                 loss: 0.3934
env0_second_0:                 episode reward: 9.6000,                 loss: 0.3986
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1749.15,                last time consumption/overall running time: 2731.8910s / 351953.4245 s
env0_first_0:                 episode reward: -13.1500,                 loss: 0.4151
env0_second_0:                 episode reward: 13.1500,                 loss: 0.4223
env1_first_0:                 episode reward: -13.6500,                 loss: nan
env1_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 1763.05,                last time consumption/overall running time: 2770.5829s / 354724.0074 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.3884
env0_second_0:                 episode reward: 2.6500,                 loss: 0.4027
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2807.9899s / 357531.9972 s
env0_first_0:                 episode reward: -8.6500,                 loss: 0.3644
env0_second_0:                 episode reward: 8.6500,                 loss: 0.3860
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2783.2464s / 360315.2437 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.4335
env0_second_0:                 episode reward: 7.8000,                 loss: 0.4545
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1747.35,                last time consumption/overall running time: 2713.9230s / 363029.1667 s
env0_first_0:                 episode reward: -19.4500,                 loss: 0.4608
env0_second_0:                 episode reward: 19.4500,                 loss: 0.4715
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1691.0,                last time consumption/overall running time: 2630.0703s / 365659.2369 s
env0_first_0:                 episode reward: -21.9000,                 loss: 0.5613
env0_second_0:                 episode reward: 21.9000,                 loss: 0.5778
env1_first_0:                 episode reward: -30.0500,                 loss: nan
env1_second_0:                 episode reward: 30.0500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1720.55,                last time consumption/overall running time: 2696.9911s / 368356.2281 s
env0_first_0:                 episode reward: -14.5500,                 loss: 0.4973
env0_second_0:                 episode reward: 14.5500,                 loss: 0.5129
env1_first_0:                 episode reward: -21.9500,                 loss: nan
env1_second_0:                 episode reward: 21.9500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 1723.55,                last time consumption/overall running time: 2671.1852s / 371027.4133 s
env0_first_0:                 episode reward: -21.6000,                 loss: 0.5229
env0_second_0:                 episode reward: 21.6000,                 loss: 0.5128
env1_first_0:                 episode reward: -34.6000,                 loss: nan
env1_second_0:                 episode reward: 34.6000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1745.0,                last time consumption/overall running time: 2735.2834s / 373762.6967 s
env0_first_0:                 episode reward: -13.4500,                 loss: 0.4761
env0_second_0:                 episode reward: 13.4500,                 loss: 0.4698
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1761.65,                last time consumption/overall running time: 2786.4889s / 376549.1856 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.4176
env0_second_0:                 episode reward: 3.4500,                 loss: 0.4216
env1_first_0:                 episode reward: -21.5500,                 loss: nan
env1_second_0:                 episode reward: 21.5500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1746.7,                last time consumption/overall running time: 2819.1494s / 379368.3350 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.3960
env0_second_0:                 episode reward: 11.6500,                 loss: 0.4031
env1_first_0:                 episode reward: -13.4500,                 loss: nan
env1_second_0:                 episode reward: 13.4500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2861.3072s / 382229.6423 s
env0_first_0:                 episode reward: -20.9000,                 loss: 0.3757
env0_second_0:                 episode reward: 20.9000,                 loss: 0.3772
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1781.45,                last time consumption/overall running time: 2879.8673s / 385109.5096 s
env0_first_0:                 episode reward: -17.6000,                 loss: 0.4276
env0_second_0:                 episode reward: 17.6000,                 loss: 0.4326
env1_first_0:                 episode reward: -14.6000,                 loss: nan
env1_second_0:                 episode reward: 14.6000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1763.15,                last time consumption/overall running time: 2846.7315s / 387956.2411 s
env0_first_0:                 episode reward: -13.6000,                 loss: 0.4108
env0_second_0:                 episode reward: 13.6000,                 loss: 0.4192
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1722.25,                last time consumption/overall running time: 2780.3971s / 390736.6381 s
env0_first_0:                 episode reward: -8.9500,                 loss: 0.4051
env0_second_0:                 episode reward: 8.9500,                 loss: 0.4056
env1_first_0:                 episode reward: -21.2000,                 loss: nan
env1_second_0:                 episode reward: 21.2000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2861.0640s / 393597.7021 s
env0_first_0:                 episode reward: -9.8500,                 loss: 0.3595
env0_second_0:                 episode reward: 9.8500,                 loss: 0.3611
env1_first_0:                 episode reward: -12.6000,                 loss: nan
env1_second_0:                 episode reward: 12.6000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1721.15,                last time consumption/overall running time: 2681.3259s / 396279.0280 s
env0_first_0:                 episode reward: -17.9000,                 loss: 0.3731
env0_second_0:                 episode reward: 17.9000,                 loss: 0.3716
env1_first_0:                 episode reward: -23.2000,                 loss: nan
env1_second_0:                 episode reward: 23.2000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1750.3,                last time consumption/overall running time: 2731.6934s / 399010.7214 s
env0_first_0:                 episode reward: -16.0500,                 loss: 0.4305
env0_second_0:                 episode reward: 16.0500,                 loss: 0.4365
env1_first_0:                 episode reward: -12.9000,                 loss: nan
env1_second_0:                 episode reward: 12.9000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1688.95,                last time consumption/overall running time: 2633.8022s / 401644.5236 s
env0_first_0:                 episode reward: -19.8000,                 loss: 0.5015
env0_second_0:                 episode reward: 19.8000,                 loss: 0.4962
env1_first_0:                 episode reward: -22.0500,                 loss: nan
env1_second_0:                 episode reward: 22.0500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1780.1,                last time consumption/overall running time: 2770.4148s / 404414.9384 s
env0_first_0:                 episode reward: -16.5000,                 loss: 0.5559
env0_second_0:                 episode reward: 16.5000,                 loss: 0.5696
env1_first_0:                 episode reward: -19.7000,                 loss: nan
env1_second_0:                 episode reward: 19.7000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1779.4,                last time consumption/overall running time: 2781.3206s / 407196.2590 s
env0_first_0:                 episode reward: -14.1500,                 loss: 0.5793
env0_second_0:                 episode reward: 14.1500,                 loss: 0.5702
env1_first_0:                 episode reward: -15.3500,                 loss: nan
env1_second_0:                 episode reward: 15.3500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1779.75,                last time consumption/overall running time: 2767.5365s / 409963.7955 s
env0_first_0:                 episode reward: -13.4000,                 loss: 0.5480
env0_second_0:                 episode reward: 13.4000,                 loss: 0.5356
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1721.35,                last time consumption/overall running time: 2677.8319s / 412641.6274 s
env0_first_0:                 episode reward: -29.1000,                 loss: 0.5381
env0_second_0:                 episode reward: 29.1000,                 loss: 0.5195
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1712.0,                last time consumption/overall running time: 2668.9843s / 415310.6116 s
env0_first_0:                 episode reward: -16.5000,                 loss: 0.5388
env0_second_0:                 episode reward: 16.5000,                 loss: 0.5407
env1_first_0:                 episode reward: -24.5500,                 loss: nan
env1_second_0:                 episode reward: 24.5500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2773.3274s / 418083.9390 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.5042
env0_second_0:                 episode reward: 4.0500,                 loss: 0.5116
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2776.3141s / 420860.2531 s
env0_first_0:                 episode reward: -12.1500,                 loss: 0.4368
env0_second_0:                 episode reward: 12.1500,                 loss: 0.4210
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2775.8076s / 423636.0607 s
env0_first_0:                 episode reward: -13.9500,                 loss: 0.4057
env0_second_0:                 episode reward: 13.9500,                 loss: 0.4052
env1_first_0:                 episode reward: -20.1000,                 loss: nan
env1_second_0:                 episode reward: 20.1000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1743.35,                last time consumption/overall running time: 2705.3659s / 426341.4266 s
env0_first_0:                 episode reward: -17.8500,                 loss: 0.4110
env0_second_0:                 episode reward: 17.8500,                 loss: 0.4176
env1_first_0:                 episode reward: -21.0500,                 loss: nan
env1_second_0:                 episode reward: 21.0500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2773.0196s / 429114.4462 s
env0_first_0:                 episode reward: -12.0500,                 loss: 0.4034
env0_second_0:                 episode reward: 12.0500,                 loss: 0.4079
env1_first_0:                 episode reward: -15.1000,                 loss: nan
env1_second_0:                 episode reward: 15.1000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1768.85,                last time consumption/overall running time: 2756.3260s / 431870.7722 s
env0_first_0:                 episode reward: -13.9000,                 loss: 0.4015
env0_second_0:                 episode reward: 13.9000,                 loss: 0.3900
env1_first_0:                 episode reward: -16.2000,                 loss: nan
env1_second_0:                 episode reward: 16.2000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1733.0,                last time consumption/overall running time: 2713.8521s / 434584.6243 s
env0_first_0:                 episode reward: -18.4000,                 loss: 0.4180
env0_second_0:                 episode reward: 18.4000,                 loss: 0.4119
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2794.6043s / 437379.2287 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.4017
env0_second_0:                 episode reward: 8.5000,                 loss: 0.3974
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1735.15,                last time consumption/overall running time: 2801.9884s / 440181.2171 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.3834
env0_second_0:                 episode reward: 4.5000,                 loss: 0.3854
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1731.65,                last time consumption/overall running time: 2706.2261s / 442887.4432 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.4106
env0_second_0:                 episode reward: 3.9500,                 loss: 0.3947
env1_first_0:                 episode reward: -14.6000,                 loss: nan
env1_second_0:                 episode reward: 14.6000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 1751.35,                last time consumption/overall running time: 2730.6797s / 445618.1228 s
env0_first_0:                 episode reward: -15.8000,                 loss: 0.4421
env0_second_0:                 episode reward: 15.8000,                 loss: 0.4338
env1_first_0:                 episode reward: -20.2000,                 loss: nan
env1_second_0:                 episode reward: 20.2000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1756.8,                last time consumption/overall running time: 2743.6872s / 448361.8100 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.4238
env0_second_0:                 episode reward: 3.9500,                 loss: 0.4133
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2784.9210s / 451146.7310 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.3853
env0_second_0:                 episode reward: 2.1000,                 loss: 0.3708
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2780.5897s / 453927.3207 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.3748
env0_second_0:                 episode reward: 7.8000,                 loss: 0.3675
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1773.45,                last time consumption/overall running time: 2764.8772s / 456692.1979 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.3524
env0_second_0:                 episode reward: 0.1500,                 loss: 0.3457
env1_first_0:                 episode reward: -11.1500,                 loss: nan
env1_second_0:                 episode reward: 11.1500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2770.2534s / 459462.4513 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.3278
env0_second_0:                 episode reward: 7.4000,                 loss: 0.3271
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2767.2834s / 462229.7347 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.2931
env0_second_0:                 episode reward: 6.2000,                 loss: 0.3076
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2770.2548s / 464999.9894 s
env0_first_0:                 episode reward: -13.7000,                 loss: 0.2914
env0_second_0:                 episode reward: 13.7000,                 loss: 0.3051
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1779.75,                last time consumption/overall running time: 2792.1076s / 467792.0970 s
env0_first_0:                 episode reward: -17.5000,                 loss: 0.3168
env0_second_0:                 episode reward: 17.5000,                 loss: 0.3330
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1736.75,                last time consumption/overall running time: 2777.5301s / 470569.6271 s
env0_first_0:                 episode reward: -13.3500,                 loss: 0.3487
env0_second_0:                 episode reward: 13.3500,                 loss: 0.3697
env1_first_0:                 episode reward: -11.4500,                 loss: nan
env1_second_0:                 episode reward: 11.4500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1747.2,                last time consumption/overall running time: 2803.4743s / 473373.1014 s
env0_first_0:                 episode reward: -17.6000,                 loss: 0.3966
env0_second_0:                 episode reward: 17.6000,                 loss: 0.4383
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1710.45,                last time consumption/overall running time: 2738.1994s / 476111.3007 s
env0_first_0:                 episode reward: -16.6500,                 loss: 0.4279
env0_second_0:                 episode reward: 16.6500,                 loss: 0.4764
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1782.95,                last time consumption/overall running time: 2850.7605s / 478962.0612 s
env0_first_0:                 episode reward: -15.9000,                 loss: 0.4427
env0_second_0:                 episode reward: 15.9000,                 loss: 0.4635
env1_first_0:                 episode reward: -13.5500,                 loss: nan
env1_second_0:                 episode reward: 13.5500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2845.2358s / 481807.2970 s
env0_first_0:                 episode reward: -9.7500,                 loss: 0.3940
env0_second_0:                 episode reward: 9.7500,                 loss: 0.4226
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2838.0902s / 484645.3872 s
env0_first_0:                 episode reward: -8.4500,                 loss: 0.3624
env0_second_0:                 episode reward: 8.4500,                 loss: 0.3852
env1_first_0:                 episode reward: -9.3000,                 loss: nan
env1_second_0:                 episode reward: 9.3000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2833.6648s / 487479.0521 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.3681
env0_second_0:                 episode reward: 12.2000,                 loss: 0.3906
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2768.3564s / 490247.4085 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.3486
env0_second_0:                 episode reward: 7.0000,                 loss: 0.3687
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2780.3074s / 493027.7159 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.3322
env0_second_0:                 episode reward: 3.6000,                 loss: 0.3412
env1_first_0:                 episode reward: -17.6000,                 loss: nan
env1_second_0:                 episode reward: 17.6000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1779.05,                last time consumption/overall running time: 2770.6060s / 495798.3219 s
env0_first_0:                 episode reward: -11.3000,                 loss: 0.2993
env0_second_0:                 episode reward: 11.3000,                 loss: 0.3132
env1_first_0:                 episode reward: -11.0000,                 loss: nan
env1_second_0:                 episode reward: 11.0000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1768.35,                last time consumption/overall running time: 2775.0818s / 498573.4037 s
env0_first_0:                 episode reward: -12.7500,                 loss: 0.3211
env0_second_0:                 episode reward: 12.7500,                 loss: 0.3466
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1720.9,                last time consumption/overall running time: 2729.5616s / 501302.9653 s
env0_first_0:                 episode reward: -9.4500,                 loss: 0.3642
env0_second_0:                 episode reward: 9.4500,                 loss: 0.3936
env1_first_0:                 episode reward: -18.5500,                 loss: nan
env1_second_0:                 episode reward: 18.5500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1766.35,                last time consumption/overall running time: 2814.0968s / 504117.0621 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.3759
env0_second_0:                 episode reward: 3.7000,                 loss: 0.4178
env1_first_0:                 episode reward: -19.5000,                 loss: nan
env1_second_0:                 episode reward: 19.5000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1710.55,                last time consumption/overall running time: 2652.0770s / 506769.1391 s
env0_first_0:                 episode reward: -12.4000,                 loss: 0.3646
env0_second_0:                 episode reward: 12.4000,                 loss: 0.4028
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1773.45,                last time consumption/overall running time: 2764.1537s / 509533.2928 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.3700
env0_second_0:                 episode reward: 6.1000,                 loss: 0.3909
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2768.9632s / 512302.2559 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.3328
env0_second_0:                 episode reward: 3.3000,                 loss: 0.3716
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2785.9840s / 515088.2399 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.3449
env0_second_0:                 episode reward: 2.2500,                 loss: 0.3823
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2785.4315s / 517873.6714 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.3229
env0_second_0:                 episode reward: 4.5000,                 loss: 0.3533
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2781.7802s / 520655.4516 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.2950
env0_second_0:                 episode reward: -1.6000,                 loss: 0.3206
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2768.3272s / 523423.7788 s
env0_first_0:                 episode reward: -10.6500,                 loss: 0.2808
env0_second_0:                 episode reward: 10.6500,                 loss: 0.3168
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1765.15,                last time consumption/overall running time: 2751.2949s / 526175.0738 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.2838
env0_second_0:                 episode reward: 5.3500,                 loss: 0.3124
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2850.4699s / 529025.5437 s
env0_first_0:                 episode reward: -10.2000,                 loss: 0.3281
env0_second_0:                 episode reward: 10.2000,                 loss: 0.3578
env1_first_0:                 episode reward: -13.8500,                 loss: nan
env1_second_0:                 episode reward: 13.8500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2876.2294s / 531901.7731 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.3372
env0_second_0:                 episode reward: 6.8000,                 loss: 0.3724
env1_first_0:                 episode reward: -11.3000,                 loss: nan
env1_second_0:                 episode reward: 11.3000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2874.3379s / 534776.1109 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.3155
env0_second_0:                 episode reward: 5.0500,                 loss: 0.3571
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2861.0281s / 537637.1390 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.2856
env0_second_0:                 episode reward: -3.9000,                 loss: 0.3211
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1753.75,                last time consumption/overall running time: 2801.0415s / 540438.1805 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.2554
env0_second_0:                 episode reward: 6.2500,                 loss: 0.2782
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2849.6453s / 543287.8259 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.2568
env0_second_0:                 episode reward: 2.7500,                 loss: 0.2844
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2850.0325s / 546137.8584 s
env0_first_0:                 episode reward: -10.8500,                 loss: 0.2701
env0_second_0:                 episode reward: 10.8500,                 loss: 0.2996
env1_first_0:                 episode reward: -15.3500,                 loss: nan
env1_second_0:                 episode reward: 15.3500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2845.8812s / 548983.7395 s
env0_first_0:                 episode reward: -10.2500,                 loss: 0.2988
env0_second_0:                 episode reward: 10.2500,                 loss: 0.3213
env1_first_0:                 episode reward: -15.0500,                 loss: nan
env1_second_0:                 episode reward: 15.0500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2853.4586s / 551837.1981 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.3094
env0_second_0:                 episode reward: 4.4000,                 loss: 0.3424
env1_first_0:                 episode reward: -15.7000,                 loss: nan
env1_second_0:                 episode reward: 15.7000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2789.2332s / 554626.4313 s
env0_first_0:                 episode reward: -10.3000,                 loss: 0.2934
env0_second_0:                 episode reward: 10.3000,                 loss: 0.3306
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2782.4445s / 557408.8758 s
env0_first_0:                 episode reward: -12.2000,                 loss: 0.2941
env0_second_0:                 episode reward: 12.2000,                 loss: 0.3268
env1_first_0:                 episode reward: -13.0500,                 loss: nan
env1_second_0:                 episode reward: 13.0500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2784.5809s / 560193.4567 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.2596
env0_second_0:                 episode reward: 3.1500,                 loss: 0.2905
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2778.2950s / 562971.7516 s
env0_first_0:                 episode reward: -10.1500,                 loss: 0.2585
env0_second_0:                 episode reward: 10.1500,                 loss: 0.2888
env1_first_0:                 episode reward: -12.3500,                 loss: nan
env1_second_0:                 episode reward: 12.3500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2780.6852s / 565752.4368 s
env0_first_0:                 episode reward: -14.9500,                 loss: 0.2455
env0_second_0:                 episode reward: 14.9500,                 loss: 0.2826
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1747.65,                last time consumption/overall running time: 2721.2461s / 568473.6830 s
env0_first_0:                 episode reward: -9.6500,                 loss: 0.2817
env0_second_0:                 episode reward: 9.6500,                 loss: 0.3135
env1_first_0:                 episode reward: -12.9000,                 loss: nan
env1_second_0:                 episode reward: 12.9000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2770.8142s / 571244.4972 s
env0_first_0:                 episode reward: -9.9500,                 loss: 0.3035
env0_second_0:                 episode reward: 9.9500,                 loss: 0.3442
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2785.2587s / 574029.7559 s
env0_first_0:                 episode reward: -9.2000,                 loss: 0.3128
env0_second_0:                 episode reward: 9.2000,                 loss: 0.3529
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2774.8248s / 576804.5807 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.3131
env0_second_0:                 episode reward: 11.6500,                 loss: 0.3625
env1_first_0:                 episode reward: -11.7000,                 loss: nan
env1_second_0:                 episode reward: 11.7000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2779.4068s / 579583.9875 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.2989
env0_second_0:                 episode reward: 11.6500,                 loss: 0.3534
env1_first_0:                 episode reward: -10.1500,                 loss: nan
env1_second_0:                 episode reward: 10.1500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2781.3045s / 582365.2920 s
env0_first_0:                 episode reward: -14.9000,                 loss: 0.3070
env0_second_0:                 episode reward: 14.9000,                 loss: 0.3641
env1_first_0:                 episode reward: -10.5500,                 loss: nan
env1_second_0:                 episode reward: 10.5500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1769.4,                last time consumption/overall running time: 2748.5890s / 585113.8810 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.3013
env0_second_0:                 episode reward: 6.3500,                 loss: 0.3490
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2788.7674s / 587902.6485 s
env0_first_0:                 episode reward: -10.3500,                 loss: 0.3140
env0_second_0:                 episode reward: 10.3500,                 loss: 0.3500
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1781.95,                last time consumption/overall running time: 2762.7944s / 590665.4428 s
env0_first_0:                 episode reward: -13.7500,                 loss: 0.3099
env0_second_0:                 episode reward: 13.7500,                 loss: 0.3510
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2784.3190s / 593449.7619 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.3110
env0_second_0:                 episode reward: 7.5000,                 loss: 0.3622
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2809.6273s / 596259.3892 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.2991
env0_second_0:                 episode reward: 7.7500,                 loss: 0.3349
env1_first_0:                 episode reward: -14.7000,                 loss: nan
env1_second_0:                 episode reward: 14.7000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1770.0,                last time consumption/overall running time: 2774.7774s / 599034.1666 s
env0_first_0:                 episode reward: -9.5000,                 loss: 0.3395
env0_second_0:                 episode reward: 9.5000,                 loss: 0.3657
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2799.5433s / 601833.7099 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.3547
env0_second_0:                 episode reward: 7.7500,                 loss: 0.3967
env1_first_0:                 episode reward: -9.3000,                 loss: nan
env1_second_0:                 episode reward: 9.3000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2841.8878s / 604675.5977 s