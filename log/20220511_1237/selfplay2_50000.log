2022-05-11 12:37:49.088661: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-05-11 12:37:49.088744: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia:/home/quantumiracle/.mujoco/mujoco200/bin:/home/quantumiracle/.mujoco/mujoco210/bin:/usr/lib/nvidia
2022-05-11 12:37:49.088752: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
pygame 2.0.1 (SDL 2.0.14, Python 3.6.13)
Hello from the pygame community. https://www.pygame.org/contribute.html
arbitrary_mdp mdp
Load arbitrary_mdp environment in type mdp.
Env observation space: Box(0.0, 33.0, (1,), float32) action space: Discrete(3)
<mars.env.mdp.mdp_wrapper.MDPWrapper object at 0x7f46622937f0>
3 3 10
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=1, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=3, bias=True)
    )
  )
)
Error: no exploited model index given!
Agents No. [0] (index starting from 0) are not learnable.
Agent No. [0] loads model from:  data/model/20220510124814/mdp_arbitrary_mdp_selfplay2/50000_0
Arguments:  {'env_name': 'arbitrary_mdp', 'env_type': 'mdp', 'num_envs': 1, 'ram': True, 'seed': 1, 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 1.0, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 8000}, 'num_process': 1, 'batch_size': 640, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': True, 'load_model_idx': False, 'load_model_full_path': 'data/model/20220510124814/mdp_arbitrary_mdp_selfplay2/50000_0', 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 1000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True, 'selfplay_score_delta': 1.5, 'trainable_agent_idx': 0, 'opponent_idx': 1}, 'against_baseline': False}
Save models to : /home/quantumiracle/research/MARS/data/model/20220510124814_exploit_50000/mdp_arbitrary_mdp_selfplay2. 
 Save logs to: /home/quantumiracle/research/MARS/data/log/20220510124814_exploit_50000/mdp_arbitrary_mdp_selfplay2.
Episode: 1/30000 (0.0033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8625s / 0.8625 s
agent0:                 episode reward: -0.6574,                 loss: nan
agent1:                 episode reward: 0.6574,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.0923s / 0.9548 s
agent0:                 episode reward: 0.6804,                 loss: nan
agent1:                 episode reward: -0.6804,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.0883s / 1.0431 s
agent0:                 episode reward: 1.1474,                 loss: nan
agent1:                 episode reward: -1.1474,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.0843s / 1.1273 s
agent0:                 episode reward: 0.5651,                 loss: nan
agent1:                 episode reward: -0.5651,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.6419s / 1.7693 s
agent0:                 episode reward: -0.0403,                 loss: nan
agent1:                 episode reward: 0.0403,                 loss: 0.4059
Episode: 101/30000 (0.3367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.7527s / 2.5220 s
agent0:                 episode reward: 1.4920,                 loss: nan
agent1:                 episode reward: -1.4920,                 loss: 0.3794
Episode: 121/30000 (0.4033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.7581s / 3.2801 s
agent0:                 episode reward: 0.4096,                 loss: nan
agent1:                 episode reward: -0.4096,                 loss: 0.3756
Episode: 141/30000 (0.4700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8675s / 4.1476 s
agent0:                 episode reward: 0.9796,                 loss: nan
agent1:                 episode reward: -0.9796,                 loss: 0.3722
Episode: 161/30000 (0.5367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.7803s / 4.9279 s
agent0:                 episode reward: 0.9113,                 loss: nan
agent1:                 episode reward: -0.9113,                 loss: 0.3728
Episode: 181/30000 (0.6033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8191s / 5.7471 s
agent0:                 episode reward: 0.6838,                 loss: nan
agent1:                 episode reward: -0.6838,                 loss: 0.3820
Episode: 201/30000 (0.6700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8155s / 6.5626 s
agent0:                 episode reward: 0.2301,                 loss: nan
agent1:                 episode reward: -0.2301,                 loss: 0.3828
Episode: 221/30000 (0.7367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8482s / 7.4108 s
agent0:                 episode reward: 0.9275,                 loss: nan
agent1:                 episode reward: -0.9275,                 loss: 0.3798
Episode: 241/30000 (0.8033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.7680s / 8.1787 s
agent0:                 episode reward: 0.1585,                 loss: nan
agent1:                 episode reward: -0.1585,                 loss: 0.3791
Episode: 261/30000 (0.8700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8256s / 9.0044 s
agent0:                 episode reward: 0.0544,                 loss: nan
agent1:                 episode reward: -0.0544,                 loss: 0.3766
Episode: 281/30000 (0.9367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8718s / 9.8762 s
agent0:                 episode reward: 0.9628,                 loss: nan
agent1:                 episode reward: -0.9628,                 loss: 0.4090
Episode: 301/30000 (1.0033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8148s / 10.6909 s
agent0:                 episode reward: 0.7615,                 loss: nan
agent1:                 episode reward: -0.7615,                 loss: 0.4097
Episode: 321/30000 (1.0700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8281s / 11.5190 s
agent0:                 episode reward: 1.1048,                 loss: nan
agent1:                 episode reward: -1.1048,                 loss: 0.4088
Episode: 341/30000 (1.1367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.7973s / 12.3163 s
agent0:                 episode reward: 0.6541,                 loss: nan
agent1:                 episode reward: -0.6541,                 loss: 0.4070
Episode: 361/30000 (1.2033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.7966s / 13.1129 s
agent0:                 episode reward: -0.2586,                 loss: nan
agent1:                 episode reward: 0.2586,                 loss: 0.4052
Episode: 381/30000 (1.2700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8595s / 13.9724 s
agent0:                 episode reward: 0.2327,                 loss: nan
agent1:                 episode reward: -0.2327,                 loss: 0.3765
Episode: 401/30000 (1.3367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8609s / 14.8333 s
agent0:                 episode reward: 0.2593,                 loss: nan
agent1:                 episode reward: -0.2593,                 loss: 0.3609
Episode: 421/30000 (1.4033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8048s / 15.6381 s
agent0:                 episode reward: 0.2347,                 loss: nan
agent1:                 episode reward: -0.2347,                 loss: 0.3582
Episode: 441/30000 (1.4700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.7930s / 16.4311 s
agent0:                 episode reward: 0.5806,                 loss: nan
agent1:                 episode reward: -0.5806,                 loss: 0.3560
Episode: 461/30000 (1.5367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8248s / 17.2559 s
agent0:                 episode reward: 0.1976,                 loss: nan
agent1:                 episode reward: -0.1976,                 loss: 0.3549
Episode: 481/30000 (1.6033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8237s / 18.0796 s
agent0:                 episode reward: 0.2826,                 loss: nan
agent1:                 episode reward: -0.2826,                 loss: 0.2796
Episode: 501/30000 (1.6700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8409s / 18.9205 s
agent0:                 episode reward: -0.3281,                 loss: nan
agent1:                 episode reward: 0.3281,                 loss: 0.2632
Episode: 521/30000 (1.7367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8190s / 19.7394 s
agent0:                 episode reward: 0.0284,                 loss: nan
agent1:                 episode reward: -0.0284,                 loss: 0.2591
Episode: 541/30000 (1.8033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8310s / 20.5705 s
agent0:                 episode reward: 0.5338,                 loss: nan
agent1:                 episode reward: -0.5338,                 loss: 0.2533
Episode: 561/30000 (1.8700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8315s / 21.4020 s
agent0:                 episode reward: -0.1720,                 loss: nan
agent1:                 episode reward: 0.1720,                 loss: 0.2459
Episode: 581/30000 (1.9367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8373s / 22.2392 s
agent0:                 episode reward: -0.0928,                 loss: nan
agent1:                 episode reward: 0.0928,                 loss: 0.2174
Episode: 601/30000 (2.0033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8504s / 23.0896 s
agent0:                 episode reward: -0.6230,                 loss: nan
agent1:                 episode reward: 0.6230,                 loss: 0.2066
Episode: 621/30000 (2.0700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8322s / 23.9218 s
agent0:                 episode reward: 0.3497,                 loss: nan
agent1:                 episode reward: -0.3497,                 loss: 0.2024
Episode: 641/30000 (2.1367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8948s / 24.8166 s
agent0:                 episode reward: -0.2707,                 loss: nan
agent1:                 episode reward: 0.2707,                 loss: 0.2012
Episode: 661/30000 (2.2033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8647s / 25.6813 s
agent0:                 episode reward: 0.3354,                 loss: nan
agent1:                 episode reward: -0.3354,                 loss: 0.1999
Episode: 681/30000 (2.2700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8480s / 26.5294 s
agent0:                 episode reward: -0.0690,                 loss: nan
agent1:                 episode reward: 0.0690,                 loss: 0.1927
Episode: 701/30000 (2.3367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8752s / 27.4046 s
agent0:                 episode reward: 0.2022,                 loss: nan
agent1:                 episode reward: -0.2022,                 loss: 0.1898
Episode: 721/30000 (2.4033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8567s / 28.2613 s
agent0:                 episode reward: -0.0673,                 loss: nan
agent1:                 episode reward: 0.0673,                 loss: 0.1874
Episode: 741/30000 (2.4700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9394s / 29.2007 s
agent0:                 episode reward: -0.0522,                 loss: nan
agent1:                 episode reward: 0.0522,                 loss: 0.1855
Episode: 761/30000 (2.5367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8495s / 30.0502 s
agent0:                 episode reward: -1.3569,                 loss: nan
agent1:                 episode reward: 1.3569,                 loss: 0.1858
Episode: 781/30000 (2.6033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8463s / 30.8965 s
agent0:                 episode reward: -0.8754,                 loss: nan
agent1:                 episode reward: 0.8754,                 loss: 0.2176
Episode: 801/30000 (2.6700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8684s / 31.7650 s
agent0:                 episode reward: -0.7697,                 loss: nan
agent1:                 episode reward: 0.7697,                 loss: 0.2208
Episode: 821/30000 (2.7367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8722s / 32.6371 s
agent0:                 episode reward: 0.3984,                 loss: nan
agent1:                 episode reward: -0.3984,                 loss: 0.2190
Episode: 841/30000 (2.8033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8982s / 33.5353 s
agent0:                 episode reward: 0.1904,                 loss: nan
agent1:                 episode reward: -0.1904,                 loss: 0.2184
Episode: 861/30000 (2.8700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8892s / 34.4245 s
agent0:                 episode reward: -0.2817,                 loss: nan
agent1:                 episode reward: 0.2817,                 loss: 0.2184
Episode: 881/30000 (2.9367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9168s / 35.3414 s
agent0:                 episode reward: -1.0646,                 loss: nan
agent1:                 episode reward: 1.0646,                 loss: 0.2428
Episode: 901/30000 (3.0033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9010s / 36.2424 s
agent0:                 episode reward: -0.7485,                 loss: nan
agent1:                 episode reward: 0.7485,                 loss: 0.2441
Episode: 921/30000 (3.0700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9652s / 37.2076 s
agent0:                 episode reward: -0.6628,                 loss: nan
agent1:                 episode reward: 0.6628,                 loss: 0.2429/home/quantumiracle/anaconda3/envs/res/lib/python3.6/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/quantumiracle/anaconda3/envs/res/lib/python3.6/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
/home/quantumiracle/anaconda3/envs/res/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/quantumiracle/anaconda3/envs/res/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

Episode: 941/30000 (3.1367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9244s / 38.1320 s
agent0:                 episode reward: -0.7227,                 loss: nan
agent1:                 episode reward: 0.7227,                 loss: 0.2407
Episode: 961/30000 (3.2033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8942s / 39.0262 s
agent0:                 episode reward: 0.2228,                 loss: nan
agent1:                 episode reward: -0.2228,                 loss: 0.2405
Episode: 981/30000 (3.2700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.8813s / 39.9075 s
agent0:                 episode reward: -0.2152,                 loss: nan
agent1:                 episode reward: 0.2152,                 loss: 0.2785
Episode: 1001/30000 (3.3367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9038s / 40.8114 s
agent0:                 episode reward: 0.4645,                 loss: nan
agent1:                 episode reward: -0.4645,                 loss: 0.2818
Episode: 1021/30000 (3.4033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9058s / 41.7172 s
agent0:                 episode reward: -0.6289,                 loss: nan
agent1:                 episode reward: 0.6289,                 loss: 0.2783
Episode: 1041/30000 (3.4700%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9286s / 42.6458 s
agent0:                 episode reward: -0.7573,                 loss: nan
agent1:                 episode reward: 0.7573,                 loss: 0.2768
Episode: 1061/30000 (3.5367%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9035s / 43.5493 s
agent0:                 episode reward: -0.7724,                 loss: nan
agent1:                 episode reward: 0.7724,                 loss: 0.2739
Episode: 1081/30000 (3.6033%),                 avg. length: 9.0,                last time consumption/overall running time: 0.9040s / 44.4533 s
agent0:                 episode reward: -0.4456,                 loss: nan
agent1:                 episode reward: 0.4456,                 loss: 0.2945
Traceback (most recent call last):
  File "exploit_arbitrary_mdp2.py", line 51, in <module>
    launch_rollout(parser_args.method, parser_args.load_id, parser_args.epi)
  File "exploit_arbitrary_mdp2.py", line 43, in launch_rollout
    rollout(env, model, args, load_id+f'_exploit_{epi}')
  File "/home/quantumiracle/research/MARS/mars/rollout.py", line 21, in rollout
    rollout_normal(env, model, save_id, args)
  File "/home/quantumiracle/research/MARS/mars/rollout.py", line 161, in rollout_normal
    loss = model.update(
  File "/home/quantumiracle/research/MARS/mars/rl/agents/multiagent.py", line 281, in update
    loss = agent.update()
  File "/home/quantumiracle/research/MARS/mars/rl/agents/dqn.py", line 109, in update
    state, action, reward, next_state, done = self.buffer.sample(self.batch_size)
  File "/home/quantumiracle/research/MARS/mars/rl/common/storage.py", line 88, in sample
    indices = np.random.choice(len(per_env_buffer), self.per_env_batch_sizes[env_idx], replace=False)
KeyboardInterrupt
