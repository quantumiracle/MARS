Cannot import pettingzoo env:  combat_jet_v1
boxing_v2 pettingzoo
type:  pettingzoo
boxing_v2
Load boxing_v2 environment in type pettingzoo.
Env observation space: <bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f0f23322a90>> action space: <bound method aec_to_parallel_wrapper.action_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f0f23322a90>>
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f0f2331e860>
random seed: [667, 820, 989, 382, 99]
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f0f2331e860>
discrete_policy 18 Discrete(18)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=324, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 18 Discrete(18)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=324, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 18 Discrete(18)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'boxing_v2', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 5000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax', 'channel_list': [32, 16], 'kernel_size_list': [4, 4], 'stride_list': [1, 1]}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220615_2120/pettingzoo_boxing_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220615_2120/pettingzoo_boxing_v2_nash_dqn.
Episode: 1/50000 (0.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 29.5484s / 29.5484 s
first_0:                 episode reward: 1.0000,                 loss: 0.0193
second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 867.8769s / 897.4253 s
first_0:                 episode reward: 0.5500,                 loss: 0.0136
second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 879.1422s / 1776.5675 s
first_0:                 episode reward: 0.1500,                 loss: 0.0103
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 885.0021s / 2661.5695 s
first_0:                 episode reward: -0.4500,                 loss: 0.0093
second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 887.7426s / 3549.3121 s
first_0:                 episode reward: -0.5000,                 loss: 0.0098
second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 891.1742s / 4440.4863 s
first_0:                 episode reward: -0.3000,                 loss: 0.0101
second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 893.9731s / 5334.4594 s
first_0:                 episode reward: 0.0000,                 loss: 0.0103
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 903.9803s / 6238.4397 s
first_0:                 episode reward: 0.0000,                 loss: 0.0102
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 912.4667s / 7150.9064 s
first_0:                 episode reward: 0.3000,                 loss: 0.0107
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 916.4266s / 8067.3330 s
first_0:                 episode reward: -0.2000,                 loss: 0.0104
second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 918.1483s / 8985.4813 s
first_0:                 episode reward: -0.2500,                 loss: 0.0110
second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 919.8832s / 9905.3645 s
first_0:                 episode reward: 1.1000,                 loss: 0.0111
second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 919.6867s / 10825.0513 s
first_0:                 episode reward: -0.3000,                 loss: 0.0112
second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 921.5778s / 11746.6291 s
first_0:                 episode reward: 0.2000,                 loss: 0.0112
second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 927.5715s / 12674.2006 s
first_0:                 episode reward: -0.1500,                 loss: 0.0110
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 932.0365s / 13606.2371 s
first_0:                 episode reward: -0.4000,                 loss: 0.0109
second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 5705.4313s / 19311.6684 s
first_0:                 episode reward: -0.0500,                 loss: 0.0108
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 928.7326s / 20240.4009 s
first_0:                 episode reward: -0.7500,                 loss: 0.0111
second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 928.5723s / 21168.9732 s
first_0:                 episode reward: 0.4500,                 loss: 0.0109
second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 928.5633s / 22097.5365 s
first_0:                 episode reward: -0.3500,                 loss: 0.0115
second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 924.4101s / 23021.9467 s
first_0:                 episode reward: -0.2500,                 loss: 0.0115
second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 924.6501s / 23946.5968 s
first_0:                 episode reward: 0.0000,                 loss: 0.0115
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 923.6786s / 24870.2754 s
first_0:                 episode reward: 0.2000,                 loss: 0.0116
second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 924.6435s / 25794.9190 s
first_0:                 episode reward: 0.4000,                 loss: 0.0116
second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 925.6822s / 26720.6012 s
first_0:                 episode reward: 0.8000,                 loss: 0.0120
second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 921.8831s / 27642.4842 s
first_0:                 episode reward: -0.8500,                 loss: 0.0115
second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 936.9952s / 28579.4794 s
first_0:                 episode reward: 0.4000,                 loss: 0.0118
second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 922.9583s / 29502.4377 s
first_0:                 episode reward: -0.5000,                 loss: 0.0115
second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 921.8899s / 30424.3276 s
first_0:                 episode reward: -0.3500,                 loss: 0.0116
second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 920.0665s / 31344.3941 s
first_0:                 episode reward: -0.1500,                 loss: 0.0116
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 920.4539s / 32264.8479 s
first_0:                 episode reward: 0.4500,                 loss: 0.0114
second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 919.1727s / 33184.0207 s
first_0:                 episode reward: 0.0500,                 loss: 0.0116
second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 918.4668s / 34102.4875 s
first_0:                 episode reward: 0.4500,                 loss: 0.0117
second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 917.7372s / 35020.2247 s
first_0:                 episode reward: 0.1500,                 loss: 0.0116
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 915.5842s / 35935.8089 s
first_0:                 episode reward: 0.7500,                 loss: 0.0117
second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 917.7185s / 36853.5274 s
first_0:                 episode reward: -0.1500,                 loss: 0.0114
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 916.9592s / 37770.4866 s
first_0:                 episode reward: 0.7000,                 loss: 0.0112
second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 915.8756s / 38686.3622 s
first_0:                 episode reward: -1.0000,                 loss: 0.0111
second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 915.5639s / 39601.9261 s
first_0:                 episode reward: 0.1500,                 loss: 0.0116
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 918.8489s / 40520.7749 s
first_0:                 episode reward: -0.1500,                 loss: 0.0110
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 921.2358s / 41442.0107 s
first_0:                 episode reward: 0.5500,                 loss: 0.0117
second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 922.6982s / 42364.7089 s
first_0:                 episode reward: -0.2000,                 loss: 0.0115
second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 923.0947s / 43287.8037 s
first_0:                 episode reward: 0.0000,                 loss: 0.0113
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 919.7803s / 44207.5839 s
first_0:                 episode reward: 0.6000,                 loss: 0.0113
second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 920.4894s / 45128.0734 s
first_0:                 episode reward: 0.5000,                 loss: 0.0113
second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 919.4931s / 46047.5664 s
first_0:                 episode reward: -0.5000,                 loss: 0.0110
second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 918.1619s / 46965.7284 s
first_0:                 episode reward: 0.6000,                 loss: 0.0114
second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 919.3047s / 47885.0331 s
first_0:                 episode reward: 0.0500,                 loss: 0.0115
second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 918.4338s / 48803.4670 s
first_0:                 episode reward: -0.4500,                 loss: 0.0107
second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 919.1123s / 49722.5793 s
first_0:                 episode reward: -0.1500,                 loss: 0.0117
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 918.8805s / 50641.4598 s
first_0:                 episode reward: -0.5500,                 loss: 0.0114
second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 918.5719s / 51560.0317 s
first_0:                 episode reward: -0.5500,                 loss: 0.0112
second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 918.4553s / 52478.4869 s
first_0:                 episode reward: 0.3000,                 loss: 0.0114
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 920.8010s / 53399.2879 s
first_0:                 episode reward: 0.1500,                 loss: 0.0117
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 921.8213s / 54321.1093 s
first_0:                 episode reward: -1.2000,                 loss: 0.0117
second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 921.2724s / 55242.3817 s
first_0:                 episode reward: -0.7000,                 loss: 0.0117
second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 921.2066s / 56163.5883 s
first_0:                 episode reward: 0.3500,                 loss: 0.0117
second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 922.0183s / 57085.6065 s
first_0:                 episode reward: 0.6500,                 loss: 0.0117
second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 922.4994s / 58008.1060 s
first_0:                 episode reward: -0.7500,                 loss: 0.0119
second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 921.1186s / 58929.2246 s