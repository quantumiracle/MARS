pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 80, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0524/pettingzoo_boxing_v1_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0524/pettingzoo_boxing_v1_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 27.3274s / 27.3274 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0026
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 14.0000,                 loss: nan
env1_second_0:                 episode reward: -14.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1268.65,                last time consumption/overall running time: 490.1178s / 517.4452 s
env0_first_0:                 episode reward: 48.9000,                 loss: 0.0183
env0_second_0:                 episode reward: -48.9000,                 loss: nan
env1_first_0:                 episode reward: 50.8500,                 loss: nan
env1_second_0:                 episode reward: -50.8500,                 loss: nan
Score delta: 135.6, update the opponent.
Episode: 41/10000 (0.4100%),                 avg. length: 1475.2,                last time consumption/overall running time: 644.9676s / 1162.4128 s
env0_first_0:                 episode reward: -6.4500,                 loss: nan
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0640
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 799.1009s / 1961.5137 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0352
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0585
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Score delta: 92.2, update the opponent.
Episode: 81/10000 (0.8100%),                 avg. length: 1774.2,                last time consumption/overall running time: 799.4483s / 2760.9620 s
env0_first_0:                 episode reward: 9.4500,                 loss: 0.0319
env0_second_0:                 episode reward: -9.4500,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1685.65,                last time consumption/overall running time: 758.0689s / 3519.0309 s
env0_first_0:                 episode reward: 31.4000,                 loss: 0.0483
env0_second_0:                 episode reward: -31.4000,                 loss: 0.0536
env1_first_0:                 episode reward: 37.5000,                 loss: nan
env1_second_0:                 episode reward: -37.5000,                 loss: nan
Score delta: 87.4, update the opponent.
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 805.4008s / 4324.4317 s
env0_first_0:                 episode reward: 1.5000,                 loss: nan
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0382
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1729.65,                last time consumption/overall running time: 781.6864s / 5106.1181 s
env0_first_0:                 episode reward: -27.3500,                 loss: 0.0473
env0_second_0:                 episode reward: 27.3500,                 loss: 0.0339
env1_first_0:                 episode reward: -25.9000,                 loss: nan
env1_second_0:                 episode reward: 25.9000,                 loss: nan
Score delta: 88.8, update the opponent.
Episode: 161/10000 (1.6100%),                 avg. length: 1609.35,                last time consumption/overall running time: 728.4149s / 5834.5330 s
env0_first_0:                 episode reward: 30.9500,                 loss: 0.0454
env0_second_0:                 episode reward: -30.9500,                 loss: 0.0410
env1_first_0:                 episode reward: 37.4000,                 loss: nan
env1_second_0:                 episode reward: -37.4000,                 loss: nan
Score delta: 87.6, update the opponent.
Episode: 181/10000 (1.8100%),                 avg. length: 1748.75,                last time consumption/overall running time: 791.0133s / 6625.5462 s
env0_first_0:                 episode reward: 5.2000,                 loss: nan
env0_second_0:                 episode reward: -5.2000,                 loss: 0.0388
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1776.3,                last time consumption/overall running time: 801.7210s / 7427.2672 s
env0_first_0:                 episode reward: -25.6500,                 loss: 0.0450
env0_second_0:                 episode reward: 25.6500,                 loss: 0.0428
env1_first_0:                 episode reward: -18.8000,                 loss: nan
env1_second_0:                 episode reward: 18.8000,                 loss: nan
Score delta: 90.0, update the opponent.
Episode: 221/10000 (2.2100%),                 avg. length: 1561.7,                last time consumption/overall running time: 707.7807s / 8135.0479 s
env0_first_0:                 episode reward: 25.1000,                 loss: 0.0713
env0_second_0:                 episode reward: -25.1000,                 loss: 0.0320
env1_first_0:                 episode reward: 29.8500,                 loss: nan
env1_second_0:                 episode reward: -29.8500,                 loss: nan
Score delta: 96.6, update the opponent.
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 807.8947s / 8942.9426 s
env0_first_0:                 episode reward: 4.8000,                 loss: nan
env0_second_0:                 episode reward: -4.8000,                 loss: 0.0191
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1513.3,                last time consumption/overall running time: 686.2890s / 9629.2316 s
env0_first_0:                 episode reward: -34.5500,                 loss: 0.0418
env0_second_0:                 episode reward: 34.5500,                 loss: 0.0291
env1_first_0:                 episode reward: -28.0500,                 loss: nan
env1_second_0:                 episode reward: 28.0500,                 loss: nan
Score delta: 81.0, update the opponent.
Episode: 281/10000 (2.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 809.5844s / 10438.8160 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0481
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1607.5,                last time consumption/overall running time: 729.2672s / 11168.0832 s
env0_first_0:                 episode reward: 46.0500,                 loss: 0.0310
env0_second_0:                 episode reward: -46.0500,                 loss: 0.0387
env1_first_0:                 episode reward: 46.1000,                 loss: nan
env1_second_0:                 episode reward: -46.1000,                 loss: nan
Score delta: 84.8, update the opponent.
Episode: 321/10000 (3.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 807.7905s / 11975.8737 s
env0_first_0:                 episode reward: -4.8000,                 loss: nan
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0490
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 807.2567s / 12783.1304 s
env0_first_0:                 episode reward: -16.8500,                 loss: nan
env0_second_0:                 episode reward: 16.8500,                 loss: 0.0291
env1_first_0:                 episode reward: -16.3500,                 loss: nan
env1_second_0:                 episode reward: 16.3500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1258.3,                last time consumption/overall running time: 570.6613s / 13353.7917 s
env0_first_0:                 episode reward: -46.2500,                 loss: 0.0363
env0_second_0:                 episode reward: 46.2500,                 loss: 0.0262
env1_first_0:                 episode reward: -52.3000,                 loss: nan
env1_second_0:                 episode reward: 52.3000,                 loss: nan
Score delta: 91.0, update the opponent.
Episode: 381/10000 (3.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 807.0292s / 14160.8209 s
env0_first_0:                 episode reward: -8.6000,                 loss: 0.0326
env0_second_0:                 episode reward: 8.6000,                 loss: nan
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1284.9,                last time consumption/overall running time: 583.1378s / 14743.9587 s
env0_first_0:                 episode reward: 32.2000,                 loss: 0.0427
env0_second_0:                 episode reward: -32.2000,                 loss: 0.0437
env1_first_0:                 episode reward: 52.4500,                 loss: nan
env1_second_0:                 episode reward: -52.4500,                 loss: nan
Score delta: 83.6, update the opponent.
Episode: 421/10000 (4.2100%),                 avg. length: 1300.8,                last time consumption/overall running time: 590.3448s / 15334.3035 s
env0_first_0:                 episode reward: 43.3000,                 loss: nan
env0_second_0:                 episode reward: -43.3000,                 loss: 0.0321
env1_first_0:                 episode reward: 46.3000,                 loss: nan
env1_second_0:                 episode reward: -46.3000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1495.3,                last time consumption/overall running time: 679.8392s / 16014.1427 s
env0_first_0:                 episode reward: -3.1500,                 loss: nan
env0_second_0:                 episode reward: 3.1500,                 loss: 0.0653
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1511.0,                last time consumption/overall running time: 685.9203s / 16700.0630 s
env0_first_0:                 episode reward: -25.0500,                 loss: 0.0467
env0_second_0:                 episode reward: 25.0500,                 loss: 0.0757
env1_first_0:                 episode reward: -16.7000,                 loss: nan
env1_second_0:                 episode reward: 16.7000,                 loss: nan
Score delta: 90.2, update the opponent.
Episode: 481/10000 (4.8100%),                 avg. length: 1517.85,                last time consumption/overall running time: 687.8559s / 17387.9189 s
env0_first_0:                 episode reward: 31.0000,                 loss: 0.0683
env0_second_0:                 episode reward: -31.0000,                 loss: nan
env1_first_0:                 episode reward: 27.4000,                 loss: nan
env1_second_0:                 episode reward: -27.4000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1395.0,                last time consumption/overall running time: 633.5444s / 18021.4633 s
env0_first_0:                 episode reward: 33.9500,                 loss: 0.0814
env0_second_0:                 episode reward: -33.9500,                 loss: 0.0593
env1_first_0:                 episode reward: 51.6000,                 loss: nan
env1_second_0:                 episode reward: -51.6000,                 loss: nan
Score delta: 86.8, update the opponent.
Episode: 521/10000 (5.2100%),                 avg. length: 1475.1,                last time consumption/overall running time: 669.8798s / 18691.3431 s
env0_first_0:                 episode reward: -13.3500,                 loss: nan
env0_second_0:                 episode reward: 13.3500,                 loss: 0.0931
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1678.75,                last time consumption/overall running time: 763.1706s / 19454.5138 s
env0_first_0:                 episode reward: -14.2500,                 loss: 0.0554
env0_second_0:                 episode reward: 14.2500,                 loss: 0.0876
env1_first_0:                 episode reward: -15.5500,                 loss: nan
env1_second_0:                 episode reward: 15.5500,                 loss: nan
Score delta: 83.6, update the opponent.
Episode: 561/10000 (5.6100%),                 avg. length: 1366.4,                last time consumption/overall running time: 619.3741s / 20073.8879 s
env0_first_0:                 episode reward: 33.1000,                 loss: 0.0711
env0_second_0:                 episode reward: -33.1000,                 loss: 0.2187
env1_first_0:                 episode reward: 29.1000,                 loss: nan
env1_second_0:                 episode reward: -29.1000,                 loss: nan
Score delta: 90.8, update the opponent.
Episode: 581/10000 (5.8100%),                 avg. length: 1119.95,                last time consumption/overall running time: 508.7458s / 20582.6337 s
env0_first_0:                 episode reward: 45.0500,                 loss: nan
env0_second_0:                 episode reward: -45.0500,                 loss: 0.0823
env1_first_0:                 episode reward: 34.7000,                 loss: nan
env1_second_0:                 episode reward: -34.7000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1298.2,                last time consumption/overall running time: 590.4839s / 21173.1175 s
env0_first_0:                 episode reward: 25.5500,                 loss: nan
env0_second_0:                 episode reward: -25.5500,                 loss: 0.0513
env1_first_0:                 episode reward: 29.7500,                 loss: nan
env1_second_0:                 episode reward: -29.7500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1573.3,                last time consumption/overall running time: 713.0296s / 21886.1472 s
env0_first_0:                 episode reward: -5.0000,                 loss: nan
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0530
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1716.4,                last time consumption/overall running time: 777.1182s / 22663.2654 s
env0_first_0:                 episode reward: -7.9500,                 loss: nan
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0419
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 810.7882s / 23474.0535 s
env0_first_0:                 episode reward: -31.0500,                 loss: 0.0511
env0_second_0:                 episode reward: 31.0500,                 loss: 0.0336
env1_first_0:                 episode reward: -31.5500,                 loss: nan
env1_second_0:                 episode reward: 31.5500,                 loss: nan
Score delta: 81.4, update the opponent.
Episode: 681/10000 (6.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 808.4610s / 24282.5145 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0908
env0_second_0:                 episode reward: 1.6000,                 loss: nan
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1676.9,                last time consumption/overall running time: 759.5464s / 25042.0609 s
env0_first_0:                 episode reward: 29.6500,                 loss: 0.0477
env0_second_0:                 episode reward: -29.6500,                 loss: nan
env1_first_0:                 episode reward: 30.3500,                 loss: nan
env1_second_0:                 episode reward: -30.3500,                 loss: nan
Score delta: 85.4, update the opponent.
Episode: 721/10000 (7.2100%),                 avg. length: 564.3,                last time consumption/overall running time: 255.4268s / 25297.4877 s
env0_first_0:                 episode reward: 80.8000,                 loss: nan
env0_second_0:                 episode reward: -80.8000,                 loss: 0.0410
env1_first_0:                 episode reward: 84.5000,                 loss: nan
env1_second_0:                 episode reward: -84.5000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1131.2,                last time consumption/overall running time: 512.4499s / 25809.9376 s
env0_first_0:                 episode reward: 47.9500,                 loss: nan
env0_second_0:                 episode reward: -47.9500,                 loss: 0.0561
env1_first_0:                 episode reward: 59.5000,                 loss: nan
env1_second_0:                 episode reward: -59.5000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1709.3,                last time consumption/overall running time: 774.2759s / 26584.2135 s
env0_first_0:                 episode reward: -4.5000,                 loss: nan
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0962
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1599.35,                last time consumption/overall running time: 726.5186s / 27310.7321 s
env0_first_0:                 episode reward: -20.4500,                 loss: 0.0312
env0_second_0:                 episode reward: 20.4500,                 loss: 0.0449
env1_first_0:                 episode reward: -21.5500,                 loss: nan
env1_second_0:                 episode reward: 21.5500,                 loss: nan
Score delta: 90.4, update the opponent.
Episode: 801/10000 (8.0100%),                 avg. length: 1781.1,                last time consumption/overall running time: 806.7068s / 28117.4389 s
env0_first_0:                 episode reward: -22.4500,                 loss: 0.0232
env0_second_0:                 episode reward: 22.4500,                 loss: nan
env1_first_0:                 episode reward: -19.0500,                 loss: nan
env1_second_0:                 episode reward: 19.0500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 811.4801s / 28928.9190 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0136
env0_second_0:                 episode reward: -1.8500,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 809.2307s / 29738.1497 s
env0_first_0:                 episode reward: 12.2500,                 loss: 0.0100
env0_second_0:                 episode reward: -12.2500,                 loss: nan
env1_first_0:                 episode reward: 17.3000,                 loss: nan
env1_second_0:                 episode reward: -17.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1741.35,                last time consumption/overall running time: 789.6556s / 30527.8053 s
env0_first_0:                 episode reward: 45.2000,                 loss: 0.0248
env0_second_0:                 episode reward: -45.2000,                 loss: 0.0465
env1_first_0:                 episode reward: 43.0500,                 loss: nan
env1_second_0:                 episode reward: -43.0500,                 loss: nan
Score delta: 80.8, update the opponent.
Episode: 881/10000 (8.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 808.3730s / 31336.1783 s
env0_first_0:                 episode reward: -2.6500,                 loss: nan
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0605
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1711.35,                last time consumption/overall running time: 775.9385s / 32112.1168 s
env0_first_0:                 episode reward: -25.1000,                 loss: 0.0204
env0_second_0:                 episode reward: 25.1000,                 loss: 0.0650
env1_first_0:                 episode reward: -21.6000,                 loss: nan
env1_second_0:                 episode reward: 21.6000,                 loss: nan
Score delta: 83.2, update the opponent.
Episode: 921/10000 (9.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 809.2094s / 32921.3262 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0208
env0_second_0:                 episode reward: 3.4500,                 loss: nan
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1661.0,                last time consumption/overall running time: 753.5313s / 33674.8575 s
env0_first_0:                 episode reward: -30.6500,                 loss: 0.0313
env0_second_0:                 episode reward: 30.6500,                 loss: nan
env1_first_0:                 episode reward: -23.8000,                 loss: nan
env1_second_0:                 episode reward: 23.8000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 809.2141s / 34484.0716 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0562
env0_second_0:                 episode reward: -1.8000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 809.0000s / 35293.0716 s
env0_first_0:                 episode reward: 12.8000,                 loss: 0.0105
env0_second_0:                 episode reward: -12.8000,                 loss: nan
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1537.95,                last time consumption/overall running time: 697.1170s / 35990.1886 s
env0_first_0:                 episode reward: 32.9500,                 loss: 0.0182
env0_second_0:                 episode reward: -32.9500,                 loss: 0.0796
env1_first_0:                 episode reward: 36.2500,                 loss: nan
env1_second_0:                 episode reward: -36.2500,                 loss: nan
Score delta: 86.4, update the opponent.
Episode: 1021/10000 (10.2100%),                 avg. length: 923.45,                last time consumption/overall running time: 418.9486s / 36409.1372 s
env0_first_0:                 episode reward: -51.9000,                 loss: 0.0269
env0_second_0:                 episode reward: 51.9000,                 loss: 0.0843
env1_first_0:                 episode reward: -63.1500,                 loss: nan
env1_second_0:                 episode reward: 63.1500,                 loss: nan
Score delta: 88.2, update the opponent.
Episode: 1041/10000 (10.4100%),                 avg. length: 1624.25,                last time consumption/overall running time: 737.7663s / 37146.9035 s
env0_first_0:                 episode reward: -40.2000,                 loss: 0.0568
env0_second_0:                 episode reward: 40.2000,                 loss: nan
env1_first_0:                 episode reward: -37.3500,                 loss: nan
env1_second_0:                 episode reward: 37.3500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 925.7881s / 38072.6916 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0471
env0_second_0:                 episode reward: 5.3500,                 loss: nan
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1732.95,                last time consumption/overall running time: 934.7454s / 39007.4370 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0187
env0_second_0:                 episode reward: 6.1000,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1776.55,                last time consumption/overall running time: 953.5619s / 39960.9990 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0155
env0_second_0:                 episode reward: -1.9500,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1731.3,                last time consumption/overall running time: 929.8632s / 40890.8622 s
env0_first_0:                 episode reward: 17.4000,                 loss: 0.0255
env0_second_0:                 episode reward: -17.4000,                 loss: nan
env1_first_0:                 episode reward: 19.3000,                 loss: nan
env1_second_0:                 episode reward: -19.3000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1278.0,                last time consumption/overall running time: 687.0603s / 41577.9224 s
env0_first_0:                 episode reward: 37.2500,                 loss: 0.0442
env0_second_0:                 episode reward: -37.2500,                 loss: nan
env1_first_0:                 episode reward: 28.9500,                 loss: nan
env1_second_0:                 episode reward: -28.9500,                 loss: nan
Score delta: 85.2, update the opponent.
Episode: 1161/10000 (11.6100%),                 avg. length: 1550.05,                last time consumption/overall running time: 832.5060s / 42410.4284 s
env0_first_0:                 episode reward: 23.0000,                 loss: nan
env0_second_0:                 episode reward: -23.0000,                 loss: 0.0790
env1_first_0:                 episode reward: 29.8500,                 loss: nan
env1_second_0:                 episode reward: -29.8500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1590.2,                last time consumption/overall running time: 853.4844s / 43263.9128 s
env0_first_0:                 episode reward: -15.9500,                 loss: nan
env0_second_0:                 episode reward: 15.9500,                 loss: 0.0679
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 958.8817s / 44222.7945 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0561
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0548
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Score delta: 87.8, update the opponent.
Episode: 1221/10000 (12.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 958.6515s / 45181.4460 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0141
env0_second_0:                 episode reward: -1.4500,                 loss: nan
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1648.75,                last time consumption/overall running time: 884.1967s / 46065.6427 s
env0_first_0:                 episode reward: 20.5000,                 loss: 0.0135
env0_second_0:                 episode reward: -20.5000,                 loss: nan
env1_first_0:                 episode reward: 25.5000,                 loss: nan
env1_second_0:                 episode reward: -25.5000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1673.3,                last time consumption/overall running time: 895.5724s / 46961.2150 s
env0_first_0:                 episode reward: 26.3500,                 loss: 0.0284
env0_second_0:                 episode reward: -26.3500,                 loss: 0.0458
env1_first_0:                 episode reward: 23.3500,                 loss: nan
env1_second_0:                 episode reward: -23.3500,                 loss: nan
Score delta: 84.2, update the opponent.
Episode: 1281/10000 (12.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.0823s / 47915.2974 s
env0_first_0:                 episode reward: -7.3500,                 loss: nan
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0572
env1_first_0:                 episode reward: -13.7500,                 loss: nan
env1_second_0:                 episode reward: 13.7500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1761.55,                last time consumption/overall running time: 942.8215s / 48858.1188 s
env0_first_0:                 episode reward: -29.9500,                 loss: 0.0434
env0_second_0:                 episode reward: 29.9500,                 loss: 0.0506
env1_first_0:                 episode reward: -27.0000,                 loss: nan
env1_second_0:                 episode reward: 27.0000,                 loss: nan
Score delta: 83.6, update the opponent.
Episode: 1321/10000 (13.2100%),                 avg. length: 1750.05,                last time consumption/overall running time: 935.8777s / 49793.9966 s
env0_first_0:                 episode reward: -17.1000,                 loss: 0.0424
env0_second_0:                 episode reward: 17.1000,                 loss: nan
env1_first_0:                 episode reward: -24.2500,                 loss: nan
env1_second_0:                 episode reward: 24.2500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.6948s / 50748.6913 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0250
env0_second_0:                 episode reward: -1.6000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.8386s / 51703.5299 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0113
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1775.45,                last time consumption/overall running time: 949.7640s / 52653.2939 s
env0_first_0:                 episode reward: 30.9000,                 loss: 0.0133
env0_second_0:                 episode reward: -30.9000,                 loss: 0.0606
env1_first_0:                 episode reward: 33.1000,                 loss: nan
env1_second_0:                 episode reward: -33.1000,                 loss: nan
Score delta: 84.4, update the opponent.
Episode: 1401/10000 (14.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.9897s / 53607.2836 s
env0_first_0:                 episode reward: -10.2000,                 loss: nan
env0_second_0:                 episode reward: 10.2000,                 loss: 0.0613
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1553.35,                last time consumption/overall running time: 831.1338s / 54438.4174 s
env0_first_0:                 episode reward: -40.0500,                 loss: 0.0189
env0_second_0:                 episode reward: 40.0500,                 loss: 0.0748
env1_first_0:                 episode reward: -51.9000,                 loss: nan
env1_second_0:                 episode reward: 51.9000,                 loss: nan
Score delta: 82.0, update the opponent.
Episode: 1441/10000 (14.4100%),                 avg. length: 1738.65,                last time consumption/overall running time: 930.3518s / 55368.7692 s
env0_first_0:                 episode reward: -18.7000,                 loss: 0.0349
env0_second_0:                 episode reward: 18.7000,                 loss: nan
env1_first_0:                 episode reward: -16.3500,                 loss: nan
env1_second_0:                 episode reward: 16.3500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1674.6,                last time consumption/overall running time: 894.8826s / 56263.6518 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0336
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1537.05,                last time consumption/overall running time: 821.7794s / 57085.4313 s
env0_first_0:                 episode reward: 48.1000,                 loss: 0.0503
env0_second_0:                 episode reward: -48.1000,                 loss: 0.0501
env1_first_0:                 episode reward: 44.3000,                 loss: nan
env1_second_0:                 episode reward: -44.3000,                 loss: nan
Score delta: 87.2, update the opponent.
Episode: 1501/10000 (15.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.2094s / 58039.6407 s
env0_first_0:                 episode reward: 15.8000,                 loss: nan
env0_second_0:                 episode reward: -15.8000,                 loss: 0.0632
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1678.6,                last time consumption/overall running time: 896.1455s / 58935.7861 s
env0_first_0:                 episode reward: -12.2500,                 loss: nan
env0_second_0:                 episode reward: 12.2500,                 loss: 0.0775
env1_first_0:                 episode reward: -17.7000,                 loss: nan
env1_second_0:                 episode reward: 17.7000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1770.5,                last time consumption/overall running time: 946.1046s / 59881.8907 s
env0_first_0:                 episode reward: -9.5500,                 loss: nan
env0_second_0:                 episode reward: 9.5500,                 loss: 0.0582
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1690.05,                last time consumption/overall running time: 903.4255s / 60785.3163 s
env0_first_0:                 episode reward: -35.5500,                 loss: 0.0564
env0_second_0:                 episode reward: 35.5500,                 loss: 0.0304
env1_first_0:                 episode reward: -32.7500,                 loss: nan
env1_second_0:                 episode reward: 32.7500,                 loss: nan
Score delta: 87.0, update the opponent.
Episode: 1581/10000 (15.8100%),                 avg. length: 1368.05,                last time consumption/overall running time: 731.8356s / 61517.1518 s
env0_first_0:                 episode reward: 36.3000,                 loss: 0.0739
env0_second_0:                 episode reward: -36.3000,                 loss: 0.0389
env1_first_0:                 episode reward: 39.2000,                 loss: nan
env1_second_0:                 episode reward: -39.2000,                 loss: nan
Score delta: 86.8, update the opponent.
Episode: 1601/10000 (16.0100%),                 avg. length: 1776.95,                last time consumption/overall running time: 949.6438s / 62466.7956 s
env0_first_0:                 episode reward: -5.9000,                 loss: nan
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0611
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1749.4,                last time consumption/overall running time: 935.1727s / 63401.9683 s
env0_first_0:                 episode reward: -22.0500,                 loss: 0.0602
env0_second_0:                 episode reward: 22.0500,                 loss: 0.0791
env1_first_0:                 episode reward: -21.7500,                 loss: nan
env1_second_0:                 episode reward: 21.7500,                 loss: nan
Score delta: 85.0, update the opponent.
Episode: 1641/10000 (16.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.9612s / 64355.9296 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0361
env0_second_0:                 episode reward: -1.8000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.1415s / 65309.0711 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0131
env0_second_0:                 episode reward: -1.8500,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1645.4,                last time consumption/overall running time: 879.2138s / 66188.2849 s
env0_first_0:                 episode reward: 34.4500,                 loss: 0.0213
env0_second_0:                 episode reward: -34.4500,                 loss: 0.0606
env1_first_0:                 episode reward: 32.5000,                 loss: nan
env1_second_0:                 episode reward: -32.5000,                 loss: nan
Score delta: 88.0, update the opponent.
Episode: 1701/10000 (17.0100%),                 avg. length: 1706.35,                last time consumption/overall running time: 911.5440s / 67099.8289 s
env0_first_0:                 episode reward: 28.2500,                 loss: nan
env0_second_0:                 episode reward: -28.2500,                 loss: 0.0654
env1_first_0:                 episode reward: 37.2500,                 loss: nan
env1_second_0:                 episode reward: -37.2500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.9426s / 68053.7715 s
env0_first_0:                 episode reward: -8.4500,                 loss: nan
env0_second_0:                 episode reward: 8.4500,                 loss: 0.0443
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1772.75,                last time consumption/overall running time: 948.5444s / 69002.3159 s
env0_first_0:                 episode reward: -28.4500,                 loss: nan
env0_second_0:                 episode reward: 28.4500,                 loss: 0.0473
env1_first_0:                 episode reward: -32.1000,                 loss: nan
env1_second_0:                 episode reward: 32.1000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.6558s / 69955.9717 s
env0_first_0:                 episode reward: -15.6000,                 loss: 0.0347
env0_second_0:                 episode reward: 15.6000,                 loss: 0.0583
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Score delta: 83.6, update the opponent.
Episode: 1781/10000 (17.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.8163s / 70908.7880 s
env0_first_0:                 episode reward: -36.2500,                 loss: 0.0258
env0_second_0:                 episode reward: 36.2500,                 loss: nan
env1_first_0:                 episode reward: -30.3500,                 loss: nan
env1_second_0:                 episode reward: 30.3500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.8949s / 71862.6829 s
env0_first_0:                 episode reward: -15.1000,                 loss: 0.0405
env0_second_0:                 episode reward: 15.1000,                 loss: nan
env1_first_0:                 episode reward: -13.4000,                 loss: nan
env1_second_0:                 episode reward: 13.4000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.3277s / 72816.0106 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0266
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.5630s / 73769.5736 s
env0_first_0:                 episode reward: 2.0000,                 loss: 0.0133
env0_second_0:                 episode reward: -2.0000,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1736.05,                last time consumption/overall running time: 928.1722s / 74697.7458 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0149
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 18.2500,                 loss: nan
env1_second_0:                 episode reward: -18.2500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 1549.05,                last time consumption/overall running time: 828.2355s / 75525.9813 s
env0_first_0:                 episode reward: 31.4500,                 loss: 0.0279
env0_second_0:                 episode reward: -31.4500,                 loss: 0.0375
env1_first_0:                 episode reward: 17.5000,                 loss: nan
env1_second_0:                 episode reward: -17.5000,                 loss: nan
Score delta: 86.0, update the opponent.
Episode: 1901/10000 (19.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.5389s / 76479.5203 s
env0_first_0:                 episode reward: -18.9000,                 loss: nan
env0_second_0:                 episode reward: 18.9000,                 loss: 0.0590
env1_first_0:                 episode reward: -19.4500,                 loss: nan
env1_second_0:                 episode reward: 19.4500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.5230s / 77434.0432 s
env0_first_0:                 episode reward: -1.7500,                 loss: nan
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0404
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.7529s / 78387.7961 s
env0_first_0:                 episode reward: -11.5000,                 loss: nan
env0_second_0:                 episode reward: 11.5000,                 loss: 0.0236
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.0754s / 79340.8715 s
env0_first_0:                 episode reward: -26.4000,                 loss: 0.0304
env0_second_0:                 episode reward: 26.4000,                 loss: 0.0430
env1_first_0:                 episode reward: -26.4500,                 loss: nan
env1_second_0:                 episode reward: 26.4500,                 loss: nan
Score delta: 82.2, update the opponent.
Episode: 1981/10000 (19.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.0166s / 80294.8881 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0262
env0_second_0:                 episode reward: 3.8000,                 loss: nan
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.1120s / 81249.0001 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0084
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.7838s / 82202.7839 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0125
env0_second_0:                 episode reward: 6.0000,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.5448s / 83157.3288 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0116
env0_second_0:                 episode reward: 2.1500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.4044s / 84111.7331 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0139
env0_second_0:                 episode reward: 5.6000,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.6880s / 85065.4211 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0247
env0_second_0:                 episode reward: 2.2500,                 loss: nan
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.7332s / 86019.1543 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0165
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.3263s / 86973.4806 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0087
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.9090s / 87927.3896 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.0053
env0_second_0:                 episode reward: -8.3500,                 loss: nan
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.9118s / 88881.3015 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0150
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1755.55,                last time consumption/overall running time: 938.6990s / 89820.0004 s
env0_first_0:                 episode reward: 22.4000,                 loss: 0.0263
env0_second_0:                 episode reward: -22.4000,                 loss: 0.0476
env1_first_0:                 episode reward: 15.9500,                 loss: nan
env1_second_0:                 episode reward: -15.9500,                 loss: nan
Score delta: 88.2, update the opponent.
Episode: 2201/10000 (22.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.4445s / 90773.4449 s
env0_first_0:                 episode reward: -1.5500,                 loss: nan
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0352
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.6690s / 91726.1139 s
env0_first_0:                 episode reward: -1.9500,                 loss: nan
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0273
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.8345s / 92679.9485 s
env0_first_0:                 episode reward: -0.8500,                 loss: nan
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0208
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.8999s / 93633.8483 s
env0_first_0:                 episode reward: -18.2500,                 loss: nan
env0_second_0:                 episode reward: 18.2500,                 loss: 0.0318
env1_first_0:                 episode reward: -16.3000,                 loss: nan
env1_second_0:                 episode reward: 16.3000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.3703s / 94588.2187 s
env0_first_0:                 episode reward: -20.0000,                 loss: nan
env0_second_0:                 episode reward: 20.0000,                 loss: 0.0327
env1_first_0:                 episode reward: -19.4000,                 loss: nan
env1_second_0:                 episode reward: 19.4000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 954.9088s / 95543.1275 s
env0_first_0:                 episode reward: -27.3500,                 loss: 0.0470
env0_second_0:                 episode reward: 27.3500,                 loss: 0.0297
env1_first_0:                 episode reward: -28.6500,                 loss: nan
env1_second_0:                 episode reward: 28.6500,                 loss: nan
Score delta: 81.0, update the opponent.
Episode: 2321/10000 (23.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 953.3286s / 96496.4560 s
env0_first_0:                 episode reward: -11.3500,                 loss: 0.0301
env0_second_0:                 episode reward: 11.3500,                 loss: nan
env1_first_0:                 episode reward: -10.0500,                 loss: nan
env1_second_0:                 episode reward: 10.0500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.8595s / 97449.3155 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0213
env0_second_0:                 episode reward: 2.9000,                 loss: nan
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0733s / 98401.3888 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0169
env0_second_0:                 episode reward: -3.5500,                 loss: nan
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1749.55,                last time consumption/overall running time: 933.3241s / 99334.7129 s
env0_first_0:                 episode reward: 38.0000,                 loss: 0.0236
env0_second_0:                 episode reward: -38.0000,                 loss: 0.0325
env1_first_0:                 episode reward: 35.4500,                 loss: nan
env1_second_0:                 episode reward: -35.4500,                 loss: nan
Score delta: 82.6, update the opponent.
Episode: 2401/10000 (24.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0797s / 100286.7926 s
env0_first_0:                 episode reward: 10.7000,                 loss: nan
env0_second_0:                 episode reward: -10.7000,                 loss: 0.0340
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.4492s / 101238.2418 s
env0_first_0:                 episode reward: -2.7000,                 loss: nan
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0154
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.1089s / 102190.3507 s
env0_first_0:                 episode reward: 3.0500,                 loss: nan
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0180
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.3284s / 103142.6791 s
env0_first_0:                 episode reward: -9.2000,                 loss: nan
env0_second_0:                 episode reward: 9.2000,                 loss: 0.0272
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1762.25,                last time consumption/overall running time: 940.2537s / 104082.9328 s
env0_first_0:                 episode reward: -25.6000,                 loss: nan
env0_second_0:                 episode reward: 25.6000,                 loss: 0.0450
env1_first_0:                 episode reward: -32.9000,                 loss: nan
env1_second_0:                 episode reward: 32.9000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 1711.65,                last time consumption/overall running time: 913.0173s / 104995.9502 s
env0_first_0:                 episode reward: -31.8500,                 loss: 0.0395
env0_second_0:                 episode reward: 31.8500,                 loss: 0.0581
env1_first_0:                 episode reward: -44.3000,                 loss: nan
env1_second_0:                 episode reward: 44.3000,                 loss: nan
Score delta: 82.0, update the opponent.
Episode: 2521/10000 (25.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0650s / 105948.0152 s
env0_first_0:                 episode reward: -18.3500,                 loss: 0.0331
env0_second_0:                 episode reward: 18.3500,                 loss: nan
env1_first_0:                 episode reward: -16.8500,                 loss: nan
env1_second_0:                 episode reward: 16.8500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.6042s / 106899.6194 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0192
env0_second_0:                 episode reward: 0.5000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.1296s / 107851.7490 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0136
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.6326s / 108803.3816 s
env0_first_0:                 episode reward: 15.1000,                 loss: 0.0187
env0_second_0:                 episode reward: -15.1000,                 loss: nan
env1_first_0:                 episode reward: 13.2500,                 loss: nan
env1_second_0:                 episode reward: -13.2500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 1763.3,                last time consumption/overall running time: 940.4910s / 109743.8726 s
env0_first_0:                 episode reward: 31.3500,                 loss: 0.0262
env0_second_0:                 episode reward: -31.3500,                 loss: nan
env1_first_0:                 episode reward: 39.2000,                 loss: nan
env1_second_0:                 episode reward: -39.2000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 1747.8,                last time consumption/overall running time: 932.4729s / 110676.3455 s
env0_first_0:                 episode reward: 27.5500,                 loss: 0.0314
env0_second_0:                 episode reward: -27.5500,                 loss: 0.0704
env1_first_0:                 episode reward: 30.8000,                 loss: nan
env1_second_0:                 episode reward: -30.8000,                 loss: nan
Score delta: 81.2, update the opponent.
Episode: 2641/10000 (26.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.7809s / 111629.1264 s
env0_first_0:                 episode reward: 8.1500,                 loss: nan
env0_second_0:                 episode reward: -8.1500,                 loss: 0.0586
env1_first_0:                 episode reward: 19.3000,                 loss: nan
env1_second_0:                 episode reward: -19.3000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.2939s / 112581.4203 s
env0_first_0:                 episode reward: 8.0000,                 loss: nan
env0_second_0:                 episode reward: -8.0000,                 loss: 0.0371
env1_first_0:                 episode reward: 17.1500,                 loss: nan
env1_second_0:                 episode reward: -17.1500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.8001s / 113533.2205 s
env0_first_0:                 episode reward: 1.7500,                 loss: nan
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0416
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0915s / 114485.3120 s
env0_first_0:                 episode reward: -0.8000,                 loss: nan
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0426
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.9726s / 115437.2846 s
env0_first_0:                 episode reward: -2.7000,                 loss: nan
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0433
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.7336s / 116389.0182 s
env0_first_0:                 episode reward: -11.5000,                 loss: nan
env0_second_0:                 episode reward: 11.5000,                 loss: 0.0450
env1_first_0:                 episode reward: -15.3000,                 loss: nan
env1_second_0:                 episode reward: 15.3000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.5053s / 117340.5234 s
env0_first_0:                 episode reward: -7.8000,                 loss: nan
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0323
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.2261s / 118292.7495 s
env0_first_0:                 episode reward: -6.8000,                 loss: nan
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0306
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.0444s / 119243.7939 s
env0_first_0:                 episode reward: -10.9500,                 loss: nan
env0_second_0:                 episode reward: 10.9500,                 loss: 0.0286
env1_first_0:                 episode reward: -10.9000,                 loss: nan
env1_second_0:                 episode reward: 10.9000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0887s / 120195.8827 s
env0_first_0:                 episode reward: -11.5000,                 loss: nan
env0_second_0:                 episode reward: 11.5000,                 loss: 0.0277
env1_first_0:                 episode reward: -17.0000,                 loss: nan
env1_second_0:                 episode reward: 17.0000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.4892s / 121148.3719 s
env0_first_0:                 episode reward: -20.6500,                 loss: nan
env0_second_0:                 episode reward: 20.6500,                 loss: 0.0325
env1_first_0:                 episode reward: -18.1000,                 loss: nan
env1_second_0:                 episode reward: 18.1000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.3255s / 122100.6974 s
env0_first_0:                 episode reward: -21.5500,                 loss: nan
env0_second_0:                 episode reward: 21.5500,                 loss: 0.0320
env1_first_0:                 episode reward: -17.4000,                 loss: nan
env1_second_0:                 episode reward: 17.4000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.5977s / 123053.2951 s
env0_first_0:                 episode reward: -18.7500,                 loss: nan
env0_second_0:                 episode reward: 18.7500,                 loss: 0.0318
env1_first_0:                 episode reward: -29.2000,                 loss: nan
env1_second_0:                 episode reward: 29.2000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.8521s / 124006.1472 s
env0_first_0:                 episode reward: -27.1000,                 loss: 0.0435
env0_second_0:                 episode reward: 27.1000,                 loss: 0.0314
env1_first_0:                 episode reward: -23.0500,                 loss: nan
env1_second_0:                 episode reward: 23.0500,                 loss: nan
Score delta: 84.0, update the opponent.
Episode: 2921/10000 (29.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.7791s / 124957.9263 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0317
env0_second_0:                 episode reward: 5.2000,                 loss: nan
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.5823s / 125909.5087 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0194
env0_second_0:                 episode reward: 2.8000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.9586s / 126861.4672 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0178
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.3266s / 127813.7938 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0155
env0_second_0:                 episode reward: 4.8000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0536s / 128765.8474 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0155
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.1396s / 129717.9870 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0151
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0384s / 130670.0254 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0170
env0_second_0:                 episode reward: -3.5500,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0959s / 131622.1213 s
env0_first_0:                 episode reward: 13.0500,                 loss: 0.0165
env0_second_0:                 episode reward: -13.0500,                 loss: nan
env1_first_0:                 episode reward: 16.6500,                 loss: nan
env1_second_0:                 episode reward: -16.6500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.1422s / 132574.2635 s
env0_first_0:                 episode reward: 20.8500,                 loss: 0.0217
env0_second_0:                 episode reward: -20.8500,                 loss: nan
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.6914s / 133525.9549 s
env0_first_0:                 episode reward: 14.9500,                 loss: 0.0236
env0_second_0:                 episode reward: -14.9500,                 loss: nan
env1_first_0:                 episode reward: 20.9500,                 loss: nan
env1_second_0:                 episode reward: -20.9500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.9593s / 134477.9142 s
env0_first_0:                 episode reward: 17.8500,                 loss: 0.0320
env0_second_0:                 episode reward: -17.8500,                 loss: nan
env1_first_0:                 episode reward: 36.6000,                 loss: nan
env1_second_0:                 episode reward: -36.6000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.5246s / 135429.4387 s
env0_first_0:                 episode reward: 31.5000,                 loss: 0.0327
env0_second_0:                 episode reward: -31.5000,                 loss: nan
env1_first_0:                 episode reward: 26.0000,                 loss: nan
env1_second_0:                 episode reward: -26.0000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.2212s / 136381.6600 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0304
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0414
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Score delta: 81.2, update the opponent.
Episode: 3181/10000 (31.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.4808s / 137334.1408 s
env0_first_0:                 episode reward: 4.4500,                 loss: nan
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0143
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.3815s / 138286.5223 s
env0_first_0:                 episode reward: 0.2000,                 loss: nan
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0104
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.6779s / 139239.2002 s
env0_first_0:                 episode reward: -1.6000,                 loss: nan
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0104
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.0824s / 140191.2826 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0092
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.9311s / 141143.2137 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0088
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.7683s / 142094.9820 s
env0_first_0:                 episode reward: -2.9000,                 loss: nan
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0106
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.7480s / 143046.7300 s
env0_first_0:                 episode reward: -3.2500,                 loss: nan
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0140
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 952.5574s / 143999.2873 s
env0_first_0:                 episode reward: -5.3000,                 loss: nan
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0151
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.7079s / 144949.9952 s
env0_first_0:                 episode reward: -4.5000,                 loss: nan
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0155
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.9744s / 145900.9696 s
env0_first_0:                 episode reward: -24.4000,                 loss: nan
env0_second_0:                 episode reward: 24.4000,                 loss: 0.0181
env1_first_0:                 episode reward: -19.6000,                 loss: nan
env1_second_0:                 episode reward: 19.6000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.7961s / 146851.7657 s
env0_first_0:                 episode reward: -30.2500,                 loss: nan
env0_second_0:                 episode reward: 30.2500,                 loss: 0.0222
env1_first_0:                 episode reward: -21.6500,                 loss: nan
env1_second_0:                 episode reward: 21.6500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.8195s / 147802.5852 s
env0_first_0:                 episode reward: -27.8000,                 loss: nan
env0_second_0:                 episode reward: 27.8000,                 loss: 0.0281
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 1759.45,                last time consumption/overall running time: 937.7689s / 148740.3541 s
env0_first_0:                 episode reward: -31.8000,                 loss: 0.0451
env0_second_0:                 episode reward: 31.8000,                 loss: 0.0315
env1_first_0:                 episode reward: -21.0000,                 loss: nan
env1_second_0:                 episode reward: 21.0000,                 loss: nan
Score delta: 82.2, update the opponent.
Episode: 3441/10000 (34.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.4086s / 149691.7627 s
env0_first_0:                 episode reward: -14.0500,                 loss: 0.0316
env0_second_0:                 episode reward: 14.0500,                 loss: nan
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 951.3568s / 150643.1196 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0162
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.5860s / 151593.7055 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0125
env0_second_0:                 episode reward: 5.5000,                 loss: nan
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.8860s / 152544.5915 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0119
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.4248s / 153495.0163 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0120
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.6442s / 154445.6605 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0136
env0_second_0:                 episode reward: -4.8000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.3987s / 155396.0592 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0089
env0_second_0:                 episode reward: -1.0500,                 loss: nan
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.9105s / 156345.9697 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0083
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.6389s / 157296.6086 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0115
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.3244s / 158246.9330 s
env0_first_0:                 episode reward: 5.9500,                 loss: 0.0116
env0_second_0:                 episode reward: -5.9500,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.3637s / 159197.2967 s
env0_first_0:                 episode reward: 3.4500,                 loss: 0.0113
env0_second_0:                 episode reward: -3.4500,                 loss: nan
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.3273s / 160147.6241 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0100
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.3654s / 161096.9895 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0105
env0_second_0:                 episode reward: -1.5500,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.5975s / 162047.5869 s
env0_first_0:                 episode reward: 8.8000,                 loss: 0.0087
env0_second_0:                 episode reward: -8.8000,                 loss: nan
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.5888s / 162998.1757 s
env0_first_0:                 episode reward: 9.9000,                 loss: 0.0075
env0_second_0:                 episode reward: -9.9000,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.7794s / 163947.9551 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0102
env0_second_0:                 episode reward: -1.3500,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.4549s / 164897.4100 s
env0_first_0:                 episode reward: 14.1000,                 loss: 0.0139
env0_second_0:                 episode reward: -14.1000,                 loss: nan
env1_first_0:                 episode reward: 11.8500,                 loss: nan
env1_second_0:                 episode reward: -11.8500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.7093s / 165847.1193 s
env0_first_0:                 episode reward: 9.8500,                 loss: 0.0130
env0_second_0:                 episode reward: -9.8500,                 loss: nan
env1_first_0:                 episode reward: 13.3500,                 loss: nan
env1_second_0:                 episode reward: -13.3500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.2762s / 166796.3955 s
env0_first_0:                 episode reward: 13.8500,                 loss: 0.0146
env0_second_0:                 episode reward: -13.8500,                 loss: nan
env1_first_0:                 episode reward: 14.8500,                 loss: nan
env1_second_0:                 episode reward: -14.8500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.7695s / 167746.1650 s
env0_first_0:                 episode reward: 13.6500,                 loss: 0.0140
env0_second_0:                 episode reward: -13.6500,                 loss: nan
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.4584s / 168695.6234 s
env0_first_0:                 episode reward: 14.5500,                 loss: 0.0133
env0_second_0:                 episode reward: -14.5500,                 loss: nan
env1_first_0:                 episode reward: 16.5500,                 loss: nan
env1_second_0:                 episode reward: -16.5500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.4119s / 169645.0353 s
env0_first_0:                 episode reward: 19.3000,                 loss: 0.0154
env0_second_0:                 episode reward: -19.3000,                 loss: nan
env1_first_0:                 episode reward: 20.8500,                 loss: nan
env1_second_0:                 episode reward: -20.8500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.9247s / 170593.9601 s
env0_first_0:                 episode reward: 12.1500,                 loss: 0.0169
env0_second_0:                 episode reward: -12.1500,                 loss: nan
env1_first_0:                 episode reward: 13.1500,                 loss: nan
env1_second_0:                 episode reward: -13.1500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.8340s / 171542.7941 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0175
env0_second_0:                 episode reward: 3.9500,                 loss: nan
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.5777s / 172492.3717 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0145
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.7049s / 173442.0767 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0126
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.6621s / 174391.7388 s
env0_first_0:                 episode reward: 9.2500,                 loss: 0.0133
env0_second_0:                 episode reward: -9.2500,                 loss: nan
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.8419s / 175341.5807 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0126
env0_second_0:                 episode reward: -2.3000,                 loss: nan
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.4120s / 176290.9926 s
env0_first_0:                 episode reward: 11.8000,                 loss: 0.0135
env0_second_0:                 episode reward: -11.8000,                 loss: nan
env1_first_0:                 episode reward: 13.3000,                 loss: nan
env1_second_0:                 episode reward: -13.3000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 950.0408s / 177241.0334 s
env0_first_0:                 episode reward: 20.9500,                 loss: 0.0135
env0_second_0:                 episode reward: -20.9500,                 loss: nan
env1_first_0:                 episode reward: 16.2500,                 loss: nan
env1_second_0:                 episode reward: -16.2500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.6755s / 178190.7089 s
env0_first_0:                 episode reward: 12.8000,                 loss: 0.0147
env0_second_0:                 episode reward: -12.8000,                 loss: nan
env1_first_0:                 episode reward: 16.7500,                 loss: nan
env1_second_0:                 episode reward: -16.7500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.4104s / 179140.1193 s
env0_first_0:                 episode reward: 32.8000,                 loss: 0.0136
env0_second_0:                 episode reward: -32.8000,                 loss: nan
env1_first_0:                 episode reward: 29.9000,                 loss: nan
env1_second_0:                 episode reward: -29.9000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.4082s / 180089.5274 s
env0_first_0:                 episode reward: 21.3000,                 loss: 0.0156
env0_second_0:                 episode reward: -21.3000,                 loss: nan
env1_first_0:                 episode reward: 21.0500,                 loss: nan
env1_second_0:                 episode reward: -21.0500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.2438s / 181038.7712 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0180
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.3015s / 181988.0728 s
env0_first_0:                 episode reward: 14.8500,                 loss: 0.0208
env0_second_0:                 episode reward: -14.8500,                 loss: nan
env1_first_0:                 episode reward: 15.8000,                 loss: nan
env1_second_0:                 episode reward: -15.8000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.8566s / 182936.9294 s
env0_first_0:                 episode reward: 26.4500,                 loss: 0.0194
env0_second_0:                 episode reward: -26.4500,                 loss: 0.0417
env1_first_0:                 episode reward: 22.9000,                 loss: nan
env1_second_0:                 episode reward: -22.9000,                 loss: nan
Score delta: 83.0, update the opponent.
Episode: 4161/10000 (41.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 949.0481s / 183885.9775 s
env0_first_0:                 episode reward: 5.3500,                 loss: nan
env0_second_0:                 episode reward: -5.3500,                 loss: 0.0248
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.7290s / 184834.7065 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0077
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.8807s / 185783.5873 s
env0_first_0:                 episode reward: -2.0500,                 loss: nan
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0060
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.9859s / 186732.5731 s
env0_first_0:                 episode reward: -3.0000,                 loss: nan
env0_second_0:                 episode reward: 3.0000,                 loss: 0.0071
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 947.6857s / 187680.2588 s
env0_first_0:                 episode reward: -9.4000,                 loss: nan
env0_second_0:                 episode reward: 9.4000,                 loss: 0.0103
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.4309s / 188628.6897 s
env0_first_0:                 episode reward: -10.9000,                 loss: nan
env0_second_0:                 episode reward: 10.9000,                 loss: 0.0135
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.5933s / 189577.2829 s
env0_first_0:                 episode reward: -11.9000,                 loss: nan
env0_second_0:                 episode reward: 11.9000,                 loss: 0.0136
env1_first_0:                 episode reward: -12.6500,                 loss: nan
env1_second_0:                 episode reward: 12.6500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 948.2342s / 190525.5171 s
env0_first_0:                 episode reward: -13.6000,                 loss: nan
env0_second_0:                 episode reward: 13.6000,                 loss: 0.0142
env1_first_0:                 episode reward: -13.9000,                 loss: nan
env1_second_0:                 episode reward: 13.9000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 947.9083s / 191473.4254 s
env0_first_0:                 episode reward: -14.0000,                 loss: nan
env0_second_0:                 episode reward: 14.0000,                 loss: 0.0130
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 947.7461s / 192421.1715 s
env0_first_0:                 episode reward: -12.6500,                 loss: nan
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0139
env1_first_0:                 episode reward: -13.6500,                 loss: nan
env1_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 946.6304s / 193367.8020 s
env0_first_0:                 episode reward: -18.4500,                 loss: nan
env0_second_0:                 episode reward: 18.4500,                 loss: 0.0152
env1_first_0:                 episode reward: -19.1000,                 loss: nan
env1_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 946.5387s / 194314.3407 s
env0_first_0:                 episode reward: -8.1000,                 loss: nan
env0_second_0:                 episode reward: 8.1000,                 loss: 0.0128
env1_first_0:                 episode reward: -12.9500,                 loss: nan
env1_second_0:                 episode reward: 12.9500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 946.3993s / 195260.7400 s
env0_first_0:                 episode reward: -13.0500,                 loss: nan
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0113
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 945.9183s / 196206.6583 s
env0_first_0:                 episode reward: -11.7000,                 loss: nan
env0_second_0:                 episode reward: 11.7000,                 loss: 0.0111
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 946.8725s / 197153.5308 s
env0_first_0:                 episode reward: -13.3000,                 loss: nan
env0_second_0:                 episode reward: 13.3000,                 loss: 0.0140
env1_first_0:                 episode reward: -16.0000,                 loss: nan
env1_second_0:                 episode reward: 16.0000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 945.8801s / 198099.4110 s
env0_first_0:                 episode reward: -26.7000,                 loss: nan
env0_second_0:                 episode reward: 26.7000,                 loss: 0.0167
env1_first_0:                 episode reward: -21.6000,                 loss: nan
env1_second_0:                 episode reward: 21.6000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 945.7149s / 199045.1259 s
env0_first_0:                 episode reward: -27.7500,                 loss: nan
env0_second_0:                 episode reward: 27.7500,                 loss: 0.0160
env1_first_0:                 episode reward: -26.1500,                 loss: nan
env1_second_0:                 episode reward: 26.1500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 944.4082s / 199989.5340 s
env0_first_0:                 episode reward: -29.7000,                 loss: nan
env0_second_0:                 episode reward: 29.7000,                 loss: 0.0163
env1_first_0:                 episode reward: -30.5000,                 loss: nan
env1_second_0:                 episode reward: 30.5000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.2449s / 200932.7789 s
env0_first_0:                 episode reward: -31.4500,                 loss: nan
env0_second_0:                 episode reward: 31.4500,                 loss: 0.0192
env1_first_0:                 episode reward: -39.1000,                 loss: nan
env1_second_0:                 episode reward: 39.1000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.5279s / 201876.3068 s
env0_first_0:                 episode reward: -21.3500,                 loss: 0.0288
env0_second_0:                 episode reward: 21.3500,                 loss: 0.0217
env1_first_0:                 episode reward: -23.8000,                 loss: nan
env1_second_0:                 episode reward: 23.8000,                 loss: nan
Score delta: 80.2, update the opponent.
Episode: 4561/10000 (45.6100%),                 avg. length: 1752.1,                last time consumption/overall running time: 926.5256s / 202802.8324 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0257
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1783.4,                last time consumption/overall running time: 943.2400s / 203746.0724 s
env0_first_0:                 episode reward: 11.6000,                 loss: 0.0290
env0_second_0:                 episode reward: -11.6000,                 loss: nan
env1_first_0:                 episode reward: 19.4500,                 loss: nan
env1_second_0:                 episode reward: -19.4500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1782.5,                last time consumption/overall running time: 943.0038s / 204689.0763 s
env0_first_0:                 episode reward: 2.7500,                 loss: 0.0291
env0_second_0:                 episode reward: -2.7500,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.8602s / 205632.9365 s
env0_first_0:                 episode reward: 16.1500,                 loss: 0.0190
env0_second_0:                 episode reward: -16.1500,                 loss: nan
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 944.1989s / 206577.1353 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0191
env0_second_0:                 episode reward: 6.9500,                 loss: nan
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 944.5426s / 207521.6780 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0213
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 944.3291s / 208466.0071 s
env0_first_0:                 episode reward: 15.6500,                 loss: 0.0255
env0_second_0:                 episode reward: -15.6500,                 loss: nan
env1_first_0:                 episode reward: 13.6000,                 loss: nan
env1_second_0:                 episode reward: -13.6000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.9515s / 209409.9586 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0202
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 1761.3,                last time consumption/overall running time: 932.1365s / 210342.0951 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0174
env0_second_0:                 episode reward: -1.6000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.9408s / 211286.0359 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0225
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.7191s / 212229.7549 s
env0_first_0:                 episode reward: 15.6000,                 loss: 0.0323
env0_second_0:                 episode reward: -15.6000,                 loss: nan
env1_first_0:                 episode reward: 15.7500,                 loss: nan
env1_second_0:                 episode reward: -15.7500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1773.45,                last time consumption/overall running time: 938.5767s / 213168.3317 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0264
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 944.2449s / 214112.5766 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0241
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1752.5,                last time consumption/overall running time: 926.5706s / 215039.1472 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0274
env0_second_0:                 episode reward: 7.9000,                 loss: nan
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.4849s / 215982.6321 s
env0_first_0:                 episode reward: 10.1500,                 loss: 0.0274
env0_second_0:                 episode reward: -10.1500,                 loss: nan
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.2208s / 216925.8529 s
env0_first_0:                 episode reward: 7.1000,                 loss: 0.0279
env0_second_0:                 episode reward: -7.1000,                 loss: nan
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.7729s / 217868.6258 s
env0_first_0:                 episode reward: 9.9000,                 loss: 0.0241
env0_second_0:                 episode reward: -9.9000,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.1865s / 218811.8122 s
env0_first_0:                 episode reward: 3.3500,                 loss: 0.0250
env0_second_0:                 episode reward: -3.3500,                 loss: nan
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.5562s / 219754.3685 s
env0_first_0:                 episode reward: 11.0500,                 loss: 0.0248
env0_second_0:                 episode reward: -11.0500,                 loss: nan
env1_first_0:                 episode reward: 12.9500,                 loss: nan
env1_second_0:                 episode reward: -12.9500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.7937s / 220697.1621 s
env0_first_0:                 episode reward: 21.6500,                 loss: 0.0247
env0_second_0:                 episode reward: -21.6500,                 loss: nan
env1_first_0:                 episode reward: 26.4500,                 loss: nan
env1_second_0:                 episode reward: -26.4500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1774.95,                last time consumption/overall running time: 938.0415s / 221635.2037 s
env0_first_0:                 episode reward: 12.9500,                 loss: 0.0224
env0_second_0:                 episode reward: -12.9500,                 loss: nan
env1_first_0:                 episode reward: 18.8500,                 loss: nan
env1_second_0:                 episode reward: -18.8500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.1191s / 222578.3228 s
env0_first_0:                 episode reward: 27.4000,                 loss: 0.0273
env0_second_0:                 episode reward: -27.4000,                 loss: 0.0513
env1_first_0:                 episode reward: 21.4500,                 loss: nan
env1_second_0:                 episode reward: -21.4500,                 loss: nan
Score delta: 85.0, update the opponent.
Episode: 5001/10000 (50.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.3818s / 223521.7045 s
env0_first_0:                 episode reward: 10.2500,                 loss: nan
env0_second_0:                 episode reward: -10.2500,                 loss: 0.0321
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.4995s / 224464.2040 s
env0_first_0:                 episode reward: 6.9000,                 loss: nan
env0_second_0:                 episode reward: -6.9000,                 loss: 0.0151
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.6258s / 225406.8298 s
env0_first_0:                 episode reward: 9.3000,                 loss: nan
env0_second_0:                 episode reward: -9.3000,                 loss: 0.0145
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.7441s / 226349.5740 s
env0_first_0:                 episode reward: 1.4500,                 loss: nan
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0125
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.8183s / 227292.3922 s
env0_first_0:                 episode reward: 5.4000,                 loss: nan
env0_second_0:                 episode reward: -5.4000,                 loss: 0.0103
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.0868s / 228234.4790 s
env0_first_0:                 episode reward: 3.7500,                 loss: nan
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0151
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.0779s / 229176.5569 s
env0_first_0:                 episode reward: 7.0500,                 loss: nan
env0_second_0:                 episode reward: -7.0500,                 loss: 0.0174
env1_first_0:                 episode reward: 10.0500,                 loss: nan
env1_second_0:                 episode reward: -10.0500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.2698s / 230117.8267 s
env0_first_0:                 episode reward: 5.7000,                 loss: nan
env0_second_0:                 episode reward: -5.7000,                 loss: 0.0142
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.6705s / 231059.4972 s
env0_first_0:                 episode reward: -0.5000,                 loss: nan
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0175
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.9571s / 232001.4543 s
env0_first_0:                 episode reward: -6.8500,                 loss: nan
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0220
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.7224s / 232943.1767 s
env0_first_0:                 episode reward: -4.5500,                 loss: nan
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0238
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.9529s / 233884.1296 s
env0_first_0:                 episode reward: -5.1500,                 loss: nan
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0252
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.0851s / 234826.2147 s
env0_first_0:                 episode reward: -4.1500,                 loss: nan
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0216
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.8621s / 235768.0768 s
env0_first_0:                 episode reward: -4.8500,                 loss: nan
env0_second_0:                 episode reward: 4.8500,                 loss: 0.0209
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.9451s / 236710.0219 s
env0_first_0:                 episode reward: -2.4500,                 loss: nan
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0169
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.6617s / 237651.6835 s
env0_first_0:                 episode reward: -1.3500,                 loss: nan
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0165
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.5230s / 238593.2066 s
env0_first_0:                 episode reward: -2.4500,                 loss: nan
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0152
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.1339s / 239535.3405 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0130
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.7388s / 240477.0793 s
env0_first_0:                 episode reward: -5.4000,                 loss: nan
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0172
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.0539s / 241419.1332 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0243
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.4781s / 242360.6113 s
env0_first_0:                 episode reward: 5.3000,                 loss: nan
env0_second_0:                 episode reward: -5.3000,                 loss: 0.0259
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.8251s / 243302.4364 s
env0_first_0:                 episode reward: 4.8500,                 loss: nan
env0_second_0:                 episode reward: -4.8500,                 loss: 0.0218
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.2450s / 244244.6815 s
env0_first_0:                 episode reward: 3.2000,                 loss: nan
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0185
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.5055s / 245186.1870 s
env0_first_0:                 episode reward: -6.8000,                 loss: nan
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0236
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 943.0219s / 246129.2089 s
env0_first_0:                 episode reward: -3.6500,                 loss: nan
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0190
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.0928s / 247071.3017 s
env0_first_0:                 episode reward: -7.3500,                 loss: nan
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0131
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.7351s / 248013.0368 s
env0_first_0:                 episode reward: -2.1500,                 loss: nan
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0134
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.1070s / 248955.1438 s
env0_first_0:                 episode reward: -0.1000,                 loss: nan
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0174
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.4119s / 249897.5557 s
env0_first_0:                 episode reward: 3.4500,                 loss: nan
env0_second_0:                 episode reward: -3.4500,                 loss: 0.0221
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.0148s / 250839.5705 s
env0_first_0:                 episode reward: -7.4000,                 loss: nan
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0247
env1_first_0:                 episode reward: -10.4500,                 loss: nan
env1_second_0:                 episode reward: 10.4500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 942.3976s / 251781.9681 s
env0_first_0:                 episode reward: -1.4500,                 loss: nan
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0258
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 941.3734s / 252723.3415 s
env0_first_0:                 episode reward: -9.0000,                 loss: nan
env0_second_0:                 episode reward: 9.0000,                 loss: 0.0203
env1_first_0:                 episode reward: -8.8500,                 loss: nan
env1_second_0:                 episode reward: 8.8500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.5851s / 253663.9266 s
env0_first_0:                 episode reward: -7.6500,                 loss: nan
env0_second_0:                 episode reward: 7.6500,                 loss: 0.0239
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.6906s / 254604.6172 s
env0_first_0:                 episode reward: -5.6500,                 loss: nan
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0237
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.7679s / 255545.3851 s
env0_first_0:                 episode reward: -6.2500,                 loss: nan
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0279
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.4432s / 256485.8283 s
env0_first_0:                 episode reward: -12.2500,                 loss: nan
env0_second_0:                 episode reward: 12.2500,                 loss: 0.0401
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.9921s / 257426.8204 s
env0_first_0:                 episode reward: 1.3500,                 loss: nan
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0557
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.5765s / 258367.3968 s
env0_first_0:                 episode reward: -2.0500,                 loss: nan
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0599
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.2345s / 259306.6313 s
env0_first_0:                 episode reward: 1.8500,                 loss: nan
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0405
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.8387s / 260246.4700 s
env0_first_0:                 episode reward: 5.2500,                 loss: nan
env0_second_0:                 episode reward: -5.2500,                 loss: 0.0304
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.0859s / 261186.5560 s
env0_first_0:                 episode reward: 8.1000,                 loss: nan
env0_second_0:                 episode reward: -8.1000,                 loss: 0.0196
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.7265s / 262126.2824 s
env0_first_0:                 episode reward: 1.4500,                 loss: nan
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0202
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.2473s / 263066.5297 s
env0_first_0:                 episode reward: 6.3000,                 loss: nan
env0_second_0:                 episode reward: -6.3000,                 loss: 0.0184
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.0119s / 264006.5416 s
env0_first_0:                 episode reward: 6.4500,                 loss: nan
env0_second_0:                 episode reward: -6.4500,                 loss: 0.0183
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.0613s / 264946.6029 s
env0_first_0:                 episode reward: 9.0000,                 loss: nan
env0_second_0:                 episode reward: -9.0000,                 loss: 0.0163
env1_first_0:                 episode reward: 10.4500,                 loss: nan
env1_second_0:                 episode reward: -10.4500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.2720s / 265885.8749 s
env0_first_0:                 episode reward: 6.4500,                 loss: nan
env0_second_0:                 episode reward: -6.4500,                 loss: 0.0183
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.7599s / 266825.6348 s
env0_first_0:                 episode reward: 7.8500,                 loss: nan
env0_second_0:                 episode reward: -7.8500,                 loss: 0.0146
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.1510s / 267765.7858 s
env0_first_0:                 episode reward: 9.5000,                 loss: nan
env0_second_0:                 episode reward: -9.5000,                 loss: 0.0132
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.1364s / 268705.9222 s
env0_first_0:                 episode reward: 8.2500,                 loss: nan
env0_second_0:                 episode reward: -8.2500,                 loss: 0.0204
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.8442s / 269645.7664 s
env0_first_0:                 episode reward: 4.4500,                 loss: nan
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0242
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.4855s / 270585.2519 s
env0_first_0:                 episode reward: 0.6500,                 loss: nan
env0_second_0:                 episode reward: -0.6500,                 loss: 0.0173
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.0812s / 271525.3331 s
env0_first_0:                 episode reward: 6.2000,                 loss: nan
env0_second_0:                 episode reward: -6.2000,                 loss: 0.0128
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.2292s / 272465.5623 s
env0_first_0:                 episode reward: -1.2000,                 loss: nan
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0148
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.3122s / 273404.8745 s
env0_first_0:                 episode reward: -4.2000,                 loss: nan
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0162
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 940.0146s / 274344.8892 s
env0_first_0:                 episode reward: 4.6000,                 loss: nan
env0_second_0:                 episode reward: -4.6000,                 loss: 0.0188
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.9508s / 275284.8400 s
env0_first_0:                 episode reward: 5.9500,                 loss: nan
env0_second_0:                 episode reward: -5.9500,                 loss: 0.0200
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.5688s / 276224.4088 s
env0_first_0:                 episode reward: 0.2500,                 loss: nan
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0140
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.6170s / 277164.0258 s
env0_first_0:                 episode reward: 2.8500,                 loss: nan
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0123
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.7350s / 278103.7608 s
env0_first_0:                 episode reward: 3.6000,                 loss: nan
env0_second_0:                 episode reward: -3.6000,                 loss: 0.0171
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.9314s / 279043.6921 s
env0_first_0:                 episode reward: 2.8000,                 loss: nan
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0188
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.0512s / 279982.7434 s
env0_first_0:                 episode reward: 1.2000,                 loss: nan
env0_second_0:                 episode reward: -1.2000,                 loss: 0.0146
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 938.5510s / 280921.2944 s
env0_first_0:                 episode reward: 1.3000,                 loss: nan
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0165
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 938.7428s / 281860.0372 s
env0_first_0:                 episode reward: -1.2000,                 loss: nan
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0152
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 938.9310s / 282798.9682 s
env0_first_0:                 episode reward: -0.2500,                 loss: nan
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0161
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.1551s / 283738.1234 s
env0_first_0:                 episode reward: 0.9000,                 loss: nan
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0148
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.6253s / 284677.7486 s
env0_first_0:                 episode reward: -1.9000,                 loss: nan
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0144
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 938.7802s / 285616.5289 s
env0_first_0:                 episode reward: -2.7500,                 loss: nan
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0141
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 938.8858s / 286555.4146 s
env0_first_0:                 episode reward: 2.7000,                 loss: nan
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0129
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 938.9764s / 287494.3910 s
env0_first_0:                 episode reward: -6.5500,                 loss: nan
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0154
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.0371s / 288433.4281 s
env0_first_0:                 episode reward: -8.8500,                 loss: nan
env0_second_0:                 episode reward: 8.8500,                 loss: 0.0169
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 939.2981s / 289372.7263 s
env0_first_0:                 episode reward: -5.9500,                 loss: nan
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0156
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 893.1878s / 290265.9141 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0190
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 892.2713s / 291158.1854 s
env0_first_0:                 episode reward: -1.9500,                 loss: nan
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0212
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 892.2020s / 292050.3874 s
env0_first_0:                 episode reward: -12.6500,                 loss: nan
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0153
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 891.7264s / 292942.1137 s
env0_first_0:                 episode reward: -2.2000,                 loss: nan
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0151
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 891.2641s / 293833.3778 s
env0_first_0:                 episode reward: -13.1500,                 loss: nan
env0_second_0:                 episode reward: 13.1500,                 loss: 0.0190
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 891.1760s / 294724.5538 s
env0_first_0:                 episode reward: -6.5000,                 loss: nan
env0_second_0:                 episode reward: 6.5000,                 loss: 0.0173
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 892.0486s / 295616.6024 s
env0_first_0:                 episode reward: -6.0500,                 loss: nan
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0176
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 892.4761s / 296509.0785 s
env0_first_0:                 episode reward: -12.2000,                 loss: nan
env0_second_0:                 episode reward: 12.2000,                 loss: 0.0186
env1_first_0:                 episode reward: -15.5000,                 loss: nan
env1_second_0:                 episode reward: 15.5000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 891.6585s / 297400.7371 s
env0_first_0:                 episode reward: -18.2500,                 loss: nan
env0_second_0:                 episode reward: 18.2500,                 loss: 0.0143
env1_first_0:                 episode reward: -13.7500,                 loss: nan
env1_second_0:                 episode reward: 13.7500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 882.0403s / 298282.7774 s
env0_first_0:                 episode reward: -7.9000,                 loss: nan
env0_second_0:                 episode reward: 7.9000,                 loss: 0.0143
env1_first_0:                 episode reward: -18.2000,                 loss: nan
env1_second_0:                 episode reward: 18.2000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 844.0784s / 299126.8558 s
env0_first_0:                 episode reward: -6.2000,                 loss: nan
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0145
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 843.5128s / 299970.3686 s
env0_first_0:                 episode reward: -15.0500,                 loss: nan
env0_second_0:                 episode reward: 15.0500,                 loss: 0.0156
env1_first_0:                 episode reward: -19.9000,                 loss: nan
env1_second_0:                 episode reward: 19.9000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 843.7813s / 300814.1499 s
env0_first_0:                 episode reward: -9.3000,                 loss: nan
env0_second_0:                 episode reward: 9.3000,                 loss: 0.0165
env1_first_0:                 episode reward: -12.0000,                 loss: nan
env1_second_0:                 episode reward: 12.0000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 843.3546s / 301657.5045 s
env0_first_0:                 episode reward: -12.7500,                 loss: nan
env0_second_0:                 episode reward: 12.7500,                 loss: 0.0192
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 843.9742s / 302501.4787 s
env0_first_0:                 episode reward: -28.9000,                 loss: nan
env0_second_0:                 episode reward: 28.9000,                 loss: 0.0228
env1_first_0:                 episode reward: -28.0000,                 loss: nan
env1_second_0:                 episode reward: 28.0000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 843.5754s / 303345.0540 s
env0_first_0:                 episode reward: -9.3000,                 loss: 0.0221
env0_second_0:                 episode reward: 9.3000,                 loss: 0.0225
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Score delta: 87.2, update the opponent.
Episode: 6741/10000 (67.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 842.8833s / 304187.9373 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0076
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 842.5572s / 305030.4945 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0046
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 843.1562s / 305873.6507 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0045
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 843.4076s / 306717.0583 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0042
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 842.8566s / 307559.9149 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0042
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 842.4668s / 308402.3817 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 841.4363s / 309243.8180 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0027
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 834.9556s / 310078.7736 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0027
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 796.8355s / 310875.6092 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0031
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 797.0368s / 311672.6460 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0028
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 796.7637s / 312469.4096 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0036
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 796.5446s / 313265.9543 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0060
env0_second_0:                 episode reward: 2.9000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 797.1318s / 314063.0860 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0033
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 796.1860s / 314859.2720 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0057
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 796.3097s / 315655.5817 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0056
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 783.5949s / 316439.1766 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0032
env0_second_0:                 episode reward: -0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 751.5829s / 317190.7596 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0041
env0_second_0:                 episode reward: 4.0500,                 loss: nan
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 722.2874s / 317913.0470 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0058
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 707.0173s / 318620.0643 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0058
env0_second_0:                 episode reward: -4.8500,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.9642s / 319326.0285 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0090
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 706.0587s / 320032.0872 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0043
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 707.3301s / 320739.4172 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0028
env0_second_0:                 episode reward: 1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 706.4855s / 321445.9028 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0033
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 706.0933s / 322151.9960 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0033
env0_second_0:                 episode reward: -1.5000,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.4335s / 322857.4295 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0032
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.5935s / 323563.0230 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0051
env0_second_0:                 episode reward: 4.5500,                 loss: nan
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 704.4315s / 324267.4546 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0034
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.5538s / 324973.0084 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0033
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 704.9112s / 325677.9196 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0032
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 706.1472s / 326384.0668 s
env0_first_0:                 episode reward: 2.7500,                 loss: 0.0050
env0_second_0:                 episode reward: -2.7500,                 loss: nan
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.9438s / 327090.0106 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0036
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 704.4231s / 327794.4337 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0044
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.9072s / 328500.3409 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0032
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 704.5096s / 329204.8505 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0031
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.3115s / 329910.1619 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0017
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.8086s / 330615.9705 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0020
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.1121s / 331321.0826 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0025
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.6255s / 332026.7081 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0030
env0_second_0:                 episode reward: -0.5000,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 705.4713s / 332732.1794 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0027
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 703.5431s / 333435.7225 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0029
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 684.8011s / 334120.5236 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.2500,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 660.6978s / 334781.2214 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0043
env0_second_0:                 episode reward: -1.8000,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 662.5990s / 335443.8204 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.2000,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 661.7683s / 336105.5887 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0021
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 663.0974s / 336768.6861 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0029
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 662.6419s / 337431.3280 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0026
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 661.3803s / 338092.7083 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0023
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 661.0489s / 338753.7572 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0021
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 662.2958s / 339416.0530 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0022
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 660.7026s / 340076.7556 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0030
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 662.4658s / 340739.2214 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0055
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 661.5475s / 341400.7689 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 662.8994s / 342063.6683 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0039
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 662.7188s / 342726.3871 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0030
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 661.7981s / 343388.1852 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0024
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 661.2753s / 344049.4605 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0017
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 661.5859s / 344711.0464 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0019
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 614.3960s / 345325.4424 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0029
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 606.4670s / 345931.9094 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0029
env0_second_0:                 episode reward: 4.2500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.0214s / 346535.9308 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0037
env0_second_0:                 episode reward: 2.8000,                 loss: nan
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.9486s / 347141.8794 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0030
env0_second_0:                 episode reward: 4.2000,                 loss: nan
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 607.5696s / 347749.4491 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0039
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 607.1365s / 348356.5856 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0032
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.8597s / 348962.4453 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0029
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.8427s / 349568.2880 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0029
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.9708s / 350174.2588 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0043
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.1657s / 350779.4245 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0045
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.9083s / 351385.3328 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0034
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.3177s / 351990.6505 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0030
env0_second_0:                 episode reward: 3.7000,                 loss: nan
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.9073s / 352595.5578 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0037
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 606.6790s / 353202.2368 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0041
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 606.7294s / 353808.9662 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0031
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.8140s / 354414.7801 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0032
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.7170s / 355020.4971 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0034
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 603.9282s / 355624.4253 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0026
env0_second_0:                 episode reward: 3.1000,                 loss: nan
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.1934s / 356229.6187 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0025
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.0889s / 356834.7075 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0015
env0_second_0:                 episode reward: 3.3500,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.7080s / 357439.4155 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0025
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.6681s / 358044.0836 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0023
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.4006s / 358648.4842 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0019
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.9326s / 359254.4168 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0025
env0_second_0:                 episode reward: 2.5000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.6750s / 359859.0918 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0020
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 603.6220s / 360462.7138 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0018
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 603.9683s / 361066.6821 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0020
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.9077s / 361671.5897 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0018
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 603.9398s / 362275.5296 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0018
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 605.3547s / 362880.8843 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0014
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 603.8746s / 363484.7588 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0013
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.1773s / 364088.9361 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0011
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.9975s / 364693.9336 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0013
env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 604.7301s / 365298.6637 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0015
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 603.4336s / 365902.0973 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0031
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 602.5781s / 366504.6754 s
env0_first_0:                 episode reward: -12.4000,                 loss: 0.0198
env0_second_0:                 episode reward: 12.4000,                 loss: nan
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 580.9881s / 367085.6635 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0224
env0_second_0:                 episode reward: 3.7500,                 loss: nan
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 548.7053s / 367634.3688 s
env0_first_0:                 episode reward: -8.3000,                 loss: 0.0175
env0_second_0:                 episode reward: 8.3000,                 loss: nan
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 548.9349s / 368183.3037 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0118
env0_second_0:                 episode reward: 4.2000,                 loss: nan
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 548.9095s / 368732.2132 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0106
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 549.8263s / 369282.0396 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0088
env0_second_0:                 episode reward: 2.4000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 548.6447s / 369830.6843 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0051
env0_second_0:                 episode reward: 1.1500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 547.7033s / 370378.3875 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0031
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 549.5383s / 370927.9258 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0026
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 547.9119s / 371475.8378 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0082
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 512.4034s / 371988.2412 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0105
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 506.4948s / 372494.7360 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0080
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 510.1763s / 373004.9123 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0039
env0_second_0:                 episode reward: 3.6500,                 loss: nan
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 509.2956s / 373514.2078 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0032
env0_second_0:                 episode reward: 0.7000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 509.3597s / 374023.5676 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0024
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 451.1654s / 374474.7330 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0042
env0_second_0:                 episode reward: 4.4000,                 loss: nan
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 448.8026s / 374923.5355 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0029
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 451.4634s / 375374.9989 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0020
env0_second_0:                 episode reward: 2.4500,                 loss: nan
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 453.1087s / 375828.1076 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0035
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 448.2926s / 376276.4002 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0022
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 448.6422s / 376725.0424 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0026
env0_second_0:                 episode reward: 3.9500,                 loss: nan
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 450.5928s / 377175.6352 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0022
env0_second_0:                 episode reward: 0.6000,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 452.3144s / 377627.9496 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0014
env0_second_0:                 episode reward: 2.2500,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 452.1640s / 378080.1136 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0015
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 453.4886s / 378533.6022 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.0013
env0_second_0:                 episode reward: 3.0000,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 452.6968s / 378986.2990 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0011
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 450.7385s / 379437.0376 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0010
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 447.6407s / 379884.6782 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0012
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 450.0973s / 380334.7755 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0013
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 451.4974s / 380786.2729 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0010
env0_second_0:                 episode reward: 1.5000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 451.7560s / 381238.0289 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0015
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 450.7821s / 381688.8111 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0016
env0_second_0:                 episode reward: 2.2500,                 loss: nan
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 449.3312s / 382138.1423 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0019
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 450.9912s / 382589.1335 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0023
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 448.8221s / 383037.9556 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0029
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 448.5282s / 383486.4838 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0027
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 451.4874s / 383937.9712 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0050
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 450.2475s / 384388.2187 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0081
env0_second_0:                 episode reward: 1.9500,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 449.2130s / 384837.4317 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0095
env0_second_0:                 episode reward: 3.9000,                 loss: nan
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 457.6010s / 385295.0328 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0055
env0_second_0:                 episode reward: 1.5500,                 loss: nan
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 640.7090s / 385935.7418 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0036
env0_second_0:                 episode reward: 1.4000,                 loss: nan
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 641.0844s / 386576.8262 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.1000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 639.5090s / 387216.3352 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0022
env0_second_0:                 episode reward: 2.3000,                 loss: nan
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 640.3354s / 387856.6705 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0033
env0_second_0:                 episode reward: 3.5000,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 643.7754s / 388500.4459 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0024
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 641.1672s / 389141.6132 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0020
env0_second_0:                 episode reward: 4.2500,                 loss: nan
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 638.5001s / 389780.1132 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0015
env0_second_0:                 episode reward: 2.2500,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 641.0747s / 390421.1879 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0017
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 639.2465s / 391060.4344 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0017
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 641.0601s / 391701.4945 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0017
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 639.5761s / 392341.0705 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0017
env0_second_0:                 episode reward: 0.0000,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 638.5543s / 392979.6248 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0018
env0_second_0:                 episode reward: 4.0500,                 loss: nan
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 635.9211s / 393615.5460 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0017
env0_second_0:                 episode reward: -0.1000,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 636.8336s / 394252.3796 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0010
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 638.8515s / 394891.2311 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0013
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 638.7604s / 395529.9915 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0012
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 637.8965s / 396167.8880 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0013
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 636.4597s / 396804.3477 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0013
env0_second_0:                 episode reward: -0.2500,                 loss: nan
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 636.0795s / 397440.4272 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0013Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_second_0:                 episode reward: 2.6500,                 loss: nan
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 631.6403s / 398072.0675 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0013
env0_second_0:                 episode reward: 2.9500,                 loss: nan
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 600.5832s / 398672.6507 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0014
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 599.5968s / 399272.2475 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0012
env0_second_0:                 episode reward: 3.4500,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 599.5574s / 399871.8050 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0015
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 600.3963s / 400472.2013 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0023
env0_second_0:                 episode reward: 0.8000,                 loss: nan
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 599.5961s / 401071.7974 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0028
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 599.0515s / 401670.8489 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0022
env0_second_0:                 episode reward: 1.8500,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 601.0295s / 402271.8784 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0021
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 603.2324s / 402875.1108 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0013
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 599.5497s / 403474.6605 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0018
env0_second_0:                 episode reward: 3.6500,                 loss: nan
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 599.1452s / 404073.8056 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0012
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 597.3176s / 404671.1232 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0009
env0_second_0:                 episode reward: 2.6000,                 loss: nan
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
