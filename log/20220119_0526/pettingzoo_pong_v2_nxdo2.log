pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nxdo2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 30, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0526/pettingzoo_pong_v2_nxdo2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0526/pettingzoo_pong_v2_nxdo2.
pong_v2 pettingzoo
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
random seed: 11
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
DQNBase(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
Episode: 1/10000 (0.0100%),                 avg. length: 1226.0,                last time consumption/overall running time: 27.2510s / 27.2510 s
env0_first_0:                 episode reward: -13.0000,                 loss: 0.0103
env0_second_0:                 episode reward: 13.0000,                 loss: nan
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 979.25,                last time consumption/overall running time: 406.2569s / 433.5079 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0021
env0_second_0:                 episode reward: -3.1500,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 897.1,                last time consumption/overall running time: 380.6903s / 814.1982 s
env0_first_0:                 episode reward: 12.3500,                 loss: 0.0017
env0_second_0:                 episode reward: -12.3500,                 loss: 0.0051
env1_first_0:                 episode reward: 14.0500,                 loss: nan
env1_second_0:                 episode reward: -14.0500,                 loss: nan
Score delta: 33.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/33_0.
Episode: 61/10000 (0.6100%),                 avg. length: 1120.1,                last time consumption/overall running time: 484.1044s / 1298.3026 s
env0_first_0:                 episode reward: 8.8000,                 loss: nan
env0_second_0:                 episode reward: -8.8000,                 loss: 0.0030
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1275.3,                last time consumption/overall running time: 575.2971s / 1873.5997 s
env0_first_0:                 episode reward: -12.6500,                 loss: 0.0022
env0_second_0:                 episode reward: 12.6500,                 loss: 0.0080
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Score delta: 32.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/79_1.
Episode: 101/10000 (1.0100%),                 avg. length: 1500.6,                last time consumption/overall running time: 664.7181s / 2538.3178 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0073
env0_second_0:                 episode reward: 3.9500,                 loss: nan
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1356.4,                last time consumption/overall running time: 600.9813s / 3139.2991 s
env0_first_0:                 episode reward: 11.9000,                 loss: 0.0123
env0_second_0:                 episode reward: -11.9000,                 loss: nan
env1_first_0:                 episode reward: 11.2000,                 loss: nan
env1_second_0:                 episode reward: -11.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1228.6,                last time consumption/overall running time: 544.7404s / 3684.0395 s
env0_first_0:                 episode reward: 12.5000,                 loss: 0.0124
env0_second_0:                 episode reward: -12.5000,                 loss: nan
env1_first_0:                 episode reward: 14.8500,                 loss: nan
env1_second_0:                 episode reward: -14.8500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1456.35,                last time consumption/overall running time: 676.4317s / 4360.4713 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0106
env0_second_0:                 episode reward: -4.4000,                 loss: 0.0122
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Score delta: 30.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/151_0.
Episode: 181/10000 (1.8100%),                 avg. length: 1435.15,                last time consumption/overall running time: 1043.8039s / 5404.2751 s
env0_first_0:                 episode reward: -10.2000,                 loss: 0.0114
env0_second_0:                 episode reward: 10.2000,                 loss: 0.0109
env1_first_0:                 episode reward: -12.0000,                 loss: nan
env1_second_0:                 episode reward: 12.0000,                 loss: nan
Score delta: 33.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/180_1.
Episode: 201/10000 (2.0100%),                 avg. length: 1227.95,                last time consumption/overall running time: 607.4188s / 6011.6939 s
env0_first_0:                 episode reward: 15.4000,                 loss: 0.0118
env0_second_0:                 episode reward: -15.4000,                 loss: nan
env1_first_0:                 episode reward: 12.2000,                 loss: nan
env1_second_0:                 episode reward: -12.2000,                 loss: nan
Score delta: 35.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/201_0.
Episode: 221/10000 (2.2100%),                 avg. length: 1609.6,                last time consumption/overall running time: 713.6818s / 6725.3757 s
env0_first_0:                 episode reward: -11.4000,                 loss: nan
env0_second_0:                 episode reward: 11.4000,                 loss: 0.0142
env1_first_0:                 episode reward: -11.6500,                 loss: nan
env1_second_0:                 episode reward: 11.6500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1276.9,                last time consumption/overall running time: 985.4990s / 7710.8746 s
env0_first_0:                 episode reward: 11.9500,                 loss: 0.0115
env0_second_0:                 episode reward: -11.9500,                 loss: 0.0104
env1_first_0:                 episode reward: 12.1000,                 loss: nan
env1_second_0:                 episode reward: -12.1000,                 loss: nan
Score delta: 30.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/223_1.
Episode: 261/10000 (2.6100%),                 avg. length: 1354.0,                last time consumption/overall running time: 735.7017s / 8446.5763 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0091
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0135
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Score delta: 33.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/247_0.
Episode: 281/10000 (2.8100%),                 avg. length: 1221.3,                last time consumption/overall running time: 846.7326s / 9293.3090 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0120
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0105
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Score delta: 32.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/274_1.
Episode: 301/10000 (3.0100%),                 avg. length: 1381.85,                last time consumption/overall running time: 928.6870s / 10221.9960 s
env0_first_0:                 episode reward: 10.4500,                 loss: 0.0098
env0_second_0:                 episode reward: -10.4500,                 loss: 0.0133
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Score delta: 33.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/295_0.
Episode: 321/10000 (3.2100%),                 avg. length: 1322.95,                last time consumption/overall running time: 587.8095s / 10809.8055 s
env0_first_0:                 episode reward: -12.3500,                 loss: nan
env0_second_0:                 episode reward: 12.3500,                 loss: 0.0108
env1_first_0:                 episode reward: -16.4500,                 loss: nan
env1_second_0:                 episode reward: 16.4500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1325.75,                last time consumption/overall running time: 1047.2182s / 11857.0237 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0070
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Score delta: 31.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/327_1.
Episode: 361/10000 (3.6100%),                 avg. length: 1914.4,                last time consumption/overall running time: 845.3460s / 12702.3697 s
env0_first_0:                 episode reward: 9.2000,                 loss: 0.0090
env0_second_0:                 episode reward: -9.2000,                 loss: nan
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1665.75,                last time consumption/overall running time: 1294.9512s / 13997.3209 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0102
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0065
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Score delta: 35.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/368_0.
Episode: 401/10000 (4.0100%),                 avg. length: 2790.1,                last time consumption/overall running time: 1505.0293s / 15502.3502 s
env0_first_0:                 episode reward: 20.2000,                 loss: 0.0096
env0_second_0:                 episode reward: -20.2000,                 loss: 0.0066
env1_first_0:                 episode reward: 13.0500,                 loss: nan
env1_second_0:                 episode reward: -13.0500,                 loss: nan
Score delta: 32.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/389_1.
Episode: 421/10000 (4.2100%),                 avg. length: 2591.5,                last time consumption/overall running time: 1677.9460s / 17180.2962 s
env0_first_0:                 episode reward: 28.9500,                 loss: 0.0121
env0_second_0:                 episode reward: -28.9500,                 loss: 0.0071
env1_first_0:                 episode reward: 30.4000,                 loss: nan
env1_second_0:                 episode reward: -30.4000,                 loss: nan
Score delta: 194.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/410_0.
Episode: 441/10000 (4.4100%),                 avg. length: 1598.0,                last time consumption/overall running time: 1926.5178s / 19106.8140 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0125
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0064
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Score delta: 43.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/431_1.
Episode: 461/10000 (4.6100%),                 avg. length: 2259.85,                last time consumption/overall running time: 1640.3076s / 20747.1216 s
env0_first_0:                 episode reward: 11.0000,                 loss: 0.0105
env0_second_0:                 episode reward: -11.0000,                 loss: 0.0128
env1_first_0:                 episode reward: 16.6000,                 loss: nan
env1_second_0:                 episode reward: -16.6000,                 loss: nan
Score delta: 32.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/456_0.
Episode: 481/10000 (4.8100%),                 avg. length: 2229.9,                last time consumption/overall running time: 981.2300s / 21728.3516 s
env0_first_0:                 episode reward: -6.8000,                 loss: nan
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0095
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1582.5,                last time consumption/overall running time: 2303.1501s / 24031.5017 s
env0_first_0:                 episode reward: -10.0000,                 loss: 0.0105
env0_second_0:                 episode reward: 10.0000,                 loss: 0.0061
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
Score delta: 30.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/497_1.
Episode: 521/10000 (5.2100%),                 avg. length: 1550.5,                last time consumption/overall running time: 688.5910s / 24720.0927 s
env0_first_0:                 episode reward: 8.7500,                 loss: 0.0104
env0_second_0:                 episode reward: -8.7500,                 loss: nan
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1916.05,                last time consumption/overall running time: 2243.4018s / 26963.4945 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0095
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0130
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Score delta: 33.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/523_0.
Episode: 561/10000 (5.6100%),                 avg. length: 1413.95,                last time consumption/overall running time: 2422.0729s / 29385.5674 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0141
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0069
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Score delta: 32.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/552_1.
Episode: 581/10000 (5.8100%),                 avg. length: 1669.95,                last time consumption/overall running time: 734.6511s / 30120.2186 s
env0_first_0:                 episode reward: 8.8500,                 loss: 0.0115
env0_second_0:                 episode reward: -8.8500,                 loss: nan
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2069.05,                last time consumption/overall running time: 1649.0117s / 31769.2303 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0105
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0138
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Score delta: 43.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/583_0.
Episode: 621/10000 (6.2100%),                 avg. length: 1517.0,                last time consumption/overall running time: 667.8868s / 32437.1171 s
env0_first_0:                 episode reward: -13.5500,                 loss: nan
env0_second_0:                 episode reward: 13.5500,                 loss: 0.0059
env1_first_0:                 episode reward: -15.0000,                 loss: nan
env1_second_0:                 episode reward: 15.0000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2130.4,                last time consumption/overall running time: 2017.3151s / 34454.4322 s
env0_first_0:                 episode reward: -8.4500,                 loss: 0.0091
env0_second_0:                 episode reward: 8.4500,                 loss: 0.0062
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Score delta: 31.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/623_1.
Episode: 661/10000 (6.6100%),                 avg. length: 1870.3,                last time consumption/overall running time: 1744.5976s / 36199.0299 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0087
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0144
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Score delta: 33.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/652_0.
Episode: 681/10000 (6.8100%),                 avg. length: 1726.6,                last time consumption/overall running time: 3358.7170s / 39557.7469 s
env0_first_0:                 episode reward: -9.7000,                 loss: 0.0077
env0_second_0:                 episode reward: 9.7000,                 loss: 0.0116
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Score delta: 62.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/673_1.
Episode: 701/10000 (7.0100%),                 avg. length: 1439.25,                last time consumption/overall running time: 2666.8849s / 42224.6318 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0077
env0_second_0:                 episode reward: -7.6500,                 loss: 0.0167
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Score delta: 30.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/694_0.
Episode: 721/10000 (7.2100%),                 avg. length: 2243.8,                last time consumption/overall running time: 3505.4331s / 45730.0649 s
env0_first_0:                 episode reward: -16.7500,                 loss: 0.0071
env0_second_0:                 episode reward: 16.7500,                 loss: 0.0116
env1_first_0:                 episode reward: -22.5000,                 loss: nan
env1_second_0:                 episode reward: 22.5000,                 loss: nan
Score delta: 51.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/715_1.
Episode: 741/10000 (7.4100%),                 avg. length: 1185.75,                last time consumption/overall running time: 624.8377s / 46354.9026 s
env0_first_0:                 episode reward: 13.6000,                 loss: 0.0083
env0_second_0:                 episode reward: -13.6000,                 loss: nan
env1_first_0:                 episode reward: 11.6000,                 loss: nan
env1_second_0:                 episode reward: -11.6000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1318.1,                last time consumption/overall running time: 1808.3422s / 48163.2448 s
env0_first_0:                 episode reward: 5.5000,                 loss: 0.0081
env0_second_0:                 episode reward: -5.5000,                 loss: 0.0173
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Score delta: 31.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/751_0.
Episode: 781/10000 (7.8100%),                 avg. length: 1477.15,                last time consumption/overall running time: 3375.3350s / 51538.5798 s
env0_first_0:                 episode reward: -15.3000,                 loss: 0.0090
env0_second_0:                 episode reward: 15.3000,                 loss: 0.0128
env1_first_0:                 episode reward: -15.1500,                 loss: nan
env1_second_0:                 episode reward: 15.1500,                 loss: nan
Score delta: 31.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/773_1.
Episode: 801/10000 (8.0100%),                 avg. length: 1345.95,                last time consumption/overall running time: 708.8312s / 52247.4110 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0090
env0_second_0:                 episode reward: 6.3000,                 loss: nan
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1541.9,                last time consumption/overall running time: 807.4194s / 53054.8303 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0096
env0_second_0:                 episode reward: 3.6500,                 loss: nan
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2344.95,                last time consumption/overall running time: 1224.3307s / 54279.1611 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0119
env0_second_0:                 episode reward: 5.2500,                 loss: nan
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2228.25,                last time consumption/overall running time: 1166.2846s / 55445.4457 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0095
env0_second_0:                 episode reward: -2.4000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2060.4,                last time consumption/overall running time: 1069.1466s / 56514.5923 s
env0_first_0:                 episode reward: 6.6500,                 loss: 0.0083
env0_second_0:                 episode reward: -6.6500,                 loss: nan
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2386.5,                last time consumption/overall running time: 1242.5633s / 57757.1556 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0076
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1997.8,                last time consumption/overall running time: 1034.0870s / 58791.2426 s
env0_first_0:                 episode reward: 8.4000,                 loss: 0.0075
env0_second_0:                 episode reward: -8.4000,                 loss: nan
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1653.7,                last time consumption/overall running time: 853.0249s / 59644.2675 s
env0_first_0:                 episode reward: 12.5000,                 loss: 0.0073
env0_second_0:                 episode reward: -12.5000,                 loss: nan
env1_first_0:                 episode reward: 12.1500,                 loss: nan
env1_second_0:                 episode reward: -12.1500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1377.9,                last time consumption/overall running time: 706.1594s / 60350.4269 s
env0_first_0:                 episode reward: 12.8500,                 loss: 0.0086
env0_second_0:                 episode reward: -12.8500,                 loss: nan
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1935.15,                last time consumption/overall running time: 1013.7462s / 61364.1731 s
env0_first_0:                 episode reward: 8.5000,                 loss: 0.0080
env0_second_0:                 episode reward: -8.5000,                 loss: nan
env1_first_0:                 episode reward: 9.3500,                 loss: nan
env1_second_0:                 episode reward: -9.3500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2051.1,                last time consumption/overall running time: 1065.0720s / 62429.2450 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0079
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1788.75,                last time consumption/overall running time: 938.6669s / 63367.9119 s
env0_first_0:                 episode reward: 7.7500,                 loss: 0.0082
env0_second_0:                 episode reward: -7.7500,                 loss: nan
env1_first_0:                 episode reward: 8.3000,                 loss: nan
env1_second_0:                 episode reward: -8.3000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1720.65,                last time consumption/overall running time: 2352.8374s / 65720.7493 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.0097
env0_second_0:                 episode reward: -8.3500,                 loss: 0.0155
env1_first_0:                 episode reward: 10.3000,                 loss: nan
env1_second_0:                 episode reward: -10.3000,                 loss: nan
Score delta: 31.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1030_0.
Episode: 1061/10000 (10.6100%),                 avg. length: 3312.8,                last time consumption/overall running time: 2909.4070s / 68630.1563 s
env0_first_0:                 episode reward: -8.2000,                 loss: 0.0109
env0_second_0:                 episode reward: 8.2000,                 loss: 0.0095
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Score delta: 123.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1051_1.
Episode: 1081/10000 (10.8100%),                 avg. length: 5889.2,                last time consumption/overall running time: 4123.5922s / 72753.7485 s
env0_first_0:                 episode reward: 60.0500,                 loss: 0.0138
env0_second_0:                 episode reward: -60.0500,                 loss: 0.0119
env1_first_0:                 episode reward: 53.4500,                 loss: nan
env1_second_0:                 episode reward: -53.4500,                 loss: nan
Score delta: 339.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1072_0.
Episode: 1101/10000 (11.0100%),                 avg. length: 2300.2,                last time consumption/overall running time: 2479.5813s / 75233.3298 s
env0_first_0:                 episode reward: -22.8500,                 loss: 0.0138
env0_second_0:                 episode reward: 22.8500,                 loss: 0.0104
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Score delta: 79.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1093_1.
Episode: 1121/10000 (11.2100%),                 avg. length: 2358.05,                last time consumption/overall running time: 1241.9768s / 76475.3066 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0100
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2268.95,                last time consumption/overall running time: 1186.8516s / 77662.1583 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0081
env0_second_0:                 episode reward: -4.1500,                 loss: nan
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2590.2,                last time consumption/overall running time: 1352.2233s / 79014.3816 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0072
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2127.6,                last time consumption/overall running time: 1110.0271s / 80124.4087 s
env0_first_0:                 episode reward: 8.7500,                 loss: 0.0086
env0_second_0:                 episode reward: -8.7500,                 loss: nan
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1823.8,                last time consumption/overall running time: 956.6510s / 81081.0597 s
env0_first_0:                 episode reward: 9.6500,                 loss: 0.0104
env0_second_0:                 episode reward: -9.6500,                 loss: nan
env1_first_0:                 episode reward: 11.7000,                 loss: nan
env1_second_0:                 episode reward: -11.7000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1757.15,                last time consumption/overall running time: 911.3685s / 81992.4282 s
env0_first_0:                 episode reward: 9.6500,                 loss: 0.0121
env0_second_0:                 episode reward: -9.6500,                 loss: nan
env1_first_0:                 episode reward: 10.5500,                 loss: nan
env1_second_0:                 episode reward: -10.5500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2070.5,                last time consumption/overall running time: 1084.2265s / 83076.6546 s
env0_first_0:                 episode reward: 8.1500,                 loss: 0.0117
env0_second_0:                 episode reward: -8.1500,                 loss: nan
env1_first_0:                 episode reward: 12.1500,                 loss: nan
env1_second_0:                 episode reward: -12.1500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1947.3,                last time consumption/overall running time: 1011.4783s / 84088.1330 s
env0_first_0:                 episode reward: 5.3000,                 loss: 0.0108
env0_second_0:                 episode reward: -5.3000,                 loss: nan
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1913.45,                last time consumption/overall running time: 2462.7954s / 86550.9284 s
env0_first_0:                 episode reward: -27.4000,                 loss: 0.0108
env0_second_0:                 episode reward: 27.4000,                 loss: 0.0094
env1_first_0:                 episode reward: -21.8500,                 loss: nan
env1_second_0:                 episode reward: 21.8500,                 loss: nan
Score delta: 30.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1264_0.
Episode: 1301/10000 (13.0100%),                 avg. length: 1830.95,                last time consumption/overall running time: 4585.8633s / 91136.7917 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0056
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0087
env1_first_0:                 episode reward: -8.2500,                 loss: nan
env1_second_0:                 episode reward: 8.2500,                 loss: nan
Score delta: 35.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1285_1.
Episode: 1321/10000 (13.2100%),                 avg. length: 2027.95,                last time consumption/overall running time: 1059.5225s / 92196.3142 s
env0_first_0:                 episode reward: 8.4500,                 loss: 0.0079
env0_second_0:                 episode reward: -8.4500,                 loss: nan
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 2054.3,                last time consumption/overall running time: 1075.4299s / 93271.7441 s
env0_first_0:                 episode reward: 8.1000,                 loss: 0.0095
env0_second_0:                 episode reward: -8.1000,                 loss: nan
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1823.35,                last time consumption/overall running time: 965.4895s / 94237.2336 s
env0_first_0:                 episode reward: 11.4500,                 loss: 0.0071
env0_second_0:                 episode reward: -11.4500,                 loss: nan
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1679.55,                last time consumption/overall running time: 2477.2670s / 96714.5006 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0064
env0_second_0:                 episode reward: -6.1500,                 loss: 0.0151
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Score delta: 31.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1367_0.
Episode: 1401/10000 (14.0100%),                 avg. length: 1827.7,                last time consumption/overall running time: 943.2488s / 97657.7494 s
env0_first_0:                 episode reward: -9.8500,                 loss: nan
env0_second_0:                 episode reward: 9.8500,                 loss: 0.0093
env1_first_0:                 episode reward: -11.2500,                 loss: nan
env1_second_0:                 episode reward: 11.2500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1787.45,                last time consumption/overall running time: 4587.8240s / 102245.5734 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0095
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0061
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Score delta: 35.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1412_1.
Episode: 1441/10000 (14.4100%),                 avg. length: 2330.3,                last time consumption/overall running time: 1228.4371s / 103474.0105 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0074
env0_second_0:                 episode reward: -3.4000,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2628.4,                last time consumption/overall running time: 1351.3343s / 104825.3448 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0061
env0_second_0:                 episode reward: -3.3000,                 loss: nan
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2476.8,                last time consumption/overall running time: 1293.1796s / 106118.5244 s
env0_first_0:                 episode reward: 7.7500,                 loss: 0.0068
env0_second_0:                 episode reward: -7.7500,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2271.9,                last time consumption/overall running time: 1178.2623s / 107296.7868 s
env0_first_0:                 episode reward: 7.9000,                 loss: 0.0077
env0_second_0:                 episode reward: -7.9000,                 loss: nan
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1923.35,                last time consumption/overall running time: 1004.1658s / 108300.9526 s
env0_first_0:                 episode reward: 8.1000,                 loss: 0.0071
env0_second_0:                 episode reward: -8.1000,                 loss: nan
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2157.4,                last time consumption/overall running time: 1131.8735s / 109432.8261 s
env0_first_0:                 episode reward: 10.1500,                 loss: 0.0068
env0_second_0:                 episode reward: -10.1500,                 loss: nan
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 2480.05,                last time consumption/overall running time: 1293.7685s / 110726.5946 s
env0_first_0:                 episode reward: 6.2500,                 loss: 0.0083
env0_second_0:                 episode reward: -6.2500,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1682.6,                last time consumption/overall running time: 870.0491s / 111596.6437 s
env0_first_0:                 episode reward: 10.5000,                 loss: 0.0080
env0_second_0:                 episode reward: -10.5000,                 loss: nan
env1_first_0:                 episode reward: 10.0500,                 loss: nan
env1_second_0:                 episode reward: -10.0500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1922.85,                last time consumption/overall running time: 1001.1114s / 112597.7551 s
env0_first_0:                 episode reward: 10.9500,                 loss: 0.0084
env0_second_0:                 episode reward: -10.9500,                 loss: nan
env1_first_0:                 episode reward: 11.4000,                 loss: nan
env1_second_0:                 episode reward: -11.4000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1784.25,                last time consumption/overall running time: 928.5788s / 113526.3339 s
env0_first_0:                 episode reward: 11.7500,                 loss: 0.0082
env0_second_0:                 episode reward: -11.7500,                 loss: nan
env1_first_0:                 episode reward: 12.7000,                 loss: nan
env1_second_0:                 episode reward: -12.7000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1756.65,                last time consumption/overall running time: 922.8052s / 114449.1392 s
env0_first_0:                 episode reward: 12.0000,                 loss: 0.0081
env0_second_0:                 episode reward: -12.0000,                 loss: nan
env1_first_0:                 episode reward: 11.7500,                 loss: nan
env1_second_0:                 episode reward: -11.7500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2091.25,                last time consumption/overall running time: 1104.6226s / 115553.7618 s
env0_first_0:                 episode reward: 10.6000,                 loss: 0.0098
env0_second_0:                 episode reward: -10.6000,                 loss: nan
env1_first_0:                 episode reward: 10.6000,                 loss: nan
env1_second_0:                 episode reward: -10.6000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1563.85,                last time consumption/overall running time: 819.6117s / 116373.3734 s
env0_first_0:                 episode reward: 12.4500,                 loss: 0.0091
env0_second_0:                 episode reward: -12.4500,                 loss: nan
env1_first_0:                 episode reward: 12.7000,                 loss: nan
env1_second_0:                 episode reward: -12.7000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1967.05,                last time consumption/overall running time: 1022.9981s / 117396.3715 s
env0_first_0:                 episode reward: 11.7500,                 loss: 0.0093
env0_second_0:                 episode reward: -11.7500,                 loss: nan
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 3246.3,                last time consumption/overall running time: 2931.4750s / 120327.8465 s
env0_first_0:                 episode reward: -28.3000,                 loss: 0.0095
env0_second_0:                 episode reward: 28.3000,                 loss: 0.0092
env1_first_0:                 episode reward: -32.6500,                 loss: nan
env1_second_0:                 episode reward: 32.6500,                 loss: nan
Score delta: 31.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1702_0.
Episode: 1741/10000 (17.4100%),                 avg. length: 2513.0,                last time consumption/overall running time: 5620.5549s / 125948.4014 s
env0_first_0:                 episode reward: 8.5000,                 loss: 0.0090
env0_second_0:                 episode reward: -8.5000,                 loss: 0.0095
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Score delta: 117.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1723_1.
Episode: 1761/10000 (17.6100%),                 avg. length: 2129.1,                last time consumption/overall running time: 2529.3370s / 128477.7384 s
env0_first_0:                 episode reward: 14.4500,                 loss: 0.0075
env0_second_0:                 episode reward: -14.4500,                 loss: 0.0135
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Score delta: 31.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1760_0.
Episode: 1781/10000 (17.8100%),                 avg. length: 3068.45,                last time consumption/overall running time: 5708.4185s / 134186.1569 s
env0_first_0:                 episode reward: -23.9000,                 loss: nan
env0_second_0:                 episode reward: 23.9000,                 loss: 0.0083
env1_first_0:                 episode reward: -23.2000,                 loss: nan
env1_second_0:                 episode reward: 23.2000,                 loss: nan
Score delta: 61.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1781_1.
Episode: 1801/10000 (18.0100%),                 avg. length: 2681.1,                last time consumption/overall running time: 1402.2804s / 135588.4373 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0090
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2697.8,                last time consumption/overall running time: 1409.6522s / 136998.0895 s
env0_first_0:                 episode reward: 9.1000,                 loss: 0.0071
env0_second_0:                 episode reward: -9.1000,                 loss: nan
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 3103.7,                last time consumption/overall running time: 3618.9376s / 140617.0271 s
env0_first_0:                 episode reward: 15.2500,                 loss: 0.0069
env0_second_0:                 episode reward: -15.2500,                 loss: 0.0090
env1_first_0:                 episode reward: 15.5000,                 loss: nan
env1_second_0:                 episode reward: -15.5000,                 loss: nan
Score delta: 79.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1827_0.
Episode: 1861/10000 (18.6100%),                 avg. length: 2392.2,                last time consumption/overall running time: 2449.8882s / 143066.9152 s
env0_first_0:                 episode reward: -21.7000,                 loss: 0.0078
env0_second_0:                 episode reward: 21.7000,                 loss: 0.0056
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Score delta: 36.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1848_1.
Episode: 1881/10000 (18.8100%),                 avg. length: 2338.3,                last time consumption/overall running time: 1220.6972s / 144287.6124 s
env0_first_0:                 episode reward: 3.4000,                 loss: 0.0069
env0_second_0:                 episode reward: -3.4000,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 3042.65,                last time consumption/overall running time: 1584.6100s / 145872.2224 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0063
env0_second_0:                 episode reward: 1.2000,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 3368.65,                last time consumption/overall running time: 1731.3442s / 147603.5666 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0050
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2903.5,                last time consumption/overall running time: 1509.4405s / 149113.0071 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0053
env0_second_0:                 episode reward: -2.9500,                 loss: nan
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 2995.4,                last time consumption/overall running time: 1554.0906s / 150667.0977 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0047
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 2393.85,                last time consumption/overall running time: 1241.1381s / 151908.2358 s
env0_first_0:                 episode reward: 9.3000,                 loss: 0.0051
env0_second_0:                 episode reward: -9.3000,                 loss: nan
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 3164.85,                last time consumption/overall running time: 3731.4286s / 155639.6644 s
env0_first_0:                 episode reward: -8.2000,                 loss: 0.0071
env0_second_0:                 episode reward: 8.2000,                 loss: 0.0076
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Score delta: 56.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/1984_0.
Episode: 2021/10000 (20.2100%),                 avg. length: 1968.75,                last time consumption/overall running time: 1823.5094s / 157463.1738 s
env0_first_0:                 episode reward: -16.4000,                 loss: 0.0067
env0_second_0:                 episode reward: 16.4000,                 loss: 0.0075
env1_first_0:                 episode reward: -14.1500,                 loss: nan
env1_second_0:                 episode reward: 14.1500,                 loss: nan
Score delta: 31.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2010_1.
Episode: 2041/10000 (20.4100%),                 avg. length: 3393.2,                last time consumption/overall running time: 3610.7950s / 161073.9688 s
env0_first_0:                 episode reward: -20.2500,                 loss: 0.0084
env0_second_0:                 episode reward: 20.2500,                 loss: 0.0079
env1_first_0:                 episode reward: -20.0500,                 loss: nan
env1_second_0:                 episode reward: 20.0500,                 loss: nan
Score delta: 61.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2031_0.
Episode: 2061/10000 (20.6100%),                 avg. length: 2751.05,                last time consumption/overall running time: 2686.7165s / 163760.6853 s
env0_first_0:                 episode reward: -8.5000,                 loss: 0.0091
env0_second_0:                 episode reward: 8.5000,                 loss: 0.0093
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Score delta: 30.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2052_1.
Episode: 2081/10000 (20.8100%),                 avg. length: 2753.75,                last time consumption/overall running time: 1439.1172s / 165199.8025 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0058
env0_second_0:                 episode reward: -2.9500,                 loss: nan
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 2780.5,                last time consumption/overall running time: 1461.5017s / 166661.3041 s
env0_first_0:                 episode reward: 6.9000,                 loss: 0.0053
env0_second_0:                 episode reward: -6.9000,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 2796.4,                last time consumption/overall running time: 1460.6455s / 168121.9497 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0057
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 2538.5,                last time consumption/overall running time: 1313.2711s / 169435.2208 s
env0_first_0:                 episode reward: 8.4000,                 loss: 0.0057
env0_second_0:                 episode reward: -8.4000,                 loss: nan
env1_first_0:                 episode reward: 11.5500,                 loss: nan
env1_second_0:                 episode reward: -11.5500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 2646.8,                last time consumption/overall running time: 1358.4880s / 170793.7087 s
env0_first_0:                 episode reward: 7.9500,                 loss: 0.0059
env0_second_0:                 episode reward: -7.9500,                 loss: nan
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 2373.9,                last time consumption/overall running time: 3750.6354s / 174544.3441 s
env0_first_0:                 episode reward: 13.0000,                 loss: 0.0059
env0_second_0:                 episode reward: -13.0000,                 loss: 0.0070
env1_first_0:                 episode reward: 12.1000,                 loss: nan
env1_second_0:                 episode reward: -12.1000,                 loss: nan
Score delta: 82.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2170_0.
Episode: 2201/10000 (22.0100%),                 avg. length: 2226.55,                last time consumption/overall running time: 6197.0360s / 180741.3801 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0110
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0055
env1_first_0:                 episode reward: -11.1000,                 loss: nan
env1_second_0:                 episode reward: 11.1000,                 loss: nan
Score delta: 31.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2194_1.
Episode: 2221/10000 (22.2100%),                 avg. length: 2956.35,                last time consumption/overall running time: 1529.2848s / 182270.6649 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0074
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 2512.4,                last time consumption/overall running time: 1313.5226s / 183584.1876 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0059
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 2673.95,                last time consumption/overall running time: 1388.4835s / 184972.6711 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0063
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 2841.95,                last time consumption/overall running time: 4133.4392s / 189106.1103 s
env0_first_0:                 episode reward: 11.0000,                 loss: 0.0064
env0_second_0:                 episode reward: -11.0000,                 loss: 0.0091
env1_first_0:                 episode reward: 14.2000,                 loss: nan
env1_second_0:                 episode reward: -14.2000,                 loss: nan
Score delta: 63.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2269_0.
Episode: 2301/10000 (23.0100%),                 avg. length: 2949.4,                last time consumption/overall running time: 4030.6006s / 193136.7108 s
env0_first_0:                 episode reward: -18.6500,                 loss: 0.0101
env0_second_0:                 episode reward: 18.6500,                 loss: 0.0071
env1_first_0:                 episode reward: -22.7500,                 loss: nan
env1_second_0:                 episode reward: 22.7500,                 loss: nan
Score delta: 76.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2290_1.
Episode: 2321/10000 (23.2100%),                 avg. length: 2816.95,                last time consumption/overall running time: 1364.2366s / 194500.9474 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0073
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 2627.75,                last time consumption/overall running time: 3811.8315s / 198312.7789 s
env0_first_0:                 episode reward: 12.8500,                 loss: 0.0068
env0_second_0:                 episode reward: -12.8500,                 loss: 0.0176
env1_first_0:                 episode reward: 14.2500,                 loss: nan
env1_second_0:                 episode reward: -14.2500,                 loss: nan
Score delta: 30.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2330_0.
Episode: 2361/10000 (23.6100%),                 avg. length: 2890.5,                last time consumption/overall running time: 1412.1787s / 199724.9576 s
env0_first_0:                 episode reward: -7.4500,                 loss: nan
env0_second_0:                 episode reward: 7.4500,                 loss: 0.0067
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 2348.05,                last time consumption/overall running time: 3197.9135s / 202922.8711 s
env0_first_0:                 episode reward: -8.7500,                 loss: 0.0098
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0054
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Score delta: 30.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2372_1.
Episode: 2401/10000 (24.0100%),                 avg. length: 2272.65,                last time consumption/overall running time: 1118.3129s / 204041.1840 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0080
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2518.45,                last time consumption/overall running time: 1219.3791s / 205260.5631 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0061
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 2550.6,                last time consumption/overall running time: 1256.5108s / 206517.0740 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0068
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 2343.4,                last time consumption/overall running time: 1159.6222s / 207676.6962 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0078
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 3046.2,                last time consumption/overall running time: 1490.5445s / 209167.2407 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0066
env0_second_0:                 episode reward: 3.3500,                 loss: nan
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 2682.65,                last time consumption/overall running time: 1301.7668s / 210469.0075 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0061
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2284.35,                last time consumption/overall running time: 1120.4497s / 211589.4571 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0074
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 2420.4,                last time consumption/overall running time: 1189.8267s / 212779.2838 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0071
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2292.65,                last time consumption/overall running time: 1141.3866s / 213920.6705 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0072
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 2118.25,                last time consumption/overall running time: 1050.6768s / 214971.3472 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0065
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 6.5000,                 loss: nan
env1_second_0:                 episode reward: -6.5000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 2811.6,                last time consumption/overall running time: 1380.4333s / 216351.7805 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0073
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 2671.65,                last time consumption/overall running time: 1254.7697s / 217606.5502 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0076
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 2918.8,                last time consumption/overall running time: 1353.2141s / 218959.7643 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0067
env0_second_0:                 episode reward: 2.7500,                 loss: nan
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 2264.7,                last time consumption/overall running time: 1063.1591s / 220022.9234 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0072
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 2682.45,                last time consumption/overall running time: 1254.9362s / 221277.8596 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0074
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 3012.95,                last time consumption/overall running time: 1410.4856s / 222688.3452 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0079
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 2543.3,                last time consumption/overall running time: 1181.1162s / 223869.4614 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0072
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 2775.7,                last time consumption/overall running time: 1273.4220s / 225142.8834 s
env0_first_0:                 episode reward: 7.1000,                 loss: 0.0065
env0_second_0:                 episode reward: -7.1000,                 loss: nan
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 2836.6,                last time consumption/overall running time: 3884.6904s / 229027.5737 s
env0_first_0:                 episode reward: 8.1500,                 loss: 0.0080
env0_second_0:                 episode reward: -8.1500,                 loss: 0.0055
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Score delta: 42.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2757_0.
Episode: 2781/10000 (27.8100%),                 avg. length: 2796.1,                last time consumption/overall running time: 1296.6568s / 230324.2306 s
env0_first_0:                 episode reward: -9.9000,                 loss: nan
env0_second_0:                 episode reward: 9.9000,                 loss: 0.0049
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 2396.15,                last time consumption/overall running time: 4394.8397s / 234719.0703 s
env0_first_0:                 episode reward: -20.8500,                 loss: 0.0101
env0_second_0:                 episode reward: 20.8500,                 loss: 0.0049
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
Score delta: 77.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/2799_1.
Episode: 2821/10000 (28.2100%),                 avg. length: 2958.6,                last time consumption/overall running time: 1394.2562s / 236113.3266 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0093
env0_second_0:                 episode reward: 7.0500,                 loss: nan
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 2539.9,                last time consumption/overall running time: 1171.0588s / 237284.3853 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0074
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 2227.8,                last time consumption/overall running time: 1018.8529s / 238303.2382 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0073
env0_second_0:                 episode reward: -1.4500,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 2558.35,                last time consumption/overall running time: 1174.9989s / 239478.2372 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0076
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 2211.7,                last time consumption/overall running time: 1028.1532s / 240506.3903 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0075
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 2322.3,                last time consumption/overall running time: 1078.9839s / 241585.3742 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0083
env0_second_0:                 episode reward: -3.5500,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2175.2,                last time consumption/overall running time: 1011.1342s / 242596.5084 s
env0_first_0:                 episode reward: 6.6000,                 loss: 0.0073
env0_second_0:                 episode reward: -6.6000,                 loss: nan
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 2939.8,                last time consumption/overall running time: 1379.1369s / 243975.6454 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0075
env0_second_0:                 episode reward: 1.3500,                 loss: nan
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 2287.4,                last time consumption/overall running time: 1059.4945s / 245035.1399 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0090
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1960.4,                last time consumption/overall running time: 903.8913s / 245939.0312 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0080
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2107.7,                last time consumption/overall running time: 968.1407s / 246907.1719 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0091
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 2134.8,                last time consumption/overall running time: 985.9012s / 247893.0731 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0089
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 2194.95,                last time consumption/overall running time: 1021.6253s / 248914.6984 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0074
env0_second_0:                 episode reward: 1.1000,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 2072.1,                last time consumption/overall running time: 965.0570s / 249879.7554 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0094
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 2065.25,                last time consumption/overall running time: 961.5196s / 250841.2750 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0085
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 2219.05,                last time consumption/overall running time: 1048.0144s / 251889.2895 s
env0_first_0:                 episode reward: 7.9000,                 loss: 0.0086
env0_second_0:                 episode reward: -7.9000,                 loss: nan
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 2017.9,                last time consumption/overall running time: 937.5789s / 252826.8684 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0094
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 2512.9,                last time consumption/overall running time: 1162.7860s / 253989.6544 s
env0_first_0:                 episode reward: 3.6000,                 loss: 0.0086
env0_second_0:                 episode reward: -3.6000,                 loss: nan
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 2367.1,                last time consumption/overall running time: 1090.9077s / 255080.5621 s
env0_first_0:                 episode reward: 3.8000,                 loss: 0.0092
env0_second_0:                 episode reward: -3.8000,                 loss: nan
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2740.7,                last time consumption/overall running time: 1264.5349s / 256345.0970 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0089
env0_second_0:                 episode reward: 1.7500,                 loss: nan
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 2630.15,                last time consumption/overall running time: 1156.9290s / 257502.0259 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0083
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 2446.3,                last time consumption/overall running time: 1018.4121s / 258520.4381 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0085
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 2591.45,                last time consumption/overall running time: 1062.3004s / 259582.7384 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0079
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 2356.8,                last time consumption/overall running time: 953.6650s / 260536.4035 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0077
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 2265.65,                last time consumption/overall running time: 914.7067s / 261451.1102 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0079
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 2766.6,                last time consumption/overall running time: 1121.5704s / 262572.6806 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0071
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 2264.8,                last time consumption/overall running time: 908.5876s / 263481.2682 s
env0_first_0:                 episode reward: 6.1000,                 loss: 0.0066
env0_second_0:                 episode reward: -6.1000,                 loss: nan
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 2285.3,                last time consumption/overall running time: 920.4373s / 264401.7055 s
env0_first_0:                 episode reward: 5.3000,                 loss: 0.0072
env0_second_0:                 episode reward: -5.3000,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 2475.75,                last time consumption/overall running time: 993.5931s / 265395.2985 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0076
env0_second_0:                 episode reward: -0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 2427.65,                last time consumption/overall running time: 973.8812s / 266369.1797 s
env0_first_0:                 episode reward: 2.7500,                 loss: 0.0079
env0_second_0:                 episode reward: -2.7500,                 loss: nan
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 2354.85,                last time consumption/overall running time: 946.2343s / 267315.4140 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0073
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 2571.4,                last time consumption/overall running time: 1044.2680s / 268359.6820 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0076
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 2382.6,                last time consumption/overall running time: 976.6304s / 269336.3125 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0083
env0_second_0:                 episode reward: -5.5500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 2599.35,                last time consumption/overall running time: 1072.0636s / 270408.3761 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0082
env0_second_0:                 episode reward: -2.5500,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 2724.65,                last time consumption/overall running time: 1102.5430s / 271510.9191 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0071
env0_second_0:                 episode reward: -1.9000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 2388.7,                last time consumption/overall running time: 964.7011s / 272475.6202 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0075
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 2566.65,                last time consumption/overall running time: 1026.3819s / 273502.0021 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0071
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 2574.75,                last time consumption/overall running time: 1042.8771s / 274544.8793 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0076
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 2449.95,                last time consumption/overall running time: 1001.5077s / 275546.3869 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0067
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 2324.1,                last time consumption/overall running time: 942.4848s / 276488.8718 s
env0_first_0:                 episode reward: 5.8000,                 loss: 0.0088
env0_second_0:                 episode reward: -5.8000,                 loss: nan
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 2235.65,                last time consumption/overall running time: 916.0080s / 277404.8798 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0085
env0_second_0:                 episode reward: -7.6500,                 loss: nan
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 2298.8,                last time consumption/overall running time: 927.0355s / 278331.9153 s
env0_first_0:                 episode reward: 6.5000,                 loss: 0.0077
env0_second_0:                 episode reward: -6.5000,                 loss: nan
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 2640.85,                last time consumption/overall running time: 1062.4677s / 279394.3830 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0065
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 2623.85,                last time consumption/overall running time: 1060.5431s / 280454.9261 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0069
env0_second_0:                 episode reward: -5.1500,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 2868.1,                last time consumption/overall running time: 1154.8327s / 281609.7588 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0072
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 2916.85,                last time consumption/overall running time: 1171.2354s / 282780.9942 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0069
env0_second_0:                 episode reward: -1.7500,                 loss: nan
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 2741.2,                last time consumption/overall running time: 1099.5610s / 283880.5552 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0062
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2763.6,                last time consumption/overall running time: 1130.1775s / 285010.7327 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0067
env0_second_0:                 episode reward: -5.1000,                 loss: nan
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2564.45,                last time consumption/overall running time: 1038.9329s / 286049.6656 s
env0_first_0:                 episode reward: 8.4000,                 loss: 0.0073
env0_second_0:                 episode reward: -8.4000,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2454.9,                last time consumption/overall running time: 979.2890s / 287028.9547 s
env0_first_0:                 episode reward: 8.2000,                 loss: 0.0066
env0_second_0:                 episode reward: -8.2000,                 loss: nan
env1_first_0:                 episode reward: 10.3000,                 loss: nan
env1_second_0:                 episode reward: -10.3000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 2272.8,                last time consumption/overall running time: 900.7252s / 287929.6799 s
env0_first_0:                 episode reward: 8.8000,                 loss: 0.0064
env0_second_0:                 episode reward: -8.8000,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2221.15,                last time consumption/overall running time: 877.1029s / 288806.7828 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0068
env0_second_0:                 episode reward: -7.6500,                 loss: nan
env1_first_0:                 episode reward: 10.0500,                 loss: nan
env1_second_0:                 episode reward: -10.0500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 2228.3,                last time consumption/overall running time: 894.5098s / 289701.2926 s
env0_first_0:                 episode reward: 8.4500,                 loss: 0.0072
env0_second_0:                 episode reward: -8.4500,                 loss: nan
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 2279.4,                last time consumption/overall running time: 935.0557s / 290636.3483 s
env0_first_0:                 episode reward: 8.9000,                 loss: 0.0063
env0_second_0:                 episode reward: -8.9000,                 loss: nan
env1_first_0:                 episode reward: 11.3000,                 loss: nan
env1_second_0:                 episode reward: -11.3000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 2287.05,                last time consumption/overall running time: 929.1539s / 291565.5022 s
env0_first_0:                 episode reward: 10.2000,                 loss: 0.0068
env0_second_0:                 episode reward: -10.2000,                 loss: nan
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1911.45,                last time consumption/overall running time: 772.4622s / 292337.9644 s
env0_first_0:                 episode reward: 12.2500,                 loss: 0.0070
env0_second_0:                 episode reward: -12.2500,                 loss: nan
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 2096.7,                last time consumption/overall running time: 780.3911s / 293118.3555 s
env0_first_0:                 episode reward: 10.9000,                 loss: 0.0078
env0_second_0:                 episode reward: -10.9000,                 loss: nan
env1_first_0:                 episode reward: 12.0000,                 loss: nan
env1_second_0:                 episode reward: -12.0000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 2483.65,                last time consumption/overall running time: 2325.6844s / 295444.0399 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0072
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0069
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Score delta: 30.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/3946_0.
Episode: 3981/10000 (39.8100%),                 avg. length: 2969.0,                last time consumption/overall running time: 1003.7745s / 296447.8144 s
env0_first_0:                 episode reward: -6.2000,                 loss: nan
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0051
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 2400.25,                last time consumption/overall running time: 818.2967s / 297266.1111 s
env0_first_0:                 episode reward: -10.5500,                 loss: nan
env0_second_0:                 episode reward: 10.5500,                 loss: 0.0059
env1_first_0:                 episode reward: -13.6500,                 loss: nan
env1_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1937.85,                last time consumption/overall running time: 4388.5133s / 301654.6244 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0075
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0062
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Score delta: 31.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4015_1.
Episode: 4041/10000 (40.4100%),                 avg. length: 2532.3,                last time consumption/overall running time: 852.4985s / 302507.1230 s
env0_first_0:                 episode reward: 9.1000,                 loss: 0.0067
env0_second_0:                 episode reward: -9.1000,                 loss: nan
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 2676.9,                last time consumption/overall running time: 2880.0271s / 305387.1501 s
env0_first_0:                 episode reward: 11.2500,                 loss: 0.0069
env0_second_0:                 episode reward: -11.2500,                 loss: 0.0051
env1_first_0:                 episode reward: 8.3000,                 loss: nan
env1_second_0:                 episode reward: -8.3000,                 loss: nan
Score delta: 42.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4057_0.
Episode: 4081/10000 (40.8100%),                 avg. length: 2605.35,                last time consumption/overall running time: 869.0190s / 306256.1691 s
env0_first_0:                 episode reward: -1.8500,                 loss: nan
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0070
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 2123.65,                last time consumption/overall running time: 4964.0465s / 311220.2156 s
env0_first_0:                 episode reward: -8.1500,                 loss: 0.0080
env0_second_0:                 episode reward: 8.1500,                 loss: 0.0068
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Score delta: 30.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4092_1.
Episode: 4121/10000 (41.2100%),                 avg. length: 2485.05,                last time consumption/overall running time: 855.8738s / 312076.0894 s
env0_first_0:                 episode reward: 8.6500,                 loss: 0.0061
env0_second_0:                 episode reward: -8.6500,                 loss: nan
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 2149.05,                last time consumption/overall running time: 2627.1865s / 314703.2760 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0065
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0056
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Score delta: 31.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4128_0.
Episode: 4161/10000 (41.6100%),                 avg. length: 2243.4,                last time consumption/overall running time: 4648.8856s / 319352.1616 s
env0_first_0:                 episode reward: -10.7000,                 loss: 0.0079
env0_second_0:                 episode reward: 10.7000,                 loss: 0.0071
env1_first_0:                 episode reward: -10.5000,                 loss: nan
env1_second_0:                 episode reward: 10.5000,                 loss: nan
Score delta: 30.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4160_1.
Episode: 4181/10000 (41.8100%),                 avg. length: 2882.45,                last time consumption/overall running time: 968.4046s / 320320.5662 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0070
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 2174.7,                last time consumption/overall running time: 733.7562s / 321054.3224 s
env0_first_0:                 episode reward: 5.9500,                 loss: 0.0064
env0_second_0:                 episode reward: -5.9500,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 2309.95,                last time consumption/overall running time: 784.3055s / 321838.6279 s
env0_first_0:                 episode reward: 6.7000,                 loss: 0.0060
env0_second_0:                 episode reward: -6.7000,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 2432.8,                last time consumption/overall running time: 832.7504s / 322671.3783 s
env0_first_0:                 episode reward: 8.4500,                 loss: 0.0068
env0_second_0:                 episode reward: -8.4500,                 loss: nan
env1_first_0:                 episode reward: 8.6500,                 loss: nan
env1_second_0:                 episode reward: -8.6500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 2541.2,                last time consumption/overall running time: 806.8805s / 323478.2588 s
env0_first_0:                 episode reward: 6.3500,                 loss: 0.0062
env0_second_0:                 episode reward: -6.3500,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2927.75,                last time consumption/overall running time: 934.2511s / 324412.5099 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0062
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 2657.2,                last time consumption/overall running time: 839.0296s / 325251.5396 s
env0_first_0:                 episode reward: 7.1500,                 loss: 0.0060
env0_second_0:                 episode reward: -7.1500,                 loss: nan
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 2352.6,                last time consumption/overall running time: 735.1498s / 325986.6893 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0066
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 10.8000,                 loss: nan
env1_second_0:                 episode reward: -10.8000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 2619.7,                last time consumption/overall running time: 817.6953s / 326804.3846 s
env0_first_0:                 episode reward: 7.4000,                 loss: 0.0066
env0_second_0:                 episode reward: -7.4000,                 loss: nan
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 2393.1,                last time consumption/overall running time: 2551.3549s / 329355.7395 s
env0_first_0:                 episode reward: 12.7500,                 loss: 0.0072
env0_second_0:                 episode reward: -12.7500,                 loss: nan
env1_first_0:                 episode reward: 10.5000,                 loss: nan
env1_second_0:                 episode reward: -10.5000,                 loss: nan
Score delta: 32.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4361_0.
Episode: 4381/10000 (43.8100%),                 avg. length: 2354.85,                last time consumption/overall running time: 739.2731s / 330095.0126 s
env0_first_0:                 episode reward: -8.3000,                 loss: nan
env0_second_0:                 episode reward: 8.3000,                 loss: 0.0055
env1_first_0:                 episode reward: -10.3000,                 loss: nan
env1_second_0:                 episode reward: 10.3000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 2187.0,                last time consumption/overall running time: 698.3194s / 330793.3320 s
env0_first_0:                 episode reward: -11.9000,                 loss: nan
env0_second_0:                 episode reward: 11.9000,                 loss: 0.0068
env1_first_0:                 episode reward: -12.6500,                 loss: nan
env1_second_0:                 episode reward: 12.6500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 984.15,                last time consumption/overall running time: 1489.5213s / 332282.8533 s
env0_first_0:                 episode reward: -20.4500,                 loss: 0.0166
env0_second_0:                 episode reward: 20.4500,                 loss: 0.0055
env1_first_0:                 episode reward: -19.7000,                 loss: nan
env1_second_0:                 episode reward: 19.7000,                 loss: nan
Score delta: 34.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4406_1.
Episode: 4441/10000 (44.4100%),                 avg. length: 1726.3,                last time consumption/overall running time: 528.1060s / 332810.9593 s
env0_first_0:                 episode reward: -16.9500,                 loss: 0.0108
env0_second_0:                 episode reward: 16.9500,                 loss: nan
env1_first_0:                 episode reward: -17.1000,                 loss: nan
env1_second_0:                 episode reward: 17.1000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 3094.65,                last time consumption/overall running time: 950.3279s / 333761.2871 s
env0_first_0:                 episode reward: -8.2500,                 loss: 0.0052
env0_second_0:                 episode reward: 8.2500,                 loss: nan
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 3575.35,                last time consumption/overall running time: 1103.6279s / 334864.9150 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0045
env0_second_0:                 episode reward: 3.8500,                 loss: nan
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 3845.9,                last time consumption/overall running time: 1200.1490s / 336065.0641 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0045
env0_second_0:                 episode reward: 3.2000,                 loss: nan
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 3807.85,                last time consumption/overall running time: 1192.0907s / 337257.1548 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.0042
env0_second_0:                 episode reward: -0.3000,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 3826.5,                last time consumption/overall running time: 3343.2706s / 340600.4255 s
env0_first_0:                 episode reward: 14.1500,                 loss: 0.0048
env0_second_0:                 episode reward: -14.1500,                 loss: 0.0045
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Score delta: 70.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4531_0.
Episode: 4561/10000 (45.6100%),                 avg. length: 2579.25,                last time consumption/overall running time: 815.2653s / 341415.6908 s
env0_first_0:                 episode reward: -11.3500,                 loss: nan
env0_second_0:                 episode reward: 11.3500,                 loss: 0.0055
env1_first_0:                 episode reward: -10.8000,                 loss: nan
env1_second_0:                 episode reward: 10.8000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 2093.3,                last time consumption/overall running time: 662.5034s / 342078.1942 s
env0_first_0:                 episode reward: -13.1500,                 loss: nan
env0_second_0:                 episode reward: 13.1500,                 loss: 0.0064
env1_first_0:                 episode reward: -11.2500,                 loss: nan
env1_second_0:                 episode reward: 11.2500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 1189.55,                last time consumption/overall running time: 1186.7647s / 343264.9589 s
env0_first_0:                 episode reward: -20.1500,                 loss: 0.0151
env0_second_0:                 episode reward: 20.1500,                 loss: 0.0060
env1_first_0:                 episode reward: -20.2500,                 loss: nan
env1_second_0:                 episode reward: 20.2500,                 loss: nan
Score delta: 31.8, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4582_1.
Episode: 4621/10000 (46.2100%),                 avg. length: 1607.8,                last time consumption/overall running time: 500.2295s / 343765.1884 s
env0_first_0:                 episode reward: -15.9000,                 loss: 0.0085
env0_second_0:                 episode reward: 15.9000,                 loss: nan
env1_first_0:                 episode reward: -17.1500,                 loss: nan
env1_second_0:                 episode reward: 17.1500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 3069.1,                last time consumption/overall running time: 942.7003s / 344707.8887 s
env0_first_0:                 episode reward: -10.5500,                 loss: 0.0061
env0_second_0:                 episode reward: 10.5500,                 loss: nan
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 3423.4,                last time consumption/overall running time: 1053.1074s / 345760.9962 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0047
env0_second_0:                 episode reward: 5.3500,                 loss: nan
env1_first_0:                 episode reward: -10.4500,                 loss: nan
env1_second_0:                 episode reward: 10.4500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 3549.15,                last time consumption/overall running time: 1105.3431s / 346866.3393 s
env0_first_0:                 episode reward: -9.0500,                 loss: 0.0044
env0_second_0:                 episode reward: 9.0500,                 loss: nan
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 3368.55,                last time consumption/overall running time: 1063.0624s / 347929.4017 s
env0_first_0:                 episode reward: -8.0500,                 loss: 0.0045
env0_second_0:                 episode reward: 8.0500,                 loss: nan
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 3240.25,                last time consumption/overall running time: 1018.4941s / 348947.8957 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0044
env0_second_0:                 episode reward: 4.4500,                 loss: nan
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 3261.75,                last time consumption/overall running time: 2697.1481s / 351645.0439 s
env0_first_0:                 episode reward: 11.0500,                 loss: 0.0050
env0_second_0:                 episode reward: -11.0500,                 loss: 0.0064
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Score delta: 55.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4731_0.
Episode: 4761/10000 (47.6100%),                 avg. length: 2470.05,                last time consumption/overall running time: 685.1653s / 352330.2092 s
env0_first_0:                 episode reward: -9.3500,                 loss: nan
env0_second_0:                 episode reward: 9.3500,                 loss: 0.0060
env1_first_0:                 episode reward: -10.1000,                 loss: nan
env1_second_0:                 episode reward: 10.1000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 2495.0,                last time consumption/overall running time: 696.2054s / 353026.4145 s
env0_first_0:                 episode reward: -10.2500,                 loss: nan
env0_second_0:                 episode reward: 10.2500,                 loss: 0.0059
env1_first_0:                 episode reward: -12.0000,                 loss: nan
env1_second_0:                 episode reward: 12.0000,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1364.65,                last time consumption/overall running time: 1286.8462s / 354313.2607 s
env0_first_0:                 episode reward: -16.3500,                 loss: 0.0193
env0_second_0:                 episode reward: 16.3500,                 loss: 0.0055
env1_first_0:                 episode reward: -17.3500,                 loss: nan
env1_second_0:                 episode reward: 17.3500,                 loss: nan
Score delta: 30.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4794_1.
Episode: 4821/10000 (48.2100%),                 avg. length: 1047.5,                last time consumption/overall running time: 303.0327s / 354616.2934 s
env0_first_0:                 episode reward: -23.4000,                 loss: 0.0137
env0_second_0:                 episode reward: 23.4000,                 loss: nan
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 2419.7,                last time consumption/overall running time: 707.4514s / 355323.7448 s
env0_first_0:                 episode reward: -14.4500,                 loss: 0.0087
env0_second_0:                 episode reward: 14.4500,                 loss: nan
env1_first_0:                 episode reward: -14.8000,                 loss: nan
env1_second_0:                 episode reward: 14.8000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 3350.1,                last time consumption/overall running time: 966.9594s / 356290.7042 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0061
env0_second_0:                 episode reward: -1.9000,                 loss: nan
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 2536.75,                last time consumption/overall running time: 3225.9390s / 359516.6431 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0073
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0058
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Score delta: 50.4, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4863_0.
Episode: 4901/10000 (49.0100%),                 avg. length: 3181.45,                last time consumption/overall running time: 2568.2802s / 362084.9233 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0072
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0063
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Score delta: 32.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4895_1.
Episode: 4921/10000 (49.2100%),                 avg. length: 3181.7,                last time consumption/overall running time: 2722.4808s / 364807.4041 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0074
env0_second_0:                 episode reward: -6.7500,                 loss: 0.0060
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Score delta: 45.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4916_0.
Episode: 4941/10000 (49.4100%),                 avg. length: 1880.8,                last time consumption/overall running time: 1793.6513s / 366601.0554 s
env0_first_0:                 episode reward: -14.6500,                 loss: 0.0234
env0_second_0:                 episode reward: 14.6500,                 loss: 0.0055
env1_first_0:                 episode reward: -17.0500,                 loss: nan
env1_second_0:                 episode reward: 17.0500,                 loss: nan
Score delta: 30.6, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/4940_1.
Episode: 4961/10000 (49.6100%),                 avg. length: 1149.5,                last time consumption/overall running time: 300.7356s / 366901.7910 s
env0_first_0:                 episode reward: -19.6500,                 loss: 0.0157
env0_second_0:                 episode reward: 19.6500,                 loss: nan
env1_first_0:                 episode reward: -19.2000,                 loss: nan
env1_second_0:                 episode reward: 19.2000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 2390.9,                last time consumption/overall running time: 634.1374s / 367535.9284 s
env0_first_0:                 episode reward: -11.0000,                 loss: 0.0068
env0_second_0:                 episode reward: 11.0000,                 loss: nan
env1_first_0:                 episode reward: -14.5000,                 loss: nan
env1_second_0:                 episode reward: 14.5000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 2897.15,                last time consumption/overall running time: 757.6543s / 368293.5826 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0049
env0_second_0:                 episode reward: 4.4500,                 loss: nan
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 2665.3,                last time consumption/overall running time: 691.9114s / 368985.4941 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0058
env0_second_0:                 episode reward: 0.5500,                 loss: nan
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 3070.55,                last time consumption/overall running time: 789.8340s / 369775.3281 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0055
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 3334.75,                last time consumption/overall running time: 868.3060s / 370643.6341 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.9000,                 loss: nan
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 3187.65,                last time consumption/overall running time: 843.6889s / 371487.3230 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0056
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 3434.5,                last time consumption/overall running time: 894.4571s / 372381.7802 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0046
env0_second_0:                 episode reward: 2.7000,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 3180.0,                last time consumption/overall running time: 838.5257s / 373220.3058 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0047
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 3300.35,                last time consumption/overall running time: 849.8161s / 374070.1219 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0053
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 3320.25,                last time consumption/overall running time: 859.3569s / 374929.4788 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0042
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 3335.3,                last time consumption/overall running time: 855.1670s / 375784.6458 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0055
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 3156.15,                last time consumption/overall running time: 818.4961s / 376603.1419 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0049
env0_second_0:                 episode reward: -3.1500,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 3052.7,                last time consumption/overall running time: 784.6290s / 377387.7709 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0050
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 2947.8,                last time consumption/overall running time: 764.7104s / 378152.4813 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0054
env0_second_0:                 episode reward: -1.6000,                 loss: nan
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 3364.15,                last time consumption/overall running time: 869.9101s / 379022.3914 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0050
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 3587.4,                last time consumption/overall running time: 937.2056s / 379959.5969 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0047
env0_second_0:                 episode reward: 0.1500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 3531.8,                last time consumption/overall running time: 921.1808s / 380880.7778 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 0.9000,                 loss: nan
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 3236.25,                last time consumption/overall running time: 840.4055s / 381721.1833 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0049
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 3419.85,                last time consumption/overall running time: 888.1549s / 382609.3382 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0047
env0_second_0:                 episode reward: -2.8000,                 loss: nan
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 3216.95,                last time consumption/overall running time: 838.0435s / 383447.3817 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0044
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 3342.8,                last time consumption/overall running time: 878.4446s / 384325.8263 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0048
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 3112.85,                last time consumption/overall running time: 807.2761s / 385133.1024 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0055
env0_second_0:                 episode reward: 1.3000,                 loss: nan
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 3442.0,                last time consumption/overall running time: 1249.7912s / 386382.8936 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0048
env0_second_0:                 episode reward: 0.4500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 3412.2,                last time consumption/overall running time: 1237.9037s / 387620.7973 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0052
env0_second_0:                 episode reward: -2.0500,                 loss: nan
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 2638.4,                last time consumption/overall running time: 957.7545s / 388578.5517 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0071
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 3373.7,                last time consumption/overall running time: 1227.5850s / 389806.1367 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0051
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 2994.45,                last time consumption/overall running time: 1094.7171s / 390900.8538 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0048
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 3503.95,                last time consumption/overall running time: 1284.8753s / 392185.7291 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0047
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 3337.4,                last time consumption/overall running time: 1211.7813s / 393397.5104 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0045
env0_second_0:                 episode reward: 1.4500,                 loss: nan
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 3175.0,                last time consumption/overall running time: 1150.8178s / 394548.3282 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.0055
env0_second_0:                 episode reward: -1.1500,                 loss: nan
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 3578.1,                last time consumption/overall running time: 1277.7204s / 395826.0486 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0051
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 3498.85,                last time consumption/overall running time: 1245.4106s / 397071.4592 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0048
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 3466.75,                last time consumption/overall running time: 1243.3181s / 398314.7774 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0046
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 3194.15,                last time consumption/overall running time: 1164.7993s / 399479.5766 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0051
env0_second_0:                 episode reward: -1.6500,                 loss: nan
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 3173.75,                last time consumption/overall running time: 1163.8371s / 400643.4137 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0063
env0_second_0:                 episode reward: -3.6500,                 loss: nan
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 3372.9,                last time consumption/overall running time: 1229.4814s / 401872.8951 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0053
env0_second_0:                 episode reward: -1.0500,                 loss: nan
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 3433.4,                last time consumption/overall running time: 1239.9921s / 403112.8872 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0054
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 3579.6,                last time consumption/overall running time: 1261.7031s / 404374.5903 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0050
env0_second_0:                 episode reward: -4.8000,                 loss: nan
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 3354.25,                last time consumption/overall running time: 1195.8658s / 405570.4561 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0053
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 3270.25,                last time consumption/overall running time: 1178.0408s / 406748.4969 s
env0_first_0:                 episode reward: 3.3500,                 loss: 0.0056
env0_second_0:                 episode reward: -3.3500,                 loss: nan
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 3304.9,                last time consumption/overall running time: 1213.8696s / 407962.3665 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.0048
env0_second_0:                 episode reward: -0.6500,                 loss: nan
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 3233.55,                last time consumption/overall running time: 1190.9723s / 409153.3387 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0056
env0_second_0:                 episode reward: -1.9000,                 loss: nan
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 3296.2,                last time consumption/overall running time: 1190.3749s / 410343.7137 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.8000,                 loss: nan
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 3205.0,                last time consumption/overall running time: 1151.1661s / 411494.8797 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0044
env0_second_0:                 episode reward: -1.2500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 3039.75,                last time consumption/overall running time: 1090.1342s / 412585.0139 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0055
env0_second_0:                 episode reward: -2.8500,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 3339.5,                last time consumption/overall running time: 1197.7938s / 413782.8078 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0056
env0_second_0:                 episode reward: 0.6500,                 loss: nan
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 3505.75,                last time consumption/overall running time: 1247.0083s / 415029.8161 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0052
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 3312.6,                last time consumption/overall running time: 1204.0866s / 416233.9027 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0052
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 3060.45,                last time consumption/overall running time: 1120.5147s / 417354.4174 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0049
env0_second_0:                 episode reward: -2.3000,                 loss: nan
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 3337.0,                last time consumption/overall running time: 1209.7494s / 418564.1668 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0056
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 2698.7,                last time consumption/overall running time: 970.3472s / 419534.5140 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0055
env0_second_0:                 episode reward: -0.9500,                 loss: nan
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 3402.2,                last time consumption/overall running time: 1211.0800s / 420745.5940 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 2.0000,                 loss: nan
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 3542.3,                last time consumption/overall running time: 1253.1578s / 421998.7518 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0053
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 3532.45,                last time consumption/overall running time: 1256.0989s / 423254.8507 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0046
env0_second_0:                 episode reward: -0.4000,                 loss: nan
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 3094.05,                last time consumption/overall running time: 1131.6763s / 424386.5271 s
env0_first_0:                 episode reward: 6.8000,                 loss: 0.0051
env0_second_0:                 episode reward: -6.8000,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 3067.35,                last time consumption/overall running time: 1133.3879s / 425519.9150 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0056
env0_second_0:                 episode reward: -2.6500,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 3055.85,                last time consumption/overall running time: 1105.8236s / 426625.7386 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0053
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 3066.1,                last time consumption/overall running time: 1092.7456s / 427718.4842 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0063
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 3770.7,                last time consumption/overall running time: 1331.7595s / 429050.2436 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0049
env0_second_0:                 episode reward: -0.9000,                 loss: nan
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 3316.05,                last time consumption/overall running time: 1175.8838s / 430226.1275 s
env0_first_0:                 episode reward: 2.0500,                 loss: 0.0048
env0_second_0:                 episode reward: -2.0500,                 loss: nan
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 3574.65,                last time consumption/overall running time: 1294.4222s / 431520.5497 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0052
env0_second_0:                 episode reward: -0.7500,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 3347.35,                last time consumption/overall running time: 1221.1471s / 432741.6968 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0048
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 3941.8,                last time consumption/overall running time: 1426.2354s / 434167.9322 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0042
env0_second_0:                 episode reward: -2.3000,                 loss: nan
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 3566.2,                last time consumption/overall running time: 1268.7099s / 435436.6421 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.0040
env0_second_0:                 episode reward: 0.3000,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 3560.15,                last time consumption/overall running time: 1270.7361s / 436707.3782 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0047
env0_second_0:                 episode reward: -0.8000,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 3443.55,                last time consumption/overall running time: 1239.8460s / 437947.2242 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0044
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 3403.55,                last time consumption/overall running time: 1210.7579s / 439157.9821 s
env0_first_0:                 episode reward: 2.2500,                 loss: 0.0044
env0_second_0:                 episode reward: -2.2500,                 loss: nan
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 3183.3,                last time consumption/overall running time: 1132.7943s / 440290.7764 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0045
env0_second_0:                 episode reward: -2.5000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 3496.3,                last time consumption/overall running time: 1245.7818s / 441536.5582 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0046
env0_second_0:                 episode reward: 0.3500,                 loss: nan
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 3358.7,                last time consumption/overall running time: 1202.7919s / 442739.3501 s
env0_first_0:                 episode reward: 1.9000,                 loss: 0.0050
env0_second_0:                 episode reward: -1.9000,                 loss: nan
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 3275.6,                last time consumption/overall running time: 1181.0064s / 443920.3565 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0048
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 3706.15,                last time consumption/overall running time: 1308.7726s / 445229.1291 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0044
env0_second_0:                 episode reward: -2.4000,                 loss: nan
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 3339.35,                last time consumption/overall running time: 1187.2920s / 446416.4211 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0045
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 3113.65,                last time consumption/overall running time: 1114.4882s / 447530.9093 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0050
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 3016.75,                last time consumption/overall running time: 1083.8822s / 448614.7915 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0048
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 2758.2,                last time consumption/overall running time: 990.7313s / 449605.5228 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0052
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 2997.3,                last time consumption/overall running time: 1077.7034s / 450683.2263 s
env0_first_0:                 episode reward: 3.3500,                 loss: 0.0050
env0_second_0:                 episode reward: -3.3500,                 loss: nan
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 2487.65,                last time consumption/overall running time: 880.0261s / 451563.2523 s
env0_first_0:                 episode reward: 9.0000,                 loss: 0.0053
env0_second_0:                 episode reward: -9.0000,                 loss: nan
env1_first_0:                 episode reward: 10.1000,                 loss: nan
env1_second_0:                 episode reward: -10.1000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 2952.95,                last time consumption/overall running time: 1027.1909s / 452590.4432 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0052
env0_second_0:                 episode reward: -6.1500,                 loss: nan
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 2590.95,                last time consumption/overall running time: 903.8003s / 453494.2435 s
env0_first_0:                 episode reward: 8.1500,                 loss: 0.0055
env0_second_0:                 episode reward: -8.1500,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 2680.6,                last time consumption/overall running time: 944.1268s / 454438.3703 s
env0_first_0:                 episode reward: 6.6500,                 loss: 0.0054
env0_second_0:                 episode reward: -6.6500,                 loss: nan
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 2606.55,                last time consumption/overall running time: 922.3901s / 455360.7604 s
env0_first_0:                 episode reward: 9.6500,                 loss: 0.0065
env0_second_0:                 episode reward: -9.6500,                 loss: nan
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 3192.6,                last time consumption/overall running time: 1120.9207s / 456481.6811 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0056
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 3018.45,                last time consumption/overall running time: 1083.9445s / 457565.6256 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0048
env0_second_0:                 episode reward: -3.7000,                 loss: nan
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 3087.1,                last time consumption/overall running time: 1109.2424s / 458674.8680 s
env0_first_0:                 episode reward: 7.1500,                 loss: 0.0055
env0_second_0:                 episode reward: -7.1500,                 loss: nan
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 2835.95,                last time consumption/overall running time: 1009.7989s / 459684.6669 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0049
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 3217.8,                last time consumption/overall running time: 1133.8291s / 460818.4960 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0054
env0_second_0:                 episode reward: -0.6000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 3001.95,                last time consumption/overall running time: 1043.6297s / 461862.1257 s
env0_first_0:                 episode reward: 8.4500,                 loss: 0.0050
env0_second_0:                 episode reward: -8.4500,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 2787.65,                last time consumption/overall running time: 976.0234s / 462838.1492 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0053
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 2399.8,                last time consumption/overall running time: 838.7360s / 463676.8852 s
env0_first_0:                 episode reward: 8.1500,                 loss: 0.0055
env0_second_0:                 episode reward: -8.1500,                 loss: nan
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 2956.9,                last time consumption/overall running time: 1043.0073s / 464719.8924 s
env0_first_0:                 episode reward: 7.8500,                 loss: 0.0062
env0_second_0:                 episode reward: -7.8500,                 loss: nan
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 2615.8,                last time consumption/overall running time: 936.5536s / 465656.4460 s
env0_first_0:                 episode reward: 6.3500,                 loss: 0.0054
env0_second_0:                 episode reward: -6.3500,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 2136.0,                last time consumption/overall running time: 787.9881s / 466444.4341 s
env0_first_0:                 episode reward: 11.9000,                 loss: 0.0061
env0_second_0:                 episode reward: -11.9000,                 loss: nan
env1_first_0:                 episode reward: 14.2500,                 loss: nan
env1_second_0:                 episode reward: -14.2500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 2987.2,                last time consumption/overall running time: 1296.2389s / 467740.6730 s
env0_first_0:                 episode reward: 7.4500,                 loss: 0.0055
env0_second_0:                 episode reward: -7.4500,                 loss: nan
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 2781.3,                last time consumption/overall running time: 1209.4438s / 468950.1168 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0049
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 3165.75,                last time consumption/overall running time: 1384.1863s / 470334.3031 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0056
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 2809.8,                last time consumption/overall running time: 1226.9387s / 471561.2418 s
env0_first_0:                 episode reward: 7.2500,                 loss: 0.0049
env0_second_0:                 episode reward: -7.2500,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 2745.1,                last time consumption/overall running time: 1200.9385s / 472762.1803 s
env0_first_0:                 episode reward: 10.5500,                 loss: 0.0055
env0_second_0:                 episode reward: -10.5500,                 loss: nan
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 3144.8,                last time consumption/overall running time: 1382.2583s / 474144.4385 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0055
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 2952.3,                last time consumption/overall running time: 1280.0019s / 475424.4404 s
env0_first_0:                 episode reward: 7.3500,                 loss: 0.0052
env0_second_0:                 episode reward: -7.3500,                 loss: nan
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 2660.3,                last time consumption/overall running time: 1142.3181s / 476566.7584 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.0054
env0_second_0:                 episode reward: -8.3500,                 loss: nan
env1_first_0:                 episode reward: 10.1500,                 loss: nan
env1_second_0:                 episode reward: -10.1500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 2535.3,                last time consumption/overall running time: 1090.9004s / 477657.6588 s
env0_first_0:                 episode reward: 9.5500,                 loss: 0.0054
env0_second_0:                 episode reward: -9.5500,                 loss: nan
env1_first_0:                 episode reward: 9.3000,                 loss: nan
env1_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 2290.8,                last time consumption/overall running time: 986.6645s / 478644.3233 s
env0_first_0:                 episode reward: 9.2000,                 loss: 0.0054
env0_second_0:                 episode reward: -9.2000,                 loss: nan
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 2381.05,                last time consumption/overall running time: 1026.0624s / 479670.3857 s
env0_first_0:                 episode reward: 9.3500,                 loss: 0.0061
env0_second_0:                 episode reward: -9.3500,                 loss: nan
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 2513.9,                last time consumption/overall running time: 1076.0006s / 480746.3863 s
env0_first_0:                 episode reward: 9.6000,                 loss: 0.0060
env0_second_0:                 episode reward: -9.6000,                 loss: nan
env1_first_0:                 episode reward: 8.8000,                 loss: nan
env1_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 2563.6,                last time consumption/overall running time: 1113.6197s / 481860.0061 s
env0_first_0:                 episode reward: 4.7500,                 loss: 0.0062
env0_second_0:                 episode reward: -4.7500,                 loss: nan
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 2666.0,                last time consumption/overall running time: 1172.7508s / 483032.7569 s
env0_first_0:                 episode reward: 7.5000,                 loss: 0.0059
env0_second_0:                 episode reward: -7.5000,                 loss: nan
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 2671.45,                last time consumption/overall running time: 1171.0008s / 484203.7578 s
env0_first_0:                 episode reward: 8.5000,                 loss: 0.0060
env0_second_0:                 episode reward: -8.5000,                 loss: nan
env1_first_0:                 episode reward: 9.8000,                 loss: nan
env1_second_0:                 episode reward: -9.8000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 3142.75,                last time consumption/overall running time: 1369.7964s / 485573.5541 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0055
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 2843.15,                last time consumption/overall running time: 1229.9236s / 486803.4777 s
env0_first_0:                 episode reward: 6.6000,                 loss: 0.0049
env0_second_0:                 episode reward: -6.6000,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 2433.9,                last time consumption/overall running time: 1055.4023s / 487858.8800 s
env0_first_0:                 episode reward: 9.5000,                 loss: 0.0055
env0_second_0:                 episode reward: -9.5000,                 loss: nan
env1_first_0:                 episode reward: 11.9000,                 loss: nan
env1_second_0:                 episode reward: -11.9000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 3001.15,                last time consumption/overall running time: 1296.7553s / 489155.6353 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0053
env0_second_0:                 episode reward: -3.8500,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 2412.45,                last time consumption/overall running time: 1040.6567s / 490196.2920 s
env0_first_0:                 episode reward: 9.6000,                 loss: 0.0053
env0_second_0:                 episode reward: -9.6000,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 2983.15,                last time consumption/overall running time: 1293.5907s / 491489.8827 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0062
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 2576.25,                last time consumption/overall running time: 1116.4580s / 492606.3407 s
env0_first_0:                 episode reward: 7.3000,                 loss: 0.0058
env0_second_0:                 episode reward: -7.3000,                 loss: nan
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 2809.35,                last time consumption/overall running time: 1219.1557s / 493825.4964 s
env0_first_0:                 episode reward: 4.2500,                 loss: 0.0065
env0_second_0:                 episode reward: -4.2500,                 loss: nan
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 2701.8,                last time consumption/overall running time: 1176.4342s / 495001.9306 s
env0_first_0:                 episode reward: 8.6500,                 loss: 0.0065
env0_second_0:                 episode reward: -8.6500,                 loss: nan
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 2370.65,                last time consumption/overall running time: 993.5764s / 495995.5070 s
env0_first_0:                 episode reward: 10.0500,                 loss: 0.0061
env0_second_0:                 episode reward: -10.0500,                 loss: nan
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 3222.35,                last time consumption/overall running time: 1317.0770s / 497312.5840 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0061
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 2580.05,                last time consumption/overall running time: 1054.3118s / 498366.8958 s
env0_first_0:                 episode reward: 8.8000,                 loss: 0.0061
env0_second_0:                 episode reward: -8.8000,                 loss: nan
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 2486.2,                last time consumption/overall running time: 1019.5217s / 499386.4175 s
env0_first_0:                 episode reward: 9.7500,                 loss: 0.0059
env0_second_0:                 episode reward: -9.7500,                 loss: nan
env1_first_0:                 episode reward: 10.4000,                 loss: nan
env1_second_0:                 episode reward: -10.4000,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 2548.2,                last time consumption/overall running time: 1046.3958s / 500432.8133 s
env0_first_0:                 episode reward: 7.6000,                 loss: 0.0057
env0_second_0:                 episode reward: -7.6000,                 loss: nan
env1_first_0:                 episode reward: 8.8000,                 loss: nan
env1_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 2778.1,                last time consumption/overall running time: 1139.3707s / 501572.1840 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.0062
env0_second_0:                 episode reward: -8.3500,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 2313.65,                last time consumption/overall running time: 950.3975s / 502522.5815 s
env0_first_0:                 episode reward: 9.7000,                 loss: 0.0062
env0_second_0:                 episode reward: -9.7000,                 loss: nan
env1_first_0:                 episode reward: 8.9000,                 loss: nan
env1_second_0:                 episode reward: -8.9000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 2398.35,                last time consumption/overall running time: 987.8491s / 503510.4306 s
env0_first_0:                 episode reward: 7.5500,                 loss: 0.0061
env0_second_0:                 episode reward: -7.5500,                 loss: nan
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 2326.6,                last time consumption/overall running time: 952.4196s / 504462.8502 s
env0_first_0:                 episode reward: 9.3500,                 loss: 0.0065
env0_second_0:                 episode reward: -9.3500,                 loss: nan
env1_first_0:                 episode reward: 10.4000,                 loss: nan
env1_second_0:                 episode reward: -10.4000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 2184.6,                last time consumption/overall running time: 889.1984s / 505352.0486 s
env0_first_0:                 episode reward: 11.2500,                 loss: 0.0068
env0_second_0:                 episode reward: -11.2500,                 loss: nan
env1_first_0:                 episode reward: 11.3000,                 loss: nan
env1_second_0:                 episode reward: -11.3000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 2319.15,                last time consumption/overall running time: 947.5837s / 506299.6323 s
env0_first_0:                 episode reward: 10.2000,                 loss: 0.0064
env0_second_0:                 episode reward: -10.2000,                 loss: nan
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 2747.6,                last time consumption/overall running time: 1116.2451s / 507415.8774 s
env0_first_0:                 episode reward: 10.9000,                 loss: 0.0060
env0_second_0:                 episode reward: -10.9000,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 2163.05,                last time consumption/overall running time: 885.7587s / 508301.6361 s
env0_first_0:                 episode reward: 10.2500,                 loss: 0.0060
env0_second_0:                 episode reward: -10.2500,                 loss: nan
env1_first_0:                 episode reward: 11.4500,                 loss: nan
env1_second_0:                 episode reward: -11.4500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 2806.9,                last time consumption/overall running time: 1405.4628s / 509707.0989 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.0064
env0_second_0:                 episode reward: -8.3500,                 loss: nan
env1_first_0:                 episode reward: 8.2500,                 loss: nan
env1_second_0:                 episode reward: -8.2500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 2561.55,                last time consumption/overall running time: 1399.8254s / 511106.9244 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0059
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 8.8000,                 loss: nan
env1_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 2299.95,                last time consumption/overall running time: 1269.2076s / 512376.1320 s
env0_first_0:                 episode reward: 9.8000,                 loss: 0.0057
env0_second_0:                 episode reward: -9.8000,                 loss: nan
env1_first_0:                 episode reward: 8.9000,                 loss: nan
env1_second_0:                 episode reward: -8.9000,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 2637.3,                last time consumption/overall running time: 1451.1364s / 513827.2684 s
env0_first_0:                 episode reward: 6.4000,                 loss: 0.0060
env0_second_0:                 episode reward: -6.4000,                 loss: nan
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 2490.5,                last time consumption/overall running time: 1367.1653s / 515194.4337 s
env0_first_0:                 episode reward: 7.4000,                 loss: 0.0061
env0_second_0:                 episode reward: -7.4000,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 2545.85,                last time consumption/overall running time: 1390.8018s / 516585.2355 s
env0_first_0:                 episode reward: 5.6500,                 loss: 0.0068
env0_second_0:                 episode reward: -5.6500,                 loss: nan
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 2159.25,                last time consumption/overall running time: 1182.2378s / 517767.4732 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0070
env0_second_0:                 episode reward: -7.0500,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 2362.4,                last time consumption/overall running time: 1300.8401s / 519068.3133 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0070
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 2519.8,                last time consumption/overall running time: 1380.2482s / 520448.5615 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0068
env0_second_0:                 episode reward: -6.5500,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 2374.55,                last time consumption/overall running time: 1305.4029s / 521753.9644 s
env0_first_0:                 episode reward: 8.0000,                 loss: 0.0066
env0_second_0:                 episode reward: -8.0000,                 loss: nan
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 2321.2,                last time consumption/overall running time: 1274.4161s / 523028.3805 s
env0_first_0:                 episode reward: 10.8500,                 loss: 0.0069
env0_second_0:                 episode reward: -10.8500,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 2629.15,                last time consumption/overall running time: 1443.9569s / 524472.3373 s
env0_first_0:                 episode reward: 8.4500,                 loss: 0.0060
env0_second_0:                 episode reward: -8.4500,                 loss: nan
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 2413.8,                last time consumption/overall running time: 1335.9209s / 525808.2582 s
env0_first_0:                 episode reward: 7.8000,                 loss: 0.0054
env0_second_0:                 episode reward: -7.8000,                 loss: nan
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 2647.0,                last time consumption/overall running time: 1462.5105s / 527270.7687 s
env0_first_0:                 episode reward: 7.9500,                 loss: 0.0060
env0_second_0:                 episode reward: -7.9500,                 loss: nan
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 2231.55,                last time consumption/overall running time: 1223.0829s / 528493.8516 s
env0_first_0:                 episode reward: 8.2500,                 loss: 0.0055
env0_second_0:                 episode reward: -8.2500,                 loss: nan
env1_first_0:                 episode reward: 10.1000,                 loss: nan
env1_second_0:                 episode reward: -10.1000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 2320.65,                last time consumption/overall running time: 1263.3237s / 529757.1754 s
env0_first_0:                 episode reward: 7.3000,                 loss: 0.0057
env0_second_0:                 episode reward: -7.3000,                 loss: nan
env1_first_0:                 episode reward: 10.5500,                 loss: nan
env1_second_0:                 episode reward: -10.5500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 2092.95,                last time consumption/overall running time: 1136.6659s / 530893.8412 s
env0_first_0:                 episode reward: 10.8500,                 loss: 0.0068
env0_second_0:                 episode reward: -10.8500,                 loss: nan
env1_first_0:                 episode reward: 10.4000,                 loss: nan
env1_second_0:                 episode reward: -10.4000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 2527.65,                last time consumption/overall running time: 4783.9036s / 535677.7448 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0067
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0047
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Score delta: 30.2, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/7907_0.
Episode: 7941/10000 (79.4100%),                 avg. length: 2677.4,                last time consumption/overall running time: 1458.4613s / 537136.2061 s
env0_first_0:                 episode reward: -12.1000,                 loss: nan
env0_second_0:                 episode reward: 12.1000,                 loss: 0.0060
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 1406.1,                last time consumption/overall running time: 9708.6467s / 546844.8528 s
env0_first_0:                 episode reward: -19.8000,                 loss: 0.0191
env0_second_0:                 episode reward: 19.8000,                 loss: 0.0059
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
Score delta: 31.0, save the model to .//data/model/20220119_0526/pettingzoo_pong_v2_nxdo2/7942_1.
Episode: 7981/10000 (79.8100%),                 avg. length: 2435.45,                last time consumption/overall running time: 1332.5086s / 548177.3614 s
env0_first_0:                 episode reward: -15.1000,                 loss: 0.0117
env0_second_0:                 episode reward: 15.1000,                 loss: nan
env1_first_0:                 episode reward: -12.6000,                 loss: nan
env1_second_0:                 episode reward: 12.6000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 2705.3,                last time consumption/overall running time: 1484.8284s / 549662.1898 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0066
env0_second_0:                 episode reward: 4.4500,                 loss: nan
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 3152.2,                last time consumption/overall running time: 1732.5472s / 551394.7369 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0067
env0_second_0:                 episode reward: 7.7000,                 loss: nan
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 3586.85,                last time consumption/overall running time: 1973.9929s / 553368.7299 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0056
env0_second_0:                 episode reward: 1.0000,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 3692.05,                last time consumption/overall running time: 2026.8863s / 555395.6162 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0052
env0_second_0:                 episode reward: 0.0500,                 loss: nan
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 3440.0,                last time consumption/overall running time: 1884.6879s / 557280.3041 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0048
env0_second_0:                 episode reward: -0.7000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 2881.5,                last time consumption/overall running time: 1566.6884s / 558846.9925 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0052
env0_second_0:                 episode reward: -2.1500,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 3246.3,                last time consumption/overall running time: 1761.6141s / 560608.6066 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0061
env0_second_0:                 episode reward: -1.5000,                 loss: nan
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 3160.15,                last time consumption/overall running time: 1720.3550s / 562328.9616 s
env0_first_0:                 episode reward: 1.2000,                 loss: 0.0058
env0_second_0:                 episode reward: -1.2000,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 3088.5,                last time consumption/overall running time: 1682.8840s / 564011.8455 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0059
env0_second_0:                 episode reward: -2.7000,                 loss: nan
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 2869.95,                last time consumption/overall running time: 1557.7655s / 565569.6110 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0059
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 3060.85,                last time consumption/overall running time: 1669.3591s / 567238.9702 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0064
env0_second_0:                 episode reward: 0.4000,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 3251.3,                last time consumption/overall running time: 1777.6704s / 569016.6406 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0058
env0_second_0:                 episode reward: -0.0500,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 3024.15,                last time consumption/overall running time: 1652.8546s / 570669.4952 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0058
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 2742.4,                last time consumption/overall running time: 1495.3015s / 572164.7967 s
env0_first_0:                 episode reward: 6.5000,                 loss: 0.0051
env0_second_0:                 episode reward: -6.5000,                 loss: nan
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 3022.65,                last time consumption/overall running time: 1645.0700s / 573809.8667 s
env0_first_0:                 episode reward: 5.0500,                 loss: 0.0054
env0_second_0:                 episode reward: -5.0500,                 loss: nan
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 2635.25,                last time consumption/overall running time: 1443.4873s / 575253.3539 s
env0_first_0:                 episode reward: 6.8500,                 loss: 0.0058
env0_second_0:                 episode reward: -6.8500,                 loss: nan
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 2828.35,                last time consumption/overall running time: 1547.6906s / 576801.0445 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0064
env0_second_0:                 episode reward: -3.1500,                 loss: nan
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 2631.35,                last time consumption/overall running time: 1440.3119s / 578241.3564 s
env0_first_0:                 episode reward: 7.7500,                 loss: 0.0069
env0_second_0:                 episode reward: -7.7500,                 loss: nan
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 3219.95,                last time consumption/overall running time: 1761.1378s / 580002.4942 s
env0_first_0:                 episode reward: 3.1000,                 loss: 0.0055
env0_second_0:                 episode reward: -3.1000,                 loss: nan
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 2673.7,                last time consumption/overall running time: 1469.5572s / 581472.0514 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0060
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 2864.4,                last time consumption/overall running time: 1570.9085s / 583042.9599 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0060
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 2605.45,                last time consumption/overall running time: 1429.4841s / 584472.4440 s
env0_first_0:                 episode reward: 6.8000,                 loss: 0.0065
env0_second_0:                 episode reward: -6.8000,                 loss: nan
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 2772.8,                last time consumption/overall running time: 1518.6640s / 585991.1080 s
env0_first_0:                 episode reward: 3.9000,                 loss: 0.0074
env0_second_0:                 episode reward: -3.9000,                 loss: nan
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 3010.0,                last time consumption/overall running time: 1639.2333s / 587630.3414 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0081
env0_second_0:                 episode reward: 3.8000,                 loss: nan
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 2511.65,                last time consumption/overall running time: 1364.6222s / 588994.9636 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0077
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 2837.75,                last time consumption/overall running time: 1545.2950s / 590540.2586 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0075
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 2790.8,                last time consumption/overall running time: 1515.5536s / 592055.8123 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0072
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 2833.3,                last time consumption/overall running time: 1530.2548s / 593586.0671 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0071
env0_second_0:                 episode reward: -4.6000,                 loss: nan
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 2993.75,                last time consumption/overall running time: 1617.0565s / 595203.1236 s
env0_first_0:                 episode reward: 3.7500,                 loss: 0.0059
env0_second_0:                 episode reward: -3.7500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 2354.3,                last time consumption/overall running time: 1280.8852s / 596484.0088 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0062
env0_second_0:                 episode reward: -6.0000,                 loss: nan
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 2869.75,                last time consumption/overall running time: 1555.0440s / 598039.0528 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0065
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 2590.0,                last time consumption/overall running time: 1402.0740s / 599441.1268 s
env0_first_0:                 episode reward: 5.9500,                 loss: 0.0062
env0_second_0:                 episode reward: -5.9500,                 loss: nan
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 3248.75,                last time consumption/overall running time: 1756.2008s / 601197.3275 s
env0_first_0:                 episode reward: 4.7000,                 loss: 0.0071
env0_second_0:                 episode reward: -4.7000,                 loss: nan
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 2629.0,                last time consumption/overall running time: 1416.1705s / 602613.4981 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0069
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 2480.55,                last time consumption/overall running time: 1341.2039s / 603954.7020 s
env0_first_0:                 episode reward: 7.2000,                 loss: 0.0064
env0_second_0:                 episode reward: -7.2000,                 loss: nan
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 2564.8,                last time consumption/overall running time: 1390.7914s / 605345.4934 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0058
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 2781.4,                last time consumption/overall running time: 1502.3347s / 606847.8281 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0066
env0_second_0:                 episode reward: -2.4500,                 loss: nan
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 2513.5,                last time consumption/overall running time: 1360.1249s / 608207.9530 s
env0_first_0:                 episode reward: 7.5000,                 loss: 0.0061
env0_second_0:                 episode reward: -7.5000,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 3037.15,                last time consumption/overall running time: 1641.9219s / 609849.8749 s
env0_first_0:                 episode reward: 5.7500,                 loss: 0.0058
env0_second_0:                 episode reward: -5.7500,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 2756.8,                last time consumption/overall running time: 1488.8698s / 611338.7447 s
env0_first_0:                 episode reward: 3.1000,                 loss: 0.0057
env0_second_0:                 episode reward: -3.1000,                 loss: nan
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 2500.35,                last time consumption/overall running time: 1351.5616s / 612690.3063 s
env0_first_0:                 episode reward: 8.8500,                 loss: 0.0065
env0_second_0:                 episode reward: -8.8500,                 loss: nan
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 2545.65,                last time consumption/overall running time: 1353.9743s / 614044.2807 s
env0_first_0:                 episode reward: 7.3000,                 loss: 0.0068
env0_second_0:                 episode reward: -7.3000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 2475.6,                last time consumption/overall running time: 1276.4822s / 615320.7628 s
env0_first_0:                 episode reward: 7.3000,                 loss: 0.0075
env0_second_0:                 episode reward: -7.3000,                 loss: nan
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 2432.75,                last time consumption/overall running time: 1204.0841s / 616524.8469 s
env0_first_0:                 episode reward: 7.1500,                 loss: 0.0065
env0_second_0:                 episode reward: -7.1500,                 loss: nan
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 2311.6,                last time consumption/overall running time: 1139.6936s / 617664.5405 s
env0_first_0:                 episode reward: 7.3500,                 loss: 0.0068
env0_second_0:                 episode reward: -7.3500,                 loss: nan
env1_first_0:                 episode reward: 8.7500,                 loss: nan
env1_second_0:                 episode reward: -8.7500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 2474.9,                last time consumption/overall running time: 1214.1242s / 618878.6647 s
env0_first_0:                 episode reward: 9.1500,                 loss: 0.0069
env0_second_0:                 episode reward: -9.1500,                 loss: nan
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 2664.15,                last time consumption/overall running time: 1310.7226s / 620189.3873 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0064
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 2495.7,                last time consumption/overall running time: 1224.0122s / 621413.3995 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0066
env0_second_0:                 episode reward: -7.0500,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 3010.35,                last time consumption/overall running time: 1414.5010s / 622827.9004 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0061
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 2873.75,                last time consumption/overall running time: 1326.3291s / 624154.2295 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0061
env0_second_0:                 episode reward: -1.7000,                 loss: nan
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 2549.85,                last time consumption/overall running time: 1177.9630s / 625332.1925 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0066
env0_second_0:                 episode reward: -6.0500,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 2551.8,                last time consumption/overall running time: 1182.5009s / 626514.6934 s
env0_first_0:                 episode reward: 4.3000,                 loss: 0.0076
env0_second_0:                 episode reward: -4.3000,                 loss: nan
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 2504.35,                last time consumption/overall running time: 1156.9168s / 627671.6102 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0070
env0_second_0:                 episode reward: -7.6500,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 2511.0,                last time consumption/overall running time: 1159.4195s / 628831.0297 s
env0_first_0:                 episode reward: 6.1000,                 loss: 0.0073
env0_second_0:                 episode reward: -6.1000,                 loss: nan
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 2575.8,                last time consumption/overall running time: 1182.9886s / 630014.0183 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0069
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 2830.25,                last time consumption/overall running time: 1281.2648s / 631295.2831 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0076
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 2775.55,                last time consumption/overall running time: 1256.8783s / 632552.1614 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0083
env0_second_0:                 episode reward: -1.1000,                 loss: nan
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 2210.55,                last time consumption/overall running time: 1006.6341s / 633558.7955 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0078
env0_second_0:                 episode reward: -2.4000,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 2593.2,                last time consumption/overall running time: 1169.8142s / 634728.6097 s
env0_first_0:                 episode reward: 6.0500,                 loss: 0.0081
env0_second_0:                 episode reward: -6.0500,                 loss: nan
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 2540.95,                last time consumption/overall running time: 1149.2617s / 635877.8714 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0084
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 2203.4,                last time consumption/overall running time: 1006.7107s / 636884.5820 s
env0_first_0:                 episode reward: 8.9000,                 loss: 0.0071
env0_second_0:                 episode reward: -8.9000,                 loss: nan
env1_first_0:                 episode reward: 7.1500,                 loss: nan
env1_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 2300.7,                last time consumption/overall running time: 1052.9882s / 637937.5702 s
env0_first_0:                 episode reward: 5.5500,                 loss: 0.0064
env0_second_0:                 episode reward: -5.5500,                 loss: nan
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 2408.7,                last time consumption/overall running time: 1109.3863s / 639046.9565 s
env0_first_0:                 episode reward: 7.9000,                 loss: 0.0066
env0_second_0:                 episode reward: -7.9000,                 loss: nan
env1_first_0:                 episode reward: 8.6500,                 loss: nan
env1_second_0:                 episode reward: -8.6500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 2559.15,                last time consumption/overall running time: 1175.5519s / 640222.5084 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0066
env0_second_0:                 episode reward: -6.5500,                 loss: nan
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 2730.9,                last time consumption/overall running time: 1241.1848s / 641463.6932 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0080
env0_second_0:                 episode reward: -2.9500,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 2718.1,                last time consumption/overall running time: 1232.0618s / 642695.7550 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0073
env0_second_0:                 episode reward: -1.6000,                 loss: nan
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 2412.5,                last time consumption/overall running time: 1093.6749s / 643789.4299 s
env0_first_0:                 episode reward: 6.6000,                 loss: 0.0078
env0_second_0:                 episode reward: -6.6000,                 loss: nan
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 2683.15,                last time consumption/overall running time: 1208.8129s / 644998.2428 s
env0_first_0:                 episode reward: 7.1000,                 loss: 0.0080
env0_second_0:                 episode reward: -7.1000,                 loss: nan
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 2506.2,                last time consumption/overall running time: 1133.0137s / 646131.2565 s
env0_first_0:                 episode reward: 8.1000,                 loss: 0.0075
env0_second_0:                 episode reward: -8.1000,                 loss: nan
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 2462.6,                last time consumption/overall running time: 1110.9669s / 647242.2234 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0076
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 2517.45,                last time consumption/overall running time: 1132.0234s / 648374.2468 s
env0_first_0:                 episode reward: 7.0000,                 loss: 0.0075
env0_second_0:                 episode reward: -7.0000,                 loss: nan
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 2442.85,                last time consumption/overall running time: 1042.4056s / 649416.6524 s
env0_first_0:                 episode reward: 7.9500,                 loss: 0.0075
env0_second_0:                 episode reward: -7.9500,                 loss: nan
env1_first_0:                 episode reward: 7.6000,                 loss: nan
env1_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 2281.75,                last time consumption/overall running time: 982.7998s / 650399.4522 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0076
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 10.9000,                 loss: nan
env1_second_0:                 episode reward: -10.9000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 2702.25,                last time consumption/overall running time: 1162.4809s / 651561.9332 s
env0_first_0:                 episode reward: 7.1000,                 loss: 0.0074
env0_second_0:                 episode reward: -7.1000,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 2525.95,                last time consumption/overall running time: 1094.9922s / 652656.9253 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0074
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 2433.3,                last time consumption/overall running time: 1052.0658s / 653708.9911 s
env0_first_0:                 episode reward: 8.6000,                 loss: 0.0074
env0_second_0:                 episode reward: -8.6000,                 loss: nan
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 2670.2,                last time consumption/overall running time: 1138.1876s / 654847.1787 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0074
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 2668.0,                last time consumption/overall running time: 1129.6225s / 655976.8012 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0068
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 2382.95,                last time consumption/overall running time: 1006.8851s / 656983.6862 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0072
env0_second_0:                 episode reward: -5.2000,                 loss: nan
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 2547.55,                last time consumption/overall running time: 1075.6646s / 658059.3508 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0079
env0_second_0:                 episode reward: -6.5500,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 2388.75,                last time consumption/overall running time: 1007.6329s / 659066.9837 s
env0_first_0:                 episode reward: 8.2000,                 loss: 0.0071
env0_second_0:                 episode reward: -8.2000,                 loss: nan
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 2551.85,                last time consumption/overall running time: 1081.0212s / 660148.0049 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0078
env0_second_0:                 episode reward: -4.0000,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 2297.85,                last time consumption/overall running time: 977.2701s / 661125.2750 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0072
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 2522.15,                last time consumption/overall running time: 1092.9027s / 662218.1777 s
env0_first_0:                 episode reward: 7.5500,                 loss: 0.0071
env0_second_0:                 episode reward: -7.5500,                 loss: nan
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 2467.15,                last time consumption/overall running time: 1073.7534s / 663291.9311 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0081
env0_second_0:                 episode reward: -7.6500,                 loss: nan
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 2124.6,                last time consumption/overall running time: 917.2528s / 664209.1838 s
env0_first_0:                 episode reward: 8.3500,                 loss: 0.0080
env0_second_0:                 episode reward: -8.3500,                 loss: nan
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 1983.55,                last time consumption/overall running time: 858.8137s / 665067.9975 s
env0_first_0:                 episode reward: 8.8000,                 loss: 0.0073
env0_second_0:                 episode reward: -8.8000,                 loss: nan
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 2560.1,                last time consumption/overall running time: 1104.0709s / 666172.0684 s
env0_first_0:                 episode reward: 4.6500,                 loss: 0.0073
env0_second_0:                 episode reward: -4.6500,                 loss: nan
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 2186.45,                last time consumption/overall running time: 935.1431s / 667107.2116 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0087
env0_second_0:                 episode reward: -7.0500,                 loss: nan
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 2227.8,                last time consumption/overall running time: 949.1010s / 668056.3126 s
env0_first_0:                 episode reward: 2.9000,                 loss: 0.0091
env0_second_0:                 episode reward: -2.9000,                 loss: nan
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nanLoad pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
Load pong_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(6)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr = np.asanyarray(arr)

Episode: 9801/10000 (98.0100%),                 avg. length: 2265.2,                last time consumption/overall running time: 965.2536s / 669021.5662 s
env0_first_0:                 episode reward: 6.4500,                 loss: 0.0085
env0_second_0:                 episode reward: -6.4500,                 loss: nan
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 2609.25,                last time consumption/overall running time: 1112.7682s / 670134.3343 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0077
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 2031.2,                last time consumption/overall running time: 865.1105s / 670999.4449 s
env0_first_0:                 episode reward: 7.4000,                 loss: 0.0080
env0_second_0:                 episode reward: -7.4000,                 loss: nan
env1_first_0:                 episode reward: 9.9500,                 loss: nan
env1_second_0:                 episode reward: -9.9500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 2177.55,                last time consumption/overall running time: 926.9317s / 671926.3766 s
env0_first_0:                 episode reward: 7.5500,                 loss: 0.0083
env0_second_0:                 episode reward: -7.5500,                 loss: nan
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 2204.7,                last time consumption/overall running time: 952.6302s / 672879.0068 s
env0_first_0:                 episode reward: 8.7000,                 loss: 0.0073
env0_second_0:                 episode reward: -8.7000,                 loss: nan
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 2345.85,                last time consumption/overall running time: 1016.4576s / 673895.4645 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0072
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 2444.5,                last time consumption/overall running time: 1062.7697s / 674958.2342 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0078
env0_second_0:                 episode reward: -4.1000,                 loss: nan
env1_first_0:                 episode reward: 8.6500,                 loss: nan
env1_second_0:                 episode reward: -8.6500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 2634.95,                last time consumption/overall running time: 1133.7958s / 676092.0300 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0074
env0_second_0:                 episode reward: -2.3000,                 loss: nan
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 3009.0,                last time consumption/overall running time: 1286.0909s / 677378.1209 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0072
env0_second_0:                 episode reward: -1.4500,                 loss: nan
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 2561.5,                last time consumption/overall running time: 1087.5500s / 678465.6709 s
env0_first_0:                 episode reward: 7.1000,                 loss: 0.0069
env0_second_0:                 episode reward: -7.1000,                 loss: nan
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
