<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Exploitability Test &mdash; MARS 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Single-Agent Test" href="single_agent.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MARS
            <img src="../_static/mars_label.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/marl_background.html">MARL Backgrounds</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Install MARS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/quick_start.html">A Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/general.html">General Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/sarl.html">Single-Agent RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/marl.html">Multi-Agent RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/notes.html">Some Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MARS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mars/rl/agent.html">Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/rl/dqn.html">DQN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/rl/ppo.html">PPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/marl/meta_learner.html">MARL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/equilibrium_solver/solvers.html">Equilibrium Solvers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/data_struct.html">Data Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/func.html">Functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/env/env.html">Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Primal Experiments</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="single_agent.html">Single-Agent Test</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Exploitability Test</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#self-play">Self-Play</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#slimevolley-v0">SlimeVolley-v0</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pettingzoo-boxing-v1">PettingZoo Boxing-v1</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#nash-dqn-with-exploiter">Nash-DQN with Exploiter</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">SlimeVolley-v0</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#neural-fictitious-self-play">Neural Fictitious Self-Play</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">PettingZoo Boxing-v1</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lasertag-lasertag-small3-v0">LaserTag LaserTag-small3-v0</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">SlimeVolley-v0</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#neural-extensive-form-double-oracle-nxdo">Neural Extensive-Form Double Oracle (NXDO)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">SlimeVolley-v0</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MARS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Exploitability Test</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/experiments/exploit.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="exploitability-test">
<h1>Exploitability Test<a class="headerlink" href="#exploitability-test" title="Permalink to this headline">¶</a></h1>
<section id="self-play">
<h2>Self-Play<a class="headerlink" href="#self-play" title="Permalink to this headline">¶</a></h2>
<section id="slimevolley-v0">
<h3>SlimeVolley-v0<a class="headerlink" href="#slimevolley-v0" title="Permalink to this headline">¶</a></h3>
<p>We exploit the models trained with self-play using the exploiter model, which has the same basic settings (network architecture, optimizer, learning hyper-parameters, etc) as the trained ones. To demonstrate the improved exploitability via self-play, we compare two models: a ‘good’ model (trained for 5000 episodes via self-play, converge to some almost unexploitable policy) and a ‘bad’ model (trained for 500 episodes via self-play). For exploiting the two models, the exploiter is trained via RL setting for 2000 episodes. In experiments, we set the trained model as ‘first_0’ player and the exploiter as ‘second_0’ in <em>SlimeVolley-v0</em> environment.</p>
<p><strong>Exploit the Bad Model</strong>:</p>
<p float="left">
<img src="img/slimevolley_exploit/slimevolley_badmodel_reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_badmodel_length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_badmodel_loss.png" alt="drawing" width=230/>
</p>
**Exploit the Good Model**:
<p>Exploit for 2000 episodes:</p>
<p float="left">
<img src="img/slimevolley_exploit/slimevolley_goodmodel_reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_goodmodel_length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_goodmodel_loss.png" alt="drawing" width=230/>
</p>
<p>Exploit for 10000 episodes:</p>
<p float="left">
<img src="img/slimevolley_exploit/slimevolley_goodmodel_reward_10000.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_goodmodel_length_10000.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_goodmodel_loss_10000.png" alt="drawing" width=230/>
</p>
<p><strong>Observations</strong>:</p>
<ul class="simple">
<li><p>The good trained model is significantly harder to be exploited by the exploiter, compared with the bad model. It can be seen that for the good model, ‘second_0’ almost never reach &gt;0 reward value during the whole training process. Even if the it is exploited for longer time (from 2000 to 10000 episodes), it is still hard to be exploited.</p></li>
<li><p>At the initial training stage of the exploiter against the good model, the relatively high reward of the exploiter is probably caused by its atypical behavior due to the learning from scratch setting, while the good model in its lastest self-play stage is playing against an opponent also with good performance.</p></li>
<li><p>The large episode length in the learning of exploiter against the good model shows that the two players both reach some high-performance behaviors, but the exploiter can sill not exploit the good model with positive reward. For the bad model, the episode length is not that high because the bad model is so easier to exploit without the need of playing a full episode of the game.</p></li>
</ul>
<p><strong>Large Model</strong>:</p>
<p>Considering a large model, which has hidden nodes [1024, 1024, 1024, 1024] instead of [64, 64, 64] used in above tests, we also train the model to almost converge (a significant longer time for new tournament) with self-play. Then we exploit the large model with a small model ([64, 64, 64]) used before, the results look like:</p>
<p float="left">
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_False_DQN_20210726160205reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_False_DQN_20210726160205length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_False_DQN_20210726160205loss.png" alt="drawing" width=230/>
</p>
</section>
<section id="pettingzoo-boxing-v1">
<h3>PettingZoo Boxing-v1<a class="headerlink" href="#pettingzoo-boxing-v1" title="Permalink to this headline">¶</a></h3>
<p>We also exploit a ‘good’ model trained with self-play on PettingZoo <em>Boxing-v1</em> environment. The exploitation is conducted for long enough time (10000 episodes).</p>
<p><strong>Exploit the Good Model</strong>:</p>
<p float="left">
<img src="img/slimevolley_exploit/pettingzoo_boxing_goodmodel_reward_10000.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/pettingzoo_boxing_goodmodel_length_10000.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/pettingzoo_boxing_goodmodel_loss_10000.png" alt="drawing" width=230/>
</p>
<p>We see from the above images, the good model is also easily to be exploited in this environment.</p>
</section>
</section>
<section id="nash-dqn-with-exploiter">
<h2>Nash-DQN with Exploiter<a class="headerlink" href="#nash-dqn-with-exploiter" title="Permalink to this headline">¶</a></h2>
<section id="id1">
<h3>SlimeVolley-v0<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>The bad model is trained for 1000 episodes and the good model is trained for 3500 episodes, both with 5 parallel environments.</p>
<p><strong>Exploit the Bad Model</strong>:</p>
<p float="left">
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nash_NashDQNExploiter_20210723162318reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nash_NashDQNExploiter_20210723162318length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nash_NashDQNExploiter_20210723162318loss.png" alt="drawing" width=230/>
</p>
<p><strong>Exploit the Good Model</strong>:</p>
<p float="left">
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nash_NashDQNExploiter_20210911143314reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nash_NashDQNExploiter_20210911143314length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nash_NashDQNExploiter_20210911143314loss.png" alt="drawing" width=230/>
</p>
</section>
</section>
<section id="neural-fictitious-self-play">
<h2>Neural Fictitious Self-Play<a class="headerlink" href="#neural-fictitious-self-play" title="Permalink to this headline">¶</a></h2>
<section id="id2">
<h3>PettingZoo Boxing-v1<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p><strong>Exploit the Bad Model (Average Policy)</strong>:</p>
<p>The bad model is not trained.</p>
<p float="left">
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823163305reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823163305length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823163305loss.png" alt="drawing" width=230/>
</p>
<p><strong>Exploit the Good Model (Average Policy)</strong>:</p>
<p>The good model is trained for 5000 episodes. Average policy is the average historical best response policy learned with SL.</p>
<p float="left">
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823163222reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823163222length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823163222loss.png" alt="drawing" width=230/>
</p>
<p><strong>Exploit the Good Model (Best Response Policy)</strong>:</p>
<p>The good model is trained for 5000 episodes. Best response policy is the final best response policy learned with RL.</p>
<p float="left">
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823195838reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823195838length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/pettingzoo_boxing_v1_nfsp_NFSP_20210823195838loss.png" alt="drawing" width=230/>
</p>
</section>
<section id="lasertag-lasertag-small3-v0">
<h3>LaserTag LaserTag-small3-v0<a class="headerlink" href="#lasertag-lasertag-small3-v0" title="Permalink to this headline">¶</a></h3>
<p>LaserTag environments have a fixed time horizon of 1000 timesteps. The environments are actually not zero-sum games, but general-sum ones. The reward for each timestep can be np.array([0, 0]) or np.array([1, 0]) or np.array([0, 1]), therefore not constant-sum (zero-sum). As shown in exploitation test, an improvement of the reward for one agent does not necessarily indicate an decrease of performance in another agent. Unlike the <a class="reference external" href="https://arxiv.org/abs/1603.01121">original paper</a>, which only evaluate the NFSP method on two-player zero-sum games, we also test it here on this general-sum game as done by <a class="reference external" href="https://github.com/younggyoseo/pytorch-nfsp">this repo</a>.</p>
<p>In NFSP training, both <code class="docutils literal notranslate"><span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">2</span></code> agents are learned with an individual NFSP agent. In exploitation,  <code class="docutils literal notranslate"><span class="pre">1</span></code> is the NFSP agent and <code class="docutils literal notranslate"><span class="pre">2</span></code> is the exploiter.</p>
<p><strong>Train NFSP</strong>:</p>
<p>Since the LaserTag environments have image observations, it typically requires a CNN backbone in either policy or value network. We tried two types of CNN architecture as following:</p>
<ol class="arabic simple">
<li><p>A standard CNN with channels [8, 8, 16], kernel sizes [4, 4, 4], strides [2, 1, 1], using ReLU hidden activation, and followed by a two-layer MLP of [32, 32]:</p></li>
</ol>
<p float="left">
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210820185940reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210820185940length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210820185940loss.png" alt="drawing" width=230/>
</p>
2. A CNN architecture borrowed from [IMPALA](https://arxiv.org/abs/1802.01561) with residual blocks inside, with the exact specification almost the same as [OpenAI baselines version](https://github.com/openai/baselines/blob/master/baselines/common/models.py), also followed by a two-layer MLP of [32, 32]:
<p float="left">
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210902205815reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210902205815length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210902205815loss.png" alt="drawing" width=230/>
</p>
<p>The following exploitation tests are conducted on the first model (i.e., with standard CNN).</p>
<p><strong>Exploit the Bad Model (Average Policy)</strong>:</p>
<p>The bad model is not trained.</p>
<p float="left">
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210824110514reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210824110514length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210824110514loss.png" alt="drawing" width=230/>
</p>
<p><strong>Exploit the Good Model (Average Policy)</strong>:</p>
<p>The good model is trained for 10000 episodes.</p>
<p float="left">
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210824134349reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210824134349length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210824134349loss.png" alt="drawing" width=230/>
</p>
**Exploit the Good Model (Best Response Policy)**:
<p>The good model is trained for 10000 episodes.</p>
<p float="left">
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210823205316reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210823205316length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/lasertag_LaserTag-small3-v0_nfsp_NFSP_20210823205316loss.png" alt="drawing" width=230/>
</p>
## Fictitious Self-Play
</section>
<section id="id3">
<h3>SlimeVolley-v0<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p><strong>Exploit the Good Model (Best Response Policy)</strong>:</p>
<p float="left">
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_fictitious_selfplay_DQN_20210919232841reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_fictitious_selfplay_DQN_20210919232841length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_fictitious_selfplay_DQN_20210919232841loss.png" alt="drawing" width=230/>
</p>
</section>
</section>
<section id="neural-extensive-form-double-oracle-nxdo">
<h2>Neural Extensive-Form Double Oracle (NXDO)<a class="headerlink" href="#neural-extensive-form-double-oracle-nxdo" title="Permalink to this headline">¶</a></h2>
<section id="id4">
<h3>SlimeVolley-v0<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p><strong>Exploit the Good Model (Best Response Policy)</strong>:</p>
<p float="left">
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nxdo_DQN_20210919232746reward.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nxdo_DQN_20210919232746length.png" alt="drawing" width=230/>
<img src="img/slimevolley_exploit/slimevolley_SlimeVolley-v0_nxdo_DQN_20210919232746loss.png" alt="drawing" width=230/>
</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="single_agent.html" class="btn btn-neutral float-left" title="Single-Agent Test" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Zihan Ding.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>