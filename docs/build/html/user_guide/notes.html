<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Some Notes &mdash; MARS 0.0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Agent" href="../mars/rl/agent.html" />
    <link rel="prev" title="Multi-Agent RL" href="marl.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> MARS
            <img src="../_static/mars_label.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/marl_background.html">MARL Backgrounds</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Install MARS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/install.html">Installation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">A Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="general.html">General Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="sarl.html">Single-Agent RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="marl.html">Multi-Agent RL</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Some Notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#environments">Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rl">RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#marl">MARL</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#self-play">Self-Play</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#configurations">Configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#testing">Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exploitation">Exploitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#model-zoo">Model Zoo</a></li>
<li class="toctree-l2"><a class="reference internal" href="#others">Others</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">MARS</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../mars/rl/agent.html">Agent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/rl/dqn.html">DQN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/rl/ppo.html">PPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/marl/meta_learner.html">MARL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/equilibrium_solver/solvers.html">Equilibrium Solvers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/data_struct.html">Data Structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/func.html">Functionality</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/utils/logger.html">Logger</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mars/env/env.html">Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Primal Experiments</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../experiments/single_agent.html">Single-Agent Test</a></li>
<li class="toctree-l1"><a class="reference internal" href="../experiments/exploit.html">Exploitability Test</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">MARS</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Some Notes</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/user_guide/notes.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="some-notes">
<h1>Some Notes<a class="headerlink" href="#some-notes" title="Permalink to this headline">¶</a></h1>
<section id="environments">
<h2>Environments<a class="headerlink" href="#environments" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>For SlimeVolley environments, if you want to train single agent against the baseline provided by the environment, you need to:</p>
<ol class="arabic simple">
<li><p>set <em>against_baseline</em> as <em>True</em> in either the yaml file or input arguments; then you can use it as the single-agent Gym environment.</p></li>
</ol>
</li>
<li><p>For SlimeVolley environments, if you want to train agents with self-play, you need to:</p>
<ol class="arabic simple">
<li><p>set <em>against_baseline</em> as <em>False</em> in either the yaml file or input arguments;</p></li>
<li><p>do not need to fix any model and just set the configuration yaml file to have the agents to be self-updated (using RL loss) as [‘marl_spec’][‘trainable_agent_idx’].</p></li>
</ol>
</li>
<li><p>Single- or multiple-agent support:</p>
<ul>
<li><p>SlimeVolley environments support both single-agent and two-agent games, by setting the <em>against_baseline</em> configuration to be <em>True</em> or <em>False</em> in either case;</p></li>
<li><p>Openai Gym environments only support single-agent games;</p></li>
<li><p>PettingZoo environments only support multiple-agent games, unless you can provide some agents as the opponents and set those models with <em>.fix()</em> then you can view the learnable agent to play in a single-agent game.</p></li>
</ul>
</li>
</ul>
</section>
<section id="rl">
<h2>RL<a class="headerlink" href="#rl" title="Permalink to this headline">¶</a></h2>
<p>The single-agent reinforcement learning is supported in MARS.</p>
<p>The list of supported algorithms includes: (list here)</p>
</section>
<section id="marl">
<h2>MARL<a class="headerlink" href="#marl" title="Permalink to this headline">¶</a></h2>
<section id="self-play">
<h3>Self-Play<a class="headerlink" href="#self-play" title="Permalink to this headline">¶</a></h3>
<p>We provide several algorithms, in either reinforcement learning (RL) or evolutionary strategy (ES), with self-play learning mechanism in multi-agent environments, including deep-Q networks (DQN), proximal policy optimization (PPO), genetic algorithm (GA), etc. Therefore, they can be generally classified as <strong>Self-Play + RL</strong> or <strong>Self-Play + ES</strong>.</p>
<ul>
<li><p>Self-Play + RL:</p>
<div class="highlight-tex notranslate"><div class="highlight"><pre><span></span>Champion List:
Initially, this list contains a random policy agent.

Environment:
At the beginning of each episode, load the most recent agent archived in the Champion List.
Set this agent to be the Opponent.

Agent:
Trains inside the Environment against the Opponent with our choice of RL method.
Once the performance exceeds some threshold, checkpoint the agent into the Champion List.
</pre></div>
</div>
<p>There are some requirements for an environment to be conveniently learned with self-play + RL method: (1) the environment needs to be symmetric for each agent, including their state space, action space, transition dynamics, random start, etc; (2) to conveniently apply the above mechanism for learning a single model controlling the two sides of the game, the perspectives for different agents needs to be the same, which means, the same model could be applied on each side of the game without modification. The <em>first</em> point is obviously satisfied in some games like Go, <em>SlimeVolley</em>, and most of the Atari games like <em>Boxing</em> and <em>Pong</em> , etc. The <em>second</em> point is not always available for most multi-agent game although it seems rather like a trivial implementation issue. For example, in all Atari games (OpenAI Gym or PettingZoo), there is only one perspective of observation for all agents in the game, which is the full view of the game (either image or RAM) and contains all information for both the current agent and its opponent. Thus, all agents have the same observation in Atari games by default, which makes the model lack of knowledge it is currently taking charge of. An <a class="reference external" href="https://github.com/PettingZoo-Team/PettingZoo/issues/423">issue</a> for reference is provided. The direct solution for making a game to  provide same observation perspective for each agent is to transform the perspectives of observation from all agents to the same one. If this is hard or infeasible in practice (e.g. transforming and recoloring the Atari games can take considerable efforts),  an alternative to achieve a similar effect is to add an indicator of the agent to its observation, which is a one-hot vector indicating which agent the sample is collected with, and use all samples to update the model.</p>
<p>If both the above two points are satisfied in an environment, we can simply learn one model to control each agent in a game. Moreover, samples from all agents will be symmetric for the model, therefore the model can and should learn from all those samples to maximize its learning efficiency. As an example, with DQN algorithm, we should put samples from all agents into the buffer of the model for it to learn from. Due to this reason, the implementation of self-play in our repository is different from <a class="reference external" href="https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo_selfplay.py">the previous one</a>. Since the perspective transformation is provide with in the <em>SlimeVolley</em> environment, it can use samples from only one agent to update the model.</p>
</li>
<li><p>Self-Play + GA:</p>
<div class="highlight-tex notranslate"><div class="highlight"><pre><span></span>Create a population of agents with random initial parameters.
Play a total of N games. For each game:
  Randomly choose two agents in the population and have them play against each other.
  If the game is tied, add a bit of noise to the second agent&#39;s parameters.
  Otherwise, replace the loser with a clone of the winner, and add a bit of noise to the clone.
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="configurations">
<h2>Configurations<a class="headerlink" href="#configurations" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>In files under the folder <code class="docutils literal notranslate"><span class="pre">mars_core/confs/</span></code>, the configuration entry with value <em>False</em> means it is intended to left empty, we do not use <em>None</em> since it is not properly recognized as a Python None type but a string type in our file reading process.</p></li>
<li><p>Training Configuration:</p>
<p>The overall training configurations can be specified through either (1) a <em>yaml</em> file or (2) using a parser for input arguments.</p>
<p>The configurations are classified linguistically into three sets:</p>
<p>(1) <code class="docutils literal notranslate"><span class="pre">env_args</span></code> contains arguments for specifying the environments, including the name and type of environments etc;</p>
<p>(2) <code class="docutils literal notranslate"><span class="pre">agent_args</span></code> contains arguments for specifying the learning agents, including the algorithm details etc;</p>
<p>(3) <code class="docutils literal notranslate"><span class="pre">train_args</span></code> contains arguments for specifying the training details, including network architectures, optimizers, etc.</p>
</li>
</ol>
<p>Default configurations:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">env_args</span></code>: (dict) # arguments for environments</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">env_name</span></code>: (str) None # name of the environment</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">env_type</span></code>: (str) None # type of the environment, one of [gym, pettingzoo, slimevolley, lasertag]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_envs</span></code>: (int) 1 # number of environments, &gt;1 when using parallel environment sampling</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ram</span></code>: (bool) True # whether using RAM observation (instead of using image observation)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">seed</span></code>: (int) 1122 # random seed</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">agent_args</span></code>: (dict) # arguments for specifying the learning agent</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">algorithm</span></code>: (str) DQN # the algorithm name, take ‘DQN’ as an example</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">algorithm_spec</span></code>: (dict) # algorithm specific hyper-parameters</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">episodic_update</span></code>: (bool) False # whether using episodic update or not, if not, take the update per timestep</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dueling</span></code>: (bool) False # whether using dueling networks in DQN</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">replay_buffer_size</span></code>: (int) 1e5 # size of experience replay buffer</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gamma</span></code>: (float) 0.99 # discount factor, range [0, 1]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">multi_step</span></code>: (int) 1 # whether using multi-step reward, i.e. TD(\lambda)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target_update_interval</span></code>: (int) 1000 # how many updates are skipped to update the target</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps_start</span></code>: (float) 1. # the \epsilon value at the start</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps_final</span></code>: (float) 0.01 # the \epsilon value at the end</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps_decay</span></code>: (int) 30000  # approximate episodes required to decay from \epsilon value at the start to the value at the end, this needs to be tuned for specific environment</p></li>
</ul>
</li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_args</span></code>: (dict) # arguments for the training/testing/exploiting process</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code>: (int) 32 # training batch size for off-policy algorithms, e.g. DQN</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_episodes</span></code>: (int) 10000 # maximal episodes for training</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_steps_per_episode</span></code>: (int) 10000 # maximal timesteps per episode</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_start_frame</span></code>: (int) 10000 # the number of timesteps skipped before training starts</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optimizer</span></code>: (str) adam # optimizer type, one of [adam, sgd]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>: (float) 1e-4 # learning rate for optimizers</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">device</span></code>: (str) gpu # training device, one of [gpu, cpu]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">update_itr</span></code>: (int) 1  # iterations of updates per frame, 0~inf; &lt;1 means several steps are skipped per update</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_avg_window</span></code>: (int) 20 # average window length in logging</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">log_interval</span></code>: (int) 20  # log print interval</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">render</span></code>: (bool) False # whether rendering the visualization window</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test</span></code>: (bool) False # test mode or not</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exploit</span></code>: (bool) False # exploit mode or not, used for exploiting a trained model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">load_model_idx</span></code>: (bool/str) False # the index of trained model, default format as ‘Timestamp/EpisodeForSavingModel’</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">load_model_full_path</span></code>: (bool/str) False # the complete path to locate the model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">net_architecture</span></code>: (dict) # network architecture</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_dim_list</span></code>: (list) [64, 64, 64]  # list of numbers of hidden units</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hidden_activation</span></code>: (bool/str) ReLU  # activation function for hidden layers, use torch.nn (in Sequential) style rather than torch.nn.functional (in forward)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_activation</span></code>: (bool/str) False # activation function for output layers, False means nan</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Note:</p>
<ul class="simple">
<li><p>Different algorithms will have different <code class="docutils literal notranslate"><span class="pre">algorithm_spec</span></code> entries, for example, PPO may use:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">algorithm_spec</span></code>: (dict)</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">episodic_update</span></code>: (bool) True  # as PPO is on-policy, it uses episodic update instead of update per timestep</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gamma</span></code>: (float) 0.99 # discount factor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lambda</span></code>: (float) 0.95 # hyper-parameter for GAE</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eps_clip</span></code>: (float) 0.2 # clipping factor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">K_epoch</span></code>: (int) 4  # epochs for each update</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GAE</span></code>: (bool) False  # generalized advantage estimation</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>The followings are required in the main script, for either training/testing/exploitation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils.func</span> <span class="kn">import</span> <span class="n">LoadYAML2Dict</span>
<span class="kn">from</span> <span class="nn">env.import_env</span> <span class="kn">import</span> <span class="n">make_env</span>
<span class="kn">from</span> <span class="nn">rollout</span> <span class="kn">import</span> <span class="n">rollout</span>
<span class="kn">from</span> <span class="nn">rl.algorithm</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</li>
<li><p>Typical usage for a single-agent game, e.g. <em>CartPole-v1 OpenAI Gym</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;PATH TO YAML&#39;</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mergeDefault</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

</pre></div>
</div>
</li>
<li><p>Typical usage for a two-agent game, e.g. <em>boxing-v1 PettingZoo</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;PATH TO YAML&#39;</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mergeDefault</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model1</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">model2</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="c1"># model1.fix()  # fix a model if you don&#39;t want it to learn</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

</pre></div>
</div>
</li>
</ul>
</section>
<section id="testing">
<h2>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>Typical usage for a single-agent game, e.g. <em>CartPole-v1 OpenAI Gym</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;PATH TO YAML&#39;</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="c1">## Change/specify some arguments if necessary</span>
<span class="n">args</span><span class="o">.</span><span class="n">test</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># the test mode will automatically fix all models</span>
<span class="n">args</span><span class="o">.</span><span class="n">render</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">args</span><span class="o">.</span><span class="n">load_model_full_path</span> <span class="o">=</span> <span class="s1">&#39;PATH TO THE TRAINED MODEL&#39;</span> <span class="c1"># or use args.load_model_idx</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Typical usage for a two-agent game, e.g. <em>boxing-v1 PettingZoo</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;PATH TO YAML&#39;</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="c1">## Change/specify some arguments if necessary</span>
<span class="n">args</span><span class="o">.</span><span class="n">test</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># the test mode will automatically fix all models</span>
<span class="n">args</span><span class="o">.</span><span class="n">render</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">args</span><span class="o">.</span><span class="n">load_model_full_path</span> <span class="o">=</span> <span class="s1">&#39;PATH TO THE TRAINED MODEL&#39;</span> <span class="c1"># or use args.load_model_idx</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">model1</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">model2</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">model1</span><span class="p">,</span> <span class="n">model2</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="exploitation">
<h2>Exploitation<a class="headerlink" href="#exploitation" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>When you use SlimeVolley environments and want to exploit a trained model in this type of environment, you need to set the <em>yaml</em> file with <em>against_baseline</em> as <em>False</em> and <em>exploit</em> as <em>True</em>, so that you can input two models to the <em>MultiAgent</em> object, one is the trained model you want to exploit, another one is the exploiter with whatever model you want to use. A typical example would be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">### Load configurations</span>
<span class="n">yaml_file</span> <span class="o">=</span> <span class="s1">&#39;PATH TO YAML&#39;</span>
<span class="n">args</span> <span class="o">=</span> <span class="n">LoadYAML2Dict</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">toAttr</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>

<span class="c1">## Change/specify some arguments if necessary</span>
<span class="n">args</span><span class="o">.</span><span class="n">against_baseline</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">args</span><span class="o">.</span><span class="n">exploit</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">args</span><span class="o">.</span><span class="n">load_model_full_path</span> <span class="o">=</span> <span class="s1">&#39;PATH TO THE TRAINED MODEL&#39;</span>

<span class="c1">### Create env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">make_env</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="c1">### Specify models for each agent</span>
<span class="n">trained_model</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">exploiter</span> <span class="o">=</span> <span class="n">DQN</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
<span class="n">trained_model</span><span class="o">.</span><span class="n">fix</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MultiAgent</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="p">[</span><span class="n">trained_model</span><span class="p">,</span> <span class="n">exploiter</span><span class="p">],</span> <span class="n">args</span><span class="p">)</span>

<span class="c1">### Rollout</span>
<span class="n">rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="model-zoo">
<h2>Model Zoo<a class="headerlink" href="#model-zoo" title="Permalink to this headline">¶</a></h2>
<p>​	We provide a zoo of trained agents using default <em>yaml</em> configuration files with either single-agent RL or self-play.</p>
</section>
<section id="others">
<h2>Others<a class="headerlink" href="#others" title="Permalink to this headline">¶</a></h2>
<p>There are a bunch of points worth of taking care of during usage, which are listed as below:</p>
<ul class="simple">
<li><p>For greedy-action selection when using stochastic policy (pseudo-policy), like in DQN or PPO, all agent should be <strong>not</strong> be greedy for training; all agents should be greedy for testing; the trained models should be greedy and the exploiters should <strong>not</strong> be greedy for exploitation.</p></li>
<li><p>In iterative best response algorithms, like self-play, fictitious self-play, NXDO, the <code class="docutils literal notranslate"><span class="pre">score_avg_window</span></code> and <code class="docutils literal notranslate"><span class="pre">selfplay_score_delta</span></code> affect the (minimal) interval for storing the model and updating the opponent thus the number of models in the league, the larger the values the fewer the models stored, so choose proper values for each environment to store a proper number of models.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="marl.html" class="btn btn-neutral float-left" title="Multi-Agent RL" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../mars/rl/agent.html" class="btn btn-neutral float-right" title="Agent" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Zihan Ding.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>