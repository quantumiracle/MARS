wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/MARS/wandb/run-20221007_045958-2gog0x3i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robosumo_RoboSumo-Spider-vs-Spider-v0_nash_ppo_202210070459
wandb: ‚≠êÔ∏è View project at https://wandb.ai/quantumiracle/Pettingzoo_MARS
wandb: üöÄ View run at https://wandb.ai/quantumiracle/Pettingzoo_MARS/runs/2gog0x3i
robosumo_RoboSumo-Spider-vs-Spider-v0
default:  {'env_name': 'RoboSumo-Spider-vs-Spider-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': False, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': False, 'wandb_entity': '', 'wandb_project': '', 'wandb_group': '', 'wandb_name': '', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
{'env_name': 'RoboSumo-Spider-vs-Spider-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': '', 'wandb_group': '', 'wandb_name': '', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
args:  {'env_name': 'RoboSumo-Spider-vs-Spider-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': 'Pettingzoo_MARS', 'wandb_group': '202210070459', 'wandb_name': 'robosumo_RoboSumo-Spider-vs-Spider-v0_nash_ppo_202210070459', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
RoboSumo-Spider-vs-Spider-v0 robosumo
record video: interval 1000, length 300
Load RoboSumo-Spider-vs-Spider-v0 environment in type robosumo.
Env observation space: Box(-inf, inf, (208,), float32) action space: Box(-1.0, 1.0, (16,), float32)
random seed: 526
<RecordVideo<mars.env.wrappers.mars_wrappers.RoboSumoWrapper object at 0x7f50f42b52e8>>
gaussian_policy 16 Box(-1.0, 1.0, (16,), float32)
[MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=208, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=208, bias=True)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=32, bias=True)
    (2): Tanh()
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=32, bias=True)
    (2): Tanh()
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)] MLP(
  (body): Sequential(
    (0): Linear(in_features=416, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)
gaussian_policy 16 Box(-1.0, 1.0, (16,), float32)
[MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=208, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=208, bias=True)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=32, bias=True)
    (2): Tanh()
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=32, bias=True)
    (2): Tanh()
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=208, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)] MLP(
  (body): Sequential(
    (0): Linear(in_features=416, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)
gaussian_policy 16 Box(-1.0, 1.0, (16,), float32)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'RoboSumo-Spider-vs-Spider-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': 'Pettingzoo_MARS', 'wandb_group': '202210070459', 'wandb_name': 'robosumo_RoboSumo-Spider-vs-Spider-v0_nash_ppo_202210070459', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
Save models to : /data/zihan/research/MARS/data/model/202210070459/robosumo_RoboSumo-Spider-vs-Spider-v0_nash_ppo. 
 Save logs to: /data/zihan/research/MARS/data/log/202210070459/robosumo_RoboSumo-Spider-vs-Spider-v0_nash_ppo.
Episode: 1/50000 (0.0020%),                 avg. length: 26.0,                last time consumption/overall running time: 0.28s / 0.28 s
first_0:                 episode reward: 1888.2949,                 loss: 1186063.9453
second_0:                 episode reward: -2150.9136,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 34.3,                last time consumption/overall running time: 5.38s / 5.66 s
first_0:                 episode reward: -778.1144,                 loss: 1181319.7363
second_0:                 episode reward: 446.4170,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 45.45,                last time consumption/overall running time: 6.35s / 12.00 s
first_0:                 episode reward: -106.4890,                 loss: 1001399.0030
second_0:                 episode reward: -315.6172,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 33.75,                last time consumption/overall running time: 5.20s / 17.21 s
first_0:                 episode reward: 536.3809,                 loss: 974843.5231
second_0:                 episode reward: -892.1115,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 31.75,                last time consumption/overall running time: 5.06s / 22.26 s
first_0:                 episode reward: -550.1019,                 loss: 1095500.2912
second_0:                 episode reward: 244.5745,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 36.4,                last time consumption/overall running time: 5.33s / 27.60 s
first_0:                 episode reward: 14.0357,                 loss: 938328.4494
second_0:                 episode reward: -411.9334,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 27.8,                last time consumption/overall running time: 4.69s / 32.29 s
first_0:                 episode reward: -338.4178,                 loss: 1220582.3725
second_0:                 episode reward: 44.3797,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 29.45,                last time consumption/overall running time: 4.73s / 37.01 s
first_0:                 episode reward: 854.3474,                 loss: 1218302.8532
second_0:                 episode reward: -1183.1256,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 32.3,                last time consumption/overall running time: 5.20s / 42.21 s
first_0:                 episode reward: -566.4221,                 loss: 1067135.3384
second_0:                 episode reward: 200.0573,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 44.6,                last time consumption/overall running time: 6.24s / 48.45 s
first_0:                 episode reward: 64.9593,                 loss: 1008419.4302
second_0:                 episode reward: -579.3364,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 39.65,                last time consumption/overall running time: 5.73s / 54.19 s
first_0:                 episode reward: -237.1516,                 loss: 1080479.3548
second_0:                 episode reward: -246.1877,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 31.65,                last time consumption/overall running time: 5.10s / 59.29 s
first_0:                 episode reward: -368.3619,                 loss: 1199184.1762
second_0:                 episode reward: -3.8095,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 71.05,                last time consumption/overall running time: 8.86s / 68.15 s
first_0:                 episode reward: -393.1946,                 loss: 815399.8596
second_0:                 episode reward: -486.0003,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 56.15,                last time consumption/overall running time: 7.33s / 75.48 s
first_0:                 episode reward: -323.8532,                 loss: 1128985.6603
second_0:                 episode reward: -349.2761,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 44.15,                last time consumption/overall running time: 6.18s / 81.66 s
first_0:                 episode reward: -280.0968,                 loss: 893956.7672
second_0:                 episode reward: -319.1206,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 52.8,                last time consumption/overall running time: 7.80s / 89.46 s
first_0:                 episode reward: 202.6299,                 loss: 895853.4714
second_0:                 episode reward: -895.3337,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 63.9,                last time consumption/overall running time: 8.51s / 97.97 s
first_0:                 episode reward: -353.4355,                 loss: 846834.9018
second_0:                 episode reward: -566.3409,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 47.6,                last time consumption/overall running time: 6.32s / 104.29 s
first_0:                 episode reward: 391.2933,                 loss: 1028591.6328
second_0:                 episode reward: -1087.4796,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 44.75,                last time consumption/overall running time: 6.24s / 110.52 s
first_0:                 episode reward: -114.0559,                 loss: 1051910.3085
second_0:                 episode reward: -549.6954,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 54.1,                last time consumption/overall running time: 7.35s / 117.88 s
first_0:                 episode reward: -56.0291,                 loss: 821450.1890
second_0:                 episode reward: -729.2926,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 49.7,                last time consumption/overall running time: 6.66s / 124.54 s
first_0:                 episode reward: -34.3868,                 loss: 1087027.6290
second_0:                 episode reward: -732.7990,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 34.3,                last time consumption/overall running time: 5.21s / 129.75 s
first_0:                 episode reward: 379.3077,                 loss: 1067660.1632
second_0:                 episode reward: -908.3167,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 95.05,                last time consumption/overall running time: 11.25s / 141.00 s
first_0:                 episode reward: -695.8336,                 loss: 715128.9679
second_0:                 episode reward: -793.2073,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 66.65,                last time consumption/overall running time: 8.43s / 149.43 s
first_0:                 episode reward: -22.1498,                 loss: 1013643.4232
second_0:                 episode reward: -978.3781,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 40.7,                last time consumption/overall running time: 5.82s / 155.25 s
first_0:                 episode reward: -785.8910,                 loss: 908054.0531
second_0:                 episode reward: 151.9437,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 47.35,                last time consumption/overall running time: 6.60s / 161.86 s
first_0:                 episode reward: -1040.7354,                 loss: 1089932.1660
second_0:                 episode reward: 318.7501,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 64.65,                last time consumption/overall running time: 8.44s / 170.30 s
first_0:                 episode reward: -694.3486,                 loss: 809659.0422
second_0:                 episode reward: -339.0357,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 40.35,                last time consumption/overall running time: 5.91s / 176.21 s
first_0:                 episode reward: 602.8051,                 loss: 1072266.8070
second_0:                 episode reward: -1250.6552,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 48.5,                last time consumption/overall running time: 6.88s / 183.09 s
first_0:                 episode reward: -344.9610,                 loss: 976113.1459
second_0:                 episode reward: -439.0946,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 39.9,                last time consumption/overall running time: 5.74s / 188.82 s
first_0:                 episode reward: 320.3111,                 loss: 935779.6730
second_0:                 episode reward: -963.9765,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 72.25,                last time consumption/overall running time: 9.09s / 197.91 s
first_0:                 episode reward: -996.1484,                 loss: 909368.5657
second_0:                 episode reward: -214.1384,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 86.45,                last time consumption/overall running time: 10.39s / 208.30 s
first_0:                 episode reward: -315.1371,                 loss: 696074.7502
second_0:                 episode reward: -982.0572,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 41.95,                last time consumption/overall running time: 6.03s / 214.32 s
first_0:                 episode reward: 90.1676,                 loss: 977722.2859
second_0:                 episode reward: -769.8755,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 65.95,                last time consumption/overall running time: 8.31s / 222.63 s
first_0:                 episode reward: -792.2248,                 loss: 922063.2459
second_0:                 episode reward: -260.0197,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 67.4,                last time consumption/overall running time: 8.61s / 231.24 s
first_0:                 episode reward: 67.3011,                 loss: 800489.1625
second_0:                 episode reward: -1177.4903,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 73.55,                last time consumption/overall running time: 9.25s / 240.49 s
first_0:                 episode reward: 494.4978,                 loss: 762267.5182
second_0:                 episode reward: -1653.9654,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 60.7,                last time consumption/overall running time: 7.97s / 248.45 s
first_0:                 episode reward: 272.3609,                 loss: 732788.5586
second_0:                 episode reward: -1215.1022,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 85.95,                last time consumption/overall running time: 10.53s / 258.98 s
first_0:                 episode reward: -383.9163,                 loss: 674703.4976
second_0:                 episode reward: -950.5285,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 75.95,                last time consumption/overall running time: 9.48s / 268.46 s
first_0:                 episode reward: -242.6814,                 loss: 861159.6570
second_0:                 episode reward: -946.7247,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 67.2,                last time consumption/overall running time: 8.59s / 277.06 s
first_0:                 episode reward: -601.5400,                 loss: 838674.0263
second_0:                 episode reward: -479.2162,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 58.9,                last time consumption/overall running time: 7.64s / 284.70 s
first_0:                 episode reward: -655.7857,                 loss: 843327.4069
second_0:                 episode reward: -311.6003,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 76.45,                last time consumption/overall running time: 9.40s / 294.11 s
first_0:                 episode reward: -478.4687,                 loss: 926668.1601
second_0:                 episode reward: -718.8672,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 56.55,                last time consumption/overall running time: 7.60s / 301.70 s
first_0:                 episode reward: 194.0624,                 loss: 964762.7792
second_0:                 episode reward: -1123.7628,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 71.5,                last time consumption/overall running time: 8.94s / 310.64 s
first_0:                 episode reward: 679.1705,                 loss: 863887.5998
second_0:                 episode reward: -1837.7380,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 42.6,                last time consumption/overall running time: 6.16s / 316.79 s
first_0:                 episode reward: 292.7677,                 loss: 960343.9990
second_0:                 episode reward: -973.4181,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 54.35,                last time consumption/overall running time: 7.23s / 324.02 s
first_0:                 episode reward: 97.9263,                 loss: 887331.0248
second_0:                 episode reward: -980.1296,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 52.75,                last time consumption/overall running time: 7.06s / 331.08 s
first_0:                 episode reward: -1040.1147,                 loss: 846562.9873