pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 3}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0526/pettingzoo_boxing_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0526/pettingzoo_boxing_v1_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 69.4915s / 69.4915 s
env0_first_0:                 episode reward: 7.0000,                 loss: 0.0093
env0_second_0:                 episode reward: -7.0000,                 loss: 0.0103
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 1732.3071s / 1801.7987 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0216
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0212
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2076.9714s / 3878.7701 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0248
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0252
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2167.4035s / 6046.1735 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0232
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0225
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2210.5538s / 8256.7274 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0245
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0246
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2209.1611s / 10465.8885 s
env0_first_0:                 episode reward: -8.8500,                 loss: 0.0287
env0_second_0:                 episode reward: 8.8500,                 loss: 0.0280
env1_first_0:                 episode reward: -6.8000,                 loss: nan
env1_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2211.9448s / 12677.8333 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0419
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0391
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2215.6960s / 14893.5294 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0376
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0349
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2207.9891s / 17101.5185 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0311
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0310
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2202.8025s / 19304.3210 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0243
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0212
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2201.5421s / 21505.8631 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0200
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0179
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2213.5867s / 23719.4497 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0214
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0196
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2203.9018s / 25923.3515 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0203
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0204
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 2211.6967s / 28135.0483 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0329
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0329
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1779.8,                last time consumption/overall running time: 2203.0133s / 30338.0616 s
env0_first_0:                 episode reward: -19.7500,                 loss: 0.0519
env0_second_0:                 episode reward: 19.7500,                 loss: 0.0489
env1_first_0:                 episode reward: -24.0500,                 loss: nan
env1_second_0:                 episode reward: 24.0500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1555.55,                last time consumption/overall running time: 1937.3282s / 32275.3898 s
env0_first_0:                 episode reward: -41.5000,                 loss: 0.0715
env0_second_0:                 episode reward: 41.5000,                 loss: 0.0644
env1_first_0:                 episode reward: -41.3500,                 loss: nan
env1_second_0:                 episode reward: 41.3500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1069.35,                last time consumption/overall running time: 1334.4998s / 33609.8896 s
env0_first_0:                 episode reward: -65.6000,                 loss: 0.0939
env0_second_0:                 episode reward: 65.6000,                 loss: 0.0827
env1_first_0:                 episode reward: -63.3500,                 loss: nan
env1_second_0:                 episode reward: 63.3500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1070.7,                last time consumption/overall running time: 1358.0252s / 34967.9147 s
env0_first_0:                 episode reward: -56.3000,                 loss: 0.1252
env0_second_0:                 episode reward: 56.3000,                 loss: 0.1038
env1_first_0:                 episode reward: -72.5500,                 loss: nan
env1_second_0:                 episode reward: 72.5500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 650.7,                last time consumption/overall running time: 810.3824s / 35778.2972 s
env0_first_0:                 episode reward: -77.2500,                 loss: 0.1483
env0_second_0:                 episode reward: 77.2500,                 loss: 0.1272
env1_first_0:                 episode reward: -70.2500,                 loss: nan
env1_second_0:                 episode reward: 70.2500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 663.05,                last time consumption/overall running time: 826.5415s / 36604.8387 s
env0_first_0:                 episode reward: -77.8500,                 loss: 0.1787
env0_second_0:                 episode reward: 77.8500,                 loss: 0.1612
env1_first_0:                 episode reward: -66.8500,                 loss: nan
env1_second_0:                 episode reward: 66.8500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 655.75,                last time consumption/overall running time: 869.4178s / 37474.2565 s
env0_first_0:                 episode reward: -72.7000,                 loss: 0.2299
env0_second_0:                 episode reward: 72.7000,                 loss: 0.2025
env1_first_0:                 episode reward: -69.9000,                 loss: nan
env1_second_0:                 episode reward: 69.9000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 358.65,                last time consumption/overall running time: 527.7998s / 38002.0562 s
env0_first_0:                 episode reward: -77.1000,                 loss: 0.2669
env0_second_0:                 episode reward: 77.1000,                 loss: 0.2362
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 540.2,                last time consumption/overall running time: 796.5577s / 38798.6139 s
env0_first_0:                 episode reward: -77.8500,                 loss: 0.3088
env0_second_0:                 episode reward: 77.8500,                 loss: 0.2701
env1_first_0:                 episode reward: -78.9500,                 loss: nan
env1_second_0:                 episode reward: 78.9500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 479.45,                last time consumption/overall running time: 712.1986s / 39510.8126 s
env0_first_0:                 episode reward: -82.1500,                 loss: 0.3409
env0_second_0:                 episode reward: 82.1500,                 loss: 0.2856
env1_first_0:                 episode reward: -73.3500,                 loss: nan
env1_second_0:                 episode reward: 73.3500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 299.3,                last time consumption/overall running time: 443.6070s / 39954.4196 s
env0_first_0:                 episode reward: -85.7500,                 loss: 0.3582
env0_second_0:                 episode reward: 85.7500,                 loss: 0.2797
env1_first_0:                 episode reward: -78.9500,                 loss: nan
env1_second_0:                 episode reward: 78.9500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 277.7,                last time consumption/overall running time: 411.8934s / 40366.3130 s
env0_first_0:                 episode reward: -81.9000,                 loss: 0.3922
env0_second_0:                 episode reward: 81.9000,                 loss: 0.3039
env1_first_0:                 episode reward: -83.4000,                 loss: nan
env1_second_0:                 episode reward: 83.4000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 300.95,                last time consumption/overall running time: 447.1151s / 40813.4280 s
env0_first_0:                 episode reward: -82.7000,                 loss: 0.3608
env0_second_0:                 episode reward: 82.7000,                 loss: 0.2904
env1_first_0:                 episode reward: -89.9500,                 loss: nan
env1_second_0:                 episode reward: 89.9500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 348.4,                last time consumption/overall running time: 520.6528s / 41334.0808 s
env0_first_0:                 episode reward: -73.0500,                 loss: 0.4385
env0_second_0:                 episode reward: 73.0500,                 loss: 0.2995
env1_first_0:                 episode reward: -91.7000,                 loss: nan
env1_second_0:                 episode reward: 91.7000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 360.2,                last time consumption/overall running time: 538.9602s / 41873.0410 s
env0_first_0:                 episode reward: -81.1500,                 loss: 0.4643
env0_second_0:                 episode reward: 81.1500,                 loss: 0.3141
env1_first_0:                 episode reward: -83.9000,                 loss: nan
env1_second_0:                 episode reward: 83.9000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 320.0,                last time consumption/overall running time: 482.6801s / 42355.7211 s
env0_first_0:                 episode reward: -79.0500,                 loss: 0.4600
env0_second_0:                 episode reward: 79.0500,                 loss: 0.3028
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 292.85,                last time consumption/overall running time: 433.0617s / 42788.7828 s
env0_first_0:                 episode reward: -88.1500,                 loss: 0.5000
env0_second_0:                 episode reward: 88.1500,                 loss: 0.3278
env1_first_0:                 episode reward: -69.5000,                 loss: nan
env1_second_0:                 episode reward: 69.5000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 263.15,                last time consumption/overall running time: 392.9687s / 43181.7516 s
env0_first_0:                 episode reward: -77.4500,                 loss: 0.5266
env0_second_0:                 episode reward: 77.4500,                 loss: 0.3434
env1_first_0:                 episode reward: -86.7000,                 loss: nan
env1_second_0:                 episode reward: 86.7000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 290.05,                last time consumption/overall running time: 434.7310s / 43616.4826 s
env0_first_0:                 episode reward: -80.5500,                 loss: 0.5559
env0_second_0:                 episode reward: 80.5500,                 loss: 0.3548
env1_first_0:                 episode reward: -84.1500,                 loss: nan
env1_second_0:                 episode reward: 84.1500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 269.45,                last time consumption/overall running time: 397.1398s / 44013.6223 s
env0_first_0:                 episode reward: -75.5500,                 loss: 0.5820
env0_second_0:                 episode reward: 75.5500,                 loss: 0.3806
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 309.35,                last time consumption/overall running time: 456.2650s / 44469.8874 s
env0_first_0:                 episode reward: -80.4000,                 loss: 0.6211
env0_second_0:                 episode reward: 80.4000,                 loss: 0.3831
env1_first_0:                 episode reward: -68.8500,                 loss: nan
env1_second_0:                 episode reward: 68.8500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 259.6,                last time consumption/overall running time: 385.4256s / 44855.3129 s
env0_first_0:                 episode reward: -80.3500,                 loss: 0.6746
env0_second_0:                 episode reward: 80.3500,                 loss: 0.4362
env1_first_0:                 episode reward: -89.0000,                 loss: nan
env1_second_0:                 episode reward: 89.0000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 249.55,                last time consumption/overall running time: 372.2430s / 45227.5559 s
env0_first_0:                 episode reward: -84.7000,                 loss: 0.6438
env0_second_0:                 episode reward: 84.7000,                 loss: 0.4108
env1_first_0:                 episode reward: -82.6000,                 loss: nan
env1_second_0:                 episode reward: 82.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 243.95,                last time consumption/overall running time: 363.3620s / 45590.9180 s
env0_first_0:                 episode reward: -97.6000,                 loss: 0.6405
env0_second_0:                 episode reward: 97.6000,                 loss: 0.4540
env1_first_0:                 episode reward: -82.7000,                 loss: nan
env1_second_0:                 episode reward: 82.7000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 240.6,                last time consumption/overall running time: 355.4797s / 45946.3977 s
env0_first_0:                 episode reward: -89.0500,                 loss: 0.6034
env0_second_0:                 episode reward: 89.0500,                 loss: 0.4280
env1_first_0:                 episode reward: -90.5000,                 loss: nan
env1_second_0:                 episode reward: 90.5000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 261.7,                last time consumption/overall running time: 391.4736s / 46337.8713 s
env0_first_0:                 episode reward: -83.8500,                 loss: 0.6535
env0_second_0:                 episode reward: 83.8500,                 loss: 0.4483
env1_first_0:                 episode reward: -87.8000,                 loss: nan
env1_second_0:                 episode reward: 87.8000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 267.75,                last time consumption/overall running time: 399.2240s / 46737.0953 s
env0_first_0:                 episode reward: -84.9000,                 loss: 0.6177
env0_second_0:                 episode reward: 84.9000,                 loss: 0.4359
env1_first_0:                 episode reward: -81.3500,                 loss: nan
env1_second_0:                 episode reward: 81.3500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 254.7,                last time consumption/overall running time: 378.2539s / 47115.3492 s
env0_first_0:                 episode reward: -84.7000,                 loss: 0.6321
env0_second_0:                 episode reward: 84.7000,                 loss: 0.4889
env1_first_0:                 episode reward: -89.8500,                 loss: nan
env1_second_0:                 episode reward: 89.8500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 239.75,                last time consumption/overall running time: 354.0585s / 47469.4078 s
env0_first_0:                 episode reward: -90.8000,                 loss: 0.6079
env0_second_0:                 episode reward: 90.8000,                 loss: 0.4788
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 235.0,                last time consumption/overall running time: 349.7827s / 47819.1904 s
env0_first_0:                 episode reward: -87.8000,                 loss: 0.6029
env0_second_0:                 episode reward: 87.8000,                 loss: 0.5024
env1_first_0:                 episode reward: -89.5000,                 loss: nan
env1_second_0:                 episode reward: 89.5000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 234.5,                last time consumption/overall running time: 349.8981s / 48169.0885 s
env0_first_0:                 episode reward: -91.7500,                 loss: 0.5813
env0_second_0:                 episode reward: 91.7500,                 loss: 0.4717
env1_first_0:                 episode reward: -92.3500,                 loss: nan
env1_second_0:                 episode reward: 92.3500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 259.7,                last time consumption/overall running time: 383.3918s / 48552.4803 s
env0_first_0:                 episode reward: -88.3000,                 loss: 0.6335
env0_second_0:                 episode reward: 88.3000,                 loss: 0.4833
env1_first_0:                 episode reward: -84.0500,                 loss: nan
env1_second_0:                 episode reward: 84.0500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 282.35,                last time consumption/overall running time: 420.5029s / 48972.9832 s
env0_first_0:                 episode reward: -87.1000,                 loss: 0.6415
env0_second_0:                 episode reward: 87.1000,                 loss: 0.5129
env1_first_0:                 episode reward: -80.6000,                 loss: nan
env1_second_0:                 episode reward: 80.6000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 271.5,                last time consumption/overall running time: 402.1053s / 49375.0885 s
env0_first_0:                 episode reward: -80.6500,                 loss: 0.6664
env0_second_0:                 episode reward: 80.6500,                 loss: 0.5596
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 274.55,                last time consumption/overall running time: 401.8546s / 49776.9432 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.6756
env0_second_0:                 episode reward: 86.4000,                 loss: 0.6131
env1_first_0:                 episode reward: -93.0500,                 loss: nan
env1_second_0:                 episode reward: 93.0500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 235.25,                last time consumption/overall running time: 349.9289s / 50126.8720 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.7206
env0_second_0:                 episode reward: 93.0500,                 loss: 0.6269
env1_first_0:                 episode reward: -91.8000,                 loss: nan
env1_second_0:                 episode reward: 91.8000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 234.85,                last time consumption/overall running time: 346.2486s / 50473.1206 s
env0_first_0:                 episode reward: -87.6500,                 loss: 0.6664
env0_second_0:                 episode reward: 87.6500,                 loss: 0.5731
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 232.2,                last time consumption/overall running time: 344.8634s / 50817.9840 s
env0_first_0:                 episode reward: -92.6000,                 loss: 0.6672
env0_second_0:                 episode reward: 92.6000,                 loss: 0.6173
env1_first_0:                 episode reward: -87.5000,                 loss: nan
env1_second_0:                 episode reward: 87.5000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 240.75,                last time consumption/overall running time: 361.0815s / 51179.0655 s
env0_first_0:                 episode reward: -89.4500,                 loss: 0.6870
env0_second_0:                 episode reward: 89.4500,                 loss: 0.5651
env1_first_0:                 episode reward: -87.3500,                 loss: nan
env1_second_0:                 episode reward: 87.3500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 243.85,                last time consumption/overall running time: 362.3378s / 51541.4033 s
env0_first_0:                 episode reward: -86.3500,                 loss: 0.6607
env0_second_0:                 episode reward: 86.3500,                 loss: 0.6573
env1_first_0:                 episode reward: -95.4500,                 loss: nan
env1_second_0:                 episode reward: 95.4500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 238.7,                last time consumption/overall running time: 359.0137s / 51900.4170 s
env0_first_0:                 episode reward: -93.4500,                 loss: 0.6689
env0_second_0:                 episode reward: 93.4500,                 loss: 0.6267
env1_first_0:                 episode reward: -88.9000,                 loss: nan
env1_second_0:                 episode reward: 88.9000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 284.7,                last time consumption/overall running time: 423.5584s / 52323.9754 s
env0_first_0:                 episode reward: -82.8500,                 loss: 0.6530
env0_second_0:                 episode reward: 82.8500,                 loss: 0.7284
env1_first_0:                 episode reward: -81.3000,                 loss: nan
env1_second_0:                 episode reward: 81.3000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 245.85,                last time consumption/overall running time: 368.5174s / 52692.4928 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.6673
env0_second_0:                 episode reward: 85.4000,                 loss: 0.6988
env1_first_0:                 episode reward: -92.0000,                 loss: nan
env1_second_0:                 episode reward: 92.0000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 233.3,                last time consumption/overall running time: 347.1750s / 53039.6678 s
env0_first_0:                 episode reward: -94.1500,                 loss: 0.6209
env0_second_0:                 episode reward: 94.1500,                 loss: 0.7875
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 225.55,                last time consumption/overall running time: 333.9433s / 53373.6110 s
env0_first_0:                 episode reward: -94.5000,                 loss: 0.5945
env0_second_0:                 episode reward: 94.5000,                 loss: 0.7202
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 235.05,                last time consumption/overall running time: 349.5896s / 53723.2006 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.5819
env0_second_0:                 episode reward: 93.2000,                 loss: 0.7462
env1_first_0:                 episode reward: -92.1000,                 loss: nan
env1_second_0:                 episode reward: 92.1000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 229.85,                last time consumption/overall running time: 338.6566s / 54061.8572 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.5872
env0_second_0:                 episode reward: 92.6500,                 loss: 0.6980
env1_first_0:                 episode reward: -87.9500,                 loss: nan
env1_second_0:                 episode reward: 87.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 230.7,                last time consumption/overall running time: 344.7076s / 54406.5648 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.6261
env0_second_0:                 episode reward: 89.6000,                 loss: 0.7588
env1_first_0:                 episode reward: -94.2000,                 loss: nan
env1_second_0:                 episode reward: 94.2000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 229.9,                last time consumption/overall running time: 340.1897s / 54746.7545 s
env0_first_0:                 episode reward: -94.6000,                 loss: 0.5999
env0_second_0:                 episode reward: 94.6000,                 loss: 0.7394
env1_first_0:                 episode reward: -94.8000,                 loss: nan
env1_second_0:                 episode reward: 94.8000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 224.9,                last time consumption/overall running time: 333.0500s / 55079.8045 s
env0_first_0:                 episode reward: -90.9000,                 loss: 0.6166
env0_second_0:                 episode reward: 90.9000,                 loss: 0.7555
env1_first_0:                 episode reward: -90.5500,                 loss: nan
env1_second_0:                 episode reward: 90.5500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 221.6,                last time consumption/overall running time: 330.4949s / 55410.2994 s
env0_first_0:                 episode reward: -97.5500,                 loss: 0.6162
env0_second_0:                 episode reward: 97.5500,                 loss: 0.7373
env1_first_0:                 episode reward: -94.9500,                 loss: nan
env1_second_0:                 episode reward: 94.9500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 235.75,                last time consumption/overall running time: 343.9434s / 55754.2428 s
env0_first_0:                 episode reward: -87.8000,                 loss: 0.6355
env0_second_0:                 episode reward: 87.8000,                 loss: 0.7431
env1_first_0:                 episode reward: -90.1500,                 loss: nan
env1_second_0:                 episode reward: 90.1500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 228.45,                last time consumption/overall running time: 338.8163s / 56093.0591 s
env0_first_0:                 episode reward: -96.9500,                 loss: 0.5613
env0_second_0:                 episode reward: 96.9500,                 loss: 0.7462
env1_first_0:                 episode reward: -91.3000,                 loss: nan
env1_second_0:                 episode reward: 91.3000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 230.7,                last time consumption/overall running time: 339.5941s / 56432.6532 s
env0_first_0:                 episode reward: -87.3000,                 loss: 0.5931
env0_second_0:                 episode reward: 87.3000,                 loss: 0.6889
env1_first_0:                 episode reward: -91.4000,                 loss: nan
env1_second_0:                 episode reward: 91.4000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 220.55,                last time consumption/overall running time: 326.6422s / 56759.2954 s
env0_first_0:                 episode reward: -90.6500,                 loss: 0.5529
env0_second_0:                 episode reward: 90.6500,                 loss: 0.7544
env1_first_0:                 episode reward: -91.2000,                 loss: nan
env1_second_0:                 episode reward: 91.2000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 227.9,                last time consumption/overall running time: 340.0875s / 57099.3829 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.5027
env0_second_0:                 episode reward: 92.6500,                 loss: 0.7489
env1_first_0:                 episode reward: -93.5500,                 loss: nan
env1_second_0:                 episode reward: 93.5500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 227.95,                last time consumption/overall running time: 340.9705s / 57440.3534 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.5283
env0_second_0:                 episode reward: 93.2000,                 loss: 0.7801
env1_first_0:                 episode reward: -96.2500,                 loss: nan
env1_second_0:                 episode reward: 96.2500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 219.0,                last time consumption/overall running time: 322.0272s / 57762.3806 s
env0_first_0:                 episode reward: -95.3000,                 loss: 0.5224
env0_second_0:                 episode reward: 95.3000,                 loss: 0.6924
env1_first_0:                 episode reward: -93.9000,                 loss: nan
env1_second_0:                 episode reward: 93.9000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 224.8,                last time consumption/overall running time: 334.3863s / 58096.7669 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.5042
env0_second_0:                 episode reward: 92.4000,                 loss: 0.6965
env1_first_0:                 episode reward: -94.9500,                 loss: nan
env1_second_0:                 episode reward: 94.9500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 219.15,                last time consumption/overall running time: 322.7680s / 58419.5349 s
env0_first_0:                 episode reward: -96.7000,                 loss: 0.4698
env0_second_0:                 episode reward: 96.7000,                 loss: 0.7279
env1_first_0:                 episode reward: -96.1000,                 loss: nan
env1_second_0:                 episode reward: 96.1000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 227.5,                last time consumption/overall running time: 336.6874s / 58756.2224 s
env0_first_0:                 episode reward: -88.8500,                 loss: 0.5229
env0_second_0:                 episode reward: 88.8500,                 loss: 0.7012
env1_first_0:                 episode reward: -92.3500,                 loss: nan
env1_second_0:                 episode reward: 92.3500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 233.3,                last time consumption/overall running time: 345.2425s / 59101.4649 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.5097
env0_second_0:                 episode reward: 92.6500,                 loss: 0.7488
env1_first_0:                 episode reward: -93.0500,                 loss: nan
env1_second_0:                 episode reward: 93.0500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 228.55,                last time consumption/overall running time: 339.1824s / 59440.6472 s
env0_first_0:                 episode reward: -83.3000,                 loss: 0.5293
env0_second_0:                 episode reward: 83.3000,                 loss: 0.7312
env1_first_0:                 episode reward: -94.8000,                 loss: nan
env1_second_0:                 episode reward: 94.8000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 263.1,                last time consumption/overall running time: 389.2082s / 59829.8554 s
env0_first_0:                 episode reward: -91.1500,                 loss: 0.4610
env0_second_0:                 episode reward: 91.1500,                 loss: 0.6965
env1_first_0:                 episode reward: -92.0000,                 loss: nan
env1_second_0:                 episode reward: 92.0000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 234.25,                last time consumption/overall running time: 346.7511s / 60176.6065 s
env0_first_0:                 episode reward: -87.8000,                 loss: 0.5083
env0_second_0:                 episode reward: 87.8000,                 loss: 0.7692
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 224.4,                last time consumption/overall running time: 331.1766s / 60507.7831 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.5665
env0_second_0:                 episode reward: 87.7000,                 loss: 0.7300
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 224.15,                last time consumption/overall running time: 336.2680s / 60844.0511 s
env0_first_0:                 episode reward: -95.5000,                 loss: 0.5611
env0_second_0:                 episode reward: 95.5000,                 loss: 0.7818
env1_first_0:                 episode reward: -93.5500,                 loss: nan
env1_second_0:                 episode reward: 93.5500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 226.8,                last time consumption/overall running time: 337.4320s / 61181.4831 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.5296
env0_second_0:                 episode reward: 93.5500,                 loss: 0.8312
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 224.1,                last time consumption/overall running time: 330.6955s / 61512.1786 s
env0_first_0:                 episode reward: -94.2500,                 loss: 0.5456
env0_second_0:                 episode reward: 94.2500,                 loss: 0.8237
env1_first_0:                 episode reward: -92.7000,                 loss: nan
env1_second_0:                 episode reward: 92.7000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 223.5,                last time consumption/overall running time: 328.9353s / 61841.1138 s
env0_first_0:                 episode reward: -92.3500,                 loss: 0.5570
env0_second_0:                 episode reward: 92.3500,                 loss: 0.8001
env1_first_0:                 episode reward: -91.3000,                 loss: nan
env1_second_0:                 episode reward: 91.3000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 223.05,                last time consumption/overall running time: 332.6009s / 62173.7147 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.5461
env0_second_0:                 episode reward: 88.5500,                 loss: 0.8149
env1_first_0:                 episode reward: -92.9500,                 loss: nan
env1_second_0:                 episode reward: 92.9500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 233.15,                last time consumption/overall running time: 343.2523s / 62516.9670 s
env0_first_0:                 episode reward: -91.7000,                 loss: 0.5470
env0_second_0:                 episode reward: 91.7000,                 loss: 0.8077
env1_first_0:                 episode reward: -87.9000,                 loss: nan
env1_second_0:                 episode reward: 87.9000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 230.3,                last time consumption/overall running time: 341.3654s / 62858.3324 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.6082
env0_second_0:                 episode reward: 88.6000,                 loss: 0.8757
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 226.3,                last time consumption/overall running time: 335.6201s / 63193.9526 s
env0_first_0:                 episode reward: -93.1500,                 loss: 0.5329
env0_second_0:                 episode reward: 93.1500,                 loss: 0.7428
env1_first_0:                 episode reward: -94.1500,                 loss: nan
env1_second_0:                 episode reward: 94.1500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 231.65,                last time consumption/overall running time: 344.9844s / 63538.9369 s
env0_first_0:                 episode reward: -91.3500,                 loss: 0.6062
env0_second_0:                 episode reward: 91.3500,                 loss: 0.8311
env1_first_0:                 episode reward: -90.8500,                 loss: nan
env1_second_0:                 episode reward: 90.8500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 237.05,                last time consumption/overall running time: 351.9430s / 63890.8799 s
env0_first_0:                 episode reward: -88.9500,                 loss: 0.5803
env0_second_0:                 episode reward: 88.9500,                 loss: 0.8281
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 232.65,                last time consumption/overall running time: 345.8214s / 64236.7013 s
env0_first_0:                 episode reward: -95.4000,                 loss: 0.5643
env0_second_0:                 episode reward: 95.4000,                 loss: 0.8576
env1_first_0:                 episode reward: -87.3500,                 loss: nan
env1_second_0:                 episode reward: 87.3500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 226.55,                last time consumption/overall running time: 337.3942s / 64574.0955 s
env0_first_0:                 episode reward: -96.3000,                 loss: 0.5080
env0_second_0:                 episode reward: 96.3000,                 loss: 0.8305
env1_first_0:                 episode reward: -93.1500,                 loss: nan
env1_second_0:                 episode reward: 93.1500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 225.2,                last time consumption/overall running time: 330.4783s / 64904.5738 s
env0_first_0:                 episode reward: -93.1000,                 loss: 0.5552
env0_second_0:                 episode reward: 93.1000,                 loss: 0.8137
env1_first_0:                 episode reward: -94.9000,                 loss: nan
env1_second_0:                 episode reward: 94.9000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 227.1,                last time consumption/overall running time: 333.2755s / 65237.8493 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.5139
env0_second_0:                 episode reward: 93.5500,                 loss: 0.8229
env1_first_0:                 episode reward: -95.8500,                 loss: nan
env1_second_0:                 episode reward: 95.8500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 224.45,                last time consumption/overall running time: 334.2650s / 65572.1143 s
env0_first_0:                 episode reward: -97.3000,                 loss: 0.5230
env0_second_0:                 episode reward: 97.3000,                 loss: 0.8033
env1_first_0:                 episode reward: -92.4500,                 loss: nan
env1_second_0:                 episode reward: 92.4500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 226.25,                last time consumption/overall running time: 334.3395s / 65906.4538 s
env0_first_0:                 episode reward: -95.1000,                 loss: 0.5223
env0_second_0:                 episode reward: 95.1000,                 loss: 0.8019
env1_first_0:                 episode reward: -87.2000,                 loss: nan
env1_second_0:                 episode reward: 87.2000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 239.1,                last time consumption/overall running time: 349.0789s / 66255.5327 s
env0_first_0:                 episode reward: -90.2500,                 loss: 0.5786
env0_second_0:                 episode reward: 90.2500,                 loss: 0.8477
env1_first_0:                 episode reward: -89.6500,                 loss: nan
env1_second_0:                 episode reward: 89.6500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 231.8,                last time consumption/overall running time: 342.8273s / 66598.3600 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.5937
env0_second_0:                 episode reward: 93.3500,                 loss: 0.8398
env1_first_0:                 episode reward: -94.0000,                 loss: nan
env1_second_0:                 episode reward: 94.0000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 224.8,                last time consumption/overall running time: 332.5480s / 66930.9080 s
env0_first_0:                 episode reward: -93.1000,                 loss: 0.5699
env0_second_0:                 episode reward: 93.1000,                 loss: 0.8158
env1_first_0:                 episode reward: -95.8500,                 loss: nan
env1_second_0:                 episode reward: 95.8500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 235.1,                last time consumption/overall running time: 348.1406s / 67279.0486 s
env0_first_0:                 episode reward: -91.5000,                 loss: 0.5301
env0_second_0:                 episode reward: 91.5000,                 loss: 0.7690
env1_first_0:                 episode reward: -93.9500,                 loss: nan
env1_second_0:                 episode reward: 93.9500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 227.6,                last time consumption/overall running time: 338.7833s / 67617.8319 s
env0_first_0:                 episode reward: -93.8000,                 loss: 0.6097
env0_second_0:                 episode reward: 93.8000,                 loss: 0.7391
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 242.1,                last time consumption/overall running time: 358.4552s / 67976.2871 s
env0_first_0:                 episode reward: -93.3000,                 loss: 0.5740
env0_second_0:                 episode reward: 93.3000,                 loss: 0.7774
env1_first_0:                 episode reward: -93.3000,                 loss: nan
env1_second_0:                 episode reward: 93.3000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 260.1,                last time consumption/overall running time: 384.4864s / 68360.7734 s
env0_first_0:                 episode reward: -88.1000,                 loss: 0.5721
env0_second_0:                 episode reward: 88.1000,                 loss: 0.8043
env1_first_0:                 episode reward: -91.2500,                 loss: nan
env1_second_0:                 episode reward: 91.2500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 231.5,                last time consumption/overall running time: 346.0412s / 68706.8146 s
env0_first_0:                 episode reward: -93.4000,                 loss: 0.5482
env0_second_0:                 episode reward: 93.4000,                 loss: 0.6720
env1_first_0:                 episode reward: -93.2000,                 loss: nan
env1_second_0:                 episode reward: 93.2000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 225.45,                last time consumption/overall running time: 333.3537s / 69040.1683 s
env0_first_0:                 episode reward: -91.7000,                 loss: 0.5875
env0_second_0:                 episode reward: 91.7000,                 loss: 0.7980
env1_first_0:                 episode reward: -95.9000,                 loss: nan
env1_second_0:                 episode reward: 95.9000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 241.55,                last time consumption/overall running time: 358.1373s / 69398.3056 s
env0_first_0:                 episode reward: -87.0000,                 loss: 0.6170
env0_second_0:                 episode reward: 87.0000,                 loss: 0.7315
env1_first_0:                 episode reward: -94.4500,                 loss: nan
env1_second_0:                 episode reward: 94.4500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 232.15,                last time consumption/overall running time: 348.4441s / 69746.7497 s
env0_first_0:                 episode reward: -91.3500,                 loss: 0.5302
env0_second_0:                 episode reward: 91.3500,                 loss: 0.7391
env1_first_0:                 episode reward: -97.1000,                 loss: nan
env1_second_0:                 episode reward: 97.1000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 231.8,                last time consumption/overall running time: 344.7189s / 70091.4685 s
env0_first_0:                 episode reward: -96.4500,                 loss: 0.5086
env0_second_0:                 episode reward: 96.4500,                 loss: 0.7664
env1_first_0:                 episode reward: -88.0000,                 loss: nan
env1_second_0:                 episode reward: 88.0000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 229.7,                last time consumption/overall running time: 341.5866s / 70433.0551 s
env0_first_0:                 episode reward: -96.4500,                 loss: 0.5815
env0_second_0:                 episode reward: 96.4500,                 loss: 0.7526
env1_first_0:                 episode reward: -83.1000,                 loss: nan
env1_second_0:                 episode reward: 83.1000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 248.9,                last time consumption/overall running time: 369.7743s / 70802.8294 s
env0_first_0:                 episode reward: -88.8500,                 loss: 0.5404
env0_second_0:                 episode reward: 88.8500,                 loss: 0.8513
env1_first_0:                 episode reward: -78.7000,                 loss: nan
env1_second_0:                 episode reward: 78.7000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 252.5,                last time consumption/overall running time: 374.6472s / 71177.4766 s
env0_first_0:                 episode reward: -88.0500,                 loss: 0.5710
env0_second_0:                 episode reward: 88.0500,                 loss: 0.8639
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 232.35,                last time consumption/overall running time: 343.9429s / 71521.4195 s
env0_first_0:                 episode reward: -92.5500,                 loss: 0.5854
env0_second_0:                 episode reward: 92.5500,                 loss: 0.8266
env1_first_0:                 episode reward: -88.5500,                 loss: nan
env1_second_0:                 episode reward: 88.5500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 224.65,                last time consumption/overall running time: 335.7273s / 71857.1468 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.5850
env0_second_0:                 episode reward: 88.6000,                 loss: 0.7884
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 243.45,                last time consumption/overall running time: 360.1317s / 72217.2785 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.6169
env0_second_0:                 episode reward: 89.6000,                 loss: 0.8644
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 223.2,                last time consumption/overall running time: 327.2621s / 72544.5406 s
env0_first_0:                 episode reward: -91.4500,                 loss: 0.6217
env0_second_0:                 episode reward: 91.4500,                 loss: 0.8711
env1_first_0:                 episode reward: -91.7000,                 loss: nan
env1_second_0:                 episode reward: 91.7000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 238.1,                last time consumption/overall running time: 354.2930s / 72898.8336 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.6620
env0_second_0:                 episode reward: 89.3000,                 loss: 0.9204
env1_first_0:                 episode reward: -95.5000,                 loss: nan
env1_second_0:                 episode reward: 95.5000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 232.85,                last time consumption/overall running time: 341.9071s / 73240.7407 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.6537
env0_second_0:                 episode reward: 93.0500,                 loss: 0.9056
env1_first_0:                 episode reward: -93.0500,                 loss: nan
env1_second_0:                 episode reward: 93.0500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 225.55,                last time consumption/overall running time: 335.9845s / 73576.7252 s
env0_first_0:                 episode reward: -88.3500,                 loss: 0.6605
env0_second_0:                 episode reward: 88.3500,                 loss: 0.9476
env1_first_0:                 episode reward: -92.1000,                 loss: nan
env1_second_0:                 episode reward: 92.1000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 221.05,                last time consumption/overall running time: 329.3462s / 73906.0714 s
env0_first_0:                 episode reward: -89.4500,                 loss: 0.6146
env0_second_0:                 episode reward: 89.4500,                 loss: 0.8803
env1_first_0:                 episode reward: -96.4000,                 loss: nan
env1_second_0:                 episode reward: 96.4000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 224.0,                last time consumption/overall running time: 332.5207s / 74238.5921 s
env0_first_0:                 episode reward: -89.2500,                 loss: 0.6708
env0_second_0:                 episode reward: 89.2500,                 loss: 0.9580
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 226.65,                last time consumption/overall running time: 334.8342s / 74573.4264 s
env0_first_0:                 episode reward: -85.0000,                 loss: 0.5638
env0_second_0:                 episode reward: 85.0000,                 loss: 0.9336
env1_first_0:                 episode reward: -91.5500,                 loss: nan
env1_second_0:                 episode reward: 91.5500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 225.65,                last time consumption/overall running time: 336.7441s / 74910.1705 s
env0_first_0:                 episode reward: -94.8000,                 loss: 0.6008
env0_second_0:                 episode reward: 94.8000,                 loss: 0.8974
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 230.95,                last time consumption/overall running time: 346.0317s / 75256.2021 s
env0_first_0:                 episode reward: -92.8000,                 loss: 0.5225
env0_second_0:                 episode reward: 92.8000,                 loss: 0.9387
env1_first_0:                 episode reward: -86.1500,                 loss: nan
env1_second_0:                 episode reward: 86.1500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 241.0,                last time consumption/overall running time: 355.9593s / 75612.1615 s
env0_first_0:                 episode reward: -82.3500,                 loss: 0.5868
env0_second_0:                 episode reward: 82.3500,                 loss: 0.8317
env1_first_0:                 episode reward: -94.2500,                 loss: nan
env1_second_0:                 episode reward: 94.2500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 237.1,                last time consumption/overall running time: 352.7810s / 75964.9425 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.5995
env0_second_0:                 episode reward: 92.9500,                 loss: 0.7891
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 235.7,                last time consumption/overall running time: 351.1936s / 76316.1361 s
env0_first_0:                 episode reward: -84.4500,                 loss: 0.5626
env0_second_0:                 episode reward: 84.4500,                 loss: 0.7833
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 231.6,                last time consumption/overall running time: 343.4914s / 76659.6275 s
env0_first_0:                 episode reward: -76.8000,                 loss: 0.5845
env0_second_0:                 episode reward: 76.8000,                 loss: 0.8349
env1_first_0:                 episode reward: -93.1500,                 loss: nan
env1_second_0:                 episode reward: 93.1500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 238.8,                last time consumption/overall running time: 347.4575s / 77007.0850 s
env0_first_0:                 episode reward: -89.2000,                 loss: 0.5985
env0_second_0:                 episode reward: 89.2000,                 loss: 0.7749
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 222.5,                last time consumption/overall running time: 328.7883s / 77335.8733 s
env0_first_0:                 episode reward: -93.9500,                 loss: 0.6091
env0_second_0:                 episode reward: 93.9500,                 loss: 0.7671
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 241.45,                last time consumption/overall running time: 357.7074s / 77693.5807 s
env0_first_0:                 episode reward: -88.5000,                 loss: 0.5495
env0_second_0:                 episode reward: 88.5000,                 loss: 0.8194
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 240.5,                last time consumption/overall running time: 357.8826s / 78051.4633 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.6021
env0_second_0:                 episode reward: 88.5500,                 loss: 0.8038
env1_first_0:                 episode reward: -87.2500,                 loss: nan
env1_second_0:                 episode reward: 87.2500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 231.4,                last time consumption/overall running time: 343.4094s / 78394.8728 s
env0_first_0:                 episode reward: -90.9500,                 loss: 0.5404
env0_second_0:                 episode reward: 90.9500,                 loss: 0.7608
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 227.5,                last time consumption/overall running time: 338.9887s / 78733.8614 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.5650
env0_second_0:                 episode reward: 91.6500,                 loss: 0.8476
env1_first_0:                 episode reward: -96.7500,                 loss: nan
env1_second_0:                 episode reward: 96.7500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 225.95,                last time consumption/overall running time: 335.4820s / 79069.3434 s
env0_first_0:                 episode reward: -88.1500,                 loss: 0.5661
env0_second_0:                 episode reward: 88.1500,                 loss: 0.7713
env1_first_0:                 episode reward: -92.9000,                 loss: nan
env1_second_0:                 episode reward: 92.9000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 235.5,                last time consumption/overall running time: 350.4287s / 79419.7722 s
env0_first_0:                 episode reward: -95.3500,                 loss: 0.5870
env0_second_0:                 episode reward: 95.3500,                 loss: 0.8068
env1_first_0:                 episode reward: -86.5000,                 loss: nan
env1_second_0:                 episode reward: 86.5000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 231.8,                last time consumption/overall running time: 341.6284s / 79761.4006 s
env0_first_0:                 episode reward: -86.0500,                 loss: 0.5973
env0_second_0:                 episode reward: 86.0500,                 loss: 0.7524
env1_first_0:                 episode reward: -93.0500,                 loss: nan
env1_second_0:                 episode reward: 93.0500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 230.6,                last time consumption/overall running time: 341.3209s / 80102.7215 s
env0_first_0:                 episode reward: -92.5500,                 loss: 0.5769
env0_second_0:                 episode reward: 92.5500,                 loss: 0.7179
env1_first_0:                 episode reward: -89.0000,                 loss: nan
env1_second_0:                 episode reward: 89.0000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 242.55,                last time consumption/overall running time: 361.8275s / 80464.5490 s
env0_first_0:                 episode reward: -89.4000,                 loss: 0.5721
env0_second_0:                 episode reward: 89.4000,                 loss: 0.6713
env1_first_0:                 episode reward: -91.2500,                 loss: nan
env1_second_0:                 episode reward: 91.2500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 245.05,                last time consumption/overall running time: 362.1297s / 80826.6787 s
env0_first_0:                 episode reward: -92.2000,                 loss: 0.5910
env0_second_0:                 episode reward: 92.2000,                 loss: 0.6643
env1_first_0:                 episode reward: -86.8500,                 loss: nan
env1_second_0:                 episode reward: 86.8500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 227.85,                last time consumption/overall running time: 337.6522s / 81164.3309 s
env0_first_0:                 episode reward: -90.4500,                 loss: 0.6225
env0_second_0:                 episode reward: 90.4500,                 loss: 0.6879
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 231.15,                last time consumption/overall running time: 342.8377s / 81507.1686 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.5721
env0_second_0:                 episode reward: 89.6000,                 loss: 0.7497
env1_first_0:                 episode reward: -89.5000,                 loss: nan
env1_second_0:                 episode reward: 89.5000,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 236.4,                last time consumption/overall running time: 348.1439s / 81855.3125 s
env0_first_0:                 episode reward: -87.5000,                 loss: 0.5638
env0_second_0:                 episode reward: 87.5000,                 loss: 0.7040
env1_first_0:                 episode reward: -85.8000,                 loss: nan
env1_second_0:                 episode reward: 85.8000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 238.25,                last time consumption/overall running time: 353.0932s / 82208.4056 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.5770
env0_second_0:                 episode reward: 88.5500,                 loss: 0.7544
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 244.0,                last time consumption/overall running time: 361.4113s / 82569.8169 s
env0_first_0:                 episode reward: -88.2000,                 loss: 0.6005
env0_second_0:                 episode reward: 88.2000,                 loss: 0.8176
env1_first_0:                 episode reward: -80.4500,                 loss: nan
env1_second_0:                 episode reward: 80.4500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 251.4,                last time consumption/overall running time: 374.8808s / 82944.6978 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.6790
env0_second_0:                 episode reward: 85.6500,                 loss: 0.7163
env1_first_0:                 episode reward: -84.4500,                 loss: nan
env1_second_0:                 episode reward: 84.4500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 310.45,                last time consumption/overall running time: 458.0575s / 83402.7552 s
env0_first_0:                 episode reward: -86.5500,                 loss: 0.6248
env0_second_0:                 episode reward: 86.5500,                 loss: 0.7123
env1_first_0:                 episode reward: -87.3000,                 loss: nan
env1_second_0:                 episode reward: 87.3000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 288.05,                last time consumption/overall running time: 427.6550s / 83830.4103 s
env0_first_0:                 episode reward: -88.0500,                 loss: 0.6009
env0_second_0:                 episode reward: 88.0500,                 loss: 0.6946
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 234.55,                last time consumption/overall running time: 349.7290s / 84180.1392 s
env0_first_0:                 episode reward: -88.2000,                 loss: 0.5983
env0_second_0:                 episode reward: 88.2000,                 loss: 0.6545
env1_first_0:                 episode reward: -94.9000,                 loss: nan
env1_second_0:                 episode reward: 94.9000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 255.35,                last time consumption/overall running time: 379.1302s / 84559.2694 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.5251
env0_second_0:                 episode reward: 87.3500,                 loss: 0.6340
env1_first_0:                 episode reward: -90.4000,                 loss: nan
env1_second_0:                 episode reward: 90.4000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 278.45,                last time consumption/overall running time: 408.9717s / 84968.2411 s
env0_first_0:                 episode reward: -95.0500,                 loss: 0.5562
env0_second_0:                 episode reward: 95.0500,                 loss: 0.7164
env1_first_0:                 episode reward: -95.2000,                 loss: nan
env1_second_0:                 episode reward: 95.2000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 243.25,                last time consumption/overall running time: 359.9896s / 85328.2306 s
env0_first_0:                 episode reward: -92.2500,                 loss: 0.5515
env0_second_0:                 episode reward: 92.2500,                 loss: 0.6484
env1_first_0:                 episode reward: -89.8500,                 loss: nan
env1_second_0:                 episode reward: 89.8500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 238.85,                last time consumption/overall running time: 353.4853s / 85681.7159 s
env0_first_0:                 episode reward: -92.5000,                 loss: 0.5771
env0_second_0:                 episode reward: 92.5000,                 loss: 0.7149
env1_first_0:                 episode reward: -92.2000,                 loss: nan
env1_second_0:                 episode reward: 92.2000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 249.35,                last time consumption/overall running time: 371.5230s / 86053.2388 s
env0_first_0:                 episode reward: -84.3500,                 loss: 0.6157
env0_second_0:                 episode reward: 84.3500,                 loss: 0.6408
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 251.3,                last time consumption/overall running time: 371.5422s / 86424.7811 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.5174
env0_second_0:                 episode reward: 89.6000,                 loss: 0.6401
env1_first_0:                 episode reward: -90.2000,                 loss: nan
env1_second_0:                 episode reward: 90.2000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 238.45,                last time consumption/overall running time: 354.3799s / 86779.1609 s
env0_first_0:                 episode reward: -92.1500,                 loss: 0.5470
env0_second_0:                 episode reward: 92.1500,                 loss: 0.6546
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 238.0,                last time consumption/overall running time: 353.5048s / 87132.6658 s
env0_first_0:                 episode reward: -88.5000,                 loss: 0.4636
env0_second_0:                 episode reward: 88.5000,                 loss: 0.6101
env1_first_0:                 episode reward: -88.1000,                 loss: nan
env1_second_0:                 episode reward: 88.1000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 233.95,                last time consumption/overall running time: 345.7375s / 87478.4033 s
env0_first_0:                 episode reward: -82.6000,                 loss: 0.5134
env0_second_0:                 episode reward: 82.6000,                 loss: 0.6394
env1_first_0:                 episode reward: -88.4500,                 loss: nan
env1_second_0:                 episode reward: 88.4500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 227.2,                last time consumption/overall running time: 339.7833s / 87818.1866 s
env0_first_0:                 episode reward: -94.0000,                 loss: 0.5275
env0_second_0:                 episode reward: 94.0000,                 loss: 0.6219
env1_first_0:                 episode reward: -92.5000,                 loss: nan
env1_second_0:                 episode reward: 92.5000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 236.35,                last time consumption/overall running time: 354.7138s / 88172.9004 s
env0_first_0:                 episode reward: -88.6500,                 loss: 0.5044
env0_second_0:                 episode reward: 88.6500,                 loss: 0.6056
env1_first_0:                 episode reward: -87.7000,                 loss: nan
env1_second_0:                 episode reward: 87.7000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 243.8,                last time consumption/overall running time: 361.9248s / 88534.8252 s
env0_first_0:                 episode reward: -94.6500,                 loss: 0.5524
env0_second_0:                 episode reward: 94.6500,                 loss: 0.5735
env1_first_0:                 episode reward: -84.9500,                 loss: nan
env1_second_0:                 episode reward: 84.9500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 296.75,                last time consumption/overall running time: 439.0646s / 88973.8897 s
env0_first_0:                 episode reward: -93.7000,                 loss: 0.5437
env0_second_0:                 episode reward: 93.7000,                 loss: 0.5656
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 233.5,                last time consumption/overall running time: 344.4261s / 89318.3158 s
env0_first_0:                 episode reward: -93.1000,                 loss: 0.5282
env0_second_0:                 episode reward: 93.1000,                 loss: 0.5210
env1_first_0:                 episode reward: -95.0500,                 loss: nan
env1_second_0:                 episode reward: 95.0500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 225.1,                last time consumption/overall running time: 332.6667s / 89650.9825 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.5422
env0_second_0:                 episode reward: 85.8500,                 loss: 0.5611
env1_first_0:                 episode reward: -96.9000,                 loss: nan
env1_second_0:                 episode reward: 96.9000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 235.85,                last time consumption/overall running time: 348.9730s / 89999.9555 s
env0_first_0:                 episode reward: -94.4000,                 loss: 0.5693
env0_second_0:                 episode reward: 94.4000,                 loss: 0.5262
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 251.7,                last time consumption/overall running time: 372.6468s / 90372.6024 s
env0_first_0:                 episode reward: -91.8000,                 loss: 0.5171
env0_second_0:                 episode reward: 91.8000,                 loss: 0.5679
env1_first_0:                 episode reward: -94.0000,                 loss: nan
env1_second_0:                 episode reward: 94.0000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 618.45,                last time consumption/overall running time: 908.2406s / 91280.8429 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.5423
env0_second_0:                 episode reward: 89.6000,                 loss: 0.5784
env1_first_0:                 episode reward: -91.9500,                 loss: nan
env1_second_0:                 episode reward: 91.9500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 352.95,                last time consumption/overall running time: 521.6360s / 91802.4789 s
env0_first_0:                 episode reward: -94.9500,                 loss: 0.5039
env0_second_0:                 episode reward: 94.9500,                 loss: 0.5194
env1_first_0:                 episode reward: -94.9000,                 loss: nan
env1_second_0:                 episode reward: 94.9000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 246.85,                last time consumption/overall running time: 370.6514s / 92173.1303 s
env0_first_0:                 episode reward: -92.3000,                 loss: 0.4517
env0_second_0:                 episode reward: 92.3000,                 loss: 0.4932
env1_first_0:                 episode reward: -83.8500,                 loss: nan
env1_second_0:                 episode reward: 83.8500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 400.5,                last time consumption/overall running time: 596.4559s / 92769.5862 s
env0_first_0:                 episode reward: -88.2500,                 loss: 0.4591
env0_second_0:                 episode reward: 88.2500,                 loss: 0.4979
env1_first_0:                 episode reward: -84.8500,                 loss: nan
env1_second_0:                 episode reward: 84.8500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 424.7,                last time consumption/overall running time: 631.1916s / 93400.7778 s
env0_first_0:                 episode reward: -90.5000,                 loss: 0.4283
env0_second_0:                 episode reward: 90.5000,                 loss: 0.5114
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 711.7,                last time consumption/overall running time: 1055.0569s / 94455.8348 s
env0_first_0:                 episode reward: -90.5000,                 loss: 0.4123
env0_second_0:                 episode reward: 90.5000,                 loss: 0.4014
env1_first_0:                 episode reward: -81.7000,                 loss: nan
env1_second_0:                 episode reward: 81.7000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 432.2,                last time consumption/overall running time: 643.5945s / 95099.4293 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.3934
env0_second_0:                 episode reward: 89.3000,                 loss: 0.3812
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 855.05,                last time consumption/overall running time: 1265.0542s / 96364.4835 s
env0_first_0:                 episode reward: -88.0500,                 loss: 0.3553
env0_second_0:                 episode reward: 88.0500,                 loss: 0.3660
env1_first_0:                 episode reward: -87.0500,                 loss: nan
env1_second_0:                 episode reward: 87.0500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 863.85,                last time consumption/overall running time: 1271.6518s / 97636.1353 s
env0_first_0:                 episode reward: -92.5500,                 loss: 0.3009
env0_second_0:                 episode reward: 92.5500,                 loss: 0.3060
env1_first_0:                 episode reward: -82.9000,                 loss: nan
env1_second_0:                 episode reward: 82.9000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 322.8,                last time consumption/overall running time: 472.8007s / 98108.9360 s
env0_first_0:                 episode reward: -89.5500,                 loss: 0.3047
env0_second_0:                 episode reward: 89.5500,                 loss: 0.3008
env1_first_0:                 episode reward: -83.2500,                 loss: nan
env1_second_0:                 episode reward: 83.2500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 403.45,                last time consumption/overall running time: 585.8653s / 98694.8012 s
env0_first_0:                 episode reward: -94.6500,                 loss: 0.3165
env0_second_0:                 episode reward: 94.6500,                 loss: 0.3033
env1_first_0:                 episode reward: -76.2000,                 loss: nan
env1_second_0:                 episode reward: 76.2000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 574.65,                last time consumption/overall running time: 830.7941s / 99525.5953 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.3327
env0_second_0:                 episode reward: 91.6500,                 loss: 0.3413
env1_first_0:                 episode reward: -91.9000,                 loss: nan
env1_second_0:                 episode reward: 91.9000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 327.0,                last time consumption/overall running time: 474.7993s / 100000.3946 s
env0_first_0:                 episode reward: -81.7500,                 loss: 0.3393
env0_second_0:                 episode reward: 81.7500,                 loss: 0.3141
env1_first_0:                 episode reward: -81.5500,                 loss: nan
env1_second_0:                 episode reward: 81.5500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 551.0,                last time consumption/overall running time: 799.1773s / 100799.5719 s
env0_first_0:                 episode reward: -83.9500,                 loss: 0.3542
env0_second_0:                 episode reward: 83.9500,                 loss: 0.3469
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 246.65,                last time consumption/overall running time: 359.1318s / 101158.7037 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.3754
env0_second_0:                 episode reward: 91.6500,                 loss: 0.3672
env1_first_0:                 episode reward: -84.5000,                 loss: nan
env1_second_0:                 episode reward: 84.5000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 239.65,                last time consumption/overall running time: 346.2920s / 101504.9957 s
env0_first_0:                 episode reward: -82.4000,                 loss: 0.4371
env0_second_0:                 episode reward: 82.4000,                 loss: 0.4088
env1_first_0:                 episode reward: -87.3000,                 loss: nan
env1_second_0:                 episode reward: 87.3000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 238.3,                last time consumption/overall running time: 348.6647s / 101853.6604 s
env0_first_0:                 episode reward: -82.2500,                 loss: 0.4364
env0_second_0:                 episode reward: 82.2500,                 loss: 0.4002
env1_first_0:                 episode reward: -88.6000,                 loss: nan
env1_second_0:                 episode reward: 88.6000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 238.3,                last time consumption/overall running time: 347.7457s / 102201.4061 s
env0_first_0:                 episode reward: -89.7500,                 loss: 0.4765
env0_second_0:                 episode reward: 89.7500,                 loss: 0.4503
env1_first_0:                 episode reward: -86.2500,                 loss: nan
env1_second_0:                 episode reward: 86.2500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 243.0,                last time consumption/overall running time: 351.1051s / 102552.5112 s
env0_first_0:                 episode reward: -78.6000,                 loss: 0.4764
env0_second_0:                 episode reward: 78.6000,                 loss: 0.4800
env1_first_0:                 episode reward: -84.7500,                 loss: nan
env1_second_0:                 episode reward: 84.7500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 720.75,                last time consumption/overall running time: 1047.1554s / 103599.6666 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.5336
env0_second_0:                 episode reward: 86.6500,                 loss: 0.5372
env1_first_0:                 episode reward: -84.5000,                 loss: nan
env1_second_0:                 episode reward: 84.5000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 736.15,                last time consumption/overall running time: 1071.0335s / 104670.7001 s
env0_first_0:                 episode reward: -92.6000,                 loss: 0.5359
env0_second_0:                 episode reward: 92.6000,                 loss: 0.4699
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 1506.15,                last time consumption/overall running time: 2186.8327s / 106857.5328 s
env0_first_0:                 episode reward: -90.2500,                 loss: 0.3332
env0_second_0:                 episode reward: 90.2500,                 loss: 0.3228
env1_first_0:                 episode reward: -92.9500,                 loss: nan
env1_second_0:                 episode reward: 92.9500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1574.95,                last time consumption/overall running time: 2276.3106s / 109133.8434 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.1724
env0_second_0:                 episode reward: 86.4500,                 loss: 0.1715
env1_first_0:                 episode reward: -91.6000,                 loss: nan
env1_second_0:                 episode reward: 91.6000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 1106.5,                last time consumption/overall running time: 1603.0571s / 110736.9004 s
env0_first_0:                 episode reward: -93.7000,                 loss: 0.1810
env0_second_0:                 episode reward: 93.7000,                 loss: 0.1767
env1_first_0:                 episode reward: -85.2500,                 loss: nan
env1_second_0:                 episode reward: 85.2500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 929.55,                last time consumption/overall running time: 1347.8699s / 112084.7703 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.2345
env0_second_0:                 episode reward: 92.6500,                 loss: 0.2137
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 947.1,                last time consumption/overall running time: 1368.3020s / 113453.0723 s
env0_first_0:                 episode reward: -90.6500,                 loss: 0.2842
env0_second_0:                 episode reward: 90.6500,                 loss: 0.2247
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 561.35,                last time consumption/overall running time: 821.5884s / 114274.6607 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.2993
env0_second_0:                 episode reward: 90.3000,                 loss: 0.2849
env1_first_0:                 episode reward: -91.0500,                 loss: nan
env1_second_0:                 episode reward: 91.0500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 591.8,                last time consumption/overall running time: 857.8193s / 115132.4800 s
env0_first_0:                 episode reward: -87.6500,                 loss: 0.3283
env0_second_0:                 episode reward: 87.6500,                 loss: 0.3112
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 315.25,                last time consumption/overall running time: 461.8232s / 115594.3033 s
env0_first_0:                 episode reward: -74.1500,                 loss: 0.3291
env0_second_0:                 episode reward: 74.1500,                 loss: 0.3791
env1_first_0:                 episode reward: -89.2000,                 loss: nan
env1_second_0:                 episode reward: 89.2000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 335.6,                last time consumption/overall running time: 488.7194s / 116083.0226 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.4245
env0_second_0:                 episode reward: 88.5500,                 loss: 0.4001
env1_first_0:                 episode reward: -90.6500,                 loss: nan
env1_second_0:                 episode reward: 90.6500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 314.85,                last time consumption/overall running time: 456.1227s / 116539.1453 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.4924
env0_second_0:                 episode reward: 92.9500,                 loss: 0.5429
env1_first_0:                 episode reward: -92.5500,                 loss: nan
env1_second_0:                 episode reward: 92.5500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 590.25,                last time consumption/overall running time: 854.4890s / 117393.6343 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.5008
env0_second_0:                 episode reward: 93.2000,                 loss: 0.5361
env1_first_0:                 episode reward: -92.0500,                 loss: nan
env1_second_0:                 episode reward: 92.0500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 503.15,                last time consumption/overall running time: 731.9739s / 118125.6082 s
env0_first_0:                 episode reward: -92.0000,                 loss: 0.5256
env0_second_0:                 episode reward: 92.0000,                 loss: 0.5824
env1_first_0:                 episode reward: -94.4500,                 loss: nan
env1_second_0:                 episode reward: 94.4500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 346.75,                last time consumption/overall running time: 506.1669s / 118631.7751 s
env0_first_0:                 episode reward: -88.7000,                 loss: 0.5471
env0_second_0:                 episode reward: 88.7000,                 loss: 0.6060
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 514.0,                last time consumption/overall running time: 735.1552s / 119366.9303 s
env0_first_0:                 episode reward: -89.4500,                 loss: 0.5744
env0_second_0:                 episode reward: 89.4500,                 loss: 0.6101
env1_first_0:                 episode reward: -91.2000,                 loss: nan
env1_second_0:                 episode reward: 91.2000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 880.55,                last time consumption/overall running time: 1274.2910s / 120641.2213 s
env0_first_0:                 episode reward: -88.9500,                 loss: 0.4660
env0_second_0:                 episode reward: 88.9500,                 loss: 0.5384
env1_first_0:                 episode reward: -88.4500,                 loss: nan
env1_second_0:                 episode reward: 88.4500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 741.5,                last time consumption/overall running time: 1072.8868s / 121714.1082 s
env0_first_0:                 episode reward: -95.3500,                 loss: 0.4190
env0_second_0:                 episode reward: 95.3500,                 loss: 0.4668
env1_first_0:                 episode reward: -87.4000,                 loss: nan
env1_second_0:                 episode reward: 87.4000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 667.8,                last time consumption/overall running time: 970.3374s / 122684.4456 s
env0_first_0:                 episode reward: -80.5500,                 loss: 0.3779
env0_second_0:                 episode reward: 80.5500,                 loss: 0.4193
env1_first_0:                 episode reward: -93.9500,                 loss: nan
env1_second_0:                 episode reward: 93.9500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 530.4,                last time consumption/overall running time: 774.1179s / 123458.5635 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.3414
env0_second_0:                 episode reward: 85.6500,                 loss: 0.3638
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 358.55,                last time consumption/overall running time: 519.5646s / 123978.1281 s
env0_first_0:                 episode reward: -78.1500,                 loss: 0.4231
env0_second_0:                 episode reward: 78.1500,                 loss: 0.3966
env1_first_0:                 episode reward: -83.1000,                 loss: nan
env1_second_0:                 episode reward: 83.1000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 402.4,                last time consumption/overall running time: 585.0378s / 124563.1659 s
env0_first_0:                 episode reward: -73.8000,                 loss: 0.4922
env0_second_0:                 episode reward: 73.8000,                 loss: 0.5061
env1_first_0:                 episode reward: -83.7000,                 loss: nan
env1_second_0:                 episode reward: 83.7000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 408.8,                last time consumption/overall running time: 591.6794s / 125154.8453 s
env0_first_0:                 episode reward: -78.2500,                 loss: 0.5286
env0_second_0:                 episode reward: 78.2500,                 loss: 0.5993
env1_first_0:                 episode reward: -83.5500,                 loss: nan
env1_second_0:                 episode reward: 83.5500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 244.0,                last time consumption/overall running time: 355.6106s / 125510.4560 s
env0_first_0:                 episode reward: -84.9000,                 loss: 0.5122
env0_second_0:                 episode reward: 84.9000,                 loss: 0.6352
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 491.65,                last time consumption/overall running time: 708.5895s / 126219.0454 s
env0_first_0:                 episode reward: -91.5500,                 loss: 0.5866
env0_second_0:                 episode reward: 91.5500,                 loss: 0.6703
env1_first_0:                 episode reward: -78.7500,                 loss: nan
env1_second_0:                 episode reward: 78.7500,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1112.15,                last time consumption/overall running time: 1611.6499s / 127830.6953 s
env0_first_0:                 episode reward: -94.4000,                 loss: 0.5217
env0_second_0:                 episode reward: 94.4000,                 loss: 0.6106
env1_first_0:                 episode reward: -91.0500,                 loss: nan
env1_second_0:                 episode reward: 91.0500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 883.8,                last time consumption/overall running time: 1283.9933s / 129114.6887 s
env0_first_0:                 episode reward: -83.7000,                 loss: 0.3614
env0_second_0:                 episode reward: 83.7000,                 loss: 0.4419
env1_first_0:                 episode reward: -94.7000,                 loss: nan
env1_second_0:                 episode reward: 94.7000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 685.35,                last time consumption/overall running time: 988.9772s / 130103.6659 s
env0_first_0:                 episode reward: -94.6500,                 loss: 0.2868
env0_second_0:                 episode reward: 94.6500,                 loss: 0.3560
env1_first_0:                 episode reward: -84.7000,                 loss: nan
env1_second_0:                 episode reward: 84.7000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 959.7,                last time consumption/overall running time: 1387.3831s / 131491.0489 s
env0_first_0:                 episode reward: -90.7000,                 loss: 0.2776
env0_second_0:                 episode reward: 90.7000,                 loss: 0.3742
env1_first_0:                 episode reward: -85.6500,                 loss: nan
env1_second_0:                 episode reward: 85.6500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 721.45,                last time consumption/overall running time: 1045.0632s / 132536.1121 s
env0_first_0:                 episode reward: -87.2500,                 loss: 0.2991
env0_second_0:                 episode reward: 87.2500,                 loss: 0.4123
env1_first_0:                 episode reward: -85.5000,                 loss: nan
env1_second_0:                 episode reward: 85.5000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 1124.7,                last time consumption/overall running time: 1628.6146s / 134164.7267 s
env0_first_0:                 episode reward: -90.1000,                 loss: 0.3389
env0_second_0:                 episode reward: 90.1000,                 loss: 0.4288
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 571.95,                last time consumption/overall running time: 825.5694s / 134990.2961 s
env0_first_0:                 episode reward: -94.3000,                 loss: 0.3809
env0_second_0:                 episode reward: 94.3000,                 loss: 0.4233
env1_first_0:                 episode reward: -88.9500,                 loss: nan
env1_second_0:                 episode reward: 88.9500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 417.35,                last time consumption/overall running time: 604.9693s / 135595.2653 s
env0_first_0:                 episode reward: -83.4500,                 loss: 0.4236
env0_second_0:                 episode reward: 83.4500,                 loss: 0.4589
env1_first_0:                 episode reward: -88.3000,                 loss: nan
env1_second_0:                 episode reward: 88.3000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 445.2,                last time consumption/overall running time: 645.3791s / 136240.6445 s
env0_first_0:                 episode reward: -85.3500,                 loss: 0.4859
env0_second_0:                 episode reward: 85.3500,                 loss: 0.5352
env1_first_0:                 episode reward: -82.4000,                 loss: nan
env1_second_0:                 episode reward: 82.4000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 381.3,                last time consumption/overall running time: 558.3086s / 136798.9531 s
env0_first_0:                 episode reward: -76.3500,                 loss: 0.5187
env0_second_0:                 episode reward: 76.3500,                 loss: 0.5601
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 824.2,                last time consumption/overall running time: 1194.7694s / 137993.7225 s
env0_first_0:                 episode reward: -80.1000,                 loss: 0.5720
env0_second_0:                 episode reward: 80.1000,                 loss: 0.5876
env1_first_0:                 episode reward: -83.7500,                 loss: nan
env1_second_0:                 episode reward: 83.7500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 965.05,                last time consumption/overall running time: 1393.0152s / 139386.7378 s
env0_first_0:                 episode reward: -80.3000,                 loss: 0.5395
env0_second_0:                 episode reward: 80.3000,                 loss: 0.5320
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 725.1,                last time consumption/overall running time: 1043.3628s / 140430.1006 s
env0_first_0:                 episode reward: -89.5000,                 loss: 0.3998
env0_second_0:                 episode reward: 89.5000,                 loss: 0.5153
env1_first_0:                 episode reward: -89.2000,                 loss: nan
env1_second_0:                 episode reward: 89.2000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 361.45,                last time consumption/overall running time: 518.2469s / 140948.3474 s
env0_first_0:                 episode reward: -81.9500,                 loss: 0.4141
env0_second_0:                 episode reward: 81.9500,                 loss: 0.4718
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 714.15,                last time consumption/overall running time: 1036.8802s / 141985.2276 s
env0_first_0:                 episode reward: -81.4000,                 loss: 0.4246
env0_second_0:                 episode reward: 81.4000,                 loss: 0.5537
env1_first_0:                 episode reward: -92.7000,                 loss: nan
env1_second_0:                 episode reward: 92.7000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1094.45,                last time consumption/overall running time: 1577.3791s / 143562.6067 s
env0_first_0:                 episode reward: -89.9000,                 loss: 0.3852
env0_second_0:                 episode reward: 89.9000,                 loss: 0.4133
env1_first_0:                 episode reward: -92.5000,                 loss: nan
env1_second_0:                 episode reward: 92.5000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1436.5,                last time consumption/overall running time: 2044.9104s / 145607.5170 s
env0_first_0:                 episode reward: -91.8000,                 loss: 0.3144
env0_second_0:                 episode reward: 91.8000,                 loss: 0.3411
env1_first_0:                 episode reward: -89.0500,                 loss: nan
env1_second_0:                 episode reward: 89.0500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1286.85,                last time consumption/overall running time: 1817.1138s / 147424.6308 s
env0_first_0:                 episode reward: -87.1000,                 loss: 0.2759
env0_second_0:                 episode reward: 87.1000,                 loss: 0.3041
env1_first_0:                 episode reward: -86.7500,                 loss: nan
env1_second_0:                 episode reward: 86.7500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 886.35,                last time consumption/overall running time: 1248.2126s / 148672.8434 s
env0_first_0:                 episode reward: -89.4500,                 loss: 0.3426
env0_second_0:                 episode reward: 89.4500,                 loss: 0.3890
env1_first_0:                 episode reward: -88.4500,                 loss: nan
env1_second_0:                 episode reward: 88.4500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 854.85,                last time consumption/overall running time: 1196.5475s / 149869.3908 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.3718
env0_second_0:                 episode reward: 88.8000,                 loss: 0.4565
env1_first_0:                 episode reward: -88.2000,                 loss: nan
env1_second_0:                 episode reward: 88.2000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 1007.85,                last time consumption/overall running time: 1415.2384s / 151284.6292 s
env0_first_0:                 episode reward: -91.2500,                 loss: 0.4171
env0_second_0:                 episode reward: 91.2500,                 loss: 0.4668
env1_first_0:                 episode reward: -89.5500,                 loss: nan
env1_second_0:                 episode reward: 89.5500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 557.25,                last time consumption/overall running time: 785.1399s / 152069.7692 s
env0_first_0:                 episode reward: -83.9000,                 loss: 0.4738
env0_second_0:                 episode reward: 83.9000,                 loss: 0.4886
env1_first_0:                 episode reward: -70.7000,                 loss: nan
env1_second_0:                 episode reward: 70.7000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 597.8,                last time consumption/overall running time: 834.0127s / 152903.7819 s
env0_first_0:                 episode reward: -81.2500,                 loss: 0.5371
env0_second_0:                 episode reward: 81.2500,                 loss: 0.5500
env1_first_0:                 episode reward: -76.3000,                 loss: nan
env1_second_0:                 episode reward: 76.3000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 471.7,                last time consumption/overall running time: 660.6714s / 153564.4533 s
env0_first_0:                 episode reward: -78.1500,                 loss: 0.5088
env0_second_0:                 episode reward: 78.1500,                 loss: 0.5895
env1_first_0:                 episode reward: -89.6000,                 loss: nan
env1_second_0:                 episode reward: 89.6000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 256.45,                last time consumption/overall running time: 361.0049s / 153925.4581 s
env0_first_0:                 episode reward: -87.8500,                 loss: 0.5166
env0_second_0:                 episode reward: 87.8500,                 loss: 0.6134
env1_first_0:                 episode reward: -73.3500,                 loss: nan
env1_second_0:                 episode reward: 73.3500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 330.7,                last time consumption/overall running time: 465.4145s / 154390.8726 s
env0_first_0:                 episode reward: -93.7000,                 loss: 0.5554
env0_second_0:                 episode reward: 93.7000,                 loss: 0.7162
env1_first_0:                 episode reward: -80.4000,                 loss: nan
env1_second_0:                 episode reward: 80.4000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 579.55,                last time consumption/overall running time: 809.6634s / 155200.5361 s
env0_first_0:                 episode reward: -86.7000,                 loss: 0.5898
env0_second_0:                 episode reward: 86.7000,                 loss: 0.7184
env1_first_0:                 episode reward: -79.1000,                 loss: nan
env1_second_0:                 episode reward: 79.1000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 817.15,                last time consumption/overall running time: 1142.9585s / 156343.4946 s
env0_first_0:                 episode reward: -89.9500,                 loss: 0.5163
env0_second_0:                 episode reward: 89.9500,                 loss: 0.7107
env1_first_0:                 episode reward: -66.7000,                 loss: nan
env1_second_0:                 episode reward: 66.7000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 974.15,                last time consumption/overall running time: 1354.5281s / 157698.0226 s
env0_first_0:                 episode reward: -90.9000,                 loss: 0.4565
env0_second_0:                 episode reward: 90.9000,                 loss: 0.5671
env1_first_0:                 episode reward: -81.8000,                 loss: nan
env1_second_0:                 episode reward: 81.8000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 474.8,                last time consumption/overall running time: 668.0769s / 158366.0995 s
env0_first_0:                 episode reward: -72.7000,                 loss: 0.3737
env0_second_0:                 episode reward: 72.7000,                 loss: 0.4625
env1_first_0:                 episode reward: -84.2000,                 loss: nan
env1_second_0:                 episode reward: 84.2000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 822.4,                last time consumption/overall running time: 1147.6614s / 159513.7609 s
env0_first_0:                 episode reward: -84.4000,                 loss: 0.5008
env0_second_0:                 episode reward: 84.4000,                 loss: 0.5643
env1_first_0:                 episode reward: -81.3500,                 loss: nan
env1_second_0:                 episode reward: 81.3500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 615.05,                last time consumption/overall running time: 860.1982s / 160373.9591 s
env0_first_0:                 episode reward: -87.0000,                 loss: 0.5010
env0_second_0:                 episode reward: 87.0000,                 loss: 0.6115
env1_first_0:                 episode reward: -84.0000,                 loss: nan
env1_second_0:                 episode reward: 84.0000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 875.4,                last time consumption/overall running time: 1216.2943s / 161590.2535 s
env0_first_0:                 episode reward: -77.8500,                 loss: 0.5765
env0_second_0:                 episode reward: 77.8500,                 loss: 0.6018
env1_first_0:                 episode reward: -88.8500,                 loss: nan
env1_second_0:                 episode reward: 88.8500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 933.55,                last time consumption/overall running time: 1295.9165s / 162886.1699 s
env0_first_0:                 episode reward: -86.0500,                 loss: 0.4290
env0_second_0:                 episode reward: 86.0500,                 loss: 0.4922
env1_first_0:                 episode reward: -78.6000,                 loss: nan
env1_second_0:                 episode reward: 78.6000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 455.85,                last time consumption/overall running time: 628.5253s / 163514.6952 s
env0_first_0:                 episode reward: -90.0000,                 loss: 0.4434
env0_second_0:                 episode reward: 90.0000,                 loss: 0.4847
env1_first_0:                 episode reward: -81.8000,                 loss: nan
env1_second_0:                 episode reward: 81.8000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 390.15,                last time consumption/overall running time: 544.2838s / 164058.9790 s
env0_first_0:                 episode reward: -84.5000,                 loss: 0.4113
env0_second_0:                 episode reward: 84.5000,                 loss: 0.5346
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 484.75,                last time consumption/overall running time: 674.8078s / 164733.7868 s
env0_first_0:                 episode reward: -89.9500,                 loss: 0.4704
env0_second_0:                 episode reward: 89.9500,                 loss: 0.5523
env1_first_0:                 episode reward: -89.8500,                 loss: nan
env1_second_0:                 episode reward: 89.8500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 325.45,                last time consumption/overall running time: 451.6646s / 165185.4514 s
env0_first_0:                 episode reward: -72.0500,                 loss: 0.5594
env0_second_0:                 episode reward: 72.0500,                 loss: 0.6112
env1_first_0:                 episode reward: -86.1000,                 loss: nan
env1_second_0:                 episode reward: 86.1000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 309.7,                last time consumption/overall running time: 431.2357s / 165616.6870 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.6319
env0_second_0:                 episode reward: 89.0000,                 loss: 0.7306
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 437.1,                last time consumption/overall running time: 608.7174s / 166225.4045 s
env0_first_0:                 episode reward: -81.0500,                 loss: 0.6328
env0_second_0:                 episode reward: 81.0500,                 loss: 0.7236
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 576.55,                last time consumption/overall running time: 806.4857s / 167031.8902 s
env0_first_0:                 episode reward: -75.5000,                 loss: 0.6228
env0_second_0:                 episode reward: 75.5000,                 loss: 0.7611
env1_first_0:                 episode reward: -91.2500,                 loss: nan
env1_second_0:                 episode reward: 91.2500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 602.5,                last time consumption/overall running time: 850.2907s / 167882.1809 s
env0_first_0:                 episode reward: -83.5000,                 loss: 0.6043
env0_second_0:                 episode reward: 83.5000,                 loss: 0.7706
env1_first_0:                 episode reward: -90.1000,                 loss: nan
env1_second_0:                 episode reward: 90.1000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 602.15,                last time consumption/overall running time: 836.5714s / 168718.7523 s
env0_first_0:                 episode reward: -75.3000,                 loss: 0.5350
env0_second_0:                 episode reward: 75.3000,                 loss: 0.6493
env1_first_0:                 episode reward: -77.5000,                 loss: nan
env1_second_0:                 episode reward: 77.5000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 545.85,                last time consumption/overall running time: 760.8731s / 169479.6254 s
env0_first_0:                 episode reward: -72.1500,                 loss: 0.4623
env0_second_0:                 episode reward: 72.1500,                 loss: 0.5224
env1_first_0:                 episode reward: -89.7000,                 loss: nan
env1_second_0:                 episode reward: 89.7000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 417.2,                last time consumption/overall running time: 582.9598s / 170062.5852 s
env0_first_0:                 episode reward: -89.9000,                 loss: 0.4538
env0_second_0:                 episode reward: 89.9000,                 loss: 0.6037
env1_first_0:                 episode reward: -88.1500,                 loss: nan
env1_second_0:                 episode reward: 88.1500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 317.9,                last time consumption/overall running time: 445.6706s / 170508.2558 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.5342
env0_second_0:                 episode reward: 90.3000,                 loss: 0.6746
env1_first_0:                 episode reward: -84.2500,                 loss: nan
env1_second_0:                 episode reward: 84.2500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 373.75,                last time consumption/overall running time: 521.7932s / 171030.0490 s
env0_first_0:                 episode reward: -82.9000,                 loss: 0.5667
env0_second_0:                 episode reward: 82.9000,                 loss: 0.6292
env1_first_0:                 episode reward: -88.8000,                 loss: nan
env1_second_0:                 episode reward: 88.8000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 350.75,                last time consumption/overall running time: 488.5428s / 171518.5918 s
env0_first_0:                 episode reward: -81.8500,                 loss: 0.5470
env0_second_0:                 episode reward: 81.8500,                 loss: 0.6485
env1_first_0:                 episode reward: -87.9000,                 loss: nan
env1_second_0:                 episode reward: 87.9000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 377.45,                last time consumption/overall running time: 525.4774s / 172044.0692 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.5740
env0_second_0:                 episode reward: 88.6000,                 loss: 0.6700
env1_first_0:                 episode reward: -85.3500,                 loss: nan
env1_second_0:                 episode reward: 85.3500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 379.05,                last time consumption/overall running time: 525.8619s / 172569.9310 s
env0_first_0:                 episode reward: -92.5000,                 loss: 0.6300
env0_second_0:                 episode reward: 92.5000,                 loss: 0.7742
env1_first_0:                 episode reward: -80.7000,                 loss: nan
env1_second_0:                 episode reward: 80.7000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 318.8,                last time consumption/overall running time: 449.8601s / 173019.7911 s
env0_first_0:                 episode reward: -76.2000,                 loss: 0.6638
env0_second_0:                 episode reward: 76.2000,                 loss: 0.7837
env1_first_0:                 episode reward: -88.0500,                 loss: nan
env1_second_0:                 episode reward: 88.0500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 412.45,                last time consumption/overall running time: 581.1848s / 173600.9759 s
env0_first_0:                 episode reward: -88.2000,                 loss: 0.6911
env0_second_0:                 episode reward: 88.2000,                 loss: 0.8644
env1_first_0:                 episode reward: -66.3000,                 loss: nan
env1_second_0:                 episode reward: 66.3000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 480.2,                last time consumption/overall running time: 675.4959s / 174276.4718 s
env0_first_0:                 episode reward: -90.7000,                 loss: 0.7289
env0_second_0:                 episode reward: 90.7000,                 loss: 0.8229
env1_first_0:                 episode reward: -90.4500,                 loss: nan
env1_second_0:                 episode reward: 90.4500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 484.05,                last time consumption/overall running time: 682.3497s / 174958.8214 s
env0_first_0:                 episode reward: -81.9000,                 loss: 0.6629
env0_second_0:                 episode reward: 81.9000,                 loss: 0.7993
env1_first_0:                 episode reward: -86.3500,                 loss: nan
env1_second_0:                 episode reward: 86.3500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 375.95,                last time consumption/overall running time: 526.7410s / 175485.5624 s
env0_first_0:                 episode reward: -77.2500,                 loss: 0.6225
env0_second_0:                 episode reward: 77.2500,                 loss: 0.7574
env1_first_0:                 episode reward: -90.7000,                 loss: nan
env1_second_0:                 episode reward: 90.7000,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 352.8,                last time consumption/overall running time: 490.8592s / 175976.4217 s
env0_first_0:                 episode reward: -74.8000,                 loss: 0.6003
env0_second_0:                 episode reward: 74.8000,                 loss: 0.7365
env1_first_0:                 episode reward: -68.4000,                 loss: nan
env1_second_0:                 episode reward: 68.4000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 360.0,                last time consumption/overall running time: 501.0375s / 176477.4592 s
env0_first_0:                 episode reward: -94.4000,                 loss: 0.5923
env0_second_0:                 episode reward: 94.4000,                 loss: 0.7022
env1_first_0:                 episode reward: -70.8500,                 loss: nan
env1_second_0:                 episode reward: 70.8500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 350.95,                last time consumption/overall running time: 487.2824s / 176964.7416 s
env0_first_0:                 episode reward: -65.4500,                 loss: 0.5860
env0_second_0:                 episode reward: 65.4500,                 loss: 0.6920
env1_first_0:                 episode reward: -88.0500,                 loss: nan
env1_second_0:                 episode reward: 88.0500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 346.4,                last time consumption/overall running time: 478.2063s / 177442.9479 s
env0_first_0:                 episode reward: -78.3500,                 loss: 0.5043
env0_second_0:                 episode reward: 78.3500,                 loss: 0.6649
env1_first_0:                 episode reward: -83.9000,                 loss: nan
env1_second_0:                 episode reward: 83.9000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 325.05,                last time consumption/overall running time: 442.9029s / 177885.8508 s
env0_first_0:                 episode reward: -80.4000,                 loss: 0.5315
env0_second_0:                 episode reward: 80.4000,                 loss: 0.6773
env1_first_0:                 episode reward: -68.4500,                 loss: nan
env1_second_0:                 episode reward: 68.4500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 692.75,                last time consumption/overall running time: 967.4899s / 178853.3407 s
env0_first_0:                 episode reward: -51.2000,                 loss: 0.5228
env0_second_0:                 episode reward: 51.2000,                 loss: 0.7021
env1_first_0:                 episode reward: -67.1000,                 loss: nan
env1_second_0:                 episode reward: 67.1000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 777.75,                last time consumption/overall running time: 1099.1597s / 179952.5004 s
env0_first_0:                 episode reward: -68.8500,                 loss: 0.4973
env0_second_0:                 episode reward: 68.8500,                 loss: 0.5741
env1_first_0:                 episode reward: -64.6500,                 loss: nan
env1_second_0:                 episode reward: 64.6500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 751.4,                last time consumption/overall running time: 1058.0696s / 181010.5700 s
env0_first_0:                 episode reward: -47.8500,                 loss: 0.4336
env0_second_0:                 episode reward: 47.8500,                 loss: 0.5148
env1_first_0:                 episode reward: -50.4000,                 loss: nan
env1_second_0:                 episode reward: 50.4000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1350.05,                last time consumption/overall running time: 1903.8393s / 182914.4093 s
env0_first_0:                 episode reward: -20.1000,                 loss: 0.3052
env0_second_0:                 episode reward: 20.1000,                 loss: 0.3805
env1_first_0:                 episode reward: -18.9500,                 loss: nan
env1_second_0:                 episode reward: 18.9500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 867.8,                last time consumption/overall running time: 1230.1304s / 184144.5397 s
env0_first_0:                 episode reward: -48.3500,                 loss: 0.2444
env0_second_0:                 episode reward: 48.3500,                 loss: 0.3254
env1_first_0:                 episode reward: -27.3500,                 loss: nan
env1_second_0:                 episode reward: 27.3500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1020.25,                last time consumption/overall running time: 1446.2989s / 185590.8387 s
env0_first_0:                 episode reward: -29.9000,                 loss: 0.1908
env0_second_0:                 episode reward: 29.9000,                 loss: 0.2412
env1_first_0:                 episode reward: -36.9000,                 loss: nan
env1_second_0:                 episode reward: 36.9000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 737.2,                last time consumption/overall running time: 1039.2329s / 186630.0716 s
env0_first_0:                 episode reward: -64.2000,                 loss: 0.2535
env0_second_0:                 episode reward: 64.2000,                 loss: 0.2655
env1_first_0:                 episode reward: -51.5000,                 loss: nan
env1_second_0:                 episode reward: 51.5000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 355.4,                last time consumption/overall running time: 497.0962s / 187127.1678 s
env0_first_0:                 episode reward: -81.7000,                 loss: 0.3983
env0_second_0:                 episode reward: 81.7000,                 loss: 0.3884
env1_first_0:                 episode reward: -75.0000,                 loss: nan
env1_second_0:                 episode reward: 75.0000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 323.1,                last time consumption/overall running time: 454.3413s / 187581.5090 s
env0_first_0:                 episode reward: -92.1500,                 loss: 0.5159
env0_second_0:                 episode reward: 92.1500,                 loss: 0.4722
env1_first_0:                 episode reward: -75.2500,                 loss: nan
env1_second_0:                 episode reward: 75.2500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 297.6,                last time consumption/overall running time: 417.6263s / 187999.1353 s
env0_first_0:                 episode reward: -82.9000,                 loss: 0.5498
env0_second_0:                 episode reward: 82.9000,                 loss: 0.5749
env1_first_0:                 episode reward: -62.4500,                 loss: nan
env1_second_0:                 episode reward: 62.4500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 248.0,                last time consumption/overall running time: 351.2104s / 188350.3457 s
env0_first_0:                 episode reward: -85.4000,                 loss: 0.6247
env0_second_0:                 episode reward: 85.4000,                 loss: 0.6598
env1_first_0:                 episode reward: -86.3000,                 loss: nan
env1_second_0:                 episode reward: 86.3000,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 247.3,                last time consumption/overall running time: 348.7751s / 188699.1208 s
env0_first_0:                 episode reward: -76.4000,                 loss: 0.6973
env0_second_0:                 episode reward: 76.4000,                 loss: 0.6706
env1_first_0:                 episode reward: -84.6500,                 loss: nan
env1_second_0:                 episode reward: 84.6500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 348.1,                last time consumption/overall running time: 480.6037s / 189179.7245 s
env0_first_0:                 episode reward: -71.8000,                 loss: 0.7694
env0_second_0:                 episode reward: 71.8000,                 loss: 0.6983
env1_first_0:                 episode reward: -77.0500,                 loss: nan
env1_second_0:                 episode reward: 77.0500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 258.7,                last time consumption/overall running time: 350.3950s / 189530.1195 s
env0_first_0:                 episode reward: -76.7000,                 loss: 0.7848
env0_second_0:                 episode reward: 76.7000,                 loss: 0.7275
env1_first_0:                 episode reward: -85.6500,                 loss: nan
env1_second_0:                 episode reward: 85.6500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 271.95,                last time consumption/overall running time: 365.5076s / 189895.6271 s
env0_first_0:                 episode reward: -85.3000,                 loss: 0.8799
env0_second_0:                 episode reward: 85.3000,                 loss: 0.7489
env1_first_0:                 episode reward: -83.0500,                 loss: nan
env1_second_0:                 episode reward: 83.0500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 355.0,                last time consumption/overall running time: 480.0284s / 190375.6555 s
env0_first_0:                 episode reward: -81.7500,                 loss: 0.8427
env0_second_0:                 episode reward: 81.7500,                 loss: 0.7796
env1_first_0:                 episode reward: -78.0000,                 loss: nan
env1_second_0:                 episode reward: 78.0000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 239.05,                last time consumption/overall running time: 325.2112s / 190700.8667 s
env0_first_0:                 episode reward: -70.1500,                 loss: 0.7708
env0_second_0:                 episode reward: 70.1500,                 loss: 0.7311
env1_first_0:                 episode reward: -81.7000,                 loss: nan
env1_second_0:                 episode reward: 81.7000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 386.6,                last time consumption/overall running time: 521.2822s / 191222.1489 s
env0_first_0:                 episode reward: -58.7500,                 loss: 0.8191
env0_second_0:                 episode reward: 58.7500,                 loss: 0.7638
env1_first_0:                 episode reward: -82.7000,                 loss: nan
env1_second_0:                 episode reward: 82.7000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 583.5,                last time consumption/overall running time: 775.3214s / 191997.4703 s
env0_first_0:                 episode reward: -61.0500,                 loss: 0.6569
env0_second_0:                 episode reward: 61.0500,                 loss: 0.6324
env1_first_0:                 episode reward: -43.7000,                 loss: nan
env1_second_0:                 episode reward: 43.7000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 737.55,                last time consumption/overall running time: 983.1101s / 192980.5804 s
env0_first_0:                 episode reward: -49.9500,                 loss: 0.5494
env0_second_0:                 episode reward: 49.9500,                 loss: 0.5360
env1_first_0:                 episode reward: -52.6500,                 loss: nan
env1_second_0:                 episode reward: 52.6500,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 350.65,                last time consumption/overall running time: 473.0365s / 193453.6170 s
env0_first_0:                 episode reward: -72.5000,                 loss: 0.5151
env0_second_0:                 episode reward: 72.5000,                 loss: 0.4871
env1_first_0:                 episode reward: -81.3500,                 loss: nan
env1_second_0:                 episode reward: 81.3500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 275.05,                last time consumption/overall running time: 380.5844s / 193834.2013 s
env0_first_0:                 episode reward: -80.8000,                 loss: 0.5038
env0_second_0:                 episode reward: 80.8000,                 loss: 0.4697
env1_first_0:                 episode reward: -76.9500,                 loss: nan
env1_second_0:                 episode reward: 76.9500,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 285.15,                last time consumption/overall running time: 390.9664s / 194225.1677 s
env0_first_0:                 episode reward: -89.1500,                 loss: 0.5323
env0_second_0:                 episode reward: 89.1500,                 loss: 0.4504
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 249.8,                last time consumption/overall running time: 344.8445s / 194570.0122 s
env0_first_0:                 episode reward: -70.5500,                 loss: 0.4870
env0_second_0:                 episode reward: 70.5500,                 loss: 0.4377
env1_first_0:                 episode reward: -81.4000,                 loss: nan
env1_second_0:                 episode reward: 81.4000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 309.6,                last time consumption/overall running time: 426.7659s / 194996.7782 s
env0_first_0:                 episode reward: -83.7500,                 loss: 0.6320
env0_second_0:                 episode reward: 83.7500,                 loss: 0.5328
env1_first_0:                 episode reward: -85.0000,                 loss: nan
env1_second_0:                 episode reward: 85.0000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 234.55,                last time consumption/overall running time: 319.9127s / 195316.6909 s
env0_first_0:                 episode reward: -74.4500,                 loss: 0.6006
env0_second_0:                 episode reward: 74.4500,                 loss: 0.5792
env1_first_0:                 episode reward: -87.7000,                 loss: nan
env1_second_0:                 episode reward: 87.7000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 311.6,                last time consumption/overall running time: 421.4376s / 195738.1285 s
env0_first_0:                 episode reward: -78.8000,                 loss: 0.6089
env0_second_0:                 episode reward: 78.8000,                 loss: 0.5840
env1_first_0:                 episode reward: -93.4000,                 loss: nan
env1_second_0:                 episode reward: 93.4000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 231.65,                last time consumption/overall running time: 307.2498s / 196045.3783 s
env0_first_0:                 episode reward: -92.3000,                 loss: 0.6927
env0_second_0:                 episode reward: 92.3000,                 loss: 0.5833
env1_first_0:                 episode reward: -79.9000,                 loss: nan
env1_second_0:                 episode reward: 79.9000,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 243.0,                last time consumption/overall running time: 325.2149s / 196370.5932 s
env0_first_0:                 episode reward: -75.2000,                 loss: 0.6644
env0_second_0:                 episode reward: 75.2000,                 loss: 0.6161
env1_first_0:                 episode reward: -88.0000,                 loss: nan
env1_second_0:                 episode reward: 88.0000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 271.1,                last time consumption/overall running time: 359.2662s / 196729.8594 s
env0_first_0:                 episode reward: -88.1000,                 loss: 0.6750
env0_second_0:                 episode reward: 88.1000,                 loss: 0.6106
env1_first_0:                 episode reward: -72.0500,                 loss: nan
env1_second_0:                 episode reward: 72.0500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 259.65,                last time consumption/overall running time: 345.9693s / 197075.8286 s
env0_first_0:                 episode reward: -76.8000,                 loss: 0.6151
env0_second_0:                 episode reward: 76.8000,                 loss: 0.5855
env1_first_0:                 episode reward: -89.5500,                 loss: nan
env1_second_0:                 episode reward: 89.5500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 255.65,                last time consumption/overall running time: 339.3400s / 197415.1686 s
env0_first_0:                 episode reward: -71.5500,                 loss: 0.6147
env0_second_0:                 episode reward: 71.5500,                 loss: 0.5928
env1_first_0:                 episode reward: -83.3500,                 loss: nan
env1_second_0:                 episode reward: 83.3500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 277.05,                last time consumption/overall running time: 365.1154s / 197780.2840 s
env0_first_0:                 episode reward: -85.3500,                 loss: 0.6482
env0_second_0:                 episode reward: 85.3500,                 loss: 0.6271
env1_first_0:                 episode reward: -80.5500,                 loss: nan
env1_second_0:                 episode reward: 80.5500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 261.75,                last time consumption/overall running time: 346.6995s / 198126.9835 s
env0_first_0:                 episode reward: -64.6500,                 loss: 0.6561
env0_second_0:                 episode reward: 64.6500,                 loss: 0.6363
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 273.65,                last time consumption/overall running time: 361.6123s / 198488.5957 s
env0_first_0:                 episode reward: -85.8000,                 loss: 0.6299
env0_second_0:                 episode reward: 85.8000,                 loss: 0.6322
env1_first_0:                 episode reward: -79.5500,                 loss: nan
env1_second_0:                 episode reward: 79.5500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 407.15,                last time consumption/overall running time: 533.5892s / 199022.1849 s
env0_first_0:                 episode reward: -62.7000,                 loss: 0.5875
env0_second_0:                 episode reward: 62.7000,                 loss: 0.6018
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 233.35,                last time consumption/overall running time: 302.3165s / 199324.5014 s
env0_first_0:                 episode reward: -92.3000,                 loss: 0.5694
env0_second_0:                 episode reward: 92.3000,                 loss: 0.5626
env1_first_0:                 episode reward: -79.1500,                 loss: nan
env1_second_0:                 episode reward: 79.1500,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 355.7,                last time consumption/overall running time: 461.0657s / 199785.5671 s
env0_first_0:                 episode reward: -80.9500,                 loss: 0.5922
env0_second_0:                 episode reward: 80.9500,                 loss: 0.6399
env1_first_0:                 episode reward: -85.2500,                 loss: nan
env1_second_0:                 episode reward: 85.2500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 526.2,                last time consumption/overall running time: 670.7843s / 200456.3514 s
env0_first_0:                 episode reward: -72.5500,                 loss: 0.7012
env0_second_0:                 episode reward: 72.5500,                 loss: 0.6689
env1_first_0:                 episode reward: -90.9000,                 loss: nan
env1_second_0:                 episode reward: 90.9000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 412.9,                last time consumption/overall running time: 526.6154s / 200982.9668 s
env0_first_0:                 episode reward: -87.1500,                 loss: 0.5997
env0_second_0:                 episode reward: 87.1500,                 loss: 0.5629
env1_first_0:                 episode reward: -63.0500,                 loss: nan
env1_second_0:                 episode reward: 63.0500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 249.6,                last time consumption/overall running time: 319.7699s / 201302.7367 s
env0_first_0:                 episode reward: -85.5500,                 loss: 0.6134
env0_second_0:                 episode reward: 85.5500,                 loss: 0.5591
env1_first_0:                 episode reward: -75.9500,                 loss: nan
env1_second_0:                 episode reward: 75.9500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 264.65,                last time consumption/overall running time: 340.2991s / 201643.0358 s
env0_first_0:                 episode reward: -77.1000,                 loss: 0.5474
env0_second_0:                 episode reward: 77.1000,                 loss: 0.5494
env1_first_0:                 episode reward: -71.1000,                 loss: nan
env1_second_0:                 episode reward: 71.1000,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 222.95,                last time consumption/overall running time: 289.8658s / 201932.9016 s
env0_first_0:                 episode reward: -90.4000,                 loss: 0.5746
env0_second_0:                 episode reward: 90.4000,                 loss: 0.5055
env1_first_0:                 episode reward: -82.0000,                 loss: nan
env1_second_0:                 episode reward: 82.0000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 273.9,                last time consumption/overall running time: 351.7931s / 202284.6947 s
env0_first_0:                 episode reward: -78.0500,                 loss: 0.5516
env0_second_0:                 episode reward: 78.0500,                 loss: 0.6104
env1_first_0:                 episode reward: -90.4500,                 loss: nan
env1_second_0:                 episode reward: 90.4500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 292.55,                last time consumption/overall running time: 376.5559s / 202661.2506 s
env0_first_0:                 episode reward: -86.0500,                 loss: 0.5847
env0_second_0:                 episode reward: 86.0500,                 loss: 0.6380
env1_first_0:                 episode reward: -93.4000,                 loss: nan
env1_second_0:                 episode reward: 93.4000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 297.35,                last time consumption/overall running time: 380.5206s / 203041.7712 s
env0_first_0:                 episode reward: -90.6000,                 loss: 0.5865
env0_second_0:                 episode reward: 90.6000,                 loss: 0.5958
env1_first_0:                 episode reward: -70.3500,                 loss: nan
env1_second_0:                 episode reward: 70.3500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 373.8,                last time consumption/overall running time: 479.0874s / 203520.8586 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.5760
env0_second_0:                 episode reward: 86.6500,                 loss: 0.6279
env1_first_0:                 episode reward: -71.0500,                 loss: nan
env1_second_0:                 episode reward: 71.0500,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 378.85,                last time consumption/overall running time: 494.0365s / 204014.8951 s
env0_first_0:                 episode reward: -71.5500,                 loss: 0.5587
env0_second_0:                 episode reward: 71.5500,                 loss: 0.6025
env1_first_0:                 episode reward: -76.3500,                 loss: nan
env1_second_0:                 episode reward: 76.3500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 580.55,                last time consumption/overall running time: 742.3913s / 204757.2865 s
env0_first_0:                 episode reward: -61.3000,                 loss: 0.5543
env0_second_0:                 episode reward: 61.3000,                 loss: 0.6123
env1_first_0:                 episode reward: -65.1000,                 loss: nan
env1_second_0:                 episode reward: 65.1000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 540.45,                last time consumption/overall running time: 691.6403s / 205448.9268 s
env0_first_0:                 episode reward: -85.2500,                 loss: 0.4936
env0_second_0:                 episode reward: 85.2500,                 loss: 0.4901
env1_first_0:                 episode reward: -83.8500,                 loss: nan
env1_second_0:                 episode reward: 83.8500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 582.2,                last time consumption/overall running time: 749.7528s / 206198.6796 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.5021
env0_second_0:                 episode reward: 92.4500,                 loss: 0.4151
env1_first_0:                 episode reward: -61.4000,                 loss: nan
env1_second_0:                 episode reward: 61.4000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 242.9,                last time consumption/overall running time: 311.3712s / 206510.0508 s
env0_first_0:                 episode reward: -70.5500,                 loss: 0.4138
env0_second_0:                 episode reward: 70.5500,                 loss: 0.3565
env1_first_0:                 episode reward: -84.1500,                 loss: nan
env1_second_0:                 episode reward: 84.1500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 358.85,                last time consumption/overall running time: 460.7316s / 206970.7824 s
env0_first_0:                 episode reward: -62.0500,                 loss: 0.4488
env0_second_0:                 episode reward: 62.0500,                 loss: 0.3576
env1_first_0:                 episode reward: -94.0500,                 loss: nan
env1_second_0:                 episode reward: 94.0500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 327.8,                last time consumption/overall running time: 424.5733s / 207395.3557 s
env0_first_0:                 episode reward: -71.8000,                 loss: 0.4502
env0_second_0:                 episode reward: 71.8000,                 loss: 0.3889
env1_first_0:                 episode reward: -75.4500,                 loss: nan
env1_second_0:                 episode reward: 75.4500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 267.45,                last time consumption/overall running time: 344.5252s / 207739.8809 s
env0_first_0:                 episode reward: -78.3500,                 loss: 0.5080
env0_second_0:                 episode reward: 78.3500,                 loss: 0.4311
env1_first_0:                 episode reward: -76.3000,                 loss: nan
env1_second_0:                 episode reward: 76.3000,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 249.45,                last time consumption/overall running time: 322.4800s / 208062.3609 s
env0_first_0:                 episode reward: -80.0500,                 loss: 0.4957
env0_second_0:                 episode reward: 80.0500,                 loss: 0.4598
env1_first_0:                 episode reward: -80.5000,                 loss: nan
env1_second_0:                 episode reward: 80.5000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 304.3,                last time consumption/overall running time: 393.1894s / 208455.5503 s
env0_first_0:                 episode reward: -79.0000,                 loss: 0.5428
env0_second_0:                 episode reward: 79.0000,                 loss: 0.4737
env1_first_0:                 episode reward: -92.2000,                 loss: nan
env1_second_0:                 episode reward: 92.2000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 244.1,                last time consumption/overall running time: 313.4588s / 208769.0091 s
env0_first_0:                 episode reward: -72.9500,                 loss: 0.5103
env0_second_0:                 episode reward: 72.9500,                 loss: 0.5384
env1_first_0:                 episode reward: -97.3500,                 loss: nan
env1_second_0:                 episode reward: 97.3500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 267.2,                last time consumption/overall running time: 344.2990s / 209113.3081 s
env0_first_0:                 episode reward: -80.9500,                 loss: 0.5863
env0_second_0:                 episode reward: 80.9500,                 loss: 0.5536
env1_first_0:                 episode reward: -82.9500,                 loss: nan
env1_second_0:                 episode reward: 82.9500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 263.15,                last time consumption/overall running time: 336.8635s / 209450.1716 s
env0_first_0:                 episode reward: -85.2000,                 loss: 0.6231
env0_second_0:                 episode reward: 85.2000,                 loss: 0.6488
env1_first_0:                 episode reward: -62.4500,                 loss: nan
env1_second_0:                 episode reward: 62.4500,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 241.65,                last time consumption/overall running time: 312.6855s / 209762.8571 s
env0_first_0:                 episode reward: -87.1000,                 loss: 0.6413
env0_second_0:                 episode reward: 87.1000,                 loss: 0.5870
env1_first_0:                 episode reward: -76.3500,                 loss: nan
env1_second_0:                 episode reward: 76.3500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 250.35,                last time consumption/overall running time: 321.8289s / 210084.6860 s
env0_first_0:                 episode reward: -81.3500,                 loss: 0.6647
env0_second_0:                 episode reward: 81.3500,                 loss: 0.6957
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 255.1,                last time consumption/overall running time: 325.7178s / 210410.4039 s
env0_first_0:                 episode reward: -80.1000,                 loss: 0.6572
env0_second_0:                 episode reward: 80.1000,                 loss: 0.6350
env1_first_0:                 episode reward: -69.5000,                 loss: nan
env1_second_0:                 episode reward: 69.5000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 285.05,                last time consumption/overall running time: 363.7270s / 210774.1308 s
env0_first_0:                 episode reward: -70.8000,                 loss: 0.6795
env0_second_0:                 episode reward: 70.8000,                 loss: 0.6379
env1_first_0:                 episode reward: -80.8500,                 loss: nan
env1_second_0:                 episode reward: 80.8500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 292.3,                last time consumption/overall running time: 378.5268s / 211152.6576 s
env0_first_0:                 episode reward: -81.2000,                 loss: 0.6743
env0_second_0:                 episode reward: 81.2000,                 loss: 0.6218
env1_first_0:                 episode reward: -87.3500,                 loss: nan
env1_second_0:                 episode reward: 87.3500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 239.2,                last time consumption/overall running time: 306.8946s / 211459.5522 s
env0_first_0:                 episode reward: -72.2500,                 loss: 0.6775
env0_second_0:                 episode reward: 72.2500,                 loss: 0.5771
env1_first_0:                 episode reward: -82.2000,                 loss: nan
env1_second_0:                 episode reward: 82.2000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 253.35,                last time consumption/overall running time: 327.4517s / 211787.0039 s
env0_first_0:                 episode reward: -88.1000,                 loss: 0.6276
env0_second_0:                 episode reward: 88.1000,                 loss: 0.6292
env1_first_0:                 episode reward: -83.6000,                 loss: nan
env1_second_0:                 episode reward: 83.6000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 277.5,                last time consumption/overall running time: 356.2305s / 212143.2343 s
env0_first_0:                 episode reward: -79.1000,                 loss: 0.6073
env0_second_0:                 episode reward: 79.1000,                 loss: 0.6031
env1_first_0:                 episode reward: -72.5500,                 loss: nan
env1_second_0:                 episode reward: 72.5500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 368.5,                last time consumption/overall running time: 470.6975s / 212613.9319 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.6327
env0_second_0:                 episode reward: 87.3500,                 loss: 0.6129
env1_first_0:                 episode reward: -78.1000,                 loss: nan
env1_second_0:                 episode reward: 78.1000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 391.9,                last time consumption/overall running time: 501.8228s / 213115.7547 s
env0_first_0:                 episode reward: -67.1500,                 loss: 0.6229
env0_second_0:                 episode reward: 67.1500,                 loss: 0.5542
env1_first_0:                 episode reward: -85.7500,                 loss: nan
env1_second_0:                 episode reward: 85.7500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 701.65,                last time consumption/overall running time: 895.6358s / 214011.3905 s
env0_first_0:                 episode reward: -70.8000,                 loss: 0.5469
env0_second_0:                 episode reward: 70.8000,                 loss: 0.4926
env1_first_0:                 episode reward: -76.0500,                 loss: nan
env1_second_0:                 episode reward: 76.0500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 422.05,                last time consumption/overall running time: 541.2165s / 214552.6071 s
env0_first_0:                 episode reward: -68.3000,                 loss: 0.5050
env0_second_0:                 episode reward: 68.3000,                 loss: 0.4566
env1_first_0:                 episode reward: -75.1500,                 loss: nan
env1_second_0:                 episode reward: 75.1500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 448.4,                last time consumption/overall running time: 573.9230s / 215126.5301 s
env0_first_0:                 episode reward: -81.2000,                 loss: 0.4757
env0_second_0:                 episode reward: 81.2000,                 loss: 0.4809
env1_first_0:                 episode reward: -78.9500,                 loss: nan
env1_second_0:                 episode reward: 78.9500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 313.85,                last time consumption/overall running time: 404.7337s / 215531.2638 s
env0_first_0:                 episode reward: -79.5500,                 loss: 0.4573
env0_second_0:                 episode reward: 79.5500,                 loss: 0.4320
env1_first_0:                 episode reward: -87.3500,                 loss: nan
env1_second_0:                 episode reward: 87.3500,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 316.95,                last time consumption/overall running time: 408.7434s / 215940.0073 s
env0_first_0:                 episode reward: -70.7500,                 loss: 0.4649
env0_second_0:                 episode reward: 70.7500,                 loss: 0.4143
env1_first_0:                 episode reward: -87.1000,                 loss: nan
env1_second_0:                 episode reward: 87.1000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 674.5,                last time consumption/overall running time: 858.7460s / 216798.7532 s
env0_first_0:                 episode reward: -91.0000,                 loss: 0.4626
env0_second_0:                 episode reward: 91.0000,                 loss: 0.4038
env1_first_0:                 episode reward: -77.5000,                 loss: nan
env1_second_0:                 episode reward: 77.5000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 670.2,                last time consumption/overall running time: 840.8878s / 217639.6410 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.4481
env0_second_0:                 episode reward: 88.8000,                 loss: 0.4730
env1_first_0:                 episode reward: -88.1500,                 loss: nan
env1_second_0:                 episode reward: 88.1500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 576.4,                last time consumption/overall running time: 721.3664s / 218361.0074 s
env0_first_0:                 episode reward: -92.5000,                 loss: 0.4623
env0_second_0:                 episode reward: 92.5000,                 loss: 0.4366
env1_first_0:                 episode reward: -84.8500,                 loss: nan
env1_second_0:                 episode reward: 84.8500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 542.45,                last time consumption/overall running time: 680.3416s / 219041.3490 s
env0_first_0:                 episode reward: -88.2000,                 loss: 0.4530
env0_second_0:                 episode reward: 88.2000,                 loss: 0.4347
env1_first_0:                 episode reward: -79.5500,                 loss: nan
env1_second_0:                 episode reward: 79.5500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 337.85,                last time consumption/overall running time: 423.1978s / 219464.5468 s
env0_first_0:                 episode reward: -95.5500,                 loss: 0.4294
env0_second_0:                 episode reward: 95.5500,                 loss: 0.4785
env1_first_0:                 episode reward: -87.5000,                 loss: nan
env1_second_0:                 episode reward: 87.5000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 382.7,                last time consumption/overall running time: 481.4333s / 219945.9801 s
env0_first_0:                 episode reward: -84.9000,                 loss: 0.4177
env0_second_0:                 episode reward: 84.9000,                 loss: 0.4306
env1_first_0:                 episode reward: -95.9000,                 loss: nan
env1_second_0:                 episode reward: 95.9000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 378.05,                last time consumption/overall running time: 473.8114s / 220419.7915 s
env0_first_0:                 episode reward: -84.2000,                 loss: 0.4638
env0_second_0:                 episode reward: 84.2000,                 loss: 0.4603
env1_first_0:                 episode reward: -83.1000,                 loss: nan
env1_second_0:                 episode reward: 83.1000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 322.3,                last time consumption/overall running time: 401.3662s / 220821.1578 s
env0_first_0:                 episode reward: -89.9000,                 loss: 0.4723
env0_second_0:                 episode reward: 89.9000,                 loss: 0.5204
env1_first_0:                 episode reward: -83.2000,                 loss: nan
env1_second_0:                 episode reward: 83.2000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 408.45,                last time consumption/overall running time: 516.3100s / 221337.4678 s
env0_first_0:                 episode reward: -89.2500,                 loss: 0.5373
env0_second_0:                 episode reward: 89.2500,                 loss: 0.5334
env1_first_0:                 episode reward: -80.7500,                 loss: nan
env1_second_0:                 episode reward: 80.7500,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 676.55,                last time consumption/overall running time: 849.8734s / 222187.3412 s
env0_first_0:                 episode reward: -72.1000,                 loss: 0.4843
env0_second_0:                 episode reward: 72.1000,                 loss: 0.5085
env1_first_0:                 episode reward: -75.5500,                 loss: nan
env1_second_0:                 episode reward: 75.5500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 555.95,                last time consumption/overall running time: 686.8615s / 222874.2027 s
env0_first_0:                 episode reward: -81.8000,                 loss: 0.4765
env0_second_0:                 episode reward: 81.8000,                 loss: 0.5148
env1_first_0:                 episode reward: -83.6000,                 loss: nan
env1_second_0:                 episode reward: 83.6000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 535.5,                last time consumption/overall running time: 667.5127s / 223541.7154 s
env0_first_0:                 episode reward: -82.2000,                 loss: 0.4749
env0_second_0:                 episode reward: 82.2000,                 loss: 0.4897
env1_first_0:                 episode reward: -70.3500,                 loss: nan
env1_second_0:                 episode reward: 70.3500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 694.95,                last time consumption/overall running time: 866.8719s / 224408.5874 s
env0_first_0:                 episode reward: -87.7500,                 loss: 0.4786
env0_second_0:                 episode reward: 87.7500,                 loss: 0.4701
env1_first_0:                 episode reward: -86.5000,                 loss: nan
env1_second_0:                 episode reward: 86.5000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 573.7,                last time consumption/overall running time: 715.2467s / 225123.8341 s
env0_first_0:                 episode reward: -74.6000,                 loss: 0.5060
env0_second_0:                 episode reward: 74.6000,                 loss: 0.5207
env1_first_0:                 episode reward: -81.1000,                 loss: nan
env1_second_0:                 episode reward: 81.1000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 299.45,                last time consumption/overall running time: 376.2004s / 225500.0345 s
env0_first_0:                 episode reward: -74.6000,                 loss: 0.5430
env0_second_0:                 episode reward: 74.6000,                 loss: 0.5308
env1_first_0:                 episode reward: -71.0000,                 loss: nan
env1_second_0:                 episode reward: 71.0000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 494.15,                last time consumption/overall running time: 619.3787s / 226119.4132 s
env0_first_0:                 episode reward: -77.5000,                 loss: 0.5941
env0_second_0:                 episode reward: 77.5000,                 loss: 0.5407
env1_first_0:                 episode reward: -85.7500,                 loss: nan
env1_second_0:                 episode reward: 85.7500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 290.0,                last time consumption/overall running time: 367.5486s / 226486.9617 s
env0_first_0:                 episode reward: -70.7500,                 loss: 0.5969
env0_second_0:                 episode reward: 70.7500,                 loss: 0.5298
env1_first_0:                 episode reward: -82.8000,                 loss: nan
env1_second_0:                 episode reward: 82.8000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 327.65,                last time consumption/overall running time: 414.7714s / 226901.7332 s
env0_first_0:                 episode reward: -80.2500,                 loss: 0.5995
env0_second_0:                 episode reward: 80.2500,                 loss: 0.5455
env1_first_0:                 episode reward: -77.3000,                 loss: nan
env1_second_0:                 episode reward: 77.3000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 415.45,                last time consumption/overall running time: 516.3468s / 227418.0800 s
env0_first_0:                 episode reward: -79.4000,                 loss: 0.6233
env0_second_0:                 episode reward: 79.4000,                 loss: 0.5418
env1_first_0:                 episode reward: -86.7500,                 loss: nan
env1_second_0:                 episode reward: 86.7500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 475.3,                last time consumption/overall running time: 591.7275s / 228009.8075 s
env0_first_0:                 episode reward: -76.8500,                 loss: 0.5949
env0_second_0:                 episode reward: 76.8500,                 loss: 0.5695
env1_first_0:                 episode reward: -89.5500,                 loss: nan
env1_second_0:                 episode reward: 89.5500,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 351.0,                last time consumption/overall running time: 438.9348s / 228448.7423 s
env0_first_0:                 episode reward: -93.4500,                 loss: 0.6604
env0_second_0:                 episode reward: 93.4500,                 loss: 0.5489
env1_first_0:                 episode reward: -91.2500,                 loss: nan
env1_second_0:                 episode reward: 91.2500,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 291.7,                last time consumption/overall running time: 361.9985s / 228810.7408 s
env0_first_0:                 episode reward: -80.9000,                 loss: 0.5972
env0_second_0:                 episode reward: 80.9000,                 loss: 0.5630
env1_first_0:                 episode reward: -86.1000,                 loss: nan
env1_second_0:                 episode reward: 86.1000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 361.35,                last time consumption/overall running time: 446.8838s / 229257.6246 s
env0_first_0:                 episode reward: -75.9000,                 loss: 0.5662
env0_second_0:                 episode reward: 75.9000,                 loss: 0.5804
env1_first_0:                 episode reward: -78.5500,                 loss: nan
env1_second_0:                 episode reward: 78.5500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 280.1,                last time consumption/overall running time: 347.5579s / 229605.1825 s
env0_first_0:                 episode reward: -90.4500,                 loss: 0.5625
env0_second_0:                 episode reward: 90.4500,                 loss: 0.5923
env1_first_0:                 episode reward: -81.1000,                 loss: nan
env1_second_0:                 episode reward: 81.1000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 290.25,                last time consumption/overall running time: 363.2687s / 229968.4512 s
env0_first_0:                 episode reward: -69.0000,                 loss: 0.5376
env0_second_0:                 episode reward: 69.0000,                 loss: 0.5330
env1_first_0:                 episode reward: -90.8500,                 loss: nan
env1_second_0:                 episode reward: 90.8500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 324.15,                last time consumption/overall running time: 397.9848s / 230366.4360 s
env0_first_0:                 episode reward: -74.9500,                 loss: 0.4952
env0_second_0:                 episode reward: 74.9500,                 loss: 0.5080
env1_first_0:                 episode reward: -77.7500,                 loss: nan
env1_second_0:                 episode reward: 77.7500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 465.6,                last time consumption/overall running time: 574.9783s / 230941.4144 s
env0_first_0:                 episode reward: -84.9500,                 loss: 0.4884
env0_second_0:                 episode reward: 84.9500,                 loss: 0.5055
env1_first_0:                 episode reward: -87.2000,                 loss: nan
env1_second_0:                 episode reward: 87.2000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 457.4,                last time consumption/overall running time: 568.2353s / 231509.6496 s
env0_first_0:                 episode reward: -91.5000,                 loss: 0.4570
env0_second_0:                 episode reward: 91.5000,                 loss: 0.4419
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 300.45,                last time consumption/overall running time: 370.9362s / 231880.5858 s
env0_first_0:                 episode reward: -86.1500,                 loss: 0.4640
env0_second_0:                 episode reward: 86.1500,                 loss: 0.4260
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 448.45,                last time consumption/overall running time: 552.9309s / 232433.5167 s
env0_first_0:                 episode reward: -88.9000,                 loss: 0.4960
env0_second_0:                 episode reward: 88.9000,                 loss: 0.3835
env1_first_0:                 episode reward: -87.3000,                 loss: nan
env1_second_0:                 episode reward: 87.3000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 337.95,                last time consumption/overall running time: 414.5177s / 232848.0344 s
env0_first_0:                 episode reward: -76.5000,                 loss: 0.4696
env0_second_0:                 episode reward: 76.5000,                 loss: 0.4498
env1_first_0:                 episode reward: -88.3000,                 loss: nan
env1_second_0:                 episode reward: 88.3000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 339.85,                last time consumption/overall running time: 418.0004s / 233266.0348 s
env0_first_0:                 episode reward: -72.2000,                 loss: 0.4955
env0_second_0:                 episode reward: 72.2000,                 loss: 0.4804
env1_first_0:                 episode reward: -84.4500,                 loss: nan
env1_second_0:                 episode reward: 84.4500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 303.25,                last time consumption/overall running time: 375.2556s / 233641.2904 s
env0_first_0:                 episode reward: -87.2500,                 loss: 0.5015
env0_second_0:                 episode reward: 87.2500,                 loss: 0.4836
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 232.5,                last time consumption/overall running time: 285.4164s / 233926.7068 s
env0_first_0:                 episode reward: -71.6500,                 loss: 0.4922
env0_second_0:                 episode reward: 71.6500,                 loss: 0.4975
env1_first_0:                 episode reward: -88.9500,                 loss: nan
env1_second_0:                 episode reward: 88.9500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 258.7,                last time consumption/overall running time: 323.9274s / 234250.6342 s
env0_first_0:                 episode reward: -75.8000,                 loss: 0.5095
env0_second_0:                 episode reward: 75.8000,                 loss: 0.4976
env1_first_0:                 episode reward: -77.2500,                 loss: nan
env1_second_0:                 episode reward: 77.2500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 314.05,                last time consumption/overall running time: 388.6525s / 234639.2867 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.4979
env0_second_0:                 episode reward: 89.0000,                 loss: 0.5305
env1_first_0:                 episode reward: -80.8000,                 loss: nan
env1_second_0:                 episode reward: 80.8000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 246.95,                last time consumption/overall running time: 306.7266s / 234946.0133 s
env0_first_0:                 episode reward: -78.5000,                 loss: 0.5584
env0_second_0:                 episode reward: 78.5000,                 loss: 0.5595
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 232.0,                last time consumption/overall running time: 285.6438s / 235231.6571 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.5780
env0_second_0:                 episode reward: 92.9500,                 loss: 0.5964
env1_first_0:                 episode reward: -81.5500,                 loss: nan
env1_second_0:                 episode reward: 81.5500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 270.5,                last time consumption/overall running time: 334.4605s / 235566.1175 s
env0_first_0:                 episode reward: -86.9500,                 loss: 0.6288
env0_second_0:                 episode reward: 86.9500,                 loss: 0.6034
env1_first_0:                 episode reward: -82.3500,                 loss: nan
env1_second_0:                 episode reward: 82.3500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 244.5,                last time consumption/overall running time: 304.0380s / 235870.1556 s
env0_first_0:                 episode reward: -81.7500,                 loss: 0.5880
env0_second_0:                 episode reward: 81.7500,                 loss: 0.5948
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 225.15,                last time consumption/overall running time: 281.1702s / 236151.3258 s
env0_first_0:                 episode reward: -69.5500,                 loss: 0.5860
env0_second_0:                 episode reward: 69.5500,                 loss: 0.5469
env1_first_0:                 episode reward: -85.2000,                 loss: nan
env1_second_0:                 episode reward: 85.2000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 274.35,                last time consumption/overall running time: 336.9857s / 236488.3115 s
env0_first_0:                 episode reward: -83.7500,                 loss: 0.6114
env0_second_0:                 episode reward: 83.7500,                 loss: 0.5663
env1_first_0:                 episode reward: -63.5000,                 loss: nan
env1_second_0:                 episode reward: 63.5000,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 270.6,                last time consumption/overall running time: 328.8802s / 236817.1917 s
env0_first_0:                 episode reward: -63.3500,                 loss: 0.6525
env0_second_0:                 episode reward: 63.3500,                 loss: 0.6254
env1_first_0:                 episode reward: -90.5500,                 loss: nan
env1_second_0:                 episode reward: 90.5500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 264.3,                last time consumption/overall running time: 331.0731s / 237148.2648 s
env0_first_0:                 episode reward: -85.1500,                 loss: 0.6422
env0_second_0:                 episode reward: 85.1500,                 loss: 0.5981
env1_first_0:                 episode reward: -68.3000,                 loss: nan
env1_second_0:                 episode reward: 68.3000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 483.2,                last time consumption/overall running time: 591.0109s / 237739.2757 s
env0_first_0:                 episode reward: -81.6000,                 loss: 0.7072
env0_second_0:                 episode reward: 81.6000,                 loss: 0.6094
env1_first_0:                 episode reward: -85.9500,                 loss: nan
env1_second_0:                 episode reward: 85.9500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 303.8,                last time consumption/overall running time: 374.3133s / 238113.5889 s
env0_first_0:                 episode reward: -66.7500,                 loss: 0.7012
env0_second_0:                 episode reward: 66.7500,                 loss: 0.5929
env1_first_0:                 episode reward: -85.7500,                 loss: nan
env1_second_0:                 episode reward: 85.7500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 278.35,                last time consumption/overall running time: 340.4926s / 238454.0815 s
env0_first_0:                 episode reward: -68.7000,                 loss: 0.6882
env0_second_0:                 episode reward: 68.7000,                 loss: 0.5889
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 307.1,                last time consumption/overall running time: 378.0844s / 238832.1659 s
env0_first_0:                 episode reward: -90.1000,                 loss: 0.6509
env0_second_0:                 episode reward: 90.1000,                 loss: 0.6932
env1_first_0:                 episode reward: -73.5500,                 loss: nan
env1_second_0:                 episode reward: 73.5500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 306.85,                last time consumption/overall running time: 378.0525s / 239210.2184 s
env0_first_0:                 episode reward: -83.7500,                 loss: 0.7260
env0_second_0:                 episode reward: 83.7500,                 loss: 0.6433
env1_first_0:                 episode reward: -81.4000,                 loss: nan
env1_second_0:                 episode reward: 81.4000,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 260.4,                last time consumption/overall running time: 323.0558s / 239533.2742 s
env0_first_0:                 episode reward: -77.0500,                 loss: 0.6752
env0_second_0:                 episode reward: 77.0500,                 loss: 0.5753
env1_first_0:                 episode reward: -93.4500,                 loss: nan
env1_second_0:                 episode reward: 93.4500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 237.25,                last time consumption/overall running time: 294.2352s / 239827.5095 s
env0_first_0:                 episode reward: -88.4000,                 loss: 0.6231
env0_second_0:                 episode reward: 88.4000,                 loss: 0.5980
env1_first_0:                 episode reward: -85.5000,                 loss: nan
env1_second_0:                 episode reward: 85.5000,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 233.85,                last time consumption/overall running time: 287.7349s / 240115.2444 s
env0_first_0:                 episode reward: -87.6500,                 loss: 0.5850
env0_second_0:                 episode reward: 87.6500,                 loss: 0.5772
env1_first_0:                 episode reward: -87.5500,                 loss: nan
env1_second_0:                 episode reward: 87.5500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 252.1,                last time consumption/overall running time: 311.9390s / 240427.1834 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.5608
env0_second_0:                 episode reward: 89.3000,                 loss: 0.5601
env1_first_0:                 episode reward: -87.6000,                 loss: nan
env1_second_0:                 episode reward: 87.6000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 237.3,                last time consumption/overall running time: 293.4393s / 240720.6227 s
env0_first_0:                 episode reward: -81.6000,                 loss: 0.5355
env0_second_0:                 episode reward: 81.6000,                 loss: 0.4953
env1_first_0:                 episode reward: -93.6000,                 loss: nan
env1_second_0:                 episode reward: 93.6000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 241.0,                last time consumption/overall running time: 295.6573s / 241016.2800 s
env0_first_0:                 episode reward: -86.9500,                 loss: 0.5524
env0_second_0:                 episode reward: 86.9500,                 loss: 0.5248
env1_first_0:                 episode reward: -86.3500,                 loss: nan
env1_second_0:                 episode reward: 86.3500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 379.2,                last time consumption/overall running time: 472.3214s / 241488.6014 s
env0_first_0:                 episode reward: -92.1000,                 loss: 0.6001
env0_second_0:                 episode reward: 92.1000,                 loss: 0.4744
env1_first_0:                 episode reward: -89.0500,                 loss: nan
env1_second_0:                 episode reward: 89.0500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 242.3,                last time consumption/overall running time: 295.8040s / 241784.4054 s
env0_first_0:                 episode reward: -89.2500,                 loss: 0.5738
env0_second_0:                 episode reward: 89.2500,                 loss: 0.5178
env1_first_0:                 episode reward: -82.5000,                 loss: nan
env1_second_0:                 episode reward: 82.5000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 297.6,                last time consumption/overall running time: 368.9910s / 242153.3964 s
env0_first_0:                 episode reward: -86.5000,                 loss: 0.5289
env0_second_0:                 episode reward: 86.5000,                 loss: 0.4733
env1_first_0:                 episode reward: -82.1500,                 loss: nan
env1_second_0:                 episode reward: 82.1500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 646.0,                last time consumption/overall running time: 792.5834s / 242945.9798 s
env0_first_0:                 episode reward: -74.7000,                 loss: 0.5254
env0_second_0:                 episode reward: 74.7000,                 loss: 0.4596
env1_first_0:                 episode reward: -70.6500,                 loss: nan
env1_second_0:                 episode reward: 70.6500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 365.35,                last time consumption/overall running time: 451.2005s / 243397.1803 s
env0_first_0:                 episode reward: -68.2500,                 loss: 0.5764
env0_second_0:                 episode reward: 68.2500,                 loss: 0.4828
env1_first_0:                 episode reward: -79.5500,                 loss: nan
env1_second_0:                 episode reward: 79.5500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 265.6,                last time consumption/overall running time: 329.6583s / 243726.8385 s
env0_first_0:                 episode reward: -80.5000,                 loss: 0.5840
env0_second_0:                 episode reward: 80.5000,                 loss: 0.4994
env1_first_0:                 episode reward: -73.0000,                 loss: nan
env1_second_0:                 episode reward: 73.0000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 343.4,                last time consumption/overall running time: 424.8448s / 244151.6834 s
env0_first_0:                 episode reward: -80.8500,                 loss: 0.5364
env0_second_0:                 episode reward: 80.8500,                 loss: 0.4773
env1_first_0:                 episode reward: -86.1500,                 loss: nan
env1_second_0:                 episode reward: 86.1500,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 303.05,                last time consumption/overall running time: 372.6980s / 244524.3813 s
env0_first_0:                 episode reward: -75.1000,                 loss: 0.5675
env0_second_0:                 episode reward: 75.1000,                 loss: 0.4292
env1_first_0:                 episode reward: -93.6000,                 loss: nan
env1_second_0:                 episode reward: 93.6000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 258.05,                last time consumption/overall running time: 321.3866s / 244845.7679 s
env0_first_0:                 episode reward: -90.0000,                 loss: 0.5744
env0_second_0:                 episode reward: 90.0000,                 loss: 0.4618
env1_first_0:                 episode reward: -78.3000,                 loss: nan
env1_second_0:                 episode reward: 78.3000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 271.75,                last time consumption/overall running time: 337.0119s / 245182.7798 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.5745
env0_second_0:                 episode reward: 88.5500,                 loss: 0.4501
env1_first_0:                 episode reward: -91.3500,                 loss: nan
env1_second_0:                 episode reward: 91.3500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 226.5,                last time consumption/overall running time: 283.1428s / 245465.9226 s
env0_first_0:                 episode reward: -86.8500,                 loss: 0.5382
env0_second_0:                 episode reward: 86.8500,                 loss: 0.5204
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 225.65,                last time consumption/overall running time: 277.3707s / 245743.2933 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.5627
env0_second_0:                 episode reward: 88.5500,                 loss: 0.4370
env1_first_0:                 episode reward: -97.9500,                 loss: nan
env1_second_0:                 episode reward: 97.9500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 223.4,                last time consumption/overall running time: 277.7644s / 246021.0576 s
env0_first_0:                 episode reward: -93.4000,                 loss: 0.5445
env0_second_0:                 episode reward: 93.4000,                 loss: 0.4607
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 235.2,                last time consumption/overall running time: 295.2941s / 246316.3517 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.5184
env0_second_0:                 episode reward: 93.5500,                 loss: 0.4198
env1_first_0:                 episode reward: -89.5000,                 loss: nan
env1_second_0:                 episode reward: 89.5000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 244.2,                last time consumption/overall running time: 300.4354s / 246616.7871 s
env0_first_0:                 episode reward: -89.9500,                 loss: 0.5268
env0_second_0:                 episode reward: 89.9500,                 loss: 0.4307
env1_first_0:                 episode reward: -74.9000,                 loss: nan
env1_second_0:                 episode reward: 74.9000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 294.5,                last time consumption/overall running time: 363.5446s / 246980.3317 s
env0_first_0:                 episode reward: -91.0500,                 loss: 0.5385
env0_second_0:                 episode reward: 91.0500,                 loss: 0.3590
env1_first_0:                 episode reward: -96.5500,                 loss: nan
env1_second_0:                 episode reward: 96.5500,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 243.7,                last time consumption/overall running time: 304.5434s / 247284.8751 s
env0_first_0:                 episode reward: -90.0000,                 loss: 0.4959
env0_second_0:                 episode reward: 90.0000,                 loss: 0.3959
env1_first_0:                 episode reward: -86.0500,                 loss: nan
env1_second_0:                 episode reward: 86.0500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 233.65,                last time consumption/overall running time: 290.3185s / 247575.1936 s
env0_first_0:                 episode reward: -97.2000,                 loss: 0.4500
env0_second_0:                 episode reward: 97.2000,                 loss: 0.3861
env1_first_0:                 episode reward: -85.6500,                 loss: nan
env1_second_0:                 episode reward: 85.6500,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 324.55,                last time consumption/overall running time: 398.2469s / 247973.4405 s
env0_first_0:                 episode reward: -77.8000,                 loss: 0.4716
env0_second_0:                 episode reward: 77.8000,                 loss: 0.3996
env1_first_0:                 episode reward: -83.9000,                 loss: nan
env1_second_0:                 episode reward: 83.9000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 278.1,                last time consumption/overall running time: 345.7870s / 248319.2274 s
env0_first_0:                 episode reward: -79.7500,                 loss: 0.4665
env0_second_0:                 episode reward: 79.7500,                 loss: 0.3433
env1_first_0:                 episode reward: -86.1500,                 loss: nan
env1_second_0:                 episode reward: 86.1500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 343.25,                last time consumption/overall running time: 421.6687s / 248740.8961 s
env0_first_0:                 episode reward: -91.5500,                 loss: 0.4818
env0_second_0:                 episode reward: 91.5500,                 loss: 0.3733
env1_first_0:                 episode reward: -82.2000,                 loss: nan
env1_second_0:                 episode reward: 82.2000,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 242.6,                last time consumption/overall running time: 300.1191s / 249041.0152 s
env0_first_0:                 episode reward: -73.5500,                 loss: 0.5153
env0_second_0:                 episode reward: 73.5500,                 loss: 0.4167
env1_first_0:                 episode reward: -94.0000,                 loss: nan
env1_second_0:                 episode reward: 94.0000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 297.05,                last time consumption/overall running time: 366.5669s / 249407.5821 s
env0_first_0:                 episode reward: -86.9500,                 loss: 0.5811
env0_second_0:                 episode reward: 86.9500,                 loss: 0.4463
env1_first_0:                 episode reward: -78.9500,                 loss: nan
env1_second_0:                 episode reward: 78.9500,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 515.85,                last time consumption/overall running time: 641.0954s / 250048.6775 s
env0_first_0:                 episode reward: -75.8500,                 loss: 0.4848
env0_second_0:                 episode reward: 75.8500,                 loss: 0.4284
env1_first_0:                 episode reward: -80.1000,                 loss: nan
env1_second_0:                 episode reward: 80.1000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 360.4,                last time consumption/overall running time: 447.1764s / 250495.8539 s
env0_first_0:                 episode reward: -85.0500,                 loss: 0.4889
env0_second_0:                 episode reward: 85.0500,                 loss: 0.4283
env1_first_0:                 episode reward: -80.6500,                 loss: nan
env1_second_0:                 episode reward: 80.6500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 424.65,                last time consumption/overall running time: 527.1788s / 251023.0328 s
env0_first_0:                 episode reward: -91.9500,                 loss: 0.4637
env0_second_0:                 episode reward: 91.9500,                 loss: 0.3779
env1_first_0:                 episode reward: -82.6500,                 loss: nan
env1_second_0:                 episode reward: 82.6500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 241.9,                last time consumption/overall running time: 300.1852s / 251323.2180 s
env0_first_0:                 episode reward: -74.9000,                 loss: 0.4743
env0_second_0:                 episode reward: 74.9000,                 loss: 0.3641
env1_first_0:                 episode reward: -88.5000,                 loss: nan
env1_second_0:                 episode reward: 88.5000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 241.05,                last time consumption/overall running time: 299.8922s / 251623.1102 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.5159
env0_second_0:                 episode reward: 93.3500,                 loss: 0.4040
env1_first_0:                 episode reward: -85.4000,                 loss: nan
env1_second_0:                 episode reward: 85.4000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 378.55,                last time consumption/overall running time: 465.4893s / 252088.5995 s
env0_first_0:                 episode reward: -75.8000,                 loss: 0.5395
env0_second_0:                 episode reward: 75.8000,                 loss: 0.4412
env1_first_0:                 episode reward: -78.9000,                 loss: nan
env1_second_0:                 episode reward: 78.9000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 628.9,                last time consumption/overall running time: 767.5320s / 252856.1315 s
env0_first_0:                 episode reward: -80.3500,                 loss: 0.5561
env0_second_0:                 episode reward: 80.3500,                 loss: 0.4664
env1_first_0:                 episode reward: -59.5500,                 loss: nan
env1_second_0:                 episode reward: 59.5500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 438.05,                last time consumption/overall running time: 537.1483s / 253393.2798 s
env0_first_0:                 episode reward: -74.9500,                 loss: 0.5709
env0_second_0:                 episode reward: 74.9500,                 loss: 0.4585
env1_first_0:                 episode reward: -81.3000,                 loss: nan
env1_second_0:                 episode reward: 81.3000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 247.45,                last time consumption/overall running time: 305.3358s / 253698.6156 s
env0_first_0:                 episode reward: -77.0000,                 loss: 0.5371
env0_second_0:                 episode reward: 77.0000,                 loss: 0.4842
env1_first_0:                 episode reward: -84.5500,                 loss: nan
env1_second_0:                 episode reward: 84.5500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 229.8,                last time consumption/overall running time: 280.5901s / 253979.2057 s
env0_first_0:                 episode reward: -92.3500,                 loss: 0.5111
env0_second_0:                 episode reward: 92.3500,                 loss: 0.4626
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 311.0,                last time consumption/overall running time: 385.3564s / 254364.5622 s
env0_first_0:                 episode reward: -80.8500,                 loss: 0.5703
env0_second_0:                 episode reward: 80.8500,                 loss: 0.4516
env1_first_0:                 episode reward: -79.8500,                 loss: nan
env1_second_0:                 episode reward: 79.8500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 241.95,                last time consumption/overall running time: 299.1854s / 254663.7476 s
env0_first_0:                 episode reward: -88.3500,                 loss: 0.5376
env0_second_0:                 episode reward: 88.3500,                 loss: 0.4508
env1_first_0:                 episode reward: -88.9500,                 loss: nan
env1_second_0:                 episode reward: 88.9500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 251.7,                last time consumption/overall running time: 309.9126s / 254973.6601 s
env0_first_0:                 episode reward: -93.3000,                 loss: 0.5663
env0_second_0:                 episode reward: 93.3000,                 loss: 0.4601
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 226.35,                last time consumption/overall running time: 279.3943s / 255253.0545 s
env0_first_0:                 episode reward: -95.1000,                 loss: 0.5151
env0_second_0:                 episode reward: 95.1000,                 loss: 0.4224
env1_first_0:                 episode reward: -92.7000,                 loss: nan
env1_second_0:                 episode reward: 92.7000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 259.1,                last time consumption/overall running time: 317.6828s / 255570.7373 s
env0_first_0:                 episode reward: -88.1000,                 loss: 0.5501
env0_second_0:                 episode reward: 88.1000,                 loss: 0.4142
env1_first_0:                 episode reward: -91.8000,                 loss: nan
env1_second_0:                 episode reward: 91.8000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 270.6,                last time consumption/overall running time: 332.3439s / 255903.0812 s
env0_first_0:                 episode reward: -87.3000,                 loss: 0.4842
env0_second_0:                 episode reward: 87.3000,                 loss: 0.3666
env1_first_0:                 episode reward: -85.0000,                 loss: nan
env1_second_0:                 episode reward: 85.0000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 232.2,                last time consumption/overall running time: 285.0150s / 256188.0962 s
env0_first_0:                 episode reward: -83.4500,                 loss: 0.5315
env0_second_0:                 episode reward: 83.4500,                 loss: 0.3804
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 318.4,                last time consumption/overall running time: 386.0793s / 256574.1754 s
env0_first_0:                 episode reward: -85.2000,                 loss: 0.5161
env0_second_0:                 episode reward: 85.2000,                 loss: 0.3997
env1_first_0:                 episode reward: -96.8000,                 loss: nan
env1_second_0:                 episode reward: 96.8000,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 265.85,                last time consumption/overall running time: 325.7552s / 256899.9306 s
env0_first_0:                 episode reward: -83.8500,                 loss: 0.4959
env0_second_0:                 episode reward: 83.8500,                 loss: 0.4215
env1_first_0:                 episode reward: -75.3000,                 loss: nan
env1_second_0:                 episode reward: 75.3000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 225.2,                last time consumption/overall running time: 272.6279s / 257172.5585 s
env0_first_0:                 episode reward: -77.4500,                 loss: 0.5619
env0_second_0:                 episode reward: 77.4500,                 loss: 0.4348
env1_first_0:                 episode reward: -86.7000,                 loss: nan
env1_second_0:                 episode reward: 86.7000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 245.95,                last time consumption/overall running time: 305.5506s / 257478.1091 s
env0_first_0:                 episode reward: -92.1500,                 loss: 0.4758
env0_second_0:                 episode reward: 92.1500,                 loss: 0.3728
env1_first_0:                 episode reward: -85.7000,                 loss: nan
env1_second_0:                 episode reward: 85.7000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 232.6,                last time consumption/overall running time: 284.8856s / 257762.9947 s
env0_first_0:                 episode reward: -77.2500,                 loss: 0.4637
env0_second_0:                 episode reward: 77.2500,                 loss: 0.4237
env1_first_0:                 episode reward: -94.9000,                 loss: nan
env1_second_0:                 episode reward: 94.9000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 228.55,                last time consumption/overall running time: 276.5233s / 258039.5181 s
env0_first_0:                 episode reward: -85.0500,                 loss: 0.4228
env0_second_0:                 episode reward: 85.0500,                 loss: 0.4529
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 234.75,                last time consumption/overall running time: 289.2342s / 258328.7523 s
env0_first_0:                 episode reward: -80.8000,                 loss: 0.4668
env0_second_0:                 episode reward: 80.8000,                 loss: 0.4402
env1_first_0:                 episode reward: -85.9500,                 loss: nan
env1_second_0:                 episode reward: 85.9500,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 251.95,                last time consumption/overall running time: 309.0326s / 258637.7849 s
env0_first_0:                 episode reward: -88.4000,                 loss: 0.4781
env0_second_0:                 episode reward: 88.4000,                 loss: 0.4289
env1_first_0:                 episode reward: -76.1500,                 loss: nan
env1_second_0:                 episode reward: 76.1500,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 262.45,                last time consumption/overall running time: 320.5236s / 258958.3085 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.4854
env0_second_0:                 episode reward: 89.0000,                 loss: 0.4220
env1_first_0:                 episode reward: -84.8500,                 loss: nan
env1_second_0:                 episode reward: 84.8500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 347.7,                last time consumption/overall running time: 421.4381s / 259379.7466 s
env0_first_0:                 episode reward: -81.6000,                 loss: 0.4631
env0_second_0:                 episode reward: 81.6000,                 loss: 0.4171
env1_first_0:                 episode reward: -86.0500,                 loss: nan
env1_second_0:                 episode reward: 86.0500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 521.65,                last time consumption/overall running time: 637.3366s / 260017.0831 s
env0_first_0:                 episode reward: -82.8500,                 loss: 0.4268
env0_second_0:                 episode reward: 82.8500,                 loss: 0.4378
env1_first_0:                 episode reward: -92.6500,                 loss: nan
env1_second_0:                 episode reward: 92.6500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 270.6,                last time consumption/overall running time: 327.3082s / 260344.3913 s
env0_first_0:                 episode reward: -82.2500,                 loss: 0.4137
env0_second_0:                 episode reward: 82.2500,                 loss: 0.4052
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 265.6,                last time consumption/overall running time: 322.0087s / 260666.4000 s
env0_first_0:                 episode reward: -98.0000,                 loss: 0.4388
env0_second_0:                 episode reward: 98.0000,                 loss: 0.3958
env1_first_0:                 episode reward: -75.9000,                 loss: nan
env1_second_0:                 episode reward: 75.9000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 300.75,                last time consumption/overall running time: 368.3071s / 261034.7072 s
env0_first_0:                 episode reward: -90.6000,                 loss: 0.3463
env0_second_0:                 episode reward: 90.6000,                 loss: 0.3256
env1_first_0:                 episode reward: -94.2000,                 loss: nan
env1_second_0:                 episode reward: 94.2000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 388.65,                last time consumption/overall running time: 475.2663s / 261509.9735 s
env0_first_0:                 episode reward: -81.3500,                 loss: 0.3717
env0_second_0:                 episode reward: 81.3500,                 loss: 0.3115
env1_first_0:                 episode reward: -90.4000,                 loss: nan
env1_second_0:                 episode reward: 90.4000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 223.55,                last time consumption/overall running time: 270.5473s / 261780.5208 s
env0_first_0:                 episode reward: -84.5500,                 loss: 0.3266
env0_second_0:                 episode reward: 84.5500,                 loss: 0.3657
env1_first_0:                 episode reward: -95.1000,                 loss: nan
env1_second_0:                 episode reward: 95.1000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 316.0,                last time consumption/overall running time: 377.4588s / 262157.9795 s
env0_first_0:                 episode reward: -83.4000,                 loss: 0.3483
env0_second_0:                 episode reward: 83.4000,                 loss: 0.3468
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 276.7,                last time consumption/overall running time: 336.6824s / 262494.6620 s
env0_first_0:                 episode reward: -77.6000,                 loss: 0.3504
env0_second_0:                 episode reward: 77.6000,                 loss: 0.3561
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 402.7,                last time consumption/overall running time: 484.5557s / 262979.2176 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.3298
env0_second_0:                 episode reward: 89.3000,                 loss: 0.3670
env1_first_0:                 episode reward: -77.2500,                 loss: nan
env1_second_0:                 episode reward: 77.2500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 248.8,                last time consumption/overall running time: 300.9770s / 263280.1946 s
env0_first_0:                 episode reward: -88.2000,                 loss: 0.3733
env0_second_0:                 episode reward: 88.2000,                 loss: 0.3951
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 340.45,                last time consumption/overall running time: 411.9889s / 263692.1835 s
env0_first_0:                 episode reward: -94.8500,                 loss: 0.3628
env0_second_0:                 episode reward: 94.8500,                 loss: 0.4068
env1_first_0:                 episode reward: -88.8000,                 loss: nan
env1_second_0:                 episode reward: 88.8000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 223.95,                last time consumption/overall running time: 270.7903s / 263962.9739 s
env0_first_0:                 episode reward: -95.9000,                 loss: 0.3542
env0_second_0:                 episode reward: 95.9000,                 loss: 0.3760
env1_first_0:                 episode reward: -87.6500,                 loss: nan
env1_second_0:                 episode reward: 87.6500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 229.15,                last time consumption/overall running time: 277.0709s / 264240.0447 s
env0_first_0:                 episode reward: -87.1500,                 loss: 0.3615
env0_second_0:                 episode reward: 87.1500,                 loss: 0.4057
env1_first_0:                 episode reward: -83.6500,                 loss: nan
env1_second_0:                 episode reward: 83.6500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 335.5,                last time consumption/overall running time: 404.8025s / 264644.8473 s
env0_first_0:                 episode reward: -84.0500,                 loss: 0.3981
env0_second_0:                 episode reward: 84.0500,                 loss: 0.3991
env1_first_0:                 episode reward: -82.3000,                 loss: nan
env1_second_0:                 episode reward: 82.3000,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 293.5,                last time consumption/overall running time: 360.1952s / 265005.0425 s
env0_first_0:                 episode reward: -85.4500,                 loss: 0.4218
env0_second_0:                 episode reward: 85.4500,                 loss: 0.4177
env1_first_0:                 episode reward: -92.2000,                 loss: nan
env1_second_0:                 episode reward: 92.2000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 255.5,                last time consumption/overall running time: 308.1453s / 265313.1878 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.4653
env0_second_0:                 episode reward: 93.3500,                 loss: 0.4225
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 413.1,                last time consumption/overall running time: 503.0992s / 265816.2871 s
env0_first_0:                 episode reward: -86.8500,                 loss: 0.4221
env0_second_0:                 episode reward: 86.8500,                 loss: 0.4088
env1_first_0:                 episode reward: -92.2000,                 loss: nan
env1_second_0:                 episode reward: 92.2000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 304.95,                last time consumption/overall running time: 369.6532s / 266185.9402 s
env0_first_0:                 episode reward: -88.8500,                 loss: 0.4600
env0_second_0:                 episode reward: 88.8500,                 loss: 0.4143
env1_first_0:                 episode reward: -89.5500,                 loss: nan
env1_second_0:                 episode reward: 89.5500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 225.65,                last time consumption/overall running time: 272.6541s / 266458.5943 s
env0_first_0:                 episode reward: -91.8500,                 loss: 0.4595
env0_second_0:                 episode reward: 91.8500,                 loss: 0.4129
env1_first_0:                 episode reward: -84.1000,                 loss: nan
env1_second_0:                 episode reward: 84.1000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 287.4,                last time consumption/overall running time: 344.0157s / 266802.6100 s
env0_first_0:                 episode reward: -85.3000,                 loss: 0.4583
env0_second_0:                 episode reward: 85.3000,                 loss: 0.4306
env1_first_0:                 episode reward: -89.7000,                 loss: nan
env1_second_0:                 episode reward: 89.7000,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 357.25,                last time consumption/overall running time: 430.1973s / 267232.8073 s
env0_first_0:                 episode reward: -87.5500,                 loss: 0.4203
env0_second_0:                 episode reward: 87.5500,                 loss: 0.4110
env1_first_0:                 episode reward: -85.1000,                 loss: nan
env1_second_0:                 episode reward: 85.1000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 240.75,                last time consumption/overall running time: 292.2898s / 267525.0971 s
env0_first_0:                 episode reward: -89.4000,                 loss: 0.4376
env0_second_0:                 episode reward: 89.4000,                 loss: 0.3926
env1_first_0:                 episode reward: -83.8000,                 loss: nan
env1_second_0:                 episode reward: 83.8000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 318.8,                last time consumption/overall running time: 385.4056s / 267910.5027 s
env0_first_0:                 episode reward: -79.6000,                 loss: 0.3938
env0_second_0:                 episode reward: 79.6000,                 loss: 0.3641
env1_first_0:                 episode reward: -91.6000,                 loss: nan
env1_second_0:                 episode reward: 91.6000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 295.75,                last time consumption/overall running time: 354.2449s / 268264.7476 s
env0_first_0:                 episode reward: -76.7000,                 loss: 0.3986
env0_second_0:                 episode reward: 76.7000,                 loss: 0.3620
env1_first_0:                 episode reward: -96.0000,                 loss: nan
env1_second_0:                 episode reward: 96.0000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 226.55,                last time consumption/overall running time: 277.8454s / 268542.5930 s
env0_first_0:                 episode reward: -94.5500,                 loss: 0.4123
env0_second_0:                 episode reward: 94.5500,                 loss: 0.3740
env1_first_0:                 episode reward: -91.1000,                 loss: nan
env1_second_0:                 episode reward: 91.1000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 298.7,                last time consumption/overall running time: 363.2162s / 268905.8092 s
env0_first_0:                 episode reward: -87.9500,                 loss: 0.3821
env0_second_0:                 episode reward: 87.9500,                 loss: 0.3566
env1_first_0:                 episode reward: -93.7500,                 loss: nan
env1_second_0:                 episode reward: 93.7500,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 425.7,                last time consumption/overall running time: 512.6605s / 269418.4697 s
env0_first_0:                 episode reward: -93.3000,                 loss: 0.4462
env0_second_0:                 episode reward: 93.3000,                 loss: 0.3597
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 451.2,                last time consumption/overall running time: 546.0999s / 269964.5696 s
env0_first_0:                 episode reward: -90.1000,                 loss: 0.4224
env0_second_0:                 episode reward: 90.1000,                 loss: 0.3804
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 316.65,                last time consumption/overall running time: 382.5188s / 270347.0885 s
env0_first_0:                 episode reward: -81.9000,                 loss: 0.4360
env0_second_0:                 episode reward: 81.9000,                 loss: 0.3979
env1_first_0:                 episode reward: -87.2000,                 loss: nan
env1_second_0:                 episode reward: 87.2000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 399.5,                last time consumption/overall running time: 481.6769s / 270828.7654 s
env0_first_0:                 episode reward: -95.5000,                 loss: 0.4598
env0_second_0:                 episode reward: 95.5000,                 loss: 0.4141
env1_first_0:                 episode reward: -90.7000,                 loss: nan
env1_second_0:                 episode reward: 90.7000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 266.35,                last time consumption/overall running time: 323.6983s / 271152.4637 s
env0_first_0:                 episode reward: -90.5000,                 loss: 0.5231
env0_second_0:                 episode reward: 90.5000,                 loss: 0.4308
env1_first_0:                 episode reward: -92.1000,                 loss: nan
env1_second_0:                 episode reward: 92.1000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 296.3,                last time consumption/overall running time: 359.3536s / 271511.8173 s
env0_first_0:                 episode reward: -90.1000,                 loss: 0.4527
env0_second_0:                 episode reward: 90.1000,                 loss: 0.4327
env1_first_0:                 episode reward: -77.6000,                 loss: nan
env1_second_0:                 episode reward: 77.6000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 333.5,                last time consumption/overall running time: 402.8518s / 271914.6692 s
env0_first_0:                 episode reward: -88.3500,                 loss: 0.4496
env0_second_0:                 episode reward: 88.3500,                 loss: 0.4614
env1_first_0:                 episode reward: -90.7500,                 loss: nan
env1_second_0:                 episode reward: 90.7500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 433.15,                last time consumption/overall running time: 521.5714s / 272436.2406 s
env0_first_0:                 episode reward: -82.3500,                 loss: 0.4570
env0_second_0:                 episode reward: 82.3500,                 loss: 0.4337
env1_first_0:                 episode reward: -82.4500,                 loss: nan
env1_second_0:                 episode reward: 82.4500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 393.5,                last time consumption/overall running time: 476.7020s / 272912.9426 s
env0_first_0:                 episode reward: -96.2000,                 loss: 0.4315
env0_second_0:                 episode reward: 96.2000,                 loss: 0.4009
env1_first_0:                 episode reward: -88.6000,                 loss: nan
env1_second_0:                 episode reward: 88.6000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 276.05,                last time consumption/overall running time: 335.5997s / 273248.5423 s
env0_first_0:                 episode reward: -91.3500,                 loss: 0.4044
env0_second_0:                 episode reward: 91.3500,                 loss: 0.3639
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 251.85,                last time consumption/overall running time: 307.9881s / 273556.5303 s
env0_first_0:                 episode reward: -88.1500,                 loss: 0.4350
env0_second_0:                 episode reward: 88.1500,                 loss: 0.3624
env1_first_0:                 episode reward: -90.7500,                 loss: nan
env1_second_0:                 episode reward: 90.7500,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 244.7,                last time consumption/overall running time: 296.2414s / 273852.7717 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.3858
env0_second_0:                 episode reward: 88.6000,                 loss: 0.3411
env1_first_0:                 episode reward: -90.7000,                 loss: nan
env1_second_0:                 episode reward: 90.7000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 341.65,                last time consumption/overall running time: 417.4897s / 274270.2614 s
env0_first_0:                 episode reward: -92.2000,                 loss: 0.3774
env0_second_0:                 episode reward: 92.2000,                 loss: 0.3454
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 264.25,                last time consumption/overall running time: 327.5284s / 274597.7898 sLoad boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rl/agents/nash_dqn_exploiter.py:174: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  action = torch.LongTensor(action).to(self.device)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: -85.6000,                 loss: 0.4488
env0_second_0:                 episode reward: 85.6000,                 loss: 0.3604
env1_first_0:                 episode reward: -91.6500,                 loss: nan
env1_second_0:                 episode reward: 91.6500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 277.55,                last time consumption/overall running time: 337.3074s / 274935.0972 s
env0_first_0:                 episode reward: -95.2000,                 loss: 0.4380
env0_second_0:                 episode reward: 95.2000,                 loss: 0.3810
env1_first_0:                 episode reward: -91.9000,                 loss: nan
env1_second_0:                 episode reward: 91.9000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 251.2,                last time consumption/overall running time: 303.2479s / 275238.3451 s
env0_first_0:                 episode reward: -92.3500,                 loss: 0.4306
env0_second_0:                 episode reward: 92.3500,                 loss: 0.3943
env1_first_0:                 episode reward: -82.1000,                 loss: nan
env1_second_0:                 episode reward: 82.1000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 276.6,                last time consumption/overall running time: 338.0338s / 275576.3789 s
env0_first_0:                 episode reward: -89.1000,                 loss: 0.3992
env0_second_0:                 episode reward: 89.1000,                 loss: 0.3629
env1_first_0:                 episode reward: -86.2500,                 loss: nan
env1_second_0:                 episode reward: 86.2500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 311.65,                last time consumption/overall running time: 377.8910s / 275954.2699 s
env0_first_0:                 episode reward: -77.4000,                 loss: 0.4761
env0_second_0:                 episode reward: 77.4000,                 loss: 0.4185
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 268.5,                last time consumption/overall running time: 330.5064s / 276284.7762 s
env0_first_0:                 episode reward: -79.4000,                 loss: 0.4548
env0_second_0:                 episode reward: 79.4000,                 loss: 0.4176
env1_first_0:                 episode reward: -92.1000,                 loss: nan
env1_second_0:                 episode reward: 92.1000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 260.15,                last time consumption/overall running time: 315.2331s / 276600.0093 s
env0_first_0:                 episode reward: -94.1000,                 loss: 0.4617
env0_second_0:                 episode reward: 94.1000,                 loss: 0.3970
env1_first_0:                 episode reward: -96.3000,                 loss: nan
env1_second_0:                 episode reward: 96.3000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 237.35,                last time consumption/overall running time: 286.4518s / 276886.4611 s
env0_first_0:                 episode reward: -92.7500,                 loss: 0.4208
env0_second_0:                 episode reward: 92.7500,                 loss: 0.3633
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 306.45,                last time consumption/overall running time: 373.4167s / 277259.8779 s
env0_first_0:                 episode reward: -95.9000,                 loss: 0.4685
env0_second_0:                 episode reward: 95.9000,                 loss: 0.3695
env1_first_0:                 episode reward: -92.0500,                 loss: nan
env1_second_0:                 episode reward: 92.0500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 296.35,                last time consumption/overall running time: 359.7314s / 277619.6093 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.3923
env0_second_0:                 episode reward: 87.3500,                 loss: 0.3825
env1_first_0:                 episode reward: -81.5000,                 loss: nan
env1_second_0:                 episode reward: 81.5000,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 255.1,                last time consumption/overall running time: 308.1479s / 277927.7572 s
env0_first_0:                 episode reward: -91.7000,                 loss: 0.4325
env0_second_0:                 episode reward: 91.7000,                 loss: 0.3767
env1_first_0:                 episode reward: -93.2500,                 loss: nan
env1_second_0:                 episode reward: 93.2500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 289.8,                last time consumption/overall running time: 351.5910s / 278279.3481 s
env0_first_0:                 episode reward: -91.3500,                 loss: 0.4160
env0_second_0:                 episode reward: 91.3500,                 loss: 0.3307
env1_first_0:                 episode reward: -76.4500,                 loss: nan
env1_second_0:                 episode reward: 76.4500,                 loss: nan
