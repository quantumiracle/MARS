pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [234, 822, 348, 109, 219]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 100000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 50, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 85.6501s / 85.6501 s
env0_first_0:                 episode reward: 206.0000,                 loss: 0.0716
env0_second_0:                 episode reward: -206.0000,                 loss: nan
env1_first_0:                 episode reward: 216.0000,                 loss: nan
env1_second_0:                 episode reward: -216.0000,                 loss: nan
env2_first_0:                 episode reward: 219.0000,                 loss: nan
env2_second_0:                 episode reward: -219.0000,                 loss: nan
env3_first_0:                 episode reward: 216.0000,                 loss: nan
env3_second_0:                 episode reward: -216.0000,                 loss: nan
env4_first_0:                 episode reward: 218.0000,                 loss: nan
env4_second_0:                 episode reward: -218.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 2995.8024s / 3081.4525 s
env0_first_0:                 episode reward: 214.3500,                 loss: 0.0200
env0_second_0:                 episode reward: -214.3500,                 loss: nan
env1_first_0:                 episode reward: 215.1000,                 loss: nan
env1_second_0:                 episode reward: -215.1000,                 loss: nan
env2_first_0:                 episode reward: 214.8000,                 loss: nan
env2_second_0:                 episode reward: -214.8000,                 loss: nan
env3_first_0:                 episode reward: 212.2000,                 loss: nan
env3_second_0:                 episode reward: -212.2000,                 loss: nan
env4_first_0:                 episode reward: 213.9000,                 loss: nan
env4_second_0:                 episode reward: -213.9000,                 loss: nan
Score delta: 431.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/21_0.
Episode: 41/10000 (0.4100%),                 avg. length: 2440.3,                last time consumption/overall running time: 479.0007s / 3560.4532 s
env0_first_0:                 episode reward: 11.8000,                 loss: nan
env0_second_0:                 episode reward: -11.8000,                 loss: 0.0297
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
env2_first_0:                 episode reward: 9.6500,                 loss: nan
env2_second_0:                 episode reward: -9.6500,                 loss: nan
env3_first_0:                 episode reward: 9.9500,                 loss: nan
env3_second_0:                 episode reward: -9.9500,                 loss: nan
env4_first_0:                 episode reward: 9.8500,                 loss: nan
env4_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 2129.3,                last time consumption/overall running time: 609.9567s / 4170.4099 s
env0_first_0:                 episode reward: -8.2500,                 loss: nan
env0_second_0:                 episode reward: 8.2500,                 loss: 0.0209
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
env2_first_0:                 episode reward: -8.5500,                 loss: nan
env2_second_0:                 episode reward: 8.5500,                 loss: nan
env3_first_0:                 episode reward: -7.1500,                 loss: nan
env3_second_0:                 episode reward: 7.1500,                 loss: nan
env4_first_0:                 episode reward: -6.5500,                 loss: nan
env4_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 2260.6,                last time consumption/overall running time: 783.8499s / 4954.2598 s
env0_first_0:                 episode reward: -13.0500,                 loss: nan
env0_second_0:                 episode reward: 13.0500,                 loss: 0.0141
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
env2_first_0:                 episode reward: -12.9000,                 loss: nan
env2_second_0:                 episode reward: 12.9000,                 loss: nan
env3_first_0:                 episode reward: -14.1500,                 loss: nan
env3_second_0:                 episode reward: 14.1500,                 loss: nan
env4_first_0:                 episode reward: -14.2000,                 loss: nan
env4_second_0:                 episode reward: 14.2000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1543.95,                last time consumption/overall running time: 526.0797s / 5480.3396 s
env0_first_0:                 episode reward: -18.1000,                 loss: nan
env0_second_0:                 episode reward: 18.1000,                 loss: 0.0127
env1_first_0:                 episode reward: -18.2500,                 loss: nan
env1_second_0:                 episode reward: 18.2500,                 loss: nan
env2_first_0:                 episode reward: -19.5500,                 loss: nan
env2_second_0:                 episode reward: 19.5500,                 loss: nan
env3_first_0:                 episode reward: -19.0500,                 loss: nan
env3_second_0:                 episode reward: 19.0500,                 loss: nan
env4_first_0:                 episode reward: -17.6500,                 loss: nan
env4_second_0:                 episode reward: 17.6500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1448.5,                last time consumption/overall running time: 488.9103s / 5969.2499 s
env0_first_0:                 episode reward: -19.5000,                 loss: nan
env0_second_0:                 episode reward: 19.5000,                 loss: 0.0111
env1_first_0:                 episode reward: -19.5500,                 loss: nan
env1_second_0:                 episode reward: 19.5500,                 loss: nan
env2_first_0:                 episode reward: -20.0500,                 loss: nan
env2_second_0:                 episode reward: 20.0500,                 loss: nan
env3_first_0:                 episode reward: -19.7500,                 loss: nan
env3_second_0:                 episode reward: 19.7500,                 loss: nan
env4_first_0:                 episode reward: -19.1000,                 loss: nan
env4_second_0:                 episode reward: 19.1000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1454.25,                last time consumption/overall running time: 495.0897s / 6464.3396 s
env0_first_0:                 episode reward: -19.2000,                 loss: nan
env0_second_0:                 episode reward: 19.2000,                 loss: 0.0111
env1_first_0:                 episode reward: -19.3500,                 loss: nan
env1_second_0:                 episode reward: 19.3500,                 loss: nan
env2_first_0:                 episode reward: -18.5500,                 loss: nan
env2_second_0:                 episode reward: 18.5500,                 loss: nan
env3_first_0:                 episode reward: -18.8500,                 loss: nan
env3_second_0:                 episode reward: 18.8500,                 loss: nan
env4_first_0:                 episode reward: -19.8000,                 loss: nan
env4_second_0:                 episode reward: 19.8000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1427.25,                last time consumption/overall running time: 486.9756s / 6951.3152 s
env0_first_0:                 episode reward: -21.9500,                 loss: nan
env0_second_0:                 episode reward: 21.9500,                 loss: 0.0116
env1_first_0:                 episode reward: -21.3000,                 loss: nan
env1_second_0:                 episode reward: 21.3000,                 loss: nan
env2_first_0:                 episode reward: -21.1000,                 loss: nan
env2_second_0:                 episode reward: 21.1000,                 loss: nan
env3_first_0:                 episode reward: -22.0000,                 loss: nan
env3_second_0:                 episode reward: 22.0000,                 loss: nan
env4_first_0:                 episode reward: -22.5500,                 loss: nan
env4_second_0:                 episode reward: 22.5500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1438.95,                last time consumption/overall running time: 491.5303s / 7442.8455 s
env0_first_0:                 episode reward: -22.3500,                 loss: nan
env0_second_0:                 episode reward: 22.3500,                 loss: 0.0117
env1_first_0:                 episode reward: -22.2500,                 loss: nan
env1_second_0:                 episode reward: 22.2500,                 loss: nan
env2_first_0:                 episode reward: -22.9500,                 loss: nan
env2_second_0:                 episode reward: 22.9500,                 loss: nan
env3_first_0:                 episode reward: -23.2500,                 loss: nan
env3_second_0:                 episode reward: 23.2500,                 loss: nan
env4_first_0:                 episode reward: -22.3500,                 loss: nan
env4_second_0:                 episode reward: 22.3500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1455.9,                last time consumption/overall running time: 494.7309s / 7937.5764 s
env0_first_0:                 episode reward: -21.9500,                 loss: nan
env0_second_0:                 episode reward: 21.9500,                 loss: 0.0125
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
env2_first_0:                 episode reward: -22.1000,                 loss: nan
env2_second_0:                 episode reward: 22.1000,                 loss: nan
env3_first_0:                 episode reward: -21.9000,                 loss: nan
env3_second_0:                 episode reward: 21.9000,                 loss: nan
env4_first_0:                 episode reward: -21.3000,                 loss: nan
env4_second_0:                 episode reward: 21.3000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1481.65,                last time consumption/overall running time: 507.3812s / 8444.9576 s
env0_first_0:                 episode reward: -22.2500,                 loss: nan
env0_second_0:                 episode reward: 22.2500,                 loss: 0.0124
env1_first_0:                 episode reward: -22.6000,                 loss: nan
env1_second_0:                 episode reward: 22.6000,                 loss: nan
env2_first_0:                 episode reward: -22.9500,                 loss: nan
env2_second_0:                 episode reward: 22.9500,                 loss: nan
env3_first_0:                 episode reward: -21.9500,                 loss: nan
env3_second_0:                 episode reward: 21.9500,                 loss: nan
env4_first_0:                 episode reward: -22.8500,                 loss: nan
env4_second_0:                 episode reward: 22.8500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1425.7,                last time consumption/overall running time: 487.0133s / 8931.9709 s
env0_first_0:                 episode reward: -23.4000,                 loss: nan
env0_second_0:                 episode reward: 23.4000,                 loss: 0.0119
env1_first_0:                 episode reward: -23.9000,                 loss: nan
env1_second_0:                 episode reward: 23.9000,                 loss: nan
env2_first_0:                 episode reward: -23.8000,                 loss: nan
env2_second_0:                 episode reward: 23.8000,                 loss: nan
env3_first_0:                 episode reward: -22.9000,                 loss: nan
env3_second_0:                 episode reward: 22.9000,                 loss: nan
env4_first_0:                 episode reward: -23.9000,                 loss: nan
env4_second_0:                 episode reward: 23.9000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1619.6,                last time consumption/overall running time: 549.6348s / 9481.6057 s
env0_first_0:                 episode reward: -18.6000,                 loss: nan
env0_second_0:                 episode reward: 18.6000,                 loss: 0.0121
env1_first_0:                 episode reward: -18.8000,                 loss: nan
env1_second_0:                 episode reward: 18.8000,                 loss: nan
env2_first_0:                 episode reward: -19.3500,                 loss: nan
env2_second_0:                 episode reward: 19.3500,                 loss: nan
env3_first_0:                 episode reward: -18.6500,                 loss: nan
env3_second_0:                 episode reward: 18.6500,                 loss: nan
env4_first_0:                 episode reward: -18.6000,                 loss: nan
env4_second_0:                 episode reward: 18.6000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1483.5,                last time consumption/overall running time: 503.2739s / 9984.8796 s
env0_first_0:                 episode reward: -23.3500,                 loss: nan
env0_second_0:                 episode reward: 23.3500,                 loss: 0.0116
env1_first_0:                 episode reward: -22.6500,                 loss: nan
env1_second_0:                 episode reward: 22.6500,                 loss: nan
env2_first_0:                 episode reward: -24.3500,                 loss: nan
env2_second_0:                 episode reward: 24.3500,                 loss: nan
env3_first_0:                 episode reward: -24.0000,                 loss: nan
env3_second_0:                 episode reward: 24.0000,                 loss: nan
env4_first_0:                 episode reward: -23.2500,                 loss: nan
env4_second_0:                 episode reward: 23.2500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2844.95,                last time consumption/overall running time: 976.6143s / 10961.4939 s
env0_first_0:                 episode reward: -41.1500,                 loss: 0.0165
env0_second_0:                 episode reward: 41.1500,                 loss: 0.0125
env1_first_0:                 episode reward: -41.9000,                 loss: nan
env1_second_0:                 episode reward: 41.9000,                 loss: nan
env2_first_0:                 episode reward: -41.5500,                 loss: nan
env2_second_0:                 episode reward: 41.5500,                 loss: nan
env3_first_0:                 episode reward: -39.5500,                 loss: nan
env3_second_0:                 episode reward: 39.5500,                 loss: nan
env4_first_0:                 episode reward: -39.3500,                 loss: nan
env4_second_0:                 episode reward: 39.3500,                 loss: nan
Score delta: 51.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/283_1.
Episode: 321/10000 (3.2100%),                 avg. length: 2721.85,                last time consumption/overall running time: 932.5681s / 11894.0620 s
env0_first_0:                 episode reward: -18.2000,                 loss: 0.0185
env0_second_0:                 episode reward: 18.2000,                 loss: nan
env1_first_0:                 episode reward: -18.0500,                 loss: nan
env1_second_0:                 episode reward: 18.0500,                 loss: nan
env2_first_0:                 episode reward: -16.5500,                 loss: nan
env2_second_0:                 episode reward: 16.5500,                 loss: nan
env3_first_0:                 episode reward: -17.5500,                 loss: nan
env3_second_0:                 episode reward: 17.5500,                 loss: nan
env4_first_0:                 episode reward: -15.3500,                 loss: nan
env4_second_0:                 episode reward: 15.3500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 3388.65,                last time consumption/overall running time: 1171.4799s / 13065.5419 s
env0_first_0:                 episode reward: -10.1000,                 loss: 0.0147
env0_second_0:                 episode reward: 10.1000,                 loss: nan
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
env2_first_0:                 episode reward: -12.9500,                 loss: nan
env2_second_0:                 episode reward: 12.9500,                 loss: nan
env3_first_0:                 episode reward: -3.9000,                 loss: nan
env3_second_0:                 episode reward: 3.9000,                 loss: nan
env4_first_0:                 episode reward: -11.2000,                 loss: nan
env4_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2207.25,                last time consumption/overall running time: 761.2259s / 13826.7677 s
env0_first_0:                 episode reward: 15.8500,                 loss: 0.0126
env0_second_0:                 episode reward: -15.8500,                 loss: nan
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
env2_first_0:                 episode reward: 17.9500,                 loss: nan
env2_second_0:                 episode reward: -17.9500,                 loss: nan
env3_first_0:                 episode reward: 17.9500,                 loss: nan
env3_second_0:                 episode reward: -17.9500,                 loss: nan
env4_first_0:                 episode reward: 16.9500,                 loss: nan
env4_second_0:                 episode reward: -16.9500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1958.35,                last time consumption/overall running time: 676.0434s / 14502.8111 s
env0_first_0:                 episode reward: 21.6000,                 loss: 0.0118
env0_second_0:                 episode reward: -21.6000,                 loss: nan
env1_first_0:                 episode reward: 19.3000,                 loss: nan
env1_second_0:                 episode reward: -19.3000,                 loss: nan
env2_first_0:                 episode reward: 19.4000,                 loss: nan
env2_second_0:                 episode reward: -19.4000,                 loss: nan
env3_first_0:                 episode reward: 23.0500,                 loss: nan
env3_second_0:                 episode reward: -23.0500,                 loss: nan
env4_first_0:                 episode reward: 19.4000,                 loss: nan
env4_second_0:                 episode reward: -19.4000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1970.55,                last time consumption/overall running time: 681.8176s / 15184.6288 s
env0_first_0:                 episode reward: 9.1500,                 loss: 0.0115
env0_second_0:                 episode reward: -9.1500,                 loss: 0.0131
env1_first_0:                 episode reward: 12.4000,                 loss: nan
env1_second_0:                 episode reward: -12.4000,                 loss: nan
env2_first_0:                 episode reward: 7.6000,                 loss: nan
env2_second_0:                 episode reward: -7.6000,                 loss: nan
env3_first_0:                 episode reward: 13.1500,                 loss: nan
env3_second_0:                 episode reward: -13.1500,                 loss: nan
env4_first_0:                 episode reward: 8.6000,                 loss: nan
env4_second_0:                 episode reward: -8.6000,                 loss: nan
Score delta: 50.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/390_0.
Episode: 421/10000 (4.2100%),                 avg. length: 3206.05,                last time consumption/overall running time: 1110.9052s / 16295.5339 s
env0_first_0:                 episode reward: -9.6000,                 loss: nan
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0189
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
env2_first_0:                 episode reward: -10.4000,                 loss: nan
env2_second_0:                 episode reward: 10.4000,                 loss: nan
env3_first_0:                 episode reward: -10.4500,                 loss: nan
env3_second_0:                 episode reward: 10.4500,                 loss: nan
env4_first_0:                 episode reward: -10.3000,                 loss: nan
env4_second_0:                 episode reward: 10.3000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 3657.8,                last time consumption/overall running time: 1258.8016s / 17554.3356 s
env0_first_0:                 episode reward: -0.5500,                 loss: nan
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0218
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
env2_first_0:                 episode reward: 6.6000,                 loss: nan
env2_second_0:                 episode reward: -6.6000,                 loss: nan
env3_first_0:                 episode reward: 4.6500,                 loss: nan
env3_second_0:                 episode reward: -4.6500,                 loss: nan
env4_first_0:                 episode reward: -1.9000,                 loss: nan
env4_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2386.35,                last time consumption/overall running time: 812.9482s / 18367.2838 s
env0_first_0:                 episode reward: -15.1000,                 loss: nan
env0_second_0:                 episode reward: 15.1000,                 loss: 0.0172
env1_first_0:                 episode reward: -19.6500,                 loss: nan
env1_second_0:                 episode reward: 19.6500,                 loss: nan
env2_first_0:                 episode reward: -16.2500,                 loss: nan
env2_second_0:                 episode reward: 16.2500,                 loss: nan
env3_first_0:                 episode reward: -16.9000,                 loss: nan
env3_second_0:                 episode reward: 16.9000,                 loss: nan
env4_first_0:                 episode reward: -19.3500,                 loss: nan
env4_second_0:                 episode reward: 19.3500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2463.9,                last time consumption/overall running time: 852.7743s / 19220.0581 s
env0_first_0:                 episode reward: -17.7000,                 loss: nan
env0_second_0:                 episode reward: 17.7000,                 loss: 0.0139
env1_first_0:                 episode reward: -21.2000,                 loss: nan
env1_second_0:                 episode reward: 21.2000,                 loss: nan
env2_first_0:                 episode reward: -16.7000,                 loss: nan
env2_second_0:                 episode reward: 16.7000,                 loss: nan
env3_first_0:                 episode reward: -18.9500,                 loss: nan
env3_second_0:                 episode reward: 18.9500,                 loss: nan
env4_first_0:                 episode reward: -12.9000,                 loss: nan
env4_second_0:                 episode reward: 12.9000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2985.4,                last time consumption/overall running time: 1024.9992s / 20245.0572 s
env0_first_0:                 episode reward: -10.1000,                 loss: 0.0120
env0_second_0:                 episode reward: 10.1000,                 loss: 0.0126
env1_first_0:                 episode reward: -10.3000,                 loss: nan
env1_second_0:                 episode reward: 10.3000,                 loss: nan
env2_first_0:                 episode reward: -9.8500,                 loss: nan
env2_second_0:                 episode reward: 9.8500,                 loss: nan
env3_first_0:                 episode reward: -6.5000,                 loss: nan
env3_second_0:                 episode reward: 6.5000,                 loss: nan
env4_first_0:                 episode reward: -9.9000,                 loss: nan
env4_second_0:                 episode reward: 9.9000,                 loss: nan
Score delta: 53.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/491_1.
Episode: 521/10000 (5.2100%),                 avg. length: 5685.25,                last time consumption/overall running time: 1961.1296s / 22206.1868 s
env0_first_0:                 episode reward: 8.8500,                 loss: 0.0139
env0_second_0:                 episode reward: -8.8500,                 loss: nan
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
env2_first_0:                 episode reward: 5.1000,                 loss: nan
env2_second_0:                 episode reward: -5.1000,                 loss: nan
env3_first_0:                 episode reward: 4.5500,                 loss: nan
env3_second_0:                 episode reward: -4.5500,                 loss: nan
env4_first_0:                 episode reward: 8.1500,                 loss: nan
env4_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 4237.85,                last time consumption/overall running time: 1460.7030s / 23666.8898 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0112
env0_second_0:                 episode reward: -5.9000,                 loss: nan
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
env2_first_0:                 episode reward: 8.2000,                 loss: nan
env2_second_0:                 episode reward: -8.2000,                 loss: nan
env3_first_0:                 episode reward: 12.2000,                 loss: nan
env3_second_0:                 episode reward: -12.2000,                 loss: nan
env4_first_0:                 episode reward: 5.2000,                 loss: nan
env4_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 3677.65,                last time consumption/overall running time: 1268.0009s / 24934.8907 s
env0_first_0:                 episode reward: 11.2000,                 loss: 0.0109
env0_second_0:                 episode reward: -11.2000,                 loss: nan
env1_first_0:                 episode reward: 12.0000,                 loss: nan
env1_second_0:                 episode reward: -12.0000,                 loss: nan
env2_first_0:                 episode reward: 12.1000,                 loss: nan
env2_second_0:                 episode reward: -12.1000,                 loss: nan
env3_first_0:                 episode reward: 11.6000,                 loss: nan
env3_second_0:                 episode reward: -11.6000,                 loss: nan
env4_first_0:                 episode reward: 11.1500,                 loss: nan
env4_second_0:                 episode reward: -11.1500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 3293.65,                last time consumption/overall running time: 1145.1633s / 26080.0540 s
env0_first_0:                 episode reward: 14.2000,                 loss: 0.0106
env0_second_0:                 episode reward: -14.2000,                 loss: nan
env1_first_0:                 episode reward: 15.3000,                 loss: nan
env1_second_0:                 episode reward: -15.3000,                 loss: nan
env2_first_0:                 episode reward: 16.5500,                 loss: nan
env2_second_0:                 episode reward: -16.5500,                 loss: nan
env3_first_0:                 episode reward: 16.7500,                 loss: nan
env3_second_0:                 episode reward: -16.7500,                 loss: nan
env4_first_0:                 episode reward: 15.8000,                 loss: nan
env4_second_0:                 episode reward: -15.8000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 3132.25,                last time consumption/overall running time: 1080.7508s / 27160.8048 s
env0_first_0:                 episode reward: 15.0000,                 loss: 0.0104
env0_second_0:                 episode reward: -15.0000,                 loss: nan
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
env2_first_0:                 episode reward: 15.1000,                 loss: nan
env2_second_0:                 episode reward: -15.1000,                 loss: nan
env3_first_0:                 episode reward: 14.4000,                 loss: nan
env3_second_0:                 episode reward: -14.4000,                 loss: nan
env4_first_0:                 episode reward: 14.9500,                 loss: nan
env4_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 3201.15,                last time consumption/overall running time: 1106.4220s / 28267.2269 s
env0_first_0:                 episode reward: 15.5500,                 loss: 0.0106
env0_second_0:                 episode reward: -15.5500,                 loss: nan
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
env2_first_0:                 episode reward: 9.5000,                 loss: nan
env2_second_0:                 episode reward: -9.5000,                 loss: nan
env3_first_0:                 episode reward: 7.3500,                 loss: nan
env3_second_0:                 episode reward: -7.3500,                 loss: nan
env4_first_0:                 episode reward: 12.3000,                 loss: nan
env4_second_0:                 episode reward: -12.3000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 3821.1,                last time consumption/overall running time: 1335.9847s / 29603.2116 s
env0_first_0:                 episode reward: 10.1500,                 loss: 0.0121
env0_second_0:                 episode reward: -10.1500,                 loss: nan
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
env2_first_0:                 episode reward: 8.9000,                 loss: nan
env2_second_0:                 episode reward: -8.9000,                 loss: nan
env3_first_0:                 episode reward: 10.3000,                 loss: nan
env3_second_0:                 episode reward: -10.3000,                 loss: nan
env4_first_0:                 episode reward: 9.2500,                 loss: nan
env4_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 3027.15,                last time consumption/overall running time: 1050.0284s / 30653.2400 s
env0_first_0:                 episode reward: 12.1000,                 loss: 0.0122
env0_second_0:                 episode reward: -12.1000,                 loss: nan
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
env2_first_0:                 episode reward: 13.0000,                 loss: nan
env2_second_0:                 episode reward: -13.0000,                 loss: nan
env3_first_0:                 episode reward: 13.6000,                 loss: nan
env3_second_0:                 episode reward: -13.6000,                 loss: nan
env4_first_0:                 episode reward: 11.8000,                 loss: nan
env4_second_0:                 episode reward: -11.8000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2806.15,                last time consumption/overall running time: 960.0291s / 31613.2691 s
env0_first_0:                 episode reward: 11.2000,                 loss: 0.0122
env0_second_0:                 episode reward: -11.2000,                 loss: nan
env1_first_0:                 episode reward: 11.5000,                 loss: nan
env1_second_0:                 episode reward: -11.5000,                 loss: nan
env2_first_0:                 episode reward: 10.8000,                 loss: nan
env2_second_0:                 episode reward: -10.8000,                 loss: nan
env3_first_0:                 episode reward: 11.7000,                 loss: nan
env3_second_0:                 episode reward: -11.7000,                 loss: nan
env4_first_0:                 episode reward: 11.1000,                 loss: nan
env4_second_0:                 episode reward: -11.1000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2959.55,                last time consumption/overall running time: 1022.3523s / 32635.6214 s
env0_first_0:                 episode reward: 14.9000,                 loss: 0.0108
env0_second_0:                 episode reward: -14.9000,                 loss: nan
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
env2_first_0:                 episode reward: 13.8000,                 loss: nan
env2_second_0:                 episode reward: -13.8000,                 loss: nan
env3_first_0:                 episode reward: 15.6000,                 loss: nan
env3_second_0:                 episode reward: -15.6000,                 loss: nan
env4_first_0:                 episode reward: 15.3000,                 loss: nan
env4_second_0:                 episode reward: -15.3000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 3518.8,                last time consumption/overall running time: 1199.9674s / 33835.5888 s
env0_first_0:                 episode reward: 7.2500,                 loss: 0.0109
env0_second_0:                 episode reward: -7.2500,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
env2_first_0:                 episode reward: 10.1000,                 loss: nan
env2_second_0:                 episode reward: -10.1000,                 loss: nan
env3_first_0:                 episode reward: 4.5500,                 loss: nan
env3_second_0:                 episode reward: -4.5500,                 loss: nan
env4_first_0:                 episode reward: 6.6000,                 loss: nan
env4_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2897.8,                last time consumption/overall running time: 1029.1973s / 34864.7861 s
env0_first_0:                 episode reward: 16.3000,                 loss: 0.0110
env0_second_0:                 episode reward: -16.3000,                 loss: nan
env1_first_0:                 episode reward: 16.6500,                 loss: nan
env1_second_0:                 episode reward: -16.6500,                 loss: nan
env2_first_0:                 episode reward: 16.5000,                 loss: nan
env2_second_0:                 episode reward: -16.5000,                 loss: nan
env3_first_0:                 episode reward: 18.3000,                 loss: nan
env3_second_0:                 episode reward: -18.3000,                 loss: nan
env4_first_0:                 episode reward: 16.0000,                 loss: nan
env4_second_0:                 episode reward: -16.0000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2966.05,                last time consumption/overall running time: 1035.7422s / 35900.5283 s
env0_first_0:                 episode reward: 15.8000,                 loss: 0.0124
env0_second_0:                 episode reward: -15.8000,                 loss: nan
env1_first_0:                 episode reward: 16.3500,                 loss: nan
env1_second_0:                 episode reward: -16.3500,                 loss: nan
env2_first_0:                 episode reward: 16.7000,                 loss: nan
env2_second_0:                 episode reward: -16.7000,                 loss: nan
env3_first_0:                 episode reward: 16.5500,                 loss: nan
env3_second_0:                 episode reward: -16.5500,                 loss: nan
env4_first_0:                 episode reward: 16.6000,                 loss: nan
env4_second_0:                 episode reward: -16.6000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 3317.75,                last time consumption/overall running time: 1159.1685s / 37059.6968 s
env0_first_0:                 episode reward: 12.3000,                 loss: 0.0121
env0_second_0:                 episode reward: -12.3000,                 loss: nan
env1_first_0:                 episode reward: 13.4000,                 loss: nan
env1_second_0:                 episode reward: -13.4000,                 loss: nan
env2_first_0:                 episode reward: 13.0000,                 loss: nan
env2_second_0:                 episode reward: -13.0000,                 loss: nan
env3_first_0:                 episode reward: 10.4500,                 loss: nan
env3_second_0:                 episode reward: -10.4500,                 loss: nan
env4_first_0:                 episode reward: 11.1500,                 loss: nan
env4_second_0:                 episode reward: -11.1500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 3494.6,                last time consumption/overall running time: 1209.0773s / 38268.7741 s
env0_first_0:                 episode reward: 15.0000,                 loss: 0.0101
env0_second_0:                 episode reward: -15.0000,                 loss: nan
env1_first_0:                 episode reward: 14.3000,                 loss: nan
env1_second_0:                 episode reward: -14.3000,                 loss: nan
env2_first_0:                 episode reward: 12.0500,                 loss: nan
env2_second_0:                 episode reward: -12.0500,                 loss: nan
env3_first_0:                 episode reward: 16.2000,                 loss: nan
env3_second_0:                 episode reward: -16.2000,                 loss: nan
env4_first_0:                 episode reward: 15.3500,                 loss: nan
env4_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 4117.0,                last time consumption/overall running time: 1419.4705s / 39688.2445 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0116
env0_second_0:                 episode reward: 7.7000,                 loss: nan
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
env2_first_0:                 episode reward: -3.1500,                 loss: nan
env2_second_0:                 episode reward: 3.1500,                 loss: nan
env3_first_0:                 episode reward: -6.3500,                 loss: nan
env3_second_0:                 episode reward: 6.3500,                 loss: nan
env4_first_0:                 episode reward: -4.1500,                 loss: nan
env4_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 3246.6,                last time consumption/overall running time: 1117.9044s / 40806.1490 s
env0_first_0:                 episode reward: 13.1000,                 loss: 0.0116
env0_second_0:                 episode reward: -13.1000,                 loss: nan
env1_first_0:                 episode reward: 11.1500,                 loss: nan
env1_second_0:                 episode reward: -11.1500,                 loss: nan
env2_first_0:                 episode reward: 13.4000,                 loss: nan
env2_second_0:                 episode reward: -13.4000,                 loss: nan
env3_first_0:                 episode reward: 13.4000,                 loss: nan
env3_second_0:                 episode reward: -13.4000,                 loss: nan
env4_first_0:                 episode reward: 13.3000,                 loss: nan
env4_second_0:                 episode reward: -13.3000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2642.35,                last time consumption/overall running time: 912.1009s / 41718.2499 s
env0_first_0:                 episode reward: 14.1000,                 loss: 0.0103
env0_second_0:                 episode reward: -14.1000,                 loss: nan
env1_first_0:                 episode reward: 14.4500,                 loss: nan
env1_second_0:                 episode reward: -14.4500,                 loss: nan
env2_first_0:                 episode reward: 13.8000,                 loss: nan
env2_second_0:                 episode reward: -13.8000,                 loss: nan
env3_first_0:                 episode reward: 15.6000,                 loss: nan
env3_second_0:                 episode reward: -15.6000,                 loss: nan
env4_first_0:                 episode reward: 15.7000,                 loss: nan
env4_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 3104.6,                last time consumption/overall running time: 1070.2513s / 42788.5012 s
env0_first_0:                 episode reward: 8.7500,                 loss: 0.0104
env0_second_0:                 episode reward: -8.7500,                 loss: nan
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
env2_first_0:                 episode reward: 9.1500,                 loss: nan
env2_second_0:                 episode reward: -9.1500,                 loss: nan
env3_first_0:                 episode reward: 9.0500,                 loss: nan
env3_second_0:                 episode reward: -9.0500,                 loss: nan
env4_first_0:                 episode reward: 8.1000,                 loss: nan
env4_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2742.8,                last time consumption/overall running time: 944.2641s / 43732.7653 s
env0_first_0:                 episode reward: 16.0000,                 loss: 0.0104
env0_second_0:                 episode reward: -16.0000,                 loss: nan
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
env2_first_0:                 episode reward: 13.9000,                 loss: nan
env2_second_0:                 episode reward: -13.9000,                 loss: nan
env3_first_0:                 episode reward: 14.9500,                 loss: nan
env3_second_0:                 episode reward: -14.9500,                 loss: nan
env4_first_0:                 episode reward: 14.4000,                 loss: nan
env4_second_0:                 episode reward: -14.4000,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2598.1,                last time consumption/overall running time: 891.5503s / 44624.3156 s
env0_first_0:                 episode reward: 17.7000,                 loss: 0.0104
env0_second_0:                 episode reward: -17.7000,                 loss: nan
env1_first_0:                 episode reward: 16.5500,                 loss: nan
env1_second_0:                 episode reward: -16.5500,                 loss: nan
env2_first_0:                 episode reward: 17.8500,                 loss: nan
env2_second_0:                 episode reward: -17.8500,                 loss: nan
env3_first_0:                 episode reward: 16.6500,                 loss: nan
env3_second_0:                 episode reward: -16.6500,                 loss: nan
env4_first_0:                 episode reward: 16.9000,                 loss: nan
env4_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 3134.55,                last time consumption/overall running time: 1093.6819s / 45717.9974 s
env0_first_0:                 episode reward: 11.6000,                 loss: 0.0105
env0_second_0:                 episode reward: -11.6000,                 loss: nan
env1_first_0:                 episode reward: 12.9000,                 loss: nan
env1_second_0:                 episode reward: -12.9000,                 loss: nan
env2_first_0:                 episode reward: 13.2500,                 loss: nan
env2_second_0:                 episode reward: -13.2500,                 loss: nan
env3_first_0:                 episode reward: 12.5000,                 loss: nan
env3_second_0:                 episode reward: -12.5000,                 loss: nan
env4_first_0:                 episode reward: 11.2500,                 loss: nan
env4_second_0:                 episode reward: -11.2500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2565.05,                last time consumption/overall running time: 887.6842s / 46605.6816 s
env0_first_0:                 episode reward: 14.7500,                 loss: 0.0110
env0_second_0:                 episode reward: -14.7500,                 loss: nan
env1_first_0:                 episode reward: 16.3000,                 loss: nan
env1_second_0:                 episode reward: -16.3000,                 loss: nan
env2_first_0:                 episode reward: 15.9000,                 loss: nan
env2_second_0:                 episode reward: -15.9000,                 loss: nan
env3_first_0:                 episode reward: 13.6000,                 loss: nan
env3_second_0:                 episode reward: -13.6000,                 loss: nan
env4_first_0:                 episode reward: 16.0500,                 loss: nan
env4_second_0:                 episode reward: -16.0500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2638.4,                last time consumption/overall running time: 911.7767s / 47517.4583 s
env0_first_0:                 episode reward: 18.5500,                 loss: 0.0094
env0_second_0:                 episode reward: -18.5500,                 loss: nan
env1_first_0:                 episode reward: 18.9500,                 loss: nan
env1_second_0:                 episode reward: -18.9500,                 loss: nan
env2_first_0:                 episode reward: 18.4500,                 loss: nan
env2_second_0:                 episode reward: -18.4500,                 loss: nan
env3_first_0:                 episode reward: 16.6000,                 loss: nan
env3_second_0:                 episode reward: -16.6000,                 loss: nan
env4_first_0:                 episode reward: 17.7000,                 loss: nan
env4_second_0:                 episode reward: -17.7000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2642.85,                last time consumption/overall running time: 908.7104s / 48426.1687 s
env0_first_0:                 episode reward: 15.3500,                 loss: 0.0091
env0_second_0:                 episode reward: -15.3500,                 loss: nan
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
env2_first_0:                 episode reward: 14.0000,                 loss: nan
env2_second_0:                 episode reward: -14.0000,                 loss: nan
env3_first_0:                 episode reward: 13.8500,                 loss: nan
env3_second_0:                 episode reward: -13.8500,                 loss: nan
env4_first_0:                 episode reward: 15.5500,                 loss: nan
env4_second_0:                 episode reward: -15.5500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2288.65,                last time consumption/overall running time: 795.7050s / 49221.8738 s
env0_first_0:                 episode reward: 16.6000,                 loss: 0.0092
env0_second_0:                 episode reward: -16.6000,                 loss: nan
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
env2_first_0:                 episode reward: 18.5000,                 loss: nan
env2_second_0:                 episode reward: -18.5000,                 loss: nan
env3_first_0:                 episode reward: 17.4000,                 loss: nan
env3_second_0:                 episode reward: -17.4000,                 loss: nan
env4_first_0:                 episode reward: 16.6500,                 loss: nan
env4_second_0:                 episode reward: -16.6500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2574.2,                last time consumption/overall running time: 883.4490s / 50105.3228 s
env0_first_0:                 episode reward: 14.1500,                 loss: 0.0088
env0_second_0:                 episode reward: -14.1500,                 loss: nan
env1_first_0:                 episode reward: 14.2000,                 loss: nan
env1_second_0:                 episode reward: -14.2000,                 loss: nan
env2_first_0:                 episode reward: 16.0000,                 loss: nan
env2_second_0:                 episode reward: -16.0000,                 loss: nan
env3_first_0:                 episode reward: 13.9000,                 loss: nan
env3_second_0:                 episode reward: -13.9000,                 loss: nan
env4_first_0:                 episode reward: 14.7500,                 loss: nan
env4_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2663.55,                last time consumption/overall running time: 920.1217s / 51025.4444 s
env0_first_0:                 episode reward: 7.7000,                 loss: 0.0093
env0_second_0:                 episode reward: -7.7000,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
env2_first_0:                 episode reward: 11.6000,                 loss: nan
env2_second_0:                 episode reward: -11.6000,                 loss: nan
env3_first_0:                 episode reward: 11.7000,                 loss: nan
env3_second_0:                 episode reward: -11.7000,                 loss: nan
env4_first_0:                 episode reward: 7.6000,                 loss: nan
env4_second_0:                 episode reward: -7.6000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2301.6,                last time consumption/overall running time: 792.6701s / 51818.1146 s
env0_first_0:                 episode reward: 16.0500,                 loss: 0.0094
env0_second_0:                 episode reward: -16.0500,                 loss: nan
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
env2_first_0:                 episode reward: 16.8000,                 loss: nan
env2_second_0:                 episode reward: -16.8000,                 loss: nan
env3_first_0:                 episode reward: 15.5000,                 loss: nan
env3_second_0:                 episode reward: -15.5000,                 loss: nan
env4_first_0:                 episode reward: 17.0000,                 loss: nan
env4_second_0:                 episode reward: -17.0000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2552.9,                last time consumption/overall running time: 872.9623s / 52691.0769 s
env0_first_0:                 episode reward: 14.0500,                 loss: 0.0100
env0_second_0:                 episode reward: -14.0500,                 loss: nan
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
env2_first_0:                 episode reward: 14.6500,                 loss: nan
env2_second_0:                 episode reward: -14.6500,                 loss: nan
env3_first_0:                 episode reward: 15.6500,                 loss: nan
env3_second_0:                 episode reward: -15.6500,                 loss: nan
env4_first_0:                 episode reward: 15.8500,                 loss: nan
env4_second_0:                 episode reward: -15.8500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2840.15,                last time consumption/overall running time: 986.6479s / 53677.7248 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0102
env0_second_0:                 episode reward: -7.0500,                 loss: nan
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
env2_first_0:                 episode reward: 4.7000,                 loss: nan
env2_second_0:                 episode reward: -4.7000,                 loss: nan
env3_first_0:                 episode reward: 5.7000,                 loss: nan
env3_second_0:                 episode reward: -5.7000,                 loss: nan
env4_first_0:                 episode reward: 5.5000,                 loss: nan
env4_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2426.5,                last time consumption/overall running time: 833.4830s / 54511.2078 s
env0_first_0:                 episode reward: 17.4500,                 loss: 0.0102
env0_second_0:                 episode reward: -17.4500,                 loss: nan
env1_first_0:                 episode reward: 18.8000,                 loss: nan
env1_second_0:                 episode reward: -18.8000,                 loss: nan
env2_first_0:                 episode reward: 21.2000,                 loss: nan
env2_second_0:                 episode reward: -21.2000,                 loss: nan
env3_first_0:                 episode reward: 18.4500,                 loss: nan
env3_second_0:                 episode reward: -18.4500,                 loss: nan
env4_first_0:                 episode reward: 19.9500,                 loss: nan
env4_second_0:                 episode reward: -19.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2763.55,                last time consumption/overall running time: 944.6299s / 55455.8377 s
env0_first_0:                 episode reward: 9.6000,                 loss: 0.0104
env0_second_0:                 episode reward: -9.6000,                 loss: 0.0126
env1_first_0:                 episode reward: 11.2500,                 loss: nan
env1_second_0:                 episode reward: -11.2500,                 loss: nan
env2_first_0:                 episode reward: 10.5500,                 loss: nan
env2_second_0:                 episode reward: -10.5500,                 loss: nan
env3_first_0:                 episode reward: 9.2500,                 loss: nan
env3_second_0:                 episode reward: -9.2500,                 loss: nan
env4_first_0:                 episode reward: 11.7000,                 loss: nan
env4_second_0:                 episode reward: -11.7000,                 loss: nan
Score delta: 51.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1151_0.
Episode: 1181/10000 (11.8100%),                 avg. length: 3837.45,                last time consumption/overall running time: 1336.7627s / 56792.6003 s
env0_first_0:                 episode reward: -10.8000,                 loss: nan
env0_second_0:                 episode reward: 10.8000,                 loss: 0.0130
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
env2_first_0:                 episode reward: -1.9000,                 loss: nan
env2_second_0:                 episode reward: 1.9000,                 loss: nan
env3_first_0:                 episode reward: -8.7000,                 loss: nan
env3_second_0:                 episode reward: 8.7000,                 loss: nan
env4_first_0:                 episode reward: -4.7500,                 loss: nan
env4_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 3549.75,                last time consumption/overall running time: 1229.0244s / 58021.6247 s
env0_first_0:                 episode reward: -16.8000,                 loss: nan
env0_second_0:                 episode reward: 16.8000,                 loss: 0.0125
env1_first_0:                 episode reward: -20.1000,                 loss: nan
env1_second_0:                 episode reward: 20.1000,                 loss: nan
env2_first_0:                 episode reward: -17.1500,                 loss: nan
env2_second_0:                 episode reward: 17.1500,                 loss: nan
env3_first_0:                 episode reward: -16.5000,                 loss: nan
env3_second_0:                 episode reward: 16.5000,                 loss: nan
env4_first_0:                 episode reward: -16.2500,                 loss: nan
env4_second_0:                 episode reward: 16.2500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 3411.75,                last time consumption/overall running time: 1183.1646s / 59204.7893 s
env0_first_0:                 episode reward: -11.9000,                 loss: nan
env0_second_0:                 episode reward: 11.9000,                 loss: 0.0128
env1_first_0:                 episode reward: -13.0500,                 loss: nan
env1_second_0:                 episode reward: 13.0500,                 loss: nan
env2_first_0:                 episode reward: -11.9000,                 loss: nan
env2_second_0:                 episode reward: 11.9000,                 loss: nan
env3_first_0:                 episode reward: -14.0500,                 loss: nan
env3_second_0:                 episode reward: 14.0500,                 loss: nan
env4_first_0:                 episode reward: -13.6500,                 loss: nan
env4_second_0:                 episode reward: 13.6500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 3208.8,                last time consumption/overall running time: 1134.2163s / 60339.0056 s
env0_first_0:                 episode reward: -17.3000,                 loss: nan
env0_second_0:                 episode reward: 17.3000,                 loss: 0.0121
env1_first_0:                 episode reward: -11.4000,                 loss: nan
env1_second_0:                 episode reward: 11.4000,                 loss: nan
env2_first_0:                 episode reward: -11.7000,                 loss: nan
env2_second_0:                 episode reward: 11.7000,                 loss: nan
env3_first_0:                 episode reward: -17.6000,                 loss: nan
env3_second_0:                 episode reward: 17.6000,                 loss: nan
env4_first_0:                 episode reward: -11.0500,                 loss: nan
env4_second_0:                 episode reward: 11.0500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 3859.0,                last time consumption/overall running time: 1352.2879s / 61691.2935 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0152
env0_second_0:                 episode reward: 7.6500,                 loss: 0.0124
env1_first_0:                 episode reward: -14.5500,                 loss: nan
env1_second_0:                 episode reward: 14.5500,                 loss: nan
env2_first_0:                 episode reward: -12.0500,                 loss: nan
env2_second_0:                 episode reward: 12.0500,                 loss: nan
env3_first_0:                 episode reward: -13.5000,                 loss: nan
env3_second_0:                 episode reward: 13.5000,                 loss: nan
env4_first_0:                 episode reward: -12.9000,                 loss: nan
env4_second_0:                 episode reward: 12.9000,                 loss: nan
Score delta: 54.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1248_1.
Episode: 1281/10000 (12.8100%),                 avg. length: 4255.05,                last time consumption/overall running time: 1473.9448s / 63165.2383 s
env0_first_0:                 episode reward: 16.3500,                 loss: 0.0133
env0_second_0:                 episode reward: -16.3500,                 loss: nan
env1_first_0:                 episode reward: 12.8500,                 loss: nan
env1_second_0:                 episode reward: -12.8500,                 loss: nan
env2_first_0:                 episode reward: 7.9500,                 loss: nan
env2_second_0:                 episode reward: -7.9500,                 loss: nan
env3_first_0:                 episode reward: 4.2500,                 loss: nan
env3_second_0:                 episode reward: -4.2500,                 loss: nan
env4_first_0:                 episode reward: 6.6500,                 loss: nan
env4_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 4566.55,                last time consumption/overall running time: 1559.4946s / 64724.7329 s
env0_first_0:                 episode reward: 13.4000,                 loss: 0.0115
env0_second_0:                 episode reward: -13.4000,                 loss: 0.0135
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
env2_first_0:                 episode reward: 8.5000,                 loss: nan
env2_second_0:                 episode reward: -8.5000,                 loss: nan
env3_first_0:                 episode reward: 11.3500,                 loss: nan
env3_second_0:                 episode reward: -11.3500,                 loss: nan
env4_first_0:                 episode reward: 11.4000,                 loss: nan
env4_second_0:                 episode reward: -11.4000,                 loss: nan
Score delta: 51.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1288_0.
Episode: 1321/10000 (13.2100%),                 avg. length: 3662.35,                last time consumption/overall running time: 1231.6861s / 65956.4190 s
env0_first_0:                 episode reward: -11.6000,                 loss: nan
env0_second_0:                 episode reward: 11.6000,                 loss: 0.0125
env1_first_0:                 episode reward: -10.6500,                 loss: nan
env1_second_0:                 episode reward: 10.6500,                 loss: nan
env2_first_0:                 episode reward: -12.4000,                 loss: nan
env2_second_0:                 episode reward: 12.4000,                 loss: nan
env3_first_0:                 episode reward: -14.8000,                 loss: nan
env3_second_0:                 episode reward: 14.8000,                 loss: nan
env4_first_0:                 episode reward: -9.2500,                 loss: nan
env4_second_0:                 episode reward: 9.2500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 3932.15,                last time consumption/overall running time: 1317.5811s / 67274.0001 s
env0_first_0:                 episode reward: -11.9500,                 loss: nan
env0_second_0:                 episode reward: 11.9500,                 loss: 0.0105
env1_first_0:                 episode reward: -12.2000,                 loss: nan
env1_second_0:                 episode reward: 12.2000,                 loss: nan
env2_first_0:                 episode reward: -16.6500,                 loss: nan
env2_second_0:                 episode reward: 16.6500,                 loss: nan
env3_first_0:                 episode reward: -13.4500,                 loss: nan
env3_second_0:                 episode reward: 13.4500,                 loss: nan
env4_first_0:                 episode reward: -13.8500,                 loss: nan
env4_second_0:                 episode reward: 13.8500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 3424.85,                last time consumption/overall running time: 1152.1489s / 68426.1490 s
env0_first_0:                 episode reward: -18.2500,                 loss: nan
env0_second_0:                 episode reward: 18.2500,                 loss: 0.0109
env1_first_0:                 episode reward: -16.7000,                 loss: nan
env1_second_0:                 episode reward: 16.7000,                 loss: nan
env2_first_0:                 episode reward: -18.6500,                 loss: nan
env2_second_0:                 episode reward: 18.6500,                 loss: nan
env3_first_0:                 episode reward: -16.0000,                 loss: nan
env3_second_0:                 episode reward: 16.0000,                 loss: nan
env4_first_0:                 episode reward: -15.3000,                 loss: nan
env4_second_0:                 episode reward: 15.3000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2975.25,                last time consumption/overall running time: 1008.3352s / 69434.4842 s
env0_first_0:                 episode reward: -14.4000,                 loss: nan
env0_second_0:                 episode reward: 14.4000,                 loss: 0.0113
env1_first_0:                 episode reward: -14.5500,                 loss: nan
env1_second_0:                 episode reward: 14.5500,                 loss: nan
env2_first_0:                 episode reward: -14.0500,                 loss: nan
env2_second_0:                 episode reward: 14.0500,                 loss: nan
env3_first_0:                 episode reward: -16.3000,                 loss: nan
env3_second_0:                 episode reward: 16.3000,                 loss: nan
env4_first_0:                 episode reward: -14.6000,                 loss: nan
env4_second_0:                 episode reward: 14.6000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2339.55,                last time consumption/overall running time: 795.0009s / 70229.4851 s
env0_first_0:                 episode reward: -19.2000,                 loss: nan
env0_second_0:                 episode reward: 19.2000,                 loss: 0.0109
env1_first_0:                 episode reward: -18.5500,                 loss: nan
env1_second_0:                 episode reward: 18.5500,                 loss: nan
env2_first_0:                 episode reward: -20.6500,                 loss: nan
env2_second_0:                 episode reward: 20.6500,                 loss: nan
env3_first_0:                 episode reward: -18.5000,                 loss: nan
env3_second_0:                 episode reward: 18.5000,                 loss: nan
env4_first_0:                 episode reward: -18.8500,                 loss: nan
env4_second_0:                 episode reward: 18.8500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2451.25,                last time consumption/overall running time: 828.1033s / 71057.5885 s
env0_first_0:                 episode reward: -21.6500,                 loss: nan
env0_second_0:                 episode reward: 21.6500,                 loss: 0.0113
env1_first_0:                 episode reward: -20.1500,                 loss: nan
env1_second_0:                 episode reward: 20.1500,                 loss: nan
env2_first_0:                 episode reward: -20.7500,                 loss: nan
env2_second_0:                 episode reward: 20.7500,                 loss: nan
env3_first_0:                 episode reward: -20.0500,                 loss: nan
env3_second_0:                 episode reward: 20.0500,                 loss: nan
env4_first_0:                 episode reward: -20.6500,                 loss: nan
env4_second_0:                 episode reward: 20.6500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 3256.15,                last time consumption/overall running time: 1103.3889s / 72160.9774 s
env0_first_0:                 episode reward: -58.0000,                 loss: 0.0263
env0_second_0:                 episode reward: 58.0000,                 loss: 0.0118
env1_first_0:                 episode reward: -56.5000,                 loss: nan
env1_second_0:                 episode reward: 56.5000,                 loss: nan
env2_first_0:                 episode reward: -57.9500,                 loss: nan
env2_second_0:                 episode reward: 57.9500,                 loss: nan
env3_first_0:                 episode reward: -57.9000,                 loss: nan
env3_second_0:                 episode reward: 57.9000,                 loss: nan
env4_first_0:                 episode reward: -56.4500,                 loss: nan
env4_second_0:                 episode reward: 56.4500,                 loss: nan
Score delta: 50.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1424_1.
Episode: 1461/10000 (14.6100%),                 avg. length: 4550.7,                last time consumption/overall running time: 1553.2207s / 73714.1981 s
env0_first_0:                 episode reward: -46.9000,                 loss: 0.0180
env0_second_0:                 episode reward: 46.9000,                 loss: nan
env1_first_0:                 episode reward: -45.1000,                 loss: nan
env1_second_0:                 episode reward: 45.1000,                 loss: nan
env2_first_0:                 episode reward: -47.1500,                 loss: nan
env2_second_0:                 episode reward: 47.1500,                 loss: nan
env3_first_0:                 episode reward: -45.8500,                 loss: nan
env3_second_0:                 episode reward: 45.8500,                 loss: nan
env4_first_0:                 episode reward: -46.5500,                 loss: nan
env4_second_0:                 episode reward: 46.5500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 5908.7,                last time consumption/overall running time: 1997.9271s / 75712.1252 s
env0_first_0:                 episode reward: -19.0000,                 loss: 0.0140
env0_second_0:                 episode reward: 19.0000,                 loss: nan
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
env2_first_0:                 episode reward: -20.5000,                 loss: nan
env2_second_0:                 episode reward: 20.5000,                 loss: nan
env3_first_0:                 episode reward: -22.5500,                 loss: nan
env3_second_0:                 episode reward: 22.5500,                 loss: nan
env4_first_0:                 episode reward: -21.6000,                 loss: nan
env4_second_0:                 episode reward: 21.6000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 4677.5,                last time consumption/overall running time: 1600.1162s / 77312.2414 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0126
env0_second_0:                 episode reward: 0.9500,                 loss: nan
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
env2_first_0:                 episode reward: -2.1500,                 loss: nan
env2_second_0:                 episode reward: 2.1500,                 loss: nan
env3_first_0:                 episode reward: -8.3500,                 loss: nan
env3_second_0:                 episode reward: 8.3500,                 loss: nan
env4_first_0:                 episode reward: -2.5500,                 loss: nan
env4_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 4567.05,                last time consumption/overall running time: 1583.5892s / 78895.8307 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0130
env0_second_0:                 episode reward: 4.5500,                 loss: nan
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
env2_first_0:                 episode reward: -5.5500,                 loss: nan
env2_second_0:                 episode reward: 5.5500,                 loss: nan
env3_first_0:                 episode reward: -5.7000,                 loss: nan
env3_second_0:                 episode reward: 5.7000,                 loss: nan
env4_first_0:                 episode reward: -8.3000,                 loss: nan
env4_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 4779.35,                last time consumption/overall running time: 1635.7398s / 80531.5705 s
env0_first_0:                 episode reward: 12.4500,                 loss: 0.0135
env0_second_0:                 episode reward: -12.4500,                 loss: nan
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
env2_first_0:                 episode reward: 12.0000,                 loss: nan
env2_second_0:                 episode reward: -12.0000,                 loss: nan
env3_first_0:                 episode reward: 18.6500,                 loss: nan
env3_second_0:                 episode reward: -18.6500,                 loss: nan
env4_first_0:                 episode reward: 13.0500,                 loss: nan
env4_second_0:                 episode reward: -13.0500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 5382.15,                last time consumption/overall running time: 1850.5898s / 82382.1603 s
env0_first_0:                 episode reward: 13.7500,                 loss: 0.0134
env0_second_0:                 episode reward: -13.7500,                 loss: nan
env1_first_0:                 episode reward: 15.8500,                 loss: nan
env1_second_0:                 episode reward: -15.8500,                 loss: nan
env2_first_0:                 episode reward: 14.5500,                 loss: nan
env2_second_0:                 episode reward: -14.5500,                 loss: nan
env3_first_0:                 episode reward: 16.5000,                 loss: nan
env3_second_0:                 episode reward: -16.5000,                 loss: nan
env4_first_0:                 episode reward: 18.3500,                 loss: nan
env4_second_0:                 episode reward: -18.3500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 4242.55,                last time consumption/overall running time: 1464.0783s / 83846.2386 s
env0_first_0:                 episode reward: 15.7500,                 loss: 0.0133
env0_second_0:                 episode reward: -15.7500,                 loss: nan
env1_first_0:                 episode reward: 14.4500,                 loss: nan
env1_second_0:                 episode reward: -14.4500,                 loss: nan
env2_first_0:                 episode reward: 19.2000,                 loss: nan
env2_second_0:                 episode reward: -19.2000,                 loss: nan
env3_first_0:                 episode reward: 17.0500,                 loss: nan
env3_second_0:                 episode reward: -17.0500,                 loss: nan
env4_first_0:                 episode reward: 13.8000,                 loss: nan
env4_second_0:                 episode reward: -13.8000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 4200.0,                last time consumption/overall running time: 1444.4932s / 85290.7318 s
env0_first_0:                 episode reward: 18.6500,                 loss: 0.0141
env0_second_0:                 episode reward: -18.6500,                 loss: 0.0288
env1_first_0:                 episode reward: 18.8000,                 loss: nan
env1_second_0:                 episode reward: -18.8000,                 loss: nan
env2_first_0:                 episode reward: 20.5500,                 loss: nan
env2_second_0:                 episode reward: -20.5500,                 loss: nan
env3_first_0:                 episode reward: 17.0000,                 loss: nan
env3_second_0:                 episode reward: -17.0000,                 loss: nan
env4_first_0:                 episode reward: 19.8500,                 loss: nan
env4_second_0:                 episode reward: -19.8500,                 loss: nan
Score delta: 52.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1597_0.
Episode: 1621/10000 (16.2100%),                 avg. length: 4490.5,                last time consumption/overall running time: 1535.9321s / 86826.6639 s
env0_first_0:                 episode reward: 12.7500,                 loss: nan
env0_second_0:                 episode reward: -12.7500,                 loss: 0.0165
env1_first_0:                 episode reward: 11.3500,                 loss: nan
env1_second_0:                 episode reward: -11.3500,                 loss: nan
env2_first_0:                 episode reward: 7.9500,                 loss: nan
env2_second_0:                 episode reward: -7.9500,                 loss: nan
env3_first_0:                 episode reward: 17.7500,                 loss: nan
env3_second_0:                 episode reward: -17.7500,                 loss: nan
env4_first_0:                 episode reward: 9.3000,                 loss: nan
env4_second_0:                 episode reward: -9.3000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 5473.45,                last time consumption/overall running time: 1895.3249s / 88721.9889 s
env0_first_0:                 episode reward: -14.4500,                 loss: 0.0181
env0_second_0:                 episode reward: 14.4500,                 loss: 0.0119
env1_first_0:                 episode reward: -9.7500,                 loss: nan
env1_second_0:                 episode reward: 9.7500,                 loss: nan
env2_first_0:                 episode reward: -7.6500,                 loss: nan
env2_second_0:                 episode reward: 7.6500,                 loss: nan
env3_first_0:                 episode reward: -8.4000,                 loss: nan
env3_second_0:                 episode reward: 8.4000,                 loss: nan
env4_first_0:                 episode reward: -5.2000,                 loss: nan
env4_second_0:                 episode reward: 5.2000,                 loss: nan
Score delta: 58.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1635_1.
Episode: 1661/10000 (16.6100%),                 avg. length: 4879.6,                last time consumption/overall running time: 1678.8513s / 90400.8402 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0130
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
env2_first_0:                 episode reward: -7.6000,                 loss: nan
env2_second_0:                 episode reward: 7.6000,                 loss: nan
env3_first_0:                 episode reward: -3.5500,                 loss: nan
env3_second_0:                 episode reward: 3.5500,                 loss: nan
env4_first_0:                 episode reward: -4.2500,                 loss: nan
env4_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 6000.75,                last time consumption/overall running time: 2058.8977s / 92459.7379 s
env0_first_0:                 episode reward: -11.5000,                 loss: 0.0114
env0_second_0:                 episode reward: 11.5000,                 loss: nan
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
env2_first_0:                 episode reward: -5.8000,                 loss: nan
env2_second_0:                 episode reward: 5.8000,                 loss: nan
env3_first_0:                 episode reward: -7.1500,                 loss: nan
env3_second_0:                 episode reward: 7.1500,                 loss: nan
env4_first_0:                 episode reward: -4.5000,                 loss: nan
env4_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 6299.15,                last time consumption/overall running time: 2158.9032s / 94618.6410 s
env0_first_0:                 episode reward: 7.3500,                 loss: 0.0119
env0_second_0:                 episode reward: -7.3500,                 loss: 0.0160
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
env2_first_0:                 episode reward: 7.0500,                 loss: nan
env2_second_0:                 episode reward: -7.0500,                 loss: nan
env3_first_0:                 episode reward: 4.6500,                 loss: nan
env3_second_0:                 episode reward: -4.6500,                 loss: nan
env4_first_0:                 episode reward: 6.0500,                 loss: nan
env4_second_0:                 episode reward: -6.0500,                 loss: nan
Score delta: 53.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1687_0.
Episode: 1721/10000 (17.2100%),                 avg. length: 4669.75,                last time consumption/overall running time: 1644.8961s / 96263.5371 s
env0_first_0:                 episode reward: -4.6500,                 loss: nan
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0142
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
env2_first_0:                 episode reward: 1.6500,                 loss: nan
env2_second_0:                 episode reward: -1.6500,                 loss: nan
env3_first_0:                 episode reward: -2.7500,                 loss: nan
env3_second_0:                 episode reward: 2.7500,                 loss: nan
env4_first_0:                 episode reward: -1.4000,                 loss: nan
env4_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 5080.1,                last time consumption/overall running time: 1945.7144s / 98209.2515 s
env0_first_0:                 episode reward: -3.8000,                 loss: nan
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0138
env1_first_0:                 episode reward: -9.2000,                 loss: nan
env1_second_0:                 episode reward: 9.2000,                 loss: nan
env2_first_0:                 episode reward: -1.7000,                 loss: nan
env2_second_0:                 episode reward: 1.7000,                 loss: nan
env3_first_0:                 episode reward: -6.1000,                 loss: nan
env3_second_0:                 episode reward: 6.1000,                 loss: nan
env4_first_0:                 episode reward: -16.2000,                 loss: nan
env4_second_0:                 episode reward: 16.2000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 6305.5,                last time consumption/overall running time: 2431.6167s / 100640.8682 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0108
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0127
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
env2_first_0:                 episode reward: 4.0500,                 loss: nan
env2_second_0:                 episode reward: -4.0500,                 loss: nan
env3_first_0:                 episode reward: -1.0500,                 loss: nan
env3_second_0:                 episode reward: 1.0500,                 loss: nan
env4_first_0:                 episode reward: 2.0500,                 loss: nan
env4_second_0:                 episode reward: -2.0500,                 loss: nan
Score delta: 51.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1745_1.
Episode: 1781/10000 (17.8100%),                 avg. length: 5624.55,                last time consumption/overall running time: 2140.0862s / 102780.9544 s
env0_first_0:                 episode reward: 16.6000,                 loss: 0.0111
env0_second_0:                 episode reward: -16.6000,                 loss: nan
env1_first_0:                 episode reward: 17.9500,                 loss: nan
env1_second_0:                 episode reward: -17.9500,                 loss: nan
env2_first_0:                 episode reward: 10.9000,                 loss: nan
env2_second_0:                 episode reward: -10.9000,                 loss: nan
env3_first_0:                 episode reward: 17.9000,                 loss: nan
env3_second_0:                 episode reward: -17.9000,                 loss: nan
env4_first_0:                 episode reward: 13.0000,                 loss: nan
env4_second_0:                 episode reward: -13.0000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 8127.2,                last time consumption/overall running time: 3071.7520s / 105852.7064 s
env0_first_0:                 episode reward: -46.1500,                 loss: 0.0131
env0_second_0:                 episode reward: 46.1500,                 loss: nan
env1_first_0:                 episode reward: -35.9500,                 loss: nan
env1_second_0:                 episode reward: 35.9500,                 loss: nan
env2_first_0:                 episode reward: -31.9000,                 loss: nan
env2_second_0:                 episode reward: 31.9000,                 loss: nan
env3_first_0:                 episode reward: -41.0500,                 loss: nan
env3_second_0:                 episode reward: 41.0500,                 loss: nan
env4_first_0:                 episode reward: -36.8000,                 loss: nan
env4_second_0:                 episode reward: 36.8000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 5134.7,                last time consumption/overall running time: 1951.5071s / 107804.2135 s
env0_first_0:                 episode reward: 14.5500,                 loss: 0.0132
env0_second_0:                 episode reward: -14.5500,                 loss: nan
env1_first_0:                 episode reward: 13.1500,                 loss: nan
env1_second_0:                 episode reward: -13.1500,                 loss: nan
env2_first_0:                 episode reward: 15.9500,                 loss: nan
env2_second_0:                 episode reward: -15.9500,                 loss: nan
env3_first_0:                 episode reward: 9.3000,                 loss: nan
env3_second_0:                 episode reward: -9.3000,                 loss: nan
env4_first_0:                 episode reward: 16.3000,                 loss: nan
env4_second_0:                 episode reward: -16.3000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 5211.95,                last time consumption/overall running time: 1980.1784s / 109784.3919 s
env0_first_0:                 episode reward: 10.2000,                 loss: 0.0128
env0_second_0:                 episode reward: -10.2000,                 loss: nan
env1_first_0:                 episode reward: 15.0500,                 loss: nan
env1_second_0:                 episode reward: -15.0500,                 loss: nan
env2_first_0:                 episode reward: 13.7000,                 loss: nan
env2_second_0:                 episode reward: -13.7000,                 loss: nan
env3_first_0:                 episode reward: 12.9500,                 loss: nan
env3_second_0:                 episode reward: -12.9500,                 loss: nan
env4_first_0:                 episode reward: 9.7500,                 loss: nan
env4_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 4555.45,                last time consumption/overall running time: 1735.3717s / 111519.7636 s
env0_first_0:                 episode reward: 11.8500,                 loss: 0.0125
env0_second_0:                 episode reward: -11.8500,                 loss: nan
env1_first_0:                 episode reward: 16.5000,                 loss: nan
env1_second_0:                 episode reward: -16.5000,                 loss: nan
env2_first_0:                 episode reward: 21.0000,                 loss: nan
env2_second_0:                 episode reward: -21.0000,                 loss: nan
env3_first_0:                 episode reward: 19.6000,                 loss: nan
env3_second_0:                 episode reward: -19.6000,                 loss: nan
env4_first_0:                 episode reward: 18.3500,                 loss: nan
env4_second_0:                 episode reward: -18.3500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 5972.75,                last time consumption/overall running time: 2258.3129s / 113778.0764 s
env0_first_0:                 episode reward: 11.6000,                 loss: 0.0130
env0_second_0:                 episode reward: -11.6000,                 loss: 0.0119
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 4.9000,                 loss: nan
env3_second_0:                 episode reward: -4.9000,                 loss: nan
env4_first_0:                 episode reward: 4.8500,                 loss: nan
env4_second_0:                 episode reward: -4.8500,                 loss: nan
Score delta: 61.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1872_0.
Episode: 1901/10000 (19.0100%),                 avg. length: 5899.6,                last time consumption/overall running time: 2230.0430s / 116008.1194 s
env0_first_0:                 episode reward: -5.8000,                 loss: nan
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0115
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
env2_first_0:                 episode reward: -13.2000,                 loss: nan
env2_second_0:                 episode reward: 13.2000,                 loss: nan
env3_first_0:                 episode reward: -0.9500,                 loss: nan
env3_second_0:                 episode reward: 0.9500,                 loss: nan
env4_first_0:                 episode reward: -4.0000,                 loss: nan
env4_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 4200.75,                last time consumption/overall running time: 1599.7608s / 117607.8803 s
env0_first_0:                 episode reward: -23.9000,                 loss: 0.0138
env0_second_0:                 episode reward: 23.9000,                 loss: 0.0121
env1_first_0:                 episode reward: -25.6500,                 loss: nan
env1_second_0:                 episode reward: 25.6500,                 loss: nan
env2_first_0:                 episode reward: -25.3000,                 loss: nan
env2_second_0:                 episode reward: 25.3000,                 loss: nan
env3_first_0:                 episode reward: -28.0500,                 loss: nan
env3_second_0:                 episode reward: 28.0500,                 loss: nan
env4_first_0:                 episode reward: -25.7500,                 loss: nan
env4_second_0:                 episode reward: 25.7500,                 loss: nan
Score delta: 57.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1903_1.
Episode: 1941/10000 (19.4100%),                 avg. length: 5658.8,                last time consumption/overall running time: 2131.2122s / 119739.0925 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0120
env0_second_0:                 episode reward: 6.4000,                 loss: nan
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
env2_first_0:                 episode reward: -1.9000,                 loss: nan
env2_second_0:                 episode reward: 1.9000,                 loss: nan
env3_first_0:                 episode reward: 0.9500,                 loss: nan
env3_second_0:                 episode reward: -0.9500,                 loss: nan
env4_first_0:                 episode reward: -5.0500,                 loss: nan
env4_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 6470.95,                last time consumption/overall running time: 2423.7455s / 122162.8380 s
env0_first_0:                 episode reward: 6.9500,                 loss: 0.0117
env0_second_0:                 episode reward: -6.9500,                 loss: 0.0125
env1_first_0:                 episode reward: 10.7000,                 loss: nan
env1_second_0:                 episode reward: -10.7000,                 loss: nan
env2_first_0:                 episode reward: 17.4000,                 loss: nan
env2_second_0:                 episode reward: -17.4000,                 loss: nan
env3_first_0:                 episode reward: 9.2500,                 loss: nan
env3_second_0:                 episode reward: -9.2500,                 loss: nan
env4_first_0:                 episode reward: 0.5500,                 loss: nan
env4_second_0:                 episode reward: -0.5500,                 loss: nan
Score delta: 55.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1958_0.
Episode: 1981/10000 (19.8100%),                 avg. length: 4669.9,                last time consumption/overall running time: 1729.7902s / 123892.6282 s
env0_first_0:                 episode reward: -11.3500,                 loss: nan
env0_second_0:                 episode reward: 11.3500,                 loss: 0.0127
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
env2_first_0:                 episode reward: -14.0500,                 loss: nan
env2_second_0:                 episode reward: 14.0500,                 loss: nan
env3_first_0:                 episode reward: -8.4500,                 loss: nan
env3_second_0:                 episode reward: 8.4500,                 loss: nan
env4_first_0:                 episode reward: -6.8000,                 loss: nan
env4_second_0:                 episode reward: 6.8000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 5434.6,                last time consumption/overall running time: 2042.3002s / 125934.9284 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0109
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0136
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
env2_first_0:                 episode reward: 2.1000,                 loss: nan
env2_second_0:                 episode reward: -2.1000,                 loss: nan
env3_first_0:                 episode reward: 4.5500,                 loss: nan
env3_second_0:                 episode reward: -4.5500,                 loss: nan
env4_first_0:                 episode reward: 4.9000,                 loss: nan
env4_second_0:                 episode reward: -4.9000,                 loss: nan
Score delta: 52.2, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/1982_1.
Episode: 2021/10000 (20.2100%),                 avg. length: 6416.3,                last time consumption/overall running time: 2379.2192s / 128314.1476 s
env0_first_0:                 episode reward: 15.7000,                 loss: 0.0102
env0_second_0:                 episode reward: -15.7000,                 loss: 0.0114
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
env2_first_0:                 episode reward: -1.8000,                 loss: nan
env2_second_0:                 episode reward: 1.8000,                 loss: nan
env3_first_0:                 episode reward: 1.1500,                 loss: nan
env3_second_0:                 episode reward: -1.1500,                 loss: nan
env4_first_0:                 episode reward: 4.1500,                 loss: nan
env4_second_0:                 episode reward: -4.1500,                 loss: nan
Score delta: 50.2, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2009_0.
Episode: 2041/10000 (20.4100%),                 avg. length: 4412.3,                last time consumption/overall running time: 1623.9692s / 129938.1168 s
env0_first_0:                 episode reward: -49.2000,                 loss: 0.0316
env0_second_0:                 episode reward: 49.2000,                 loss: 0.0118
env1_first_0:                 episode reward: -48.7000,                 loss: nan
env1_second_0:                 episode reward: 48.7000,                 loss: nan
env2_first_0:                 episode reward: -45.4500,                 loss: nan
env2_second_0:                 episode reward: 45.4500,                 loss: nan
env3_first_0:                 episode reward: -46.8500,                 loss: nan
env3_second_0:                 episode reward: 46.8500,                 loss: nan
env4_first_0:                 episode reward: -48.8000,                 loss: nan
env4_second_0:                 episode reward: 48.8000,                 loss: nan
Score delta: 75.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2030_1.
Episode: 2061/10000 (20.6100%),                 avg. length: 3010.55,                last time consumption/overall running time: 1119.0250s / 131057.1418 s
env0_first_0:                 episode reward: -40.3500,                 loss: 0.0197
env0_second_0:                 episode reward: 40.3500,                 loss: nan
env1_first_0:                 episode reward: -39.8500,                 loss: nan
env1_second_0:                 episode reward: 39.8500,                 loss: nan
env2_first_0:                 episode reward: -39.4500,                 loss: nan
env2_second_0:                 episode reward: 39.4500,                 loss: nan
env3_first_0:                 episode reward: -39.8000,                 loss: nan
env3_second_0:                 episode reward: 39.8000,                 loss: nan
env4_first_0:                 episode reward: -38.8500,                 loss: nan
env4_second_0:                 episode reward: 38.8500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 3524.7,                last time consumption/overall running time: 1298.4207s / 132355.5625 s
env0_first_0:                 episode reward: -34.1500,                 loss: 0.0179
env0_second_0:                 episode reward: 34.1500,                 loss: nan
env1_first_0:                 episode reward: -36.5500,                 loss: nan
env1_second_0:                 episode reward: 36.5500,                 loss: nan
env2_first_0:                 episode reward: -36.9500,                 loss: nan
env2_second_0:                 episode reward: 36.9500,                 loss: nan
env3_first_0:                 episode reward: -33.6000,                 loss: nan
env3_second_0:                 episode reward: 33.6000,                 loss: nan
env4_first_0:                 episode reward: -34.2500,                 loss: nan
env4_second_0:                 episode reward: 34.2500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 5120.55,                last time consumption/overall running time: 1877.7670s / 134233.3295 s
env0_first_0:                 episode reward: -72.6000,                 loss: 0.0185
env0_second_0:                 episode reward: 72.6000,                 loss: nan
env1_first_0:                 episode reward: -73.4500,                 loss: nan
env1_second_0:                 episode reward: 73.4500,                 loss: nan
env2_first_0:                 episode reward: -77.0000,                 loss: nan
env2_second_0:                 episode reward: 77.0000,                 loss: nan
env3_first_0:                 episode reward: -75.6000,                 loss: nan
env3_second_0:                 episode reward: 75.6000,                 loss: nan
env4_first_0:                 episode reward: -74.9000,                 loss: nan
env4_second_0:                 episode reward: 74.9000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 3954.0,                last time consumption/overall running time: 1447.6955s / 135681.0250 s
env0_first_0:                 episode reward: -31.4000,                 loss: 0.0167
env0_second_0:                 episode reward: 31.4000,                 loss: nan
env1_first_0:                 episode reward: -27.8500,                 loss: nan
env1_second_0:                 episode reward: 27.8500,                 loss: nan
env2_first_0:                 episode reward: -25.6000,                 loss: nan
env2_second_0:                 episode reward: 25.6000,                 loss: nan
env3_first_0:                 episode reward: -25.8000,                 loss: nan
env3_second_0:                 episode reward: 25.8000,                 loss: nan
env4_first_0:                 episode reward: -30.1000,                 loss: nan
env4_second_0:                 episode reward: 30.1000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 6575.85,                last time consumption/overall running time: 2458.8048s / 138139.8298 s
env0_first_0:                 episode reward: 1.7000,                 loss: 0.0136
env0_second_0:                 episode reward: -1.7000,                 loss: nan
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
env2_first_0:                 episode reward: -4.0500,                 loss: nan
env2_second_0:                 episode reward: 4.0500,                 loss: nan
env3_first_0:                 episode reward: 0.2000,                 loss: nan
env3_second_0:                 episode reward: -0.2000,                 loss: nan
env4_first_0:                 episode reward: 4.5500,                 loss: nan
env4_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 8178.05,                last time consumption/overall running time: 3021.7553s / 141161.5851 s
env0_first_0:                 episode reward: -26.5500,                 loss: 0.0133
env0_second_0:                 episode reward: 26.5500,                 loss: nan
env1_first_0:                 episode reward: -31.5500,                 loss: nan
env1_second_0:                 episode reward: 31.5500,                 loss: nan
env2_first_0:                 episode reward: -25.9000,                 loss: nan
env2_second_0:                 episode reward: 25.9000,                 loss: nan
env3_first_0:                 episode reward: -21.5500,                 loss: nan
env3_second_0:                 episode reward: 21.5500,                 loss: nan
env4_first_0:                 episode reward: -33.6500,                 loss: nan
env4_second_0:                 episode reward: 33.6500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 7163.55,                last time consumption/overall running time: 2643.1385s / 143804.7236 s
env0_first_0:                 episode reward: 13.9000,                 loss: 0.0122
env0_second_0:                 episode reward: -13.9000,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
env2_first_0:                 episode reward: 7.7000,                 loss: nan
env2_second_0:                 episode reward: -7.7000,                 loss: nan
env3_first_0:                 episode reward: 10.3000,                 loss: nan
env3_second_0:                 episode reward: -10.3000,                 loss: nan
env4_first_0:                 episode reward: 7.1500,                 loss: nan
env4_second_0:                 episode reward: -7.1500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 7306.8,                last time consumption/overall running time: 2689.3935s / 146494.1171 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0112
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
env2_first_0:                 episode reward: -5.3000,                 loss: nan
env2_second_0:                 episode reward: 5.3000,                 loss: nan
env3_first_0:                 episode reward: -4.3000,                 loss: nan
env3_second_0:                 episode reward: 4.3000,                 loss: nan
env4_first_0:                 episode reward: 3.1000,                 loss: nan
env4_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 6051.9,                last time consumption/overall running time: 2222.8770s / 148716.9941 s
env0_first_0:                 episode reward: -14.1000,                 loss: 0.0123
env0_second_0:                 episode reward: 14.1000,                 loss: 0.0117
env1_first_0:                 episode reward: -15.6000,                 loss: nan
env1_second_0:                 episode reward: 15.6000,                 loss: nan
env2_first_0:                 episode reward: -14.5500,                 loss: nan
env2_second_0:                 episode reward: 14.5500,                 loss: nan
env3_first_0:                 episode reward: -17.7500,                 loss: nan
env3_second_0:                 episode reward: 17.7500,                 loss: nan
env4_first_0:                 episode reward: -19.6500,                 loss: nan
env4_second_0:                 episode reward: 19.6500,                 loss: nan
Score delta: 64.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2203_0.
Episode: 2241/10000 (22.4100%),                 avg. length: 7447.2,                last time consumption/overall running time: 2738.9323s / 151455.9264 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0122
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0111
env1_first_0:                 episode reward: -12.3000,                 loss: nan
env1_second_0:                 episode reward: 12.3000,                 loss: nan
env2_first_0:                 episode reward: -3.9000,                 loss: nan
env2_second_0:                 episode reward: 3.9000,                 loss: nan
env3_first_0:                 episode reward: 4.1500,                 loss: nan
env3_second_0:                 episode reward: -4.1500,                 loss: nan
env4_first_0:                 episode reward: 4.5500,                 loss: nan
env4_second_0:                 episode reward: -4.5500,                 loss: nan
Score delta: 83.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2224_1.
Episode: 2261/10000 (22.6100%),                 avg. length: 6025.0,                last time consumption/overall running time: 2220.2062s / 153676.1326 s
env0_first_0:                 episode reward: 9.3500,                 loss: 0.0121
env0_second_0:                 episode reward: -9.3500,                 loss: nan
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
env2_first_0:                 episode reward: 4.2500,                 loss: nan
env2_second_0:                 episode reward: -4.2500,                 loss: nan
env3_first_0:                 episode reward: 3.0000,                 loss: nan
env3_second_0:                 episode reward: -3.0000,                 loss: nan
env4_first_0:                 episode reward: 18.0500,                 loss: nan
env4_second_0:                 episode reward: -18.0500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 5745.2,                last time consumption/overall running time: 2135.5078s / 155811.6404 s
env0_first_0:                 episode reward: 32.6500,                 loss: 0.0109
env0_second_0:                 episode reward: -32.6500,                 loss: 0.0131
env1_first_0:                 episode reward: 29.4500,                 loss: nan
env1_second_0:                 episode reward: -29.4500,                 loss: nan
env2_first_0:                 episode reward: 24.6500,                 loss: nan
env2_second_0:                 episode reward: -24.6500,                 loss: nan
env3_first_0:                 episode reward: 42.9000,                 loss: nan
env3_second_0:                 episode reward: -42.9000,                 loss: nan
env4_first_0:                 episode reward: 31.7000,                 loss: nan
env4_second_0:                 episode reward: -31.7000,                 loss: nan
Score delta: 79.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2277_0.
Episode: 2301/10000 (23.0100%),                 avg. length: 5982.25,                last time consumption/overall running time: 2188.1790s / 157999.8194 s
env0_first_0:                 episode reward: -16.6000,                 loss: 0.0129
env0_second_0:                 episode reward: 16.6000,                 loss: 0.0139
env1_first_0:                 episode reward: -22.0500,                 loss: nan
env1_second_0:                 episode reward: 22.0500,                 loss: nan
env2_first_0:                 episode reward: -19.8500,                 loss: nan
env2_second_0:                 episode reward: 19.8500,                 loss: nan
env3_first_0:                 episode reward: -16.1000,                 loss: nan
env3_second_0:                 episode reward: 16.1000,                 loss: nan
env4_first_0:                 episode reward: -14.5500,                 loss: nan
env4_second_0:                 episode reward: 14.5500,                 loss: nan
Score delta: 50.2, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2298_1.
Episode: 2321/10000 (23.2100%),                 avg. length: 6986.85,                last time consumption/overall running time: 2545.8343s / 160545.6537 s
env0_first_0:                 episode reward: 6.9000,                 loss: 0.0113
env0_second_0:                 episode reward: -6.9000,                 loss: nan
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
env2_first_0:                 episode reward: 6.7500,                 loss: nan
env2_second_0:                 episode reward: -6.7500,                 loss: nan
env3_first_0:                 episode reward: 11.4000,                 loss: nan
env3_second_0:                 episode reward: -11.4000,                 loss: nan
env4_first_0:                 episode reward: 2.1500,                 loss: nan
env4_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 7149.7,                last time consumption/overall running time: 2608.2698s / 163153.9235 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0113
env0_second_0:                 episode reward: -7.6500,                 loss: nan
env1_first_0:                 episode reward: 20.1000,                 loss: nan
env1_second_0:                 episode reward: -20.1000,                 loss: nan
env2_first_0:                 episode reward: 0.2000,                 loss: nan
env2_second_0:                 episode reward: -0.2000,                 loss: nan
env3_first_0:                 episode reward: 18.4000,                 loss: nan
env3_second_0:                 episode reward: -18.4000,                 loss: nan
env4_first_0:                 episode reward: 27.0000,                 loss: nan
env4_second_0:                 episode reward: -27.0000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 4540.65,                last time consumption/overall running time: 1671.5913s / 164825.5148 s
env0_first_0:                 episode reward: 25.3000,                 loss: 0.0121
env0_second_0:                 episode reward: -25.3000,                 loss: 0.0291
env1_first_0:                 episode reward: 20.1000,                 loss: nan
env1_second_0:                 episode reward: -20.1000,                 loss: nan
env2_first_0:                 episode reward: 26.4500,                 loss: nan
env2_second_0:                 episode reward: -26.4500,                 loss: nan
env3_first_0:                 episode reward: 17.2500,                 loss: nan
env3_second_0:                 episode reward: -17.2500,                 loss: nan
env4_first_0:                 episode reward: 22.8000,                 loss: nan
env4_second_0:                 episode reward: -22.8000,                 loss: nan
Score delta: 55.2, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2354_0.
Episode: 2381/10000 (23.8100%),                 avg. length: 5226.9,                last time consumption/overall running time: 1951.3796s / 166776.8944 s
env0_first_0:                 episode reward: 14.6000,                 loss: nan
env0_second_0:                 episode reward: -14.6000,                 loss: 0.0179
env1_first_0:                 episode reward: 20.6000,                 loss: nan
env1_second_0:                 episode reward: -20.6000,                 loss: nan
env2_first_0:                 episode reward: 17.3500,                 loss: nan
env2_second_0:                 episode reward: -17.3500,                 loss: nan
env3_first_0:                 episode reward: 17.9500,                 loss: nan
env3_second_0:                 episode reward: -17.9500,                 loss: nan
env4_first_0:                 episode reward: 17.4000,                 loss: nan
env4_second_0:                 episode reward: -17.4000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 7307.45,                last time consumption/overall running time: 2727.6612s / 169504.5556 s
env0_first_0:                 episode reward: 12.2500,                 loss: 0.0138
env0_second_0:                 episode reward: -12.2500,                 loss: 0.0146
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
env2_first_0:                 episode reward: 9.0000,                 loss: nan
env2_second_0:                 episode reward: -9.0000,                 loss: nan
env3_first_0:                 episode reward: 5.6500,                 loss: nan
env3_second_0:                 episode reward: -5.6500,                 loss: nan
env4_first_0:                 episode reward: 9.9500,                 loss: nan
env4_second_0:                 episode reward: -9.9500,                 loss: nan
Score delta: 52.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2382_1.
Episode: 2421/10000 (24.2100%),                 avg. length: 6086.95,                last time consumption/overall running time: 2261.0122s / 171765.5679 s
env0_first_0:                 episode reward: 10.8500,                 loss: 0.0142
env0_second_0:                 episode reward: -10.8500,                 loss: 0.0153
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
env2_first_0:                 episode reward: 8.1500,                 loss: nan
env2_second_0:                 episode reward: -8.1500,                 loss: nan
env3_first_0:                 episode reward: 5.0500,                 loss: nan
env3_second_0:                 episode reward: -5.0500,                 loss: nan
env4_first_0:                 episode reward: 3.3500,                 loss: nan
env4_second_0:                 episode reward: -3.3500,                 loss: nan
Score delta: 66.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2418_0.
Episode: 2441/10000 (24.4100%),                 avg. length: 6770.35,                last time consumption/overall running time: 2517.3582s / 174282.9261 s
env0_first_0:                 episode reward: 19.9500,                 loss: nan
env0_second_0:                 episode reward: -19.9500,                 loss: 0.0110
env1_first_0:                 episode reward: 15.5500,                 loss: nan
env1_second_0:                 episode reward: -15.5500,                 loss: nan
env2_first_0:                 episode reward: 17.6500,                 loss: nan
env2_second_0:                 episode reward: -17.6500,                 loss: nan
env3_first_0:                 episode reward: 15.9000,                 loss: nan
env3_second_0:                 episode reward: -15.9000,                 loss: nan
env4_first_0:                 episode reward: 25.5500,                 loss: nan
env4_second_0:                 episode reward: -25.5500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 5875.0,                last time consumption/overall running time: 2113.2757s / 176396.2018 s
env0_first_0:                 episode reward: -18.3000,                 loss: nan
env0_second_0:                 episode reward: 18.3000,                 loss: 0.0105
env1_first_0:                 episode reward: -24.6000,                 loss: nan
env1_second_0:                 episode reward: 24.6000,                 loss: nan
env2_first_0:                 episode reward: -17.9500,                 loss: nan
env2_second_0:                 episode reward: 17.9500,                 loss: nan
env3_first_0:                 episode reward: -5.2500,                 loss: nan
env3_second_0:                 episode reward: 5.2500,                 loss: nan
env4_first_0:                 episode reward: 0.3500,                 loss: nan
env4_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 2475.25,                last time consumption/overall running time: 896.7771s / 177292.9789 s
env0_first_0:                 episode reward: -36.6500,                 loss: 0.0314
env0_second_0:                 episode reward: 36.6500,                 loss: 0.0108
env1_first_0:                 episode reward: -39.1000,                 loss: nan
env1_second_0:                 episode reward: 39.1000,                 loss: nan
env2_first_0:                 episode reward: -33.3000,                 loss: nan
env2_second_0:                 episode reward: 33.3000,                 loss: nan
env3_first_0:                 episode reward: -36.3500,                 loss: nan
env3_second_0:                 episode reward: 36.3500,                 loss: nan
env4_first_0:                 episode reward: -36.6000,                 loss: nan
env4_second_0:                 episode reward: 36.6000,                 loss: nan
Score delta: 62.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2462_1.
Episode: 2501/10000 (25.0100%),                 avg. length: 3686.75,                last time consumption/overall running time: 1333.5952s / 178626.5741 s
env0_first_0:                 episode reward: -40.1000,                 loss: 0.0235
env0_second_0:                 episode reward: 40.1000,                 loss: nan
env1_first_0:                 episode reward: -45.2000,                 loss: nan
env1_second_0:                 episode reward: 45.2000,                 loss: nan
env2_first_0:                 episode reward: -42.7000,                 loss: nan
env2_second_0:                 episode reward: 42.7000,                 loss: nan
env3_first_0:                 episode reward: -39.5500,                 loss: nan
env3_second_0:                 episode reward: 39.5500,                 loss: nan
env4_first_0:                 episode reward: -44.9500,                 loss: nan
env4_second_0:                 episode reward: 44.9500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 4786.15,                last time consumption/overall running time: 1722.7809s / 180349.3550 s
env0_first_0:                 episode reward: -51.2000,                 loss: 0.0173
env0_second_0:                 episode reward: 51.2000,                 loss: nan
env1_first_0:                 episode reward: -50.5500,                 loss: nan
env1_second_0:                 episode reward: 50.5500,                 loss: nan
env2_first_0:                 episode reward: -54.6500,                 loss: nan
env2_second_0:                 episode reward: 54.6500,                 loss: nan
env3_first_0:                 episode reward: -52.8000,                 loss: nan
env3_second_0:                 episode reward: 52.8000,                 loss: nan
env4_first_0:                 episode reward: -57.7000,                 loss: nan
env4_second_0:                 episode reward: 57.7000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 5288.1,                last time consumption/overall running time: 1929.2844s / 182278.6394 s
env0_first_0:                 episode reward: -11.3500,                 loss: 0.0124
env0_second_0:                 episode reward: 11.3500,                 loss: nan
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
env2_first_0:                 episode reward: -10.3000,                 loss: nan
env2_second_0:                 episode reward: 10.3000,                 loss: nan
env3_first_0:                 episode reward: -9.8500,                 loss: nan
env3_second_0:                 episode reward: 9.8500,                 loss: nan
env4_first_0:                 episode reward: -9.0500,                 loss: nan
env4_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 8591.25,                last time consumption/overall running time: 3176.8272s / 185455.4666 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0109
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 11.3500,                 loss: nan
env1_second_0:                 episode reward: -11.3500,                 loss: nan
env2_first_0:                 episode reward: 8.5000,                 loss: nan
env2_second_0:                 episode reward: -8.5000,                 loss: nan
env3_first_0:                 episode reward: 5.9500,                 loss: nan
env3_second_0:                 episode reward: -5.9500,                 loss: nan
env4_first_0:                 episode reward: 12.8000,                 loss: nan
env4_second_0:                 episode reward: -12.8000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 6425.15,                last time consumption/overall running time: 2390.0905s / 187845.5572 s
env0_first_0:                 episode reward: 4.8500,                 loss: 0.0111
env0_second_0:                 episode reward: -4.8500,                 loss: 0.0122
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
env2_first_0:                 episode reward: 0.7500,                 loss: nan
env2_second_0:                 episode reward: -0.7500,                 loss: nan
env3_first_0:                 episode reward: 1.9500,                 loss: nan
env3_second_0:                 episode reward: -1.9500,                 loss: nan
env4_first_0:                 episode reward: 2.3000,                 loss: nan
env4_second_0:                 episode reward: -2.3000,                 loss: nan
Score delta: 52.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2575_0.
Episode: 2601/10000 (26.0100%),                 avg. length: 8023.35,                last time consumption/overall running time: 2963.5974s / 190809.1546 s
env0_first_0:                 episode reward: -5.7500,                 loss: nan
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0135
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
env2_first_0:                 episode reward: -12.5000,                 loss: nan
env2_second_0:                 episode reward: 12.5000,                 loss: nan
env3_first_0:                 episode reward: -5.4500,                 loss: nan
env3_second_0:                 episode reward: 5.4500,                 loss: nan
env4_first_0:                 episode reward: -7.2000,                 loss: nan
env4_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 6701.1,                last time consumption/overall running time: 2491.2084s / 193300.3630 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0132
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0137
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
env2_first_0:                 episode reward: -12.6500,                 loss: nan
env2_second_0:                 episode reward: 12.6500,                 loss: nan
env3_first_0:                 episode reward: -14.2000,                 loss: nan
env3_second_0:                 episode reward: 14.2000,                 loss: nan
env4_first_0:                 episode reward: 6.8000,                 loss: nan
env4_second_0:                 episode reward: -6.8000,                 loss: nan
Score delta: 81.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2603_1.
Episode: 2641/10000 (26.4100%),                 avg. length: 5183.25,                last time consumption/overall running time: 1917.3113s / 195217.6743 s
env0_first_0:                 episode reward: 48.8000,                 loss: 0.0121
env0_second_0:                 episode reward: -48.8000,                 loss: 0.0240
env1_first_0:                 episode reward: 36.8000,                 loss: nan
env1_second_0:                 episode reward: -36.8000,                 loss: nan
env2_first_0:                 episode reward: 43.5500,                 loss: nan
env2_second_0:                 episode reward: -43.5500,                 loss: nan
env3_first_0:                 episode reward: 46.4000,                 loss: nan
env3_second_0:                 episode reward: -46.4000,                 loss: nan
env4_first_0:                 episode reward: 43.0000,                 loss: nan
env4_second_0:                 episode reward: -43.0000,                 loss: nan
Score delta: 50.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2625_0.
Episode: 2661/10000 (26.6100%),                 avg. length: 6702.75,                last time consumption/overall running time: 2490.8253s / 197708.4997 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0145
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0164
env1_first_0:                 episode reward: -11.4500,                 loss: nan
env1_second_0:                 episode reward: 11.4500,                 loss: nan
env2_first_0:                 episode reward: -13.8500,                 loss: nan
env2_second_0:                 episode reward: 13.8500,                 loss: nan
env3_first_0:                 episode reward: -7.4500,                 loss: nan
env3_second_0:                 episode reward: 7.4500,                 loss: nan
env4_first_0:                 episode reward: -16.0000,                 loss: nan
env4_second_0:                 episode reward: 16.0000,                 loss: nan
Score delta: 55.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2650_1.
Episode: 2681/10000 (26.8100%),                 avg. length: 6180.15,                last time consumption/overall running time: 2269.6815s / 199978.1812 s
env0_first_0:                 episode reward: 6.0000,                 loss: 0.0116
env0_second_0:                 episode reward: -6.0000,                 loss: 0.0120
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
env2_first_0:                 episode reward: 2.1500,                 loss: nan
env2_second_0:                 episode reward: -2.1500,                 loss: nan
env3_first_0:                 episode reward: -2.4000,                 loss: nan
env3_second_0:                 episode reward: 2.4000,                 loss: nan
env4_first_0:                 episode reward: -2.4500,                 loss: nan
env4_second_0:                 episode reward: 2.4500,                 loss: nan
Score delta: 56.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2678_0.
Episode: 2701/10000 (27.0100%),                 avg. length: 5967.75,                last time consumption/overall running time: 2186.3421s / 202164.5233 s
env0_first_0:                 episode reward: -28.8000,                 loss: 0.0208
env0_second_0:                 episode reward: 28.8000,                 loss: 0.0110
env1_first_0:                 episode reward: -22.4500,                 loss: nan
env1_second_0:                 episode reward: 22.4500,                 loss: nan
env2_first_0:                 episode reward: -30.1000,                 loss: nan
env2_second_0:                 episode reward: 30.1000,                 loss: nan
env3_first_0:                 episode reward: -24.1500,                 loss: nan
env3_second_0:                 episode reward: 24.1500,                 loss: nan
env4_first_0:                 episode reward: -24.9000,                 loss: nan
env4_second_0:                 episode reward: 24.9000,                 loss: nan
Score delta: 78.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2699_1.
Episode: 2721/10000 (27.2100%),                 avg. length: 4875.55,                last time consumption/overall running time: 1808.5670s / 203973.0903 s
env0_first_0:                 episode reward: -15.7500,                 loss: 0.0121
env0_second_0:                 episode reward: 15.7500,                 loss: nan
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
env2_first_0:                 episode reward: -17.1000,                 loss: nan
env2_second_0:                 episode reward: 17.1000,                 loss: nan
env3_first_0:                 episode reward: -14.5500,                 loss: nan
env3_second_0:                 episode reward: 14.5500,                 loss: nan
env4_first_0:                 episode reward: -14.1000,                 loss: nan
env4_second_0:                 episode reward: 14.1000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 7249.6,                last time consumption/overall running time: 2705.2432s / 206678.3335 s
env0_first_0:                 episode reward: 7.5500,                 loss: 0.0112
env0_second_0:                 episode reward: -7.5500,                 loss: nan
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
env2_first_0:                 episode reward: 14.6000,                 loss: nan
env2_second_0:                 episode reward: -14.6000,                 loss: nan
env3_first_0:                 episode reward: 10.7500,                 loss: nan
env3_second_0:                 episode reward: -10.7500,                 loss: nan
env4_first_0:                 episode reward: 10.6000,                 loss: nan
env4_second_0:                 episode reward: -10.6000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 6606.65,                last time consumption/overall running time: 2463.4707s / 209141.8041 s
env0_first_0:                 episode reward: 15.9500,                 loss: 0.0112
env0_second_0:                 episode reward: -15.9500,                 loss: 0.0133
env1_first_0:                 episode reward: 12.7000,                 loss: nan
env1_second_0:                 episode reward: -12.7000,                 loss: nan
env2_first_0:                 episode reward: 19.3000,                 loss: nan
env2_second_0:                 episode reward: -19.3000,                 loss: nan
env3_first_0:                 episode reward: 14.3500,                 loss: nan
env3_second_0:                 episode reward: -14.3500,                 loss: nan
env4_first_0:                 episode reward: 25.5000,                 loss: nan
env4_second_0:                 episode reward: -25.5000,                 loss: nan
Score delta: 51.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2749_0.
Episode: 2781/10000 (27.8100%),                 avg. length: 6718.4,                last time consumption/overall running time: 2512.9655s / 211654.7697 s
env0_first_0:                 episode reward: -33.2500,                 loss: 0.0127
env0_second_0:                 episode reward: 33.2500,                 loss: 0.0135
env1_first_0:                 episode reward: -48.8000,                 loss: nan
env1_second_0:                 episode reward: 48.8000,                 loss: nan
env2_first_0:                 episode reward: -45.0000,                 loss: nan
env2_second_0:                 episode reward: 45.0000,                 loss: nan
env3_first_0:                 episode reward: -30.6500,                 loss: nan
env3_second_0:                 episode reward: 30.6500,                 loss: nan
env4_first_0:                 episode reward: -44.1000,                 loss: nan
env4_second_0:                 episode reward: 44.1000,                 loss: nan
Score delta: 53.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2770_1.
Episode: 2801/10000 (28.0100%),                 avg. length: 7913.55,                last time consumption/overall running time: 2930.9386s / 214585.7082 s
env0_first_0:                 episode reward: -90.6500,                 loss: 0.0166
env0_second_0:                 episode reward: 90.6500,                 loss: nan
env1_first_0:                 episode reward: -81.9500,                 loss: nan
env1_second_0:                 episode reward: 81.9500,                 loss: nan
env2_first_0:                 episode reward: -89.6500,                 loss: nan
env2_second_0:                 episode reward: 89.6500,                 loss: nan
env3_first_0:                 episode reward: -85.7000,                 loss: nan
env3_second_0:                 episode reward: 85.7000,                 loss: nan
env4_first_0:                 episode reward: -87.9500,                 loss: nan
env4_second_0:                 episode reward: 87.9500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 7443.6,                last time consumption/overall running time: 2705.8305s / 217291.5388 s
env0_first_0:                 episode reward: 7.9500,                 loss: 0.0135
env0_second_0:                 episode reward: -7.9500,                 loss: nan
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
env2_first_0:                 episode reward: 13.1500,                 loss: nan
env2_second_0:                 episode reward: -13.1500,                 loss: nan
env3_first_0:                 episode reward: 11.3000,                 loss: nan
env3_second_0:                 episode reward: -11.3000,                 loss: nan
env4_first_0:                 episode reward: 7.8500,                 loss: nan
env4_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 7654.75,                last time consumption/overall running time: 2789.9618s / 220081.5006 s
env0_first_0:                 episode reward: 9.1500,                 loss: 0.0136
env0_second_0:                 episode reward: -9.1500,                 loss: nan
env1_first_0:                 episode reward: 15.0000,                 loss: nan
env1_second_0:                 episode reward: -15.0000,                 loss: nan
env2_first_0:                 episode reward: 9.2000,                 loss: nan
env2_second_0:                 episode reward: -9.2000,                 loss: nan
env3_first_0:                 episode reward: -2.6000,                 loss: nan
env3_second_0:                 episode reward: 2.6000,                 loss: nan
env4_first_0:                 episode reward: -1.0000,                 loss: nan
env4_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 6613.45,                last time consumption/overall running time: 2407.0073s / 222488.5080 s
env0_first_0:                 episode reward: 13.4500,                 loss: 0.0133
env0_second_0:                 episode reward: -13.4500,                 loss: 0.0141
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
env2_first_0:                 episode reward: 6.3500,                 loss: nan
env2_second_0:                 episode reward: -6.3500,                 loss: nan
env3_first_0:                 episode reward: 8.2500,                 loss: nan
env3_second_0:                 episode reward: -8.2500,                 loss: nan
env4_first_0:                 episode reward: 9.3000,                 loss: nan
env4_second_0:                 episode reward: -9.3000,                 loss: nan
Score delta: 55.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2844_0.
Episode: 2881/10000 (28.8100%),                 avg. length: 7449.2,                last time consumption/overall running time: 2721.6459s / 225210.1539 s
env0_first_0:                 episode reward: 1.4000,                 loss: nan
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0132
env1_first_0:                 episode reward: -13.2000,                 loss: nan
env1_second_0:                 episode reward: 13.2000,                 loss: nan
env2_first_0:                 episode reward: -14.3000,                 loss: nan
env2_second_0:                 episode reward: 14.3000,                 loss: nan
env3_first_0:                 episode reward: -8.4000,                 loss: nan
env3_second_0:                 episode reward: 8.4000,                 loss: nan
env4_first_0:                 episode reward: -16.5500,                 loss: nan
env4_second_0:                 episode reward: 16.5500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 4703.2,                last time consumption/overall running time: 1763.0321s / 226973.1859 s
env0_first_0:                 episode reward: -11.8000,                 loss: 0.0149
env0_second_0:                 episode reward: 11.8000,                 loss: 0.0123
env1_first_0:                 episode reward: -17.1500,                 loss: nan
env1_second_0:                 episode reward: 17.1500,                 loss: nan
env2_first_0:                 episode reward: -18.6500,                 loss: nan
env2_second_0:                 episode reward: 18.6500,                 loss: nan
env3_first_0:                 episode reward: -16.3000,                 loss: nan
env3_second_0:                 episode reward: 16.3000,                 loss: nan
env4_first_0:                 episode reward: -13.8000,                 loss: nan
env4_second_0:                 episode reward: 13.8000,                 loss: nan
Score delta: 52.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/2887_1.
Episode: 2921/10000 (29.2100%),                 avg. length: 5172.3,                last time consumption/overall running time: 1936.2611s / 228909.4470 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0118
env0_second_0:                 episode reward: 2.5500,                 loss: nan
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
env2_first_0:                 episode reward: -0.6000,                 loss: nan
env2_second_0:                 episode reward: 0.6000,                 loss: nan
env3_first_0:                 episode reward: -1.1500,                 loss: nan
env3_second_0:                 episode reward: 1.1500,                 loss: nan
env4_first_0:                 episode reward: 2.7000,                 loss: nan
env4_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 6711.55,                last time consumption/overall running time: 2509.0144s / 231418.4615 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0116
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
env2_first_0:                 episode reward: 2.1500,                 loss: nan
env2_second_0:                 episode reward: -2.1500,                 loss: nan
env3_first_0:                 episode reward: 0.6500,                 loss: nan
env3_second_0:                 episode reward: -0.6500,                 loss: nan
env4_first_0:                 episode reward: -6.3500,                 loss: nan
env4_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 7620.7,                last time consumption/overall running time: 2845.4016s / 234263.8631 s
env0_first_0:                 episode reward: 6.2000,                 loss: 0.0110
env0_second_0:                 episode reward: -6.2000,                 loss: nan
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
env2_first_0:                 episode reward: 5.4000,                 loss: nan
env2_second_0:                 episode reward: -5.4000,                 loss: nan
env3_first_0:                 episode reward: 10.1000,                 loss: nan
env3_second_0:                 episode reward: -10.1000,                 loss: nan
env4_first_0:                 episode reward: -0.2000,                 loss: nan
env4_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 7455.5,                last time consumption/overall running time: 2780.0105s / 237043.8735 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0102
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 9.6000,                 loss: nan
env1_second_0:                 episode reward: -9.6000,                 loss: nan
env2_first_0:                 episode reward: 5.2500,                 loss: nan
env2_second_0:                 episode reward: -5.2500,                 loss: nan
env3_first_0:                 episode reward: 11.4500,                 loss: nan
env3_second_0:                 episode reward: -11.4500,                 loss: nan
env4_first_0:                 episode reward: 5.3500,                 loss: nan
env4_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 6245.3,                last time consumption/overall running time: 2295.0212s / 239338.8948 s
env0_first_0:                 episode reward: 11.3500,                 loss: 0.0108
env0_second_0:                 episode reward: -11.3500,                 loss: nan
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
env2_first_0:                 episode reward: 7.3500,                 loss: nan
env2_second_0:                 episode reward: -7.3500,                 loss: nan
env3_first_0:                 episode reward: 6.7500,                 loss: nan
env3_second_0:                 episode reward: -6.7500,                 loss: nan
env4_first_0:                 episode reward: 7.6500,                 loss: nan
env4_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 5952.9,                last time consumption/overall running time: 2160.5313s / 241499.4261 s
env0_first_0:                 episode reward: 7.9500,                 loss: 0.0110
env0_second_0:                 episode reward: -7.9500,                 loss: nan
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
env2_first_0:                 episode reward: 10.5500,                 loss: nan
env2_second_0:                 episode reward: -10.5500,                 loss: nan
env3_first_0:                 episode reward: 10.2500,                 loss: nan
env3_second_0:                 episode reward: -10.2500,                 loss: nan
env4_first_0:                 episode reward: 9.5500,                 loss: nan
env4_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 6550.1,                last time consumption/overall running time: 2387.3570s / 243886.7830 s
env0_first_0:                 episode reward: 5.3500,                 loss: 0.0094
env0_second_0:                 episode reward: -5.3500,                 loss: nan
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
env2_first_0:                 episode reward: 8.0000,                 loss: nan
env2_second_0:                 episode reward: -8.0000,                 loss: nan
env3_first_0:                 episode reward: 8.4000,                 loss: nan
env3_second_0:                 episode reward: -8.4000,                 loss: nan
env4_first_0:                 episode reward: 9.4000,                 loss: nan
env4_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 4707.05,                last time consumption/overall running time: 1710.9554s / 245597.7384 s
env0_first_0:                 episode reward: 11.9500,                 loss: 0.0096
env0_second_0:                 episode reward: -11.9500,                 loss: nan
env1_first_0:                 episode reward: 19.3500,                 loss: nan
env1_second_0:                 episode reward: -19.3500,                 loss: nan
env2_first_0:                 episode reward: 12.4000,                 loss: nan
env2_second_0:                 episode reward: -12.4000,                 loss: nan
env3_first_0:                 episode reward: 22.5000,                 loss: nan
env3_second_0:                 episode reward: -22.5000,                 loss: nan
env4_first_0:                 episode reward: 15.3500,                 loss: nan
env4_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 5407.65,                last time consumption/overall running time: 1967.3585s / 247565.0969 s
env0_first_0:                 episode reward: 10.5000,                 loss: 0.0100
env0_second_0:                 episode reward: -10.5000,                 loss: 0.0134
env1_first_0:                 episode reward: 16.6000,                 loss: nan
env1_second_0:                 episode reward: -16.6000,                 loss: nan
env2_first_0:                 episode reward: 12.7500,                 loss: nan
env2_second_0:                 episode reward: -12.7500,                 loss: nan
env3_first_0:                 episode reward: 9.9000,                 loss: nan
env3_second_0:                 episode reward: -9.9000,                 loss: nan
env4_first_0:                 episode reward: 10.9500,                 loss: nan
env4_second_0:                 episode reward: -10.9500,                 loss: nan
Score delta: 52.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3064_0.
Episode: 3101/10000 (31.0100%),                 avg. length: 7620.0,                last time consumption/overall running time: 2780.9398s / 250346.0368 s
env0_first_0:                 episode reward: -0.2500,                 loss: nan
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0149
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
env2_first_0:                 episode reward: -4.1000,                 loss: nan
env2_second_0:                 episode reward: 4.1000,                 loss: nan
env3_first_0:                 episode reward: 2.0000,                 loss: nan
env3_second_0:                 episode reward: -2.0000,                 loss: nan
env4_first_0:                 episode reward: -2.1500,                 loss: nan
env4_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 6479.55,                last time consumption/overall running time: 2454.3854s / 252800.4221 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.0117
env0_second_0:                 episode reward: 10.9000,                 loss: 0.0139
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
env2_first_0:                 episode reward: -22.2000,                 loss: nan
env2_second_0:                 episode reward: 22.2000,                 loss: nan
env3_first_0:                 episode reward: -10.9500,                 loss: nan
env3_second_0:                 episode reward: 10.9500,                 loss: nan
env4_first_0:                 episode reward: -16.6000,                 loss: nan
env4_second_0:                 episode reward: 16.6000,                 loss: nan
Score delta: 55.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3108_1.
Episode: 3141/10000 (31.4100%),                 avg. length: 7040.5,                last time consumption/overall running time: 3690.2786s / 256490.7007 s
env0_first_0:                 episode reward: 12.8000,                 loss: 0.0113
env0_second_0:                 episode reward: -12.8000,                 loss: nan
env1_first_0:                 episode reward: 11.9500,                 loss: nan
env1_second_0:                 episode reward: -11.9500,                 loss: nan
env2_first_0:                 episode reward: 12.0500,                 loss: nan
env2_second_0:                 episode reward: -12.0500,                 loss: nan
env3_first_0:                 episode reward: 14.8500,                 loss: nan
env3_second_0:                 episode reward: -14.8500,                 loss: nan
env4_first_0:                 episode reward: 20.6000,                 loss: nan
env4_second_0:                 episode reward: -20.6000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 5820.55,                last time consumption/overall running time: 3167.5738s / 259658.2745 s
env0_first_0:                 episode reward: 17.9000,                 loss: 0.0113
env0_second_0:                 episode reward: -17.9000,                 loss: 0.0122
env1_first_0:                 episode reward: 15.5500,                 loss: nan
env1_second_0:                 episode reward: -15.5500,                 loss: nan
env2_first_0:                 episode reward: 17.0000,                 loss: nan
env2_second_0:                 episode reward: -17.0000,                 loss: nan
env3_first_0:                 episode reward: 19.5000,                 loss: nan
env3_second_0:                 episode reward: -19.5000,                 loss: nan
env4_first_0:                 episode reward: 21.1500,                 loss: nan
env4_second_0:                 episode reward: -21.1500,                 loss: nan
Score delta: 77.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3154_0.
Episode: 3181/10000 (31.8100%),                 avg. length: 7748.3,                last time consumption/overall running time: 4070.7695s / 263729.0440 s
env0_first_0:                 episode reward: 24.5500,                 loss: nan
env0_second_0:                 episode reward: -24.5500,                 loss: 0.0147
env1_first_0:                 episode reward: 18.5500,                 loss: nan
env1_second_0:                 episode reward: -18.5500,                 loss: nan
env2_first_0:                 episode reward: 6.7000,                 loss: nan
env2_second_0:                 episode reward: -6.7000,                 loss: nan
env3_first_0:                 episode reward: 23.5500,                 loss: nan
env3_second_0:                 episode reward: -23.5500,                 loss: nan
env4_first_0:                 episode reward: 20.6000,                 loss: nan
env4_second_0:                 episode reward: -20.6000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 3658.35,                last time consumption/overall running time: 1889.7367s / 265618.7807 s
env0_first_0:                 episode reward: -38.5500,                 loss: 0.0335
env0_second_0:                 episode reward: 38.5500,                 loss: 0.0157
env1_first_0:                 episode reward: -39.9500,                 loss: nan
env1_second_0:                 episode reward: 39.9500,                 loss: nan
env2_first_0:                 episode reward: -39.2000,                 loss: nan
env2_second_0:                 episode reward: 39.2000,                 loss: nan
env3_first_0:                 episode reward: -41.0500,                 loss: nan
env3_second_0:                 episode reward: 41.0500,                 loss: nan
env4_first_0:                 episode reward: -47.1000,                 loss: nan
env4_second_0:                 episode reward: 47.1000,                 loss: nan
Score delta: 72.2, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3188_1.
Episode: 3221/10000 (32.2100%),                 avg. length: 3748.7,                last time consumption/overall running time: 1927.5500s / 267546.3307 s
env0_first_0:                 episode reward: -54.6000,                 loss: 0.0223
env0_second_0:                 episode reward: 54.6000,                 loss: nan
env1_first_0:                 episode reward: -52.3500,                 loss: nan
env1_second_0:                 episode reward: 52.3500,                 loss: nan
env2_first_0:                 episode reward: -51.3500,                 loss: nan
env2_second_0:                 episode reward: 51.3500,                 loss: nan
env3_first_0:                 episode reward: -51.8000,                 loss: nan
env3_second_0:                 episode reward: 51.8000,                 loss: nan
env4_first_0:                 episode reward: -50.8000,                 loss: nan
env4_second_0:                 episode reward: 50.8000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 4899.7,                last time consumption/overall running time: 2524.3174s / 270070.6481 s
env0_first_0:                 episode reward: -43.2500,                 loss: 0.0181
env0_second_0:                 episode reward: 43.2500,                 loss: nan
env1_first_0:                 episode reward: -46.5000,                 loss: nan
env1_second_0:                 episode reward: 46.5000,                 loss: nan
env2_first_0:                 episode reward: -45.1500,                 loss: nan
env2_second_0:                 episode reward: 45.1500,                 loss: nan
env3_first_0:                 episode reward: -47.5500,                 loss: nan
env3_second_0:                 episode reward: 47.5500,                 loss: nan
env4_first_0:                 episode reward: -45.5500,                 loss: nan
env4_second_0:                 episode reward: 45.5500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 6026.0,                last time consumption/overall running time: 2992.2130s / 273062.8612 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0125
env0_second_0:                 episode reward: -2.3500,                 loss: nan
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
env2_first_0:                 episode reward: -11.2500,                 loss: nan
env2_second_0:                 episode reward: 11.2500,                 loss: nan
env3_first_0:                 episode reward: -11.0500,                 loss: nan
env3_second_0:                 episode reward: 11.0500,                 loss: nan
env4_first_0:                 episode reward: 3.7000,                 loss: nan
env4_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 7666.8,                last time consumption/overall running time: 3802.1687s / 276865.0298 s
env0_first_0:                 episode reward: 8.2500,                 loss: 0.0102
env0_second_0:                 episode reward: -8.2500,                 loss: nan
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
env2_first_0:                 episode reward: 2.5500,                 loss: nan
env2_second_0:                 episode reward: -2.5500,                 loss: nan
env3_first_0:                 episode reward: 12.0500,                 loss: nan
env3_second_0:                 episode reward: -12.0500,                 loss: nan
env4_first_0:                 episode reward: 13.0500,                 loss: nan
env4_second_0:                 episode reward: -13.0500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 6775.95,                last time consumption/overall running time: 3368.5820s / 280233.6118 s
env0_first_0:                 episode reward: 7.3500,                 loss: 0.0110
env0_second_0:                 episode reward: -7.3500,                 loss: nan
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
env2_first_0:                 episode reward: 6.7500,                 loss: nan
env2_second_0:                 episode reward: -6.7500,                 loss: nan
env3_first_0:                 episode reward: 22.0500,                 loss: nan
env3_second_0:                 episode reward: -22.0500,                 loss: nan
env4_first_0:                 episode reward: 12.2000,                 loss: nan
env4_second_0:                 episode reward: -12.2000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 7477.35,                last time consumption/overall running time: 3529.3253s / 283762.9371 s
env0_first_0:                 episode reward: 24.6000,                 loss: 0.0119
env0_second_0:                 episode reward: -24.6000,                 loss: 0.0144
env1_first_0:                 episode reward: 30.2000,                 loss: nan
env1_second_0:                 episode reward: -30.2000,                 loss: nan
env2_first_0:                 episode reward: 20.1500,                 loss: nan
env2_second_0:                 episode reward: -20.1500,                 loss: nan
env3_first_0:                 episode reward: 6.3000,                 loss: nan
env3_second_0:                 episode reward: -6.3000,                 loss: nan
env4_first_0:                 episode reward: 28.3500,                 loss: nan
env4_second_0:                 episode reward: -28.3500,                 loss: nan
Score delta: 60.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3305_0.
Episode: 3341/10000 (33.4100%),                 avg. length: 5744.7,                last time consumption/overall running time: 2638.5885s / 286401.5256 s
env0_first_0:                 episode reward: -15.7500,                 loss: 0.0120
env0_second_0:                 episode reward: 15.7500,                 loss: 0.0168
env1_first_0:                 episode reward: -14.9500,                 loss: nan
env1_second_0:                 episode reward: 14.9500,                 loss: nan
env2_first_0:                 episode reward: -10.6500,                 loss: nan
env2_second_0:                 episode reward: 10.6500,                 loss: nan
env3_first_0:                 episode reward: -14.0500,                 loss: nan
env3_second_0:                 episode reward: 14.0500,                 loss: nan
env4_first_0:                 episode reward: -8.3000,                 loss: nan
env4_second_0:                 episode reward: 8.3000,                 loss: nan
Score delta: 66.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3336_1.
Episode: 3361/10000 (33.6100%),                 avg. length: 7019.3,                last time consumption/overall running time: 3233.5306s / 289635.0561 s
env0_first_0:                 episode reward: 16.9000,                 loss: 0.0133
env0_second_0:                 episode reward: -16.9000,                 loss: nan
env1_first_0:                 episode reward: 20.4000,                 loss: nan
env1_second_0:                 episode reward: -20.4000,                 loss: nan
env2_first_0:                 episode reward: 1.6500,                 loss: nan
env2_second_0:                 episode reward: -1.6500,                 loss: nan
env3_first_0:                 episode reward: 12.9500,                 loss: nan
env3_second_0:                 episode reward: -12.9500,                 loss: nan
env4_first_0:                 episode reward: 9.6500,                 loss: nan
env4_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 7619.0,                last time consumption/overall running time: 3508.9484s / 293144.0045 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0140
env0_second_0:                 episode reward: -1.4000,                 loss: nan
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
env2_first_0:                 episode reward: 6.6500,                 loss: nan
env2_second_0:                 episode reward: -6.6500,                 loss: nan
env3_first_0:                 episode reward: 0.1500,                 loss: nan
env3_second_0:                 episode reward: -0.1500,                 loss: nan
env4_first_0:                 episode reward: 5.2000,                 loss: nan
env4_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 6911.05,                last time consumption/overall running time: 3149.7618s / 296293.7663 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0140
env0_second_0:                 episode reward: 2.2000,                 loss: nan
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
env2_first_0:                 episode reward: -2.7000,                 loss: nan
env2_second_0:                 episode reward: 2.7000,                 loss: nan
env3_first_0:                 episode reward: 2.5500,                 loss: nan
env3_second_0:                 episode reward: -2.5500,                 loss: nan
env4_first_0:                 episode reward: -8.4500,                 loss: nan
env4_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 7258.7,                last time consumption/overall running time: 3257.4751s / 299551.2414 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0153
env0_second_0:                 episode reward: -1.0500,                 loss: nan
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
env2_first_0:                 episode reward: -4.6500,                 loss: nan
env2_second_0:                 episode reward: 4.6500,                 loss: nan
env3_first_0:                 episode reward: 6.3500,                 loss: nan
env3_second_0:                 episode reward: -6.3500,                 loss: nan
env4_first_0:                 episode reward: 3.2500,                 loss: nan
env4_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 7247.5,                last time consumption/overall running time: 3177.7433s / 302728.9847 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0141
env0_second_0:                 episode reward: 13.5500,                 loss: nan
env1_first_0:                 episode reward: -12.4000,                 loss: nan
env1_second_0:                 episode reward: 12.4000,                 loss: nan
env2_first_0:                 episode reward: -6.4000,                 loss: nan
env2_second_0:                 episode reward: 6.4000,                 loss: nan
env3_first_0:                 episode reward: -4.9500,                 loss: nan
env3_second_0:                 episode reward: 4.9500,                 loss: nan
env4_first_0:                 episode reward: -3.3000,                 loss: nan
env4_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 5627.75,                last time consumption/overall running time: 2303.5460s / 305032.5308 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0121
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: 21.6500,                 loss: nan
env1_second_0:                 episode reward: -21.6500,                 loss: nan
env2_first_0:                 episode reward: 8.5500,                 loss: nan
env2_second_0:                 episode reward: -8.5500,                 loss: nan
env3_first_0:                 episode reward: 1.4000,                 loss: nan
env3_second_0:                 episode reward: -1.4000,                 loss: nan
env4_first_0:                 episode reward: 23.1500,                 loss: nan
env4_second_0:                 episode reward: -23.1500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 7197.55,                last time consumption/overall running time: 2902.5735s / 307935.1043 s
env0_first_0:                 episode reward: -29.5000,                 loss: 0.0132
env0_second_0:                 episode reward: 29.5000,                 loss: nan
env1_first_0:                 episode reward: -15.7500,                 loss: nan
env1_second_0:                 episode reward: 15.7500,                 loss: nan
env2_first_0:                 episode reward: -24.9000,                 loss: nan
env2_second_0:                 episode reward: 24.9000,                 loss: nan
env3_first_0:                 episode reward: -23.0500,                 loss: nan
env3_second_0:                 episode reward: 23.0500,                 loss: nan
env4_first_0:                 episode reward: -8.9500,                 loss: nan
env4_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 6552.55,                last time consumption/overall running time: 2564.7663s / 310499.8706 s
env0_first_0:                 episode reward: -18.3500,                 loss: 0.0156
env0_second_0:                 episode reward: 18.3500,                 loss: nan
env1_first_0:                 episode reward: -11.7000,                 loss: nan
env1_second_0:                 episode reward: 11.7000,                 loss: nan
env2_first_0:                 episode reward: -15.9500,                 loss: nan
env2_second_0:                 episode reward: 15.9500,                 loss: nan
env3_first_0:                 episode reward: -10.4000,                 loss: nan
env3_second_0:                 episode reward: 10.4000,                 loss: nan
env4_first_0:                 episode reward: -15.4500,                 loss: nan
env4_second_0:                 episode reward: 15.4500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 6563.55,                last time consumption/overall running time: 2539.4477s / 313039.3183 s
env0_first_0:                 episode reward: 14.8500,                 loss: 0.0154
env0_second_0:                 episode reward: -14.8500,                 loss: 0.0151
env1_first_0:                 episode reward: 19.9500,                 loss: nan
env1_second_0:                 episode reward: -19.9500,                 loss: nan
env2_first_0:                 episode reward: 26.4000,                 loss: nan
env2_second_0:                 episode reward: -26.4000,                 loss: nan
env3_first_0:                 episode reward: 31.6000,                 loss: nan
env3_second_0:                 episode reward: -31.6000,                 loss: nan
env4_first_0:                 episode reward: 17.3000,                 loss: nan
env4_second_0:                 episode reward: -17.3000,                 loss: nan
Score delta: 60.2, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3515_0.
Episode: 3541/10000 (35.4100%),                 avg. length: 7489.0,                last time consumption/overall running time: 2871.9791s / 315911.2974 s
env0_first_0:                 episode reward: 41.4500,                 loss: nan
env0_second_0:                 episode reward: -41.4500,                 loss: 0.0145
env1_first_0:                 episode reward: 44.2500,                 loss: nan
env1_second_0:                 episode reward: -44.2500,                 loss: nan
env2_first_0:                 episode reward: 36.3500,                 loss: nan
env2_second_0:                 episode reward: -36.3500,                 loss: nan
env3_first_0:                 episode reward: 45.3500,                 loss: nan
env3_second_0:                 episode reward: -45.3500,                 loss: nan
env4_first_0:                 episode reward: 37.3500,                 loss: nan
env4_second_0:                 episode reward: -37.3500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 7187.95,                last time consumption/overall running time: 2757.5912s / 318668.8886 s
env0_first_0:                 episode reward: 33.1500,                 loss: nan
env0_second_0:                 episode reward: -33.1500,                 loss: 0.0136
env1_first_0:                 episode reward: 26.9500,                 loss: nan
env1_second_0:                 episode reward: -26.9500,                 loss: nan
env2_first_0:                 episode reward: 26.3000,                 loss: nan
env2_second_0:                 episode reward: -26.3000,                 loss: nan
env3_first_0:                 episode reward: 30.2000,                 loss: nan
env3_second_0:                 episode reward: -30.2000,                 loss: nan
env4_first_0:                 episode reward: 28.1500,                 loss: nan
env4_second_0:                 episode reward: -28.1500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 6949.55,                last time consumption/overall running time: 2574.7396s / 321243.6283 s
env0_first_0:                 episode reward: 4.7000,                 loss: nan
env0_second_0:                 episode reward: -4.7000,                 loss: 0.0118
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
env2_first_0:                 episode reward: -1.6000,                 loss: nan
env2_second_0:                 episode reward: 1.6000,                 loss: nan
env3_first_0:                 episode reward: -11.9000,                 loss: nan
env3_second_0:                 episode reward: 11.9000,                 loss: nan
env4_first_0:                 episode reward: 10.3500,                 loss: nan
env4_second_0:                 episode reward: -10.3500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 7618.9,                last time consumption/overall running time: 2685.7162s / 323929.3445 s
env0_first_0:                 episode reward: 9.8000,                 loss: nan
env0_second_0:                 episode reward: -9.8000,                 loss: 0.0134
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
env2_first_0:                 episode reward: 7.6000,                 loss: nan
env2_second_0:                 episode reward: -7.6000,                 loss: nan
env3_first_0:                 episode reward: 11.9500,                 loss: nan
env3_second_0:                 episode reward: -11.9500,                 loss: nan
env4_first_0:                 episode reward: 6.7500,                 loss: nan
env4_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 5403.15,                last time consumption/overall running time: 1877.9840s / 325807.3285 s
env0_first_0:                 episode reward: -21.0500,                 loss: 0.0117
env0_second_0:                 episode reward: 21.0500,                 loss: 0.0160
env1_first_0:                 episode reward: -15.8500,                 loss: nan
env1_second_0:                 episode reward: 15.8500,                 loss: nan
env2_first_0:                 episode reward: -10.9500,                 loss: nan
env2_second_0:                 episode reward: 10.9500,                 loss: nan
env3_first_0:                 episode reward: -3.8000,                 loss: nan
env3_second_0:                 episode reward: 3.8000,                 loss: nan
env4_first_0:                 episode reward: -14.3000,                 loss: nan
env4_second_0:                 episode reward: 14.3000,                 loss: nan
Score delta: 52.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3608_1.
Episode: 3641/10000 (36.4100%),                 avg. length: 8098.15,                last time consumption/overall running time: 2856.0214s / 328663.3499 s
env0_first_0:                 episode reward: -27.0000,                 loss: 0.0104
env0_second_0:                 episode reward: 27.0000,                 loss: nan
env1_first_0:                 episode reward: -11.7000,                 loss: nan
env1_second_0:                 episode reward: 11.7000,                 loss: nan
env2_first_0:                 episode reward: 3.8000,                 loss: nan
env2_second_0:                 episode reward: -3.8000,                 loss: nan
env3_first_0:                 episode reward: -7.5000,                 loss: nan
env3_second_0:                 episode reward: 7.5000,                 loss: nan
env4_first_0:                 episode reward: -8.9000,                 loss: nan
env4_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 7996.65,                last time consumption/overall running time: 2793.4119s / 331456.7618 s
env0_first_0:                 episode reward: -45.8500,                 loss: 0.0138
env0_second_0:                 episode reward: 45.8500,                 loss: nan
env1_first_0:                 episode reward: -31.0500,                 loss: nan
env1_second_0:                 episode reward: 31.0500,                 loss: nan
env2_first_0:                 episode reward: -36.7000,                 loss: nan
env2_second_0:                 episode reward: 36.7000,                 loss: nan
env3_first_0:                 episode reward: -34.1500,                 loss: nan
env3_second_0:                 episode reward: 34.1500,                 loss: nan
env4_first_0:                 episode reward: -35.2000,                 loss: nan
env4_second_0:                 episode reward: 35.2000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 6597.05,                last time consumption/overall running time: 2300.7229s / 333757.4847 s
env0_first_0:                 episode reward: 1.5500,                 loss: 0.0133
env0_second_0:                 episode reward: -1.5500,                 loss: nan
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
env2_first_0:                 episode reward: 5.0000,                 loss: nan
env2_second_0:                 episode reward: -5.0000,                 loss: nan
env3_first_0:                 episode reward: 4.8500,                 loss: nan
env3_second_0:                 episode reward: -4.8500,                 loss: nan
env4_first_0:                 episode reward: -0.0500,                 loss: nan
env4_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 7281.9,                last time consumption/overall running time: 2446.7499s / 336204.2346 s
env0_first_0:                 episode reward: 11.1500,                 loss: 0.0133
env0_second_0:                 episode reward: -11.1500,                 loss: nan
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
env2_first_0:                 episode reward: 9.6000,                 loss: nan
env2_second_0:                 episode reward: -9.6000,                 loss: nan
env3_first_0:                 episode reward: 9.8500,                 loss: nan
env3_second_0:                 episode reward: -9.8500,                 loss: nan
env4_first_0:                 episode reward: 5.2000,                 loss: nan
env4_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 7158.85,                last time consumption/overall running time: 2396.8827s / 338601.1173 s
env0_first_0:                 episode reward: 18.1500,                 loss: 0.0118
env0_second_0:                 episode reward: -18.1500,                 loss: 0.0138
env1_first_0:                 episode reward: 23.5000,                 loss: nan
env1_second_0:                 episode reward: -23.5000,                 loss: nan
env2_first_0:                 episode reward: 18.9000,                 loss: nan
env2_second_0:                 episode reward: -18.9000,                 loss: nan
env3_first_0:                 episode reward: 29.0000,                 loss: nan
env3_second_0:                 episode reward: -29.0000,                 loss: nan
env4_first_0:                 episode reward: 27.5000,                 loss: nan
env4_second_0:                 episode reward: -27.5000,                 loss: nan
Score delta: 57.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3720_0.
Episode: 3741/10000 (37.4100%),                 avg. length: 5690.9,                last time consumption/overall running time: 1941.1880s / 340542.3053 s
env0_first_0:                 episode reward: 4.4500,                 loss: nan
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0121
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
env2_first_0:                 episode reward: 0.9500,                 loss: nan
env2_second_0:                 episode reward: -0.9500,                 loss: nan
env3_first_0:                 episode reward: -2.3000,                 loss: nan
env3_second_0:                 episode reward: 2.3000,                 loss: nan
env4_first_0:                 episode reward: 2.4500,                 loss: nan
env4_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 7487.9,                last time consumption/overall running time: 2507.8659s / 343050.1712 s
env0_first_0:                 episode reward: 14.0500,                 loss: nan
env0_second_0:                 episode reward: -14.0500,                 loss: 0.0114
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
env2_first_0:                 episode reward: 10.9000,                 loss: nan
env2_second_0:                 episode reward: -10.9000,                 loss: nan
env3_first_0:                 episode reward: 5.4500,                 loss: nan
env3_second_0:                 episode reward: -5.4500,                 loss: nan
env4_first_0:                 episode reward: 13.1500,                 loss: nan
env4_second_0:                 episode reward: -13.1500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 7304.35,                last time consumption/overall running time: 2438.3314s / 345488.5026 s
env0_first_0:                 episode reward: 3.5000,                 loss: nan
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0116
env1_first_0:                 episode reward: -12.2000,                 loss: nan
env1_second_0:                 episode reward: 12.2000,                 loss: nan
env2_first_0:                 episode reward: -1.0000,                 loss: nan
env2_second_0:                 episode reward: 1.0000,                 loss: nan
env3_first_0:                 episode reward: -8.5000,                 loss: nan
env3_second_0:                 episode reward: 8.5000,                 loss: nan
env4_first_0:                 episode reward: 3.4000,                 loss: nan
env4_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 5879.95,                last time consumption/overall running time: 1936.2513s / 347424.7538 s
env0_first_0:                 episode reward: -20.3000,                 loss: 0.0125
env0_second_0:                 episode reward: 20.3000,                 loss: 0.0114
env1_first_0:                 episode reward: -22.1000,                 loss: nan
env1_second_0:                 episode reward: 22.1000,                 loss: nan
env2_first_0:                 episode reward: -13.9500,                 loss: nan
env2_second_0:                 episode reward: 13.9500,                 loss: nan
env3_first_0:                 episode reward: -16.6000,                 loss: nan
env3_second_0:                 episode reward: 16.6000,                 loss: nan
env4_first_0:                 episode reward: -21.2000,                 loss: nan
env4_second_0:                 episode reward: 21.2000,                 loss: nan
Score delta: 50.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3792_1.
Episode: 3821/10000 (38.2100%),                 avg. length: 7021.25,                last time consumption/overall running time: 2304.3352s / 349729.0891 s
env0_first_0:                 episode reward: 14.5000,                 loss: 0.0133
env0_second_0:                 episode reward: -14.5000,                 loss: 0.0160
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
env2_first_0:                 episode reward: -1.0000,                 loss: nan
env2_second_0:                 episode reward: 1.0000,                 loss: nan
env3_first_0:                 episode reward: 2.1000,                 loss: nan
env3_second_0:                 episode reward: -2.1000,                 loss: nan
env4_first_0:                 episode reward: 10.1000,                 loss: nan
env4_second_0:                 episode reward: -10.1000,                 loss: nan
Score delta: 55.6, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3820_0.
Episode: 3841/10000 (38.4100%),                 avg. length: 6290.8,                last time consumption/overall running time: 2072.6660s / 351801.7550 s
env0_first_0:                 episode reward: 0.1500,                 loss: nan
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0128
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
env2_first_0:                 episode reward: -3.4000,                 loss: nan
env2_second_0:                 episode reward: 3.4000,                 loss: nan
env3_first_0:                 episode reward: 4.9500,                 loss: nan
env3_second_0:                 episode reward: -4.9500,                 loss: nan
env4_first_0:                 episode reward: 3.9500,                 loss: nan
env4_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 6373.6,                last time consumption/overall running time: 2119.5952s / 353921.3503 s
env0_first_0:                 episode reward: -10.0500,                 loss: nan
env0_second_0:                 episode reward: 10.0500,                 loss: 0.0113
env1_first_0:                 episode reward: -17.4500,                 loss: nan
env1_second_0:                 episode reward: 17.4500,                 loss: nan
env2_first_0:                 episode reward: -17.9500,                 loss: nan
env2_second_0:                 episode reward: 17.9500,                 loss: nan
env3_first_0:                 episode reward: -6.7000,                 loss: nan
env3_second_0:                 episode reward: 6.7000,                 loss: nan
env4_first_0:                 episode reward: -12.3500,                 loss: nan
env4_second_0:                 episode reward: 12.3500,                 loss: nan
Score delta: 52.0, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3861_1.
Episode: 3881/10000 (38.8100%),                 avg. length: 4681.8,                last time consumption/overall running time: 1513.4637s / 355434.8140 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0128
env0_second_0:                 episode reward: 6.6000,                 loss: nan
env1_first_0:                 episode reward: -22.7500,                 loss: nan
env1_second_0:                 episode reward: 22.7500,                 loss: nan
env2_first_0:                 episode reward: -4.2500,                 loss: nan
env2_second_0:                 episode reward: 4.2500,                 loss: nan
env3_first_0:                 episode reward: -7.4500,                 loss: nan
env3_second_0:                 episode reward: 7.4500,                 loss: nan
env4_first_0:                 episode reward: -22.0000,                 loss: nan
env4_second_0:                 episode reward: 22.0000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 7032.6,                last time consumption/overall running time: 2327.6710s / 357762.4850 s
env0_first_0:                 episode reward: 13.2000,                 loss: 0.0110
env0_second_0:                 episode reward: -13.2000,                 loss: nan
env1_first_0:                 episode reward: 7.0000,                 loss: nan
env1_second_0:                 episode reward: -7.0000,                 loss: nan
env2_first_0:                 episode reward: 17.6000,                 loss: nan
env2_second_0:                 episode reward: -17.6000,                 loss: nan
env3_first_0:                 episode reward: 7.5000,                 loss: nan
env3_second_0:                 episode reward: -7.5000,                 loss: nan
env4_first_0:                 episode reward: 8.8000,                 loss: nan
env4_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 5953.7,                last time consumption/overall running time: 2001.4154s / 359763.9004 s
env0_first_0:                 episode reward: 26.2000,                 loss: 0.0116
env0_second_0:                 episode reward: -26.2000,                 loss: 0.0124
env1_first_0:                 episode reward: 25.0000,                 loss: nan
env1_second_0:                 episode reward: -25.0000,                 loss: nan
env2_first_0:                 episode reward: 25.7000,                 loss: nan
env2_second_0:                 episode reward: -25.7000,                 loss: nan
env3_first_0:                 episode reward: 31.4500,                 loss: nan
env3_second_0:                 episode reward: -31.4500,                 loss: nan
env4_first_0:                 episode reward: 26.2000,                 loss: nan
env4_second_0:                 episode reward: -26.2000,                 loss: nan
Score delta: 58.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3910_0.
Episode: 3941/10000 (39.4100%),                 avg. length: 8173.3,                last time consumption/overall running time: 2691.6939s / 362455.5944 s
env0_first_0:                 episode reward: -7.9500,                 loss: nan
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0117
env1_first_0:                 episode reward: -20.4000,                 loss: nan
env1_second_0:                 episode reward: 20.4000,                 loss: nan
env2_first_0:                 episode reward: -13.3500,                 loss: nan
env2_second_0:                 episode reward: 13.3500,                 loss: nan
env3_first_0:                 episode reward: -11.0000,                 loss: nan
env3_second_0:                 episode reward: 11.0000,                 loss: nan
env4_first_0:                 episode reward: -17.2500,                 loss: nan
env4_second_0:                 episode reward: 17.2500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 8446.55,                last time consumption/overall running time: 2760.7226s / 365216.3170 s
env0_first_0:                 episode reward: -3.0500,                 loss: nan
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0128
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
env2_first_0:                 episode reward: -3.7000,                 loss: nan
env2_second_0:                 episode reward: 3.7000,                 loss: nan
env3_first_0:                 episode reward: 2.2500,                 loss: nan
env3_second_0:                 episode reward: -2.2500,                 loss: nan
env4_first_0:                 episode reward: -3.7500,                 loss: nan
env4_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 6797.45,                last time consumption/overall running time: 2211.4498s / 367427.7667 s
env0_first_0:                 episode reward: -2.5500,                 loss: nan
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0130
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
env2_first_0:                 episode reward: 4.7500,                 loss: nan
env2_second_0:                 episode reward: -4.7500,                 loss: nan
env3_first_0:                 episode reward: 0.4500,                 loss: nan
env3_second_0:                 episode reward: -0.4500,                 loss: nan
env4_first_0:                 episode reward: 0.9000,                 loss: nan
env4_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 8603.5,                last time consumption/overall running time: 2817.2944s / 370245.0612 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0116
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0124
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
env2_first_0:                 episode reward: -7.2500,                 loss: nan
env2_second_0:                 episode reward: 7.2500,                 loss: nan
env3_first_0:                 episode reward: 3.5000,                 loss: nan
env3_second_0:                 episode reward: -3.5000,                 loss: nan
env4_first_0:                 episode reward: 0.5500,                 loss: nan
env4_second_0:                 episode reward: -0.5500,                 loss: nan
Score delta: 71.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/3989_1.
Episode: 4021/10000 (40.2100%),                 avg. length: 7313.1,                last time consumption/overall running time: 2411.3454s / 372656.4065 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0127
env0_second_0:                 episode reward: -0.4500,                 loss: nan
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
env2_first_0:                 episode reward: -9.4000,                 loss: nan
env2_second_0:                 episode reward: 9.4000,                 loss: nan
env3_first_0:                 episode reward: 5.5000,                 loss: nan
env3_second_0:                 episode reward: -5.5000,                 loss: nan
env4_first_0:                 episode reward: -0.4000,                 loss: nan
env4_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 6385.05,                last time consumption/overall running time: 2060.3621s / 374716.7686 s
env0_first_0:                 episode reward: 10.3500,                 loss: 0.0111
env0_second_0:                 episode reward: -10.3500,                 loss: 0.0109
env1_first_0:                 episode reward: 8.9000,                 loss: nan
env1_second_0:                 episode reward: -8.9000,                 loss: nan
env2_first_0:                 episode reward: 7.1500,                 loss: nan
env2_second_0:                 episode reward: -7.1500,                 loss: nan
env3_first_0:                 episode reward: 8.6500,                 loss: nan
env3_second_0:                 episode reward: -8.6500,                 loss: nan
env4_first_0:                 episode reward: 7.2500,                 loss: nan
env4_second_0:                 episode reward: -7.2500,                 loss: nan
Score delta: 61.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/4028_0.
Episode: 4061/10000 (40.6100%),                 avg. length: 8056.25,                last time consumption/overall running time: 2593.7709s / 377310.5395 s
env0_first_0:                 episode reward: -15.3000,                 loss: nan
env0_second_0:                 episode reward: 15.3000,                 loss: 0.0102
env1_first_0:                 episode reward: -19.5000,                 loss: nan
env1_second_0:                 episode reward: 19.5000,                 loss: nan
env2_first_0:                 episode reward: -14.6000,                 loss: nan
env2_second_0:                 episode reward: 14.6000,                 loss: nan
env3_first_0:                 episode reward: -10.4500,                 loss: nan
env3_second_0:                 episode reward: 10.4500,                 loss: nan
env4_first_0:                 episode reward: -26.4500,                 loss: nan
env4_second_0:                 episode reward: 26.4500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 6874.3,                last time consumption/overall running time: 2242.1862s / 379552.7256 s
env0_first_0:                 episode reward: -21.7000,                 loss: 0.0190
env0_second_0:                 episode reward: 21.7000,                 loss: 0.0108
env1_first_0:                 episode reward: -19.9500,                 loss: nan
env1_second_0:                 episode reward: 19.9500,                 loss: nan
env2_first_0:                 episode reward: -12.7000,                 loss: nan
env2_second_0:                 episode reward: 12.7000,                 loss: nan
env3_first_0:                 episode reward: -21.1500,                 loss: nan
env3_second_0:                 episode reward: 21.1500,                 loss: nan
env4_first_0:                 episode reward: -16.2500,                 loss: nan
env4_second_0:                 episode reward: 16.2500,                 loss: nan
Score delta: 50.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/4079_1.
Episode: 4101/10000 (41.0100%),                 avg. length: 6358.9,                last time consumption/overall running time: 2027.1876s / 381579.9132 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0123
env0_second_0:                 episode reward: 4.4500,                 loss: nan
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
env2_first_0:                 episode reward: -8.3000,                 loss: nan
env2_second_0:                 episode reward: 8.3000,                 loss: nan
env3_first_0:                 episode reward: -1.9000,                 loss: nan
env3_second_0:                 episode reward: 1.9000,                 loss: nan
env4_first_0:                 episode reward: -1.2000,                 loss: nan
env4_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 8352.5,                last time consumption/overall running time: 2689.0497s / 384268.9629 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0119
env0_second_0:                 episode reward: 1.0500,                 loss: nan
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
env2_first_0:                 episode reward: 0.1000,                 loss: nan
env2_second_0:                 episode reward: -0.1000,                 loss: nan
env3_first_0:                 episode reward: -2.4000,                 loss: nan
env3_second_0:                 episode reward: 2.4000,                 loss: nan
env4_first_0:                 episode reward: -0.8000,                 loss: nan
env4_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 6957.25,                last time consumption/overall running time: 2249.4750s / 386518.4379 s
env0_first_0:                 episode reward: 17.0500,                 loss: 0.0121
env0_second_0:                 episode reward: -17.0500,                 loss: 0.0119
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
env2_first_0:                 episode reward: -2.0000,                 loss: nan
env2_second_0:                 episode reward: 2.0000,                 loss: nan
env3_first_0:                 episode reward: 17.8500,                 loss: nan
env3_second_0:                 episode reward: -17.8500,                 loss: nan
env4_first_0:                 episode reward: 7.5000,                 loss: nan
env4_second_0:                 episode reward: -7.5000,                 loss: nan
Score delta: 61.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/4139_0.
Episode: 4161/10000 (41.6100%),                 avg. length: 8047.3,                last time consumption/overall running time: 2588.0884s / 389106.5263 s
env0_first_0:                 episode reward: -11.8500,                 loss: nan
env0_second_0:                 episode reward: 11.8500,                 loss: 0.0106
env1_first_0:                 episode reward: -17.5000,                 loss: nan
env1_second_0:                 episode reward: 17.5000,                 loss: nan
env2_first_0:                 episode reward: -11.4500,                 loss: nan
env2_second_0:                 episode reward: 11.4500,                 loss: nan
env3_first_0:                 episode reward: -0.9000,                 loss: nan
env3_second_0:                 episode reward: 0.9000,                 loss: nan
env4_first_0:                 episode reward: 0.0000,                 loss: nan
env4_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 8645.05,                last time consumption/overall running time: 2806.4508s / 391912.9771 s
env0_first_0:                 episode reward: -17.5500,                 loss: 0.0135
env0_second_0:                 episode reward: 17.5500,                 loss: 0.0099
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
env2_first_0:                 episode reward: -30.4500,                 loss: nan
env2_second_0:                 episode reward: 30.4500,                 loss: nan
env3_first_0:                 episode reward: -13.7000,                 loss: nan
env3_second_0:                 episode reward: 13.7000,                 loss: nan
env4_first_0:                 episode reward: -11.1500,                 loss: nan
env4_second_0:                 episode reward: 11.1500,                 loss: nan
Score delta: 75.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/4177_1.
Episode: 4201/10000 (42.0100%),                 avg. length: 7355.1,                last time consumption/overall running time: 2400.9685s / 394313.9456 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0121
env0_second_0:                 episode reward: 5.7000,                 loss: nan
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
env2_first_0:                 episode reward: -2.8000,                 loss: nan
env2_second_0:                 episode reward: 2.8000,                 loss: nan
env3_first_0:                 episode reward: -9.2500,                 loss: nan
env3_second_0:                 episode reward: 9.2500,                 loss: nan
env4_first_0:                 episode reward: -2.7000,                 loss: nan
env4_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 6531.2,                last time consumption/overall running time: 2124.0500s / 396437.9956 s
env0_first_0:                 episode reward: 7.6000,                 loss: 0.0121
env0_second_0:                 episode reward: -7.6000,                 loss: nan
env1_first_0:                 episode reward: 16.3000,                 loss: nan
env1_second_0:                 episode reward: -16.3000,                 loss: nan
env2_first_0:                 episode reward: 11.2000,                 loss: nan
env2_second_0:                 episode reward: -11.2000,                 loss: nan
env3_first_0:                 episode reward: 12.2500,                 loss: nan
env3_second_0:                 episode reward: -12.2500,                 loss: nan
env4_first_0:                 episode reward: 11.3500,                 loss: nan
env4_second_0:                 episode reward: -11.3500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 6866.85,                last time consumption/overall running time: 2228.4179s / 398666.4136 s
env0_first_0:                 episode reward: 8.8000,                 loss: 0.0115
env0_second_0:                 episode reward: -8.8000,                 loss: 0.0112
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
env2_first_0:                 episode reward: 10.6000,                 loss: nan
env2_second_0:                 episode reward: -10.6000,                 loss: nan
env3_first_0:                 episode reward: -0.6500,                 loss: nan
env3_second_0:                 episode reward: 0.6500,                 loss: nan
env4_first_0:                 episode reward: 10.9500,                 loss: nan
env4_second_0:                 episode reward: -10.9500,                 loss: nan
Score delta: 50.8, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/4230_0.
Episode: 4261/10000 (42.6100%),                 avg. length: 7155.9,                last time consumption/overall running time: 2300.2488s / 400966.6623 s
env0_first_0:                 episode reward: -32.1000,                 loss: 0.0128
env0_second_0:                 episode reward: 32.1000,                 loss: 0.0104
env1_first_0:                 episode reward: -31.2000,                 loss: nan
env1_second_0:                 episode reward: 31.2000,                 loss: nan
env2_first_0:                 episode reward: -31.5000,                 loss: nan
env2_second_0:                 episode reward: 31.5000,                 loss: nan
env3_first_0:                 episode reward: -30.3500,                 loss: nan
env3_second_0:                 episode reward: 30.3500,                 loss: nan
env4_first_0:                 episode reward: -34.4000,                 loss: nan
env4_second_0:                 episode reward: 34.4000,                 loss: nan
Score delta: 59.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/4259_1.
Episode: 4281/10000 (42.8100%),                 avg. length: 6461.25,                last time consumption/overall running time: 2116.4731s / 403083.1354 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0130
env0_second_0:                 episode reward: -6.5500,                 loss: nan
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
env2_first_0:                 episode reward: 10.7500,                 loss: nan
env2_second_0:                 episode reward: -10.7500,                 loss: nan
env3_first_0:                 episode reward: 9.0500,                 loss: nan
env3_second_0:                 episode reward: -9.0500,                 loss: nan
env4_first_0:                 episode reward: 7.2500,                 loss: nan
env4_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 6591.0,                last time consumption/overall running time: 2143.4989s / 405226.6343 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0119
env0_second_0:                 episode reward: -0.2000,                 loss: nan
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
env2_first_0:                 episode reward: -6.6500,                 loss: nan
env2_second_0:                 episode reward: 6.6500,                 loss: nan
env3_first_0:                 episode reward: 4.7500,                 loss: nan
env3_second_0:                 episode reward: -4.7500,                 loss: nan
env4_first_0:                 episode reward: 3.0000,                 loss: nan
env4_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 6578.05,                last time consumption/overall running time: 2122.7753s / 407349.4095 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0124
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: 17.9500,                 loss: nan
env1_second_0:                 episode reward: -17.9500,                 loss: nan
env2_first_0:                 episode reward: -3.1000,                 loss: nan
env2_second_0:                 episode reward: 3.1000,                 loss: nan
env3_first_0:                 episode reward: 9.3500,                 loss: nan
env3_second_0:                 episode reward: -9.3500,                 loss: nan
env4_first_0:                 episode reward: 15.3500,                 loss: nan
env4_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 7634.35,                last time consumption/overall running time: 2470.1542s / 409819.5637 s
env0_first_0:                 episode reward: 17.4000,                 loss: 0.0131
env0_second_0:                 episode reward: -17.4000,                 loss: 0.0128
env1_first_0:                 episode reward: 13.2500,                 loss: nan
env1_second_0:                 episode reward: -13.2500,                 loss: nan
env2_first_0:                 episode reward: 12.1000,                 loss: nan
env2_second_0:                 episode reward: -12.1000,                 loss: nan
env3_first_0:                 episode reward: 19.0500,                 loss: nan
env3_second_0:                 episode reward: -19.0500,                 loss: nan
env4_first_0:                 episode reward: 17.1500,                 loss: nan
env4_second_0:                 episode reward: -17.1500,                 loss: nan
Score delta: 73.4, save the model to .//data/model/20220508_1730/pettingzoo_tennis_v2_fictitious_selfplay2/4329_0.
Episode: 4361/10000 (43.6100%),                 avg. length: 7486.0,                last time consumption/overall running time: 2433.9203s / 412253.4840 s
env0_first_0:                 episode reward: 15.8000,                 loss: nan
env0_second_0:                 episode reward: -15.8000,                 loss: 0.0152
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
env2_first_0:                 episode reward: 17.5500,                 loss: nan
env2_second_0:                 episode reward: -17.5500,                 loss: nan
env3_first_0:                 episode reward: 8.2000,                 loss: nan
env3_second_0:                 episode reward: -8.2000,                 loss: nan
env4_first_0:                 episode reward: 21.2500,                 loss: nan
env4_second_0:                 episode reward: -21.2500,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 7591.25,                last time consumption/overall running time: 2454.1474s / 414707.6314 s
env0_first_0:                 episode reward: -10.3500,                 loss: 0.0131