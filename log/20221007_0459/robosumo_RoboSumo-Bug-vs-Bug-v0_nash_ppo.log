wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/MARS/wandb/run-20221007_045958-1h09kw2t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robosumo_RoboSumo-Bug-vs-Bug-v0_nash_ppo_202210070459
wandb: ‚≠êÔ∏è View project at https://wandb.ai/quantumiracle/Pettingzoo_MARS
wandb: üöÄ View run at https://wandb.ai/quantumiracle/Pettingzoo_MARS/runs/1h09kw2t
robosumo_RoboSumo-Bug-vs-Bug-v0
default:  {'env_name': 'RoboSumo-Bug-vs-Bug-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': False, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': False, 'wandb_entity': '', 'wandb_project': '', 'wandb_group': '', 'wandb_name': '', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
{'env_name': 'RoboSumo-Bug-vs-Bug-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': '', 'wandb_group': '', 'wandb_name': '', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
args:  {'env_name': 'RoboSumo-Bug-vs-Bug-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': 'Pettingzoo_MARS', 'wandb_group': '202210070459', 'wandb_name': 'robosumo_RoboSumo-Bug-vs-Bug-v0_nash_ppo_202210070459', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
RoboSumo-Bug-vs-Bug-v0 robosumo
record video: interval 1000, length 300
Load RoboSumo-Bug-vs-Bug-v0 environment in type robosumo.
Env observation space: Box(-inf, inf, (164,), float32) action space: Box(-1.0, 1.0, (12,), float32)
random seed: 547
<RecordVideo<mars.env.wrappers.mars_wrappers.RoboSumoWrapper object at 0x7f18f83d3128>>
gaussian_policy 12 Box(-1.0, 1.0, (12,), float32)
[MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=164, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=164, bias=True)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=24, bias=True)
    (2): Tanh()
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=24, bias=True)
    (2): Tanh()
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)] MLP(
  (body): Sequential(
    (0): Linear(in_features=328, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)
gaussian_policy 12 Box(-1.0, 1.0, (12,), float32)
[MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=164, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=164, bias=True)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=24, bias=True)
    (2): Tanh()
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=24, bias=True)
    (2): Tanh()
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=164, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)] MLP(
  (body): Sequential(
    (0): Linear(in_features=328, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)
gaussian_policy 12 Box(-1.0, 1.0, (12,), float32)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'RoboSumo-Bug-vs-Bug-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': 'Pettingzoo_MARS', 'wandb_group': '202210070459', 'wandb_name': 'robosumo_RoboSumo-Bug-vs-Bug-v0_nash_ppo_202210070459', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
Save models to : /data/zihan/research/MARS/data/model/202210070459/robosumo_RoboSumo-Bug-vs-Bug-v0_nash_ppo. 
 Save logs to: /data/zihan/research/MARS/data/log/202210070459/robosumo_RoboSumo-Bug-vs-Bug-v0_nash_ppo.
Episode: 1/50000 (0.0020%),                 avg. length: 31.0,                last time consumption/overall running time: 0.27s / 0.27 s
first_0:                 episode reward: -2132.4089,                 loss: 1002788.0117
second_0:                 episode reward: 1876.8221,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 41.1,                last time consumption/overall running time: 5.43s / 5.70 s
first_0:                 episode reward: -388.7743,                 loss: 1034091.5273
second_0:                 episode reward: 23.0527,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 64.2,                last time consumption/overall running time: 6.91s / 12.61 s
first_0:                 episode reward: -14.3454,                 loss: 769443.4667
second_0:                 episode reward: -573.6534,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 61.05,                last time consumption/overall running time: 7.47s / 20.09 s
first_0:                 episode reward: -928.8279,                 loss: 715245.2163
second_0:                 episode reward: 335.8515,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 56.55,                last time consumption/overall running time: 6.88s / 26.96 s
first_0:                 episode reward: 108.8134,                 loss: 762510.9780
second_0:                 episode reward: -637.3898,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 53.45,                last time consumption/overall running time: 6.51s / 33.48 s
first_0:                 episode reward: -70.5310,                 loss: 798130.4865
second_0:                 episode reward: -451.2382,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 52.15,                last time consumption/overall running time: 6.46s / 39.94 s
first_0:                 episode reward: -265.2114,                 loss: 883535.1079
second_0:                 episode reward: -223.1131,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 55.7,                last time consumption/overall running time: 6.66s / 46.60 s
first_0:                 episode reward: -214.9795,                 loss: 862413.2082
second_0:                 episode reward: -361.7060,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 75.2,                last time consumption/overall running time: 8.17s / 54.77 s
first_0:                 episode reward: 64.4932,                 loss: 627363.2422
second_0:                 episode reward: -880.5320,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 46.1,                last time consumption/overall running time: 5.89s / 60.66 s
first_0:                 episode reward: -679.1453,                 loss: 903713.4546
second_0:                 episode reward: 143.4177,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 56.25,                last time consumption/overall running time: 6.75s / 67.40 s
first_0:                 episode reward: -766.6210,                 loss: 748862.8278
second_0:                 episode reward: 103.2020,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 51.95,                last time consumption/overall running time: 6.34s / 73.75 s
first_0:                 episode reward: 686.3241,                 loss: 770591.3046
second_0:                 episode reward: -1309.8518,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 57.0,                last time consumption/overall running time: 6.81s / 80.55 s
first_0:                 episode reward: -364.3230,                 loss: 677885.6551
second_0:                 episode reward: -363.7797,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 61.8,                last time consumption/overall running time: 7.08s / 87.63 s
first_0:                 episode reward: -652.9138,                 loss: 738156.9384
second_0:                 episode reward: -95.9430,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 72.95,                last time consumption/overall running time: 7.92s / 95.55 s
first_0:                 episode reward: -288.3842,                 loss: 761518.1105
second_0:                 episode reward: -706.5141,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 71.85,                last time consumption/overall running time: 7.96s / 103.51 s
first_0:                 episode reward: -357.1866,                 loss: 644588.2775
second_0:                 episode reward: -572.9176,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 63.25,                last time consumption/overall running time: 7.22s / 110.74 s
first_0:                 episode reward: 212.1717,                 loss: 726740.5875
second_0:                 episode reward: -1048.0157,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 53.05,                last time consumption/overall running time: 6.34s / 117.08 s
first_0:                 episode reward: -119.5614,                 loss: 724954.5439
second_0:                 episode reward: -575.1330,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 73.65,                last time consumption/overall running time: 8.04s / 125.12 s
first_0:                 episode reward: -491.4938,                 loss: 600068.3185
second_0:                 episode reward: -512.4539,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 79.9,                last time consumption/overall running time: 8.56s / 133.67 s
first_0:                 episode reward: -809.4617,                 loss: 701097.2970
second_0:                 episode reward: -267.5493,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 65.9,                last time consumption/overall running time: 7.47s / 141.14 s
first_0:                 episode reward: -637.4909,                 loss: 737421.6222
second_0:                 episode reward: -277.9277,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 61.45,                last time consumption/overall running time: 7.05s / 148.19 s
first_0:                 episode reward: -706.4147,                 loss: 669580.1397
second_0:                 episode reward: -156.1766,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 68.95,                last time consumption/overall running time: 7.68s / 155.87 s
first_0:                 episode reward: 256.2255,                 loss: 674893.8931
second_0:                 episode reward: -1199.2418,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 70.65,                last time consumption/overall running time: 7.84s / 163.71 s
first_0:                 episode reward: -179.1642,                 loss: 615448.0104
second_0:                 episode reward: -784.3336,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 72.1,                last time consumption/overall running time: 7.95s / 171.66 s
first_0:                 episode reward: -281.4009,                 loss: 586397.1097
second_0:                 episode reward: -739.5241,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 75.4,                last time consumption/overall running time: 7.97s / 179.63 s
first_0:                 episode reward: 5.0231,                 loss: 603829.6715
second_0:                 episode reward: -1074.5054,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 70.5,                last time consumption/overall running time: 7.80s / 187.43 s
first_0:                 episode reward: -790.0597,                 loss: 578740.3443
second_0:                 episode reward: -192.1594,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 90.1,                last time consumption/overall running time: 9.25s / 196.67 s
first_0:                 episode reward: -371.1969,                 loss: 577410.5047
second_0:                 episode reward: -950.0408,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 77.8,                last time consumption/overall running time: 8.28s / 204.96 s
first_0:                 episode reward: -560.1168,                 loss: 637880.4260
second_0:                 episode reward: -518.8418,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 74.05,                last time consumption/overall running time: 8.00s / 212.96 s
first_0:                 episode reward: -634.4040,                 loss: 622519.4917
second_0:                 episode reward: -406.2415,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 69.75,                last time consumption/overall running time: 7.71s / 220.68 s
first_0:                 episode reward: -977.7469,                 loss: 670027.3872
second_0:                 episode reward: 22.6661,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 66.05,                last time consumption/overall running time: 7.43s / 228.10 s
first_0:                 episode reward: -240.3246,                 loss: 752145.4899
second_0:                 episode reward: -661.9885,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 81.55,                last time consumption/overall running time: 8.60s / 236.71 s
first_0:                 episode reward: -744.5868,                 loss: 524478.8504
second_0:                 episode reward: -370.2892,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 69.1,                last time consumption/overall running time: 7.74s / 244.44 s
first_0:                 episode reward: -557.0109,                 loss: 708450.7935
second_0:                 episode reward: -436.2265,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 62.15,                last time consumption/overall running time: 7.17s / 251.61 s
first_0:                 episode reward: 205.5814,                 loss: 678497.7299
second_0:                 episode reward: -1078.7036,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 73.75,                last time consumption/overall running time: 8.06s / 259.67 s
first_0:                 episode reward: 6.4540,                 loss: 619740.1175
second_0:                 episode reward: -1027.4334,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 56.15,                last time consumption/overall running time: 6.66s / 266.33 s
first_0:                 episode reward: -185.6097,                 loss: 875952.0508
second_0:                 episode reward: -617.9593,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 83.8,                last time consumption/overall running time: 8.83s / 275.16 s
first_0:                 episode reward: -381.6736,                 loss: 755757.6031
second_0:                 episode reward: -772.9653,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 68.5,                last time consumption/overall running time: 7.57s / 282.74 s
first_0:                 episode reward: -234.4801,                 loss: 615454.8374
second_0:                 episode reward: -701.8572,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 61.1,                last time consumption/overall running time: 7.01s / 289.75 s
first_0:                 episode reward: -29.9666,                 loss: 743902.7164
second_0:                 episode reward: -827.8997,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 67.3,                last time consumption/overall running time: 7.55s / 297.30 s
first_0:                 episode reward: 164.7732,                 loss: 678785.3920
second_0:                 episode reward: -1113.1019,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 69.95,                last time consumption/overall running time: 7.73s / 305.04 s
first_0:                 episode reward: -82.4763,                 loss: 629416.7802
second_0:                 episode reward: -907.3493,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 86.9,                last time consumption/overall running time: 8.86s / 313.90 s
first_0:                 episode reward: -75.1312,                 loss: 506651.4863
second_0:                 episode reward: -1168.0259,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 69.55,                last time consumption/overall running time: 7.54s / 321.43 s
first_0:                 episode reward: -183.0762,                 loss: 631476.4395
second_0:                 episode reward: -811.0937,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 74.45,                last time consumption/overall running time: 7.84s / 329.27 s
first_0:                 episode reward: -906.7425,                 loss: 618665.3450
second_0:                 episode reward: -126.8908,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 74.55,                last time consumption/overall running time: 8.02s / 337.29 s
first_0:                 episode reward: -495.4737,                 loss: 637294.2040
second_0:                 episode reward: -533.3886,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 68.95,                last time consumption/overall running time: 7.65s / 344.94 s
first_0:                 episode reward: -745.6407,                 loss: 683494.3457
second_0:                 episode reward: -224.3346,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 66.8,                last time consumption/overall running time: 7.43s / 352.37 s