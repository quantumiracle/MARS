env_args:
    env_name: arbitrary_richobs_mdp
    env_type: mdp
    num_envs: 1
    ram: True
    seed: 1

agent_args:
    algorithm: DQN
    algorithm_spec:
        dueling: false
        replay_buffer_size: 1e5
        gamma: 1.
        multi_step: 1
        target_update_interval: 1000
        eps_start: 1.0
        eps_final: 0.01
        eps_decay: 8000

train_args:
    batch_size: 640
    max_episodes: 1000000
    max_steps_per_episode: 10000
    optimizer: adam
    learning_rate: 1e-4
    device: gpu
    update_itr: 1  # iterations of updates per frame, 0~inf; <1 means several steps are skipped per update
    log_avg_window: 20
    log_interval: 20
    multiprocess: false
    # render: True
    # test: True
    # load_model_idx: 0/1
    net_architecture: 
        # hidden_dim_list: [128, 128, 128] 
        hidden_dim_list: [32, 32, 32] 
        hidden_activation: ReLU  # use torch.nn (in Sequential) style rather than torch.nn.functional (in forward)
        output_activation: False # False means nan


    marl_method: fictitious_selfplay2
    marl_spec:  # configurations for specific MARL method
        selfplay_score_delta: 1.5
        trainable_agent_idx: 0
        opponent_idx: 1
