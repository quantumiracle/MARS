pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 10000, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nfsp', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'eta': 0.1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_1545/pettingzoo_double_dunk_v2_nfsp. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_1545/pettingzoo_double_dunk_v2_nfsp.
Episode: 1/10000 (0.0100%),                 avg. length: 3353.0,                last time consumption/overall running time: 21.9030s / 21.9030 s
env0_first_0:                 episode reward: -25.0000,                 loss: nan
env0_second_0:                 episode reward: 25.0000,                 loss: nan
env1_first_0:                 episode reward: -13.0000,                 loss: nan
env1_second_0:                 episode reward: 13.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 5375.9,                last time consumption/overall running time: 4474.3693s / 4496.2723 s
env0_first_0:                 episode reward: -17.5000,                 loss: 0.0077
env0_second_0:                 episode reward: 17.5000,                 loss: 0.0091
env1_first_0:                 episode reward: -15.8500,                 loss: nan
env1_second_0:                 episode reward: 15.8500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 3755.0,                last time consumption/overall running time: 3385.5055s / 7881.7778 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0054
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0059
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 2072.35,                last time consumption/overall running time: 1865.7517s / 9747.5295 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0041
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0044
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 2017.35,                last time consumption/overall running time: 1815.7380s / 11563.2675 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0033
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0041
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1735.45,                last time consumption/overall running time: 1565.1295s / 13128.3970 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0030
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0034
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1833.3,                last time consumption/overall running time: 1653.7088s / 14782.1058 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0032
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1828.9,                last time consumption/overall running time: 1648.7982s / 16430.9040 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0027
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0030
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1772.35,                last time consumption/overall running time: 1598.1120s / 18029.0160 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0025
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0028
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1987.35,                last time consumption/overall running time: 1791.5528s / 19820.5688 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0027
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0031
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1979.3,                last time consumption/overall running time: 1784.5667s / 21605.1355 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0032
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0037
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1818.65,                last time consumption/overall running time: 1639.0369s / 23244.1724 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0035
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0039
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1713.6,                last time consumption/overall running time: 1543.9123s / 24788.0847 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0032
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0035
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2024.55,                last time consumption/overall running time: 1825.0693s / 26613.1540 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0031
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0031
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2180.25,                last time consumption/overall running time: 1964.0645s / 28577.2185 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0029
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0032
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2044.3,                last time consumption/overall running time: 1840.9512s / 30418.1697 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0040
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0046
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1995.25,                last time consumption/overall running time: 1796.8704s / 32215.0401 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0046
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0055
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2103.75,                last time consumption/overall running time: 1894.2391s / 34109.2793 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0044
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0052
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2082.4,                last time consumption/overall running time: 1874.3102s / 35983.5894 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0038
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1887.0,                last time consumption/overall running time: 1701.6465s / 37685.2360 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0038
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0051
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1832.6,                last time consumption/overall running time: 1651.2992s / 39336.5352 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0046
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1791.2,                last time consumption/overall running time: 1613.8365s / 40950.3717 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0038
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0048
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1850.55,                last time consumption/overall running time: 1665.8428s / 42616.2145 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0037
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0044
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2006.3,                last time consumption/overall running time: 1807.7235s / 44423.9379 s
env0_first_0:                 episode reward: 1.9500,                 loss: 0.0041
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0049
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1840.75,                last time consumption/overall running time: 1658.0123s / 46081.9503 s
env0_first_0:                 episode reward: 3.0500,                 loss: 0.0041
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0051
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2096.75,                last time consumption/overall running time: 1884.8424s / 47966.7927 s
env0_first_0:                 episode reward: 3.3000,                 loss: 0.0040
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0045
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1896.6,                last time consumption/overall running time: 1712.2181s / 49679.0108 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0039
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0048
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1887.0,                last time consumption/overall running time: 1698.6299s / 51377.6407 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0038
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0042
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1819.8,                last time consumption/overall running time: 1638.5718s / 53016.2125 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0034
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0039
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1888.85,                last time consumption/overall running time: 1700.9484s / 54717.1609 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0038
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0044
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2029.9,                last time consumption/overall running time: 1825.9037s / 56543.0646 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0041
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0042
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2033.2,                last time consumption/overall running time: 1827.5246s / 58370.5891 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0040
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0040
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2074.6,                last time consumption/overall running time: 1859.9365s / 60230.5257 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0036
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0041
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2122.25,                last time consumption/overall running time: 1900.6976s / 62131.2233 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0040
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2198.35,                last time consumption/overall running time: 1970.7579s / 64101.9812 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0043
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0045
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2141.95,                last time consumption/overall running time: 1920.8484s / 66022.8295 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0043
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0047
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2604.05,                last time consumption/overall running time: 2334.0061s / 68356.8356 s
env0_first_0:                 episode reward: 4.1500,                 loss: 0.0041
env0_second_0:                 episode reward: -4.1500,                 loss: 0.0049
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2353.9,                last time consumption/overall running time: 2108.1608s / 70464.9965 s
env0_first_0:                 episode reward: 2.4500,                 loss: 0.0043
env0_second_0:                 episode reward: -2.4500,                 loss: 0.0050
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2173.7,                last time consumption/overall running time: 1946.9404s / 72411.9368 s
env0_first_0:                 episode reward: 3.2000,                 loss: 0.0044
env0_second_0:                 episode reward: -3.2000,                 loss: 0.0050
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2215.85,                last time consumption/overall running time: 1983.1125s / 74395.0493 s
env0_first_0:                 episode reward: 5.9000,                 loss: 0.0044
env0_second_0:                 episode reward: -5.9000,                 loss: 0.0052
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2171.55,                last time consumption/overall running time: 1944.8955s / 76339.9448 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0036
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0046
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2130.3,                last time consumption/overall running time: 1909.2965s / 78249.2414 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0037
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0046
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2349.6,                last time consumption/overall running time: 2105.0830s / 80354.3244 s
env0_first_0:                 episode reward: 5.1500,                 loss: 0.0044
env0_second_0:                 episode reward: -5.1500,                 loss: 0.0052
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2422.75,                last time consumption/overall running time: 2168.8905s / 82523.2149 s
env0_first_0:                 episode reward: 5.2000,                 loss: 0.0049
env0_second_0:                 episode reward: -5.2000,                 loss: 0.0060
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1925.6,                last time consumption/overall running time: 1725.7926s / 84249.0074 s
env0_first_0:                 episode reward: 7.2500,                 loss: 0.0046
env0_second_0:                 episode reward: -7.2500,                 loss: 0.0057
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2184.4,                last time consumption/overall running time: 1958.8472s / 86207.8546 s
env0_first_0:                 episode reward: 4.1000,                 loss: 0.0043
env0_second_0:                 episode reward: -4.1000,                 loss: 0.0051
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2036.9,                last time consumption/overall running time: 1823.4717s / 88031.3263 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0040
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0046
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1901.85,                last time consumption/overall running time: 1703.8777s / 89735.2040 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0033
env0_second_0:                 episode reward: -4.3500,                 loss: 0.0043
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2360.7,                last time consumption/overall running time: 2112.2553s / 91847.4593 s
env0_first_0:                 episode reward: 3.5500,                 loss: 0.0038
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0051
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2149.75,                last time consumption/overall running time: 1923.5479s / 93771.0072 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0058
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2230.8,                last time consumption/overall running time: 1991.6753s / 95762.6825 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0047
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0057
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2269.1,                last time consumption/overall running time: 2029.5512s / 97792.2337 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0041
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0048
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2260.8,                last time consumption/overall running time: 2021.9848s / 99814.2185 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0043
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0054
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2209.8,                last time consumption/overall running time: 1980.5353s / 101794.7538 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0044
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0052
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2221.6,                last time consumption/overall running time: 1987.5967s / 103782.3505 s
env0_first_0:                 episode reward: 2.8500,                 loss: 0.0044
env0_second_0:                 episode reward: -2.8500,                 loss: 0.0052
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2091.55,                last time consumption/overall running time: 1872.6840s / 105655.0345 s
env0_first_0:                 episode reward: 1.3500,                 loss: 0.0041
env0_second_0:                 episode reward: -1.3500,                 loss: 0.0052
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2363.25,                last time consumption/overall running time: 2059.4300s / 107714.4645 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0044
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0051
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2117.75,                last time consumption/overall running time: 1730.4444s / 109444.9089 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0043
env0_second_0:                 episode reward: -4.4500,                 loss: 0.0054
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2081.95,                last time consumption/overall running time: 1624.7414s / 111069.6504 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0046
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0053
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1904.65,                last time consumption/overall running time: 1461.9226s / 112531.5730 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0046
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0053
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1931.45,                last time consumption/overall running time: 1482.6114s / 114014.1844 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0044
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0054
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2395.9,                last time consumption/overall running time: 1840.8292s / 115855.0137 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0041
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0047
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2310.3,                last time consumption/overall running time: 1774.8161s / 117629.8298 s
env0_first_0:                 episode reward: 4.6000,                 loss: 0.0041
env0_second_0:                 episode reward: -4.6000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2249.0,                last time consumption/overall running time: 1730.5517s / 119360.3815 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0041
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0048
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 2015.7,                last time consumption/overall running time: 1550.4499s / 120910.8313 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0043
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0047
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2167.7,                last time consumption/overall running time: 1666.8073s / 122577.6386 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0045
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1948.35,                last time consumption/overall running time: 1495.6960s / 124073.3346 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0043
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0048
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1954.35,                last time consumption/overall running time: 1500.3201s / 125573.6548 s
env0_first_0:                 episode reward: 2.7000,                 loss: 0.0041
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 2098.65,                last time consumption/overall running time: 1613.3283s / 127186.9830 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0045
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0048
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1785.45,                last time consumption/overall running time: 1373.5103s / 128560.4933 s
env0_first_0:                 episode reward: 3.1500,                 loss: 0.0050
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0054
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2024.25,                last time consumption/overall running time: 1556.1010s / 130116.5943 s
env0_first_0:                 episode reward: 3.0500,                 loss: 0.0039
env0_second_0:                 episode reward: -3.0500,                 loss: 0.0042
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2105.05,                last time consumption/overall running time: 1619.0475s / 131735.6418 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0042
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0045
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2332.15,                last time consumption/overall running time: 1791.6946s / 133527.3364 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0045
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2096.85,                last time consumption/overall running time: 1612.9284s / 135140.2648 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0045
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0045
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2192.8,                last time consumption/overall running time: 1684.7847s / 136825.0495 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0044
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0046
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2135.3,                last time consumption/overall running time: 1640.8512s / 138465.9007 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0049
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0053
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1897.7,                last time consumption/overall running time: 1457.8665s / 139923.7672 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0052
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0055
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2162.85,                last time consumption/overall running time: 1660.3567s / 141584.1239 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0049
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0056
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1875.2,                last time consumption/overall running time: 1440.1867s / 143024.3107 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0049
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 2132.0,                last time consumption/overall running time: 1635.7389s / 144660.0496 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 2100.0,                last time consumption/overall running time: 1614.6778s / 146274.7274 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0041
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0046
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 2213.65,                last time consumption/overall running time: 1702.0954s / 147976.8228 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0044
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0048
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1915.45,                last time consumption/overall running time: 1468.2130s / 149445.0358 s
env0_first_0:                 episode reward: 2.0000,                 loss: 0.0043
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0047
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2091.0,                last time consumption/overall running time: 1605.3312s / 151050.3670 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0043
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0045
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 2159.35,                last time consumption/overall running time: 1657.1582s / 152707.5251 s
env0_first_0:                 episode reward: -1.2000,                 loss: 0.0046
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1969.15,                last time consumption/overall running time: 1509.5497s / 154217.0748 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0048
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0046
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1901.15,                last time consumption/overall running time: 1455.5565s / 155672.6313 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0043
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1795.75,                last time consumption/overall running time: 1378.1182s / 157050.7495 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0039
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 2008.7,                last time consumption/overall running time: 1535.7722s / 158586.5216 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0039
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0042
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 2180.0,                last time consumption/overall running time: 1665.2739s / 160251.7956 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0043
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0045
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1913.4,                last time consumption/overall running time: 1462.9252s / 161714.7207 s
env0_first_0:                 episode reward: 2.6000,                 loss: 0.0047
env0_second_0:                 episode reward: -2.6000,                 loss: 0.0049
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1962.1,                last time consumption/overall running time: 1495.3126s / 163210.0334 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0043
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1870.8,                last time consumption/overall running time: 1422.6455s / 164632.6789 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0043
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0048
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 2128.95,                last time consumption/overall running time: 1619.8449s / 166252.5238 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0039
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0045
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 2126.35,                last time consumption/overall running time: 1618.1730s / 167870.6967 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0042
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 2102.7,                last time consumption/overall running time: 1602.5517s / 169473.2485 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0043
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0046
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 2061.0,                last time consumption/overall running time: 1568.0870s / 171041.3355 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0044
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2144.55,                last time consumption/overall running time: 1632.8611s / 172674.1966 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0049
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0052
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 1948.65,                last time consumption/overall running time: 1484.0622s / 174158.2588 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0046
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0051
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 1969.4,                last time consumption/overall running time: 1500.9930s / 175659.2519 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0044
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0044
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 1863.8,                last time consumption/overall running time: 1417.5193s / 177076.7712 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0045
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0047
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 1811.75,                last time consumption/overall running time: 1380.0597s / 178456.8309 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0044
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0045
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 1841.2,                last time consumption/overall running time: 1400.6424s / 179857.4733 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0042
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 1920.55,                last time consumption/overall running time: 1462.5917s / 181320.0650 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0040
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 1957.2,                last time consumption/overall running time: 1490.3087s / 182810.3737 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0038
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0043
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 2017.1,                last time consumption/overall running time: 1534.9850s / 184345.3587 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0040
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0043
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 2033.7,                last time consumption/overall running time: 1545.9046s / 185891.2632 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0041
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0044
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 2048.8,                last time consumption/overall running time: 1554.9478s / 187446.2110 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0049
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 1878.1,                last time consumption/overall running time: 1427.9589s / 188874.1698 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0046
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0048
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 1946.0,                last time consumption/overall running time: 1481.5647s / 190355.7345 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0044
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0048
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 1751.15,                last time consumption/overall running time: 1330.4910s / 191686.2255 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0044
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0048
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 1801.6,                last time consumption/overall running time: 1370.0397s / 193056.2652 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0037
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0044
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 1700.25,                last time consumption/overall running time: 1291.5438s / 194347.8091 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0035
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0043
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 1974.55,                last time consumption/overall running time: 1497.4932s / 195845.3022 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0038
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0044
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 1889.55,                last time consumption/overall running time: 1433.7002s / 197279.0025 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0042
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0049
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 1865.15,                last time consumption/overall running time: 1418.5576s / 198697.5601 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0040
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0044
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 1941.15,                last time consumption/overall running time: 1475.4212s / 200172.9813 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.0038
env0_second_0:                 episode reward: 0.7000,                 loss: 0.0043
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 1729.6,                last time consumption/overall running time: 1315.5349s / 201488.5161 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0035
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0040
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 1901.5,                last time consumption/overall running time: 1442.0739s / 202930.5900 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0033
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0040
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 1890.25,                last time consumption/overall running time: 1431.5547s / 204362.1447 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0037
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0041
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 2000.95,                last time consumption/overall running time: 1519.4509s / 205881.5956 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0039
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0044
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2012.95,                last time consumption/overall running time: 1527.6710s / 207409.2666 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 2098.45,                last time consumption/overall running time: 1593.9122s / 209003.1788 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0041
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0046
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 2058.9,                last time consumption/overall running time: 1564.5919s / 210567.7708 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0042
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0046
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 1949.2,                last time consumption/overall running time: 1482.8005s / 212050.5713 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0036
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0040
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 2121.8,                last time consumption/overall running time: 1608.4953s / 213659.0666 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.0038
env0_second_0:                 episode reward: 1.1500,                 loss: 0.0041
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2062.25,                last time consumption/overall running time: 1565.5102s / 215224.5768 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0041
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0044
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 1992.75,                last time consumption/overall running time: 1510.0658s / 216734.6425 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0041
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0044
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2369.25,                last time consumption/overall running time: 1793.4499s / 218528.0924 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0041
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0044
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 2226.5,                last time consumption/overall running time: 1687.8774s / 220215.9698 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0043
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0045
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 2122.3,                last time consumption/overall running time: 1606.0081s / 221821.9780 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0042
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0045
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 2324.85,                last time consumption/overall running time: 1758.4810s / 223580.4590 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0045
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 2086.85,                last time consumption/overall running time: 1581.7727s / 225162.2316 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0043
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0048
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 1871.55,                last time consumption/overall running time: 1419.1195s / 226581.3511 s
env0_first_0:                 episode reward: 3.6500,                 loss: 0.0037
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0039
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 1877.15,                last time consumption/overall running time: 1418.8343s / 228000.1854 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0038
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0042
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 1731.0,                last time consumption/overall running time: 1311.4143s / 229311.5998 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0043
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 1980.85,                last time consumption/overall running time: 1499.5417s / 230811.1414 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0035
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0039
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 2049.0,                last time consumption/overall running time: 1554.6116s / 232365.7530 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0037
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0041
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 1864.05,                last time consumption/overall running time: 1413.8767s / 233779.6297 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0037
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0038
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 1880.45,                last time consumption/overall running time: 1425.0695s / 235204.6992 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0035
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0037
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 2048.0,                last time consumption/overall running time: 1552.3857s / 236757.0849 s
env0_first_0:                 episode reward: 2.1000,                 loss: 0.0039
env0_second_0:                 episode reward: -2.1000,                 loss: 0.0041
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 1995.4,                last time consumption/overall running time: 1511.7036s / 238268.7885 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0041
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0044
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 2055.75,                last time consumption/overall running time: 1557.7223s / 239826.5108 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.0041
env0_second_0:                 episode reward: 2.2000,                 loss: 0.0043
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 1797.4,                last time consumption/overall running time: 1361.1035s / 241187.6143 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0036
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0043
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 1939.65,                last time consumption/overall running time: 1469.9322s / 242657.5465 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0038
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0043
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 1953.1,                last time consumption/overall running time: 1478.2728s / 244135.8192 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0043
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0047
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 1832.05,                last time consumption/overall running time: 1384.8953s / 245520.7146 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0039
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0044
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2018.75,                last time consumption/overall running time: 1526.2230s / 247046.9376 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0043
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 1966.6,                last time consumption/overall running time: 1488.5392s / 248535.4768 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 1862.45,                last time consumption/overall running time: 1407.1394s / 249942.6162 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0045
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0045
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 1824.9,                last time consumption/overall running time: 1380.3447s / 251322.9609 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0042
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0040
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2126.35,                last time consumption/overall running time: 1608.8597s / 252931.8206 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0043
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0046
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 2106.0,                last time consumption/overall running time: 1594.4574s / 254526.2780 s
env0_first_0:                 episode reward: 2.4000,                 loss: 0.0044
env0_second_0:                 episode reward: -2.4000,                 loss: 0.0049
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 1961.15,                last time consumption/overall running time: 1483.3135s / 256009.5915 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0043
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0046
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 1733.9,                last time consumption/overall running time: 1310.2829s / 257319.8744 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.0045
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0049
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 1841.85,                last time consumption/overall running time: 1388.2413s / 258708.1157 s
env0_first_0:                 episode reward: 1.5000,                 loss: 0.0043
env0_second_0:                 episode reward: -1.5000,                 loss: 0.0048
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 2114.1,                last time consumption/overall running time: 1594.3152s / 260302.4309 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0040
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0045
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 2382.65,                last time consumption/overall running time: 1795.8759s / 262098.3068 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0044
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0047
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 2051.7,                last time consumption/overall running time: 1545.1213s / 263643.4281 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0045
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0046
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 2153.95,                last time consumption/overall running time: 1620.6886s / 265264.1167 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.0048
env0_second_0:                 episode reward: 0.4000,                 loss: 0.0047
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2199.7,                last time consumption/overall running time: 1657.1150s / 266921.2317 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0049
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0048
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 2134.35,                last time consumption/overall running time: 1606.9104s / 268528.1420 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.0052
env0_second_0:                 episode reward: -0.6000,                 loss: 0.0055
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 1922.95,                last time consumption/overall running time: 1446.3322s / 269974.4742 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0048
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0047
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 1976.0,                last time consumption/overall running time: 1486.8212s / 271461.2955 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0047
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0049
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 1901.95,                last time consumption/overall running time: 1429.8936s / 272891.1890 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0041
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0042
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 1902.45,                last time consumption/overall running time: 1432.9083s / 274324.0974 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0044
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 2036.05,                last time consumption/overall running time: 1529.8201s / 275853.9175 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0041
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0046
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 1867.25,                last time consumption/overall running time: 1401.4073s / 277255.3248 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0046
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 1797.15,                last time consumption/overall running time: 1348.1099s / 278603.4347 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.0045
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0048
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 1991.4,                last time consumption/overall running time: 1492.9926s / 280096.4274 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0043
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0045
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 2036.05,                last time consumption/overall running time: 1523.6022s / 281620.0295 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0045
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0048
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 2163.25,                last time consumption/overall running time: 1616.1168s / 283236.1464 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0047
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0053
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 2132.75,                last time consumption/overall running time: 1594.3229s / 284830.4693 s
env0_first_0:                 episode reward: 0.8500,                 loss: 0.0042
env0_second_0:                 episode reward: -0.8500,                 loss: 0.0047
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 2120.0,                last time consumption/overall running time: 1582.4270s / 286412.8962 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0044
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0052
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 2062.15,                last time consumption/overall running time: 1539.2657s / 287952.1619 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0052
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0057
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 2191.2,                last time consumption/overall running time: 1635.3119s / 289587.4738 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0050
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0053
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 1931.5,                last time consumption/overall running time: 1442.7629s / 291030.2367 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0046
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0048
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 1784.6,                last time consumption/overall running time: 1332.8706s / 292363.1073 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0038
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0040
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 1735.65,                last time consumption/overall running time: 1295.1941s / 293658.3014 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0037
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0038
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 1610.4,                last time consumption/overall running time: 1202.9718s / 294861.2731 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0036
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0036
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 1691.0,                last time consumption/overall running time: 1260.8021s / 296122.0752 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0030
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0034
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 1791.2,                last time consumption/overall running time: 1337.2402s / 297459.3154 s
env0_first_0:                 episode reward: 7.6000,                 loss: 0.0038
env0_second_0:                 episode reward: -7.6000,                 loss: 0.0044
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 1684.0,                last time consumption/overall running time: 1254.7495s / 298714.0649 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0039
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0043
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 1804.05,                last time consumption/overall running time: 1344.4977s / 300058.5626 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0031
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0033
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 1691.65,                last time consumption/overall running time: 1259.9084s / 301318.4710 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.0032
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0034
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 2210.25,                last time consumption/overall running time: 1645.1686s / 302963.6396 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.0034
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0039
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 2146.25,                last time consumption/overall running time: 1597.0339s / 304560.6734 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0050
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0051
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 1964.1,                last time consumption/overall running time: 1460.6046s / 306021.2780 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0050
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0053
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2006.9,                last time consumption/overall running time: 1494.1905s / 307515.4685 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0044
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0051
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2087.55,                last time consumption/overall running time: 1552.8535s / 309068.3220 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0037
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0042
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2230.95,                last time consumption/overall running time: 1657.9666s / 310726.2887 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0041
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 2318.35,                last time consumption/overall running time: 1724.3818s / 312450.6705 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.0039
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0043
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2155.9,                last time consumption/overall running time: 1604.2827s / 314054.9532 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0038
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0042
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 2050.95,                last time consumption/overall running time: 1523.8285s / 315578.7816 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0039
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0040
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 1746.7,                last time consumption/overall running time: 1300.6044s / 316879.3861 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0039
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0043
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 1759.2,                last time consumption/overall running time: 1308.6752s / 318188.0613 s
env0_first_0:                 episode reward: -0.9500,                 loss: 0.0037
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0042
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 1836.05,                last time consumption/overall running time: 1365.3834s / 319553.4446 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0033
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0038
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 1786.55,                last time consumption/overall running time: 1327.0662s / 320880.5108 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0035
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0041
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 1993.1,                last time consumption/overall running time: 1480.0891s / 322360.5999 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0034
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0036
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 1893.1,                last time consumption/overall running time: 1405.6826s / 323766.2826 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0037
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0037
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 1883.05,                last time consumption/overall running time: 1400.3703s / 325166.6529 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0037
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0041
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 1790.9,                last time consumption/overall running time: 1331.4875s / 326498.1404 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0047
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1850.05,                last time consumption/overall running time: 1373.1385s / 327871.2790 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0043
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0048
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 1796.95,                last time consumption/overall running time: 1334.8364s / 329206.1154 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0049
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 2022.15,                last time consumption/overall running time: 1501.7075s / 330707.8229 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0040
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0044
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 1981.5,                last time consumption/overall running time: 1470.8135s / 332178.6364 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.0036
env0_second_0:                 episode reward: -0.2000,                 loss: 0.0041
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 2186.25,                last time consumption/overall running time: 1622.7812s / 333801.4176 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0029
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0034
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 1933.8,                last time consumption/overall running time: 1437.2136s / 335238.6312 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0032
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0036
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 2221.85,                last time consumption/overall running time: 1649.3214s / 336887.9526 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0037
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0040
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1977.7,                last time consumption/overall running time: 1465.9991s / 338353.9516 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0040
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0043
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 2045.55,                last time consumption/overall running time: 1515.9619s / 339869.9136 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0035
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0040
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 2064.15,                last time consumption/overall running time: 1530.8832s / 341400.7968 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0037
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0040
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 1810.3,                last time consumption/overall running time: 1342.9521s / 342743.7489 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0035
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0036
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 1887.8,                last time consumption/overall running time: 1398.8832s / 344142.6321 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0034
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0035
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2139.65,                last time consumption/overall running time: 1587.5401s / 345730.1722 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0033
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0038
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 1937.2,                last time consumption/overall running time: 1435.9113s / 347166.0835 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0036
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0041
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 1830.05,                last time consumption/overall running time: 1465.9067s / 348631.9903 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0034
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0038
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 1773.15,                last time consumption/overall running time: 1603.7109s / 350235.7011 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.0030
env0_second_0:                 episode reward: 1.1000,                 loss: 0.0033
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 1886.3,                last time consumption/overall running time: 1705.9035s / 351941.6046 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.0029
env0_second_0:                 episode reward: 1.7500,                 loss: 0.0033
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 2119.85,                last time consumption/overall running time: 1915.6518s / 353857.2564 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0034
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0037
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 1962.35,                last time consumption/overall running time: 1773.2910s / 355630.5474 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0042
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0048
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 2022.2,                last time consumption/overall running time: 1830.6580s / 357461.2054 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 2067.05,                last time consumption/overall running time: 1869.2282s / 359330.4336 s
env0_first_0:                 episode reward: -4.9000,                 loss: 0.0035
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0039
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 1957.45,                last time consumption/overall running time: 1768.7299s / 361099.1635 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0039
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0043
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 1906.25,                last time consumption/overall running time: 1720.6080s / 362819.7715 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0041
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0042
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 1685.0,                last time consumption/overall running time: 1521.4551s / 364341.2266 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.0035
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0036
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 1775.5,                last time consumption/overall running time: 1604.1178s / 365945.3443 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0036
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0039
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 1772.0,                last time consumption/overall running time: 1597.8765s / 367543.2209 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0041
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 1864.4,                last time consumption/overall running time: 1680.9728s / 369224.1937 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0034
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0036
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 2028.45,                last time consumption/overall running time: 1829.0294s / 371053.2231 s
env0_first_0:                 episode reward: -1.6500,                 loss: 0.0035
env0_second_0:                 episode reward: 1.6500,                 loss: 0.0040
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 2100.45,                last time consumption/overall running time: 1892.0134s / 372945.2365 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0044
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0051
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 1678.95,                last time consumption/overall running time: 1511.7914s / 374457.0279 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0045
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0047
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 1703.7,                last time consumption/overall running time: 1534.4509s / 375991.4788 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0033
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0034
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 1767.4,                last time consumption/overall running time: 1590.1088s / 377581.5876 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0031
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0032
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 1818.85,                last time consumption/overall running time: 1637.0677s / 379218.6553 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0033
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0036
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 1829.1,                last time consumption/overall running time: 1646.5957s / 380865.2510 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0031
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0034
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 2117.0,                last time consumption/overall running time: 1901.3660s / 382766.6170 s
env0_first_0:                 episode reward: -8.6500,                 loss: 0.0039
env0_second_0:                 episode reward: 8.6500,                 loss: 0.0040
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 2059.1,                last time consumption/overall running time: 1852.5239s / 384619.1409 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0046
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0049
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 1854.45,                last time consumption/overall running time: 1666.5715s / 386285.7124 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0042
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0040
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 1810.7,                last time consumption/overall running time: 1628.6633s / 387914.3757 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0035
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0037
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 1795.0,                last time consumption/overall running time: 1614.1238s / 389528.4995 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0031
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0032
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 1949.65,                last time consumption/overall running time: 1751.9718s / 391280.4712 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0034
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0033
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 1736.95,                last time consumption/overall running time: 1561.2481s / 392841.7193 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0037
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0040
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 1747.8,                last time consumption/overall running time: 1571.7407s / 394413.4601 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0034
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0035
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 1849.0,                last time consumption/overall running time: 1662.6827s / 396076.1428 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0033
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0036
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 1888.7,                last time consumption/overall running time: 1697.5540s / 397773.6968 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0036
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0040
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 2067.4,                last time consumption/overall running time: 1858.3514s / 399632.0482 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0039
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0042
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 2153.3,                last time consumption/overall running time: 1931.0811s / 401563.1292 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0037
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0042
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 1997.2,                last time consumption/overall running time: 1790.9059s / 403354.0351 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0038
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0041
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 2117.3,                last time consumption/overall running time: 1898.5173s / 405252.5524 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0036
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0039
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 1830.75,                last time consumption/overall running time: 1641.2757s / 406893.8281 s
env0_first_0:                 episode reward: -1.9000,                 loss: 0.0039
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0044
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 1871.0,                last time consumption/overall running time: 1678.2412s / 408572.0693 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0035
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0038
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 1897.1,                last time consumption/overall running time: 1697.2315s / 410269.3008 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0046
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 1953.45,                last time consumption/overall running time: 1746.5279s / 412015.8287 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.0042
env0_second_0:                 episode reward: 2.4500,                 loss: 0.0047
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 1871.05,                last time consumption/overall running time: 1671.4005s / 413687.2292 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0041
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0043
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 1988.55,                last time consumption/overall running time: 1777.3991s / 415464.6283 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0035
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0039
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 1901.3,                last time consumption/overall running time: 1699.8408s / 417164.4691 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0038
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0042
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 1863.5,                last time consumption/overall running time: 1664.7239s / 418829.1930 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0044
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 1940.1,                last time consumption/overall running time: 1730.0489s / 420559.2419 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0040
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0043
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 1842.3,                last time consumption/overall running time: 1643.0056s / 422202.2475 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0042
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0043
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 1734.75,                last time consumption/overall running time: 1550.9090s / 423753.1565 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0033
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0035
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 1812.0,                last time consumption/overall running time: 1619.4339s / 425372.5904 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0032
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 1922.7,                last time consumption/overall running time: 1714.7265s / 427087.3169 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0034
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0035
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 1825.05,                last time consumption/overall running time: 1629.4451s / 428716.7620 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0039
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0041
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 1768.0,                last time consumption/overall running time: 1733.3049s / 430450.0669 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0037
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0039
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 1745.5,                last time consumption/overall running time: 1788.8368s / 432238.9037 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0031
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0036
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 1677.7,                last time consumption/overall running time: 1719.1852s / 433958.0889 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0036
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0039
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 1671.9,                last time consumption/overall running time: 1712.0724s / 435670.1614 s
env0_first_0:                 episode reward: 1.1000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0043
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 1761.7,                last time consumption/overall running time: 1802.9592s / 437473.1205 s
env0_first_0:                 episode reward: 1.8000,                 loss: 0.0039
env0_second_0:                 episode reward: -1.8000,                 loss: 0.0043
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 1659.2,                last time consumption/overall running time: 1698.4856s / 439171.6061 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0037
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0040
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 1755.85,                last time consumption/overall running time: 1796.7997s / 440968.4059 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0061
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0039
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 1784.4,                last time consumption/overall running time: 1830.6709s / 442799.0767 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0052
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0040
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 1680.05,                last time consumption/overall running time: 1724.3813s / 444523.4580 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0045
env0_second_0:                 episode reward: 6.3500,                 loss: 0.0043
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 2233.85,                last time consumption/overall running time: 2292.5011s / 446815.9591 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0044
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0046
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 1764.1,                last time consumption/overall running time: 1808.5397s / 448624.4989 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0040
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0044
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 1745.15,                last time consumption/overall running time: 1787.9406s / 450412.4395 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.0036
env0_second_0:                 episode reward: 1.8000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 2070.55,                last time consumption/overall running time: 2118.0878s / 452530.5273 s
env0_first_0:                 episode reward: 8.9500,                 loss: 0.0036
env0_second_0:                 episode reward: -8.9500,                 loss: 0.0037
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 2012.3,                last time consumption/overall running time: 2059.1990s / 454589.7263 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0053
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0052
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 1872.25,                last time consumption/overall running time: 1918.3608s / 456508.0871 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 0.7500,                 loss: 0.0044
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 1798.55,                last time consumption/overall running time: 1842.8616s / 458350.9487 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0047
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 1797.3,                last time consumption/overall running time: 1844.4077s / 460195.3564 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0050
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0054
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 1827.55,                last time consumption/overall running time: 1871.1984s / 462066.5548 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0047
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0049
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 1834.4,                last time consumption/overall running time: 1877.8278s / 463944.3826 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0037
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0038
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 1772.0,                last time consumption/overall running time: 1814.0203s / 465758.4029 s
env0_first_0:                 episode reward: -1.5000,                 loss: 0.0036
env0_second_0:                 episode reward: 1.5000,                 loss: 0.0036
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 1976.6,                last time consumption/overall running time: 2023.2917s / 467781.6945 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0037
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0040
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 1873.9,                last time consumption/overall running time: 1917.0630s / 469698.7575 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0042
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0046
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 2026.55,                last time consumption/overall running time: 2080.5421s / 471779.2996 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0046
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0045
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 2182.05,                last time consumption/overall running time: 2297.6421s / 474076.9417 s
env0_first_0:                 episode reward: 2.2000,                 loss: 0.0040
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0041
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 2238.5,                last time consumption/overall running time: 2358.9132s / 476435.8549 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0046
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0048
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 2127.15,                last time consumption/overall running time: 2240.6253s / 478676.4802 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0044
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 2031.4,                last time consumption/overall running time: 2141.2920s / 480817.7723 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0053
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 1943.8,                last time consumption/overall running time: 2043.2128s / 482860.9851 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0044
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 1877.7,                last time consumption/overall running time: 1979.6093s / 484840.5943 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0038
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0040
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 2037.25,                last time consumption/overall running time: 2146.3363s / 486986.9307 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0043
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0045
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 1954.85,                last time consumption/overall running time: 1992.5447s / 488979.4753 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0041
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0044
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 1798.4,                last time consumption/overall running time: 1797.6212s / 490777.0965 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0039
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0044
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 2149.75,                last time consumption/overall running time: 2145.5274s / 492922.6239 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0037
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0038
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 2144.1,                last time consumption/overall running time: 2144.7271s / 495067.3510 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0039
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0041
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 2025.0,                last time consumption/overall running time: 2020.9047s / 497088.2557 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0038
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0040
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 1863.75,                last time consumption/overall running time: 1858.8044s / 498947.0601 s
env0_first_0:                 episode reward: -8.6500,                 loss: 0.0044
env0_second_0:                 episode reward: 8.6500,                 loss: 0.0045
env1_first_0:                 episode reward: -9.6000,                 loss: nan
env1_second_0:                 episode reward: 9.6000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 2189.75,                last time consumption/overall running time: 2185.5274s / 501132.5875 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0045
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0046
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 2147.5,                last time consumption/overall running time: 2137.5267s / 503270.1143 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0045
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0046
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 2160.75,                last time consumption/overall running time: 2149.2607s / 505419.3749 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0051
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0054
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 2258.95,                last time consumption/overall running time: 2245.6935s / 507665.0684 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0051
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0054
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 1995.95,                last time consumption/overall running time: 1984.3002s / 509649.3687 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0049
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0052
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 1911.6,                last time consumption/overall running time: 1900.5300s / 511549.8987 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0050
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 1761.7,                last time consumption/overall running time: 1751.6871s / 513301.5858 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0042
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0047
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 1760.55,                last time consumption/overall running time: 1750.9150s / 515052.5008 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0048
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0053
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 1816.65,                last time consumption/overall running time: 1808.2065s / 516860.7073 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0048
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 1846.35,                last time consumption/overall running time: 1836.6942s / 518697.4015 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0041
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0044
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 1766.8,                last time consumption/overall running time: 1758.3371s / 520455.7386 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.0042
env0_second_0:                 episode reward: 1.3500,                 loss: 0.0039
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 1713.3,                last time consumption/overall running time: 1705.1696s / 522160.9082 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.0031
env0_second_0:                 episode reward: 0.2000,                 loss: 0.0033
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 2110.7,                last time consumption/overall running time: 2094.2331s / 524255.1413 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0049
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0035
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 2193.1,                last time consumption/overall running time: 2177.7799s / 526432.9212 s
env0_first_0:                 episode reward: -12.5000,                 loss: 0.0073
env0_second_0:                 episode reward: 12.5000,                 loss: 0.0049
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 2004.0,                last time consumption/overall running time: 1998.7431s / 528431.6643 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0054
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0052
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 1980.6,                last time consumption/overall running time: 1966.5012s / 530398.1655 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0044
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0045
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 1874.05,                last time consumption/overall running time: 1856.3786s / 532254.5441 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0040
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0046
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 2148.85,                last time consumption/overall running time: 2131.5294s / 534386.0734 s
env0_first_0:                 episode reward: -6.9500,                 loss: 0.0046
env0_second_0:                 episode reward: 6.9500,                 loss: 0.0050
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 1940.5,                last time consumption/overall running time: 1924.0172s / 536310.0906 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.0044
env0_second_0:                 episode reward: 2.0000,                 loss: 0.0047
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 1964.4,                last time consumption/overall running time: 1950.6539s / 538260.7445 s
env0_first_0:                 episode reward: -9.6000,                 loss: 0.0039
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0043
env1_first_0:                 episode reward: -9.4000,                 loss: nan
env1_second_0:                 episode reward: 9.4000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 1781.1,                last time consumption/overall running time: 1766.1143s / 540026.8588 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0043
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0048
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 1868.35,                last time consumption/overall running time: 1851.0863s / 541877.9451 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0039
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0044
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 1741.8,                last time consumption/overall running time: 1728.2840s / 543606.2292 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0040
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0047
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 1737.6,                last time consumption/overall running time: 1630.5064s / 545236.7355 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0028
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0034
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 2031.45,                last time consumption/overall running time: 1873.1485s / 547109.8840 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0034
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0037
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 1737.6,                last time consumption/overall running time: 1532.5653s / 548642.4493 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0040
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0046
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 1841.55,                last time consumption/overall running time: 1623.6331s / 550266.0824 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0032
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0038
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 1922.05,                last time consumption/overall running time: 1692.0239s / 551958.1063 s
env0_first_0:                 episode reward: -8.3500,                 loss: 0.0040
env0_second_0:                 episode reward: 8.3500,                 loss: 0.0044
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 1794.25,                last time consumption/overall running time: 1578.0589s / 553536.1651 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0041
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0046
env1_first_0:                 episode reward: -5.0000,                 loss: nan
env1_second_0:                 episode reward: 5.0000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 1905.85,                last time consumption/overall running time: 1675.0447s / 555211.2098 s
env0_first_0:                 episode reward: -8.1500,                 loss: 0.0039
env0_second_0:                 episode reward: 8.1500,                 loss: 0.0043
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 1844.35,                last time consumption/overall running time: 1621.2939s / 556832.5038 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0040
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0045
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 1745.9,                last time consumption/overall running time: 1535.6158s / 558368.1196 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.0032
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0037
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 1824.45,                last time consumption/overall running time: 1604.9394s / 559973.0589 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0034
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0039
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 1845.55,                last time consumption/overall running time: 1624.4763s / 561597.5352 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0031
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0038
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 2044.6,                last time consumption/overall running time: 1795.2850s / 563392.8202 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0025
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0031
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 1969.75,                last time consumption/overall running time: 1735.9074s / 565128.7276 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0022
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0026
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 1994.85,                last time consumption/overall running time: 1761.0073s / 566889.7349 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0027
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0031
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 1934.4,                last time consumption/overall running time: 1703.9326s / 568593.6675 s
env0_first_0:                 episode reward: -10.6000,                 loss: 0.0040
env0_second_0:                 episode reward: 10.6000,                 loss: 0.0042
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 1878.7,                last time consumption/overall running time: 1629.1846s / 570222.8521 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0052
env0_second_0:                 episode reward: 7.9000,                 loss: 0.0054
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 1771.55,                last time consumption/overall running time: 1466.7166s / 571689.5687 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0039
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0043
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 1766.15,                last time consumption/overall running time: 1460.1223s / 573149.6909 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0031
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0035
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 1801.45,                last time consumption/overall running time: 1489.1179s / 574638.8089 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0028
env0_second_0:                 episode reward: 2.0500,                 loss: 0.0032
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 2070.45,                last time consumption/overall running time: 1709.3887s / 576348.1975 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0031
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0036
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 1894.05,                last time consumption/overall running time: 1565.0851s / 577913.2826 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.0040
env0_second_0:                 episode reward: 0.8000,                 loss: 0.0045
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 1939.75,                last time consumption/overall running time: 1604.4008s / 579517.6834 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.0039
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0044
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 2231.35,                last time consumption/overall running time: 1842.6191s / 581360.3026 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0039
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0043
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 2132.2,                last time consumption/overall running time: 1761.0397s / 583121.3423 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0039
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0044
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 2030.75,                last time consumption/overall running time: 1678.9206s / 584800.2628 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0042
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0046
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 1893.7,                last time consumption/overall running time: 1559.3913s / 586359.6542 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.0043
env0_second_0:                 episode reward: 1.6000,                 loss: 0.0049
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 2064.3,                last time consumption/overall running time: 1701.2882s / 588060.9423 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0044
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0049
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 1957.7,                last time consumption/overall running time: 1615.1922s / 589676.1345 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0038
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0042
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 2027.5,                last time consumption/overall running time: 1671.8355s / 591347.9700 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0048
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0050
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 1991.25,                last time consumption/overall running time: 1640.8936s / 592988.8636 s
env0_first_0:                 episode reward: -5.3500,                 loss: 0.0050
env0_second_0:                 episode reward: 5.3500,                 loss: 0.0049
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 2067.65,                last time consumption/overall running time: 1699.9471s / 594688.8107 s
env0_first_0:                 episode reward: -11.0500,                 loss: 0.0044
env0_second_0:                 episode reward: 11.0500,                 loss: 0.0049
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 2055.35,                last time consumption/overall running time: 1592.0248s / 596280.8355 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0040
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0047
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 1983.25,                last time consumption/overall running time: 1455.4562s / 597736.2916 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0036
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0038
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 2098.4,                last time consumption/overall running time: 1540.8088s / 599277.1005 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0042
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0041
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 2117.45,                last time consumption/overall running time: 1554.9911s / 600832.0915 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0042
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0045
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 1877.95,                last time consumption/overall running time: 1377.4552s / 602209.5467 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0034
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0045
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 2077.75,                last time consumption/overall running time: 1521.7992s / 603731.3459 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0037
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0048
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 1873.65,                last time consumption/overall running time: 1373.5055s / 605104.8514 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.0038
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0042
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 1961.15,                last time consumption/overall running time: 1430.2509s / 606535.1023 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0036
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0038
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 2029.15,                last time consumption/overall running time: 1443.6491s / 607978.7514 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0031
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0035
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 2149.05,                last time consumption/overall running time: 1378.7906s / 609357.5419 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0030
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0035
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 2055.7,                last time consumption/overall running time: 1293.5543s / 610651.0963 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.0039
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0040
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 1783.3,                last time consumption/overall running time: 1123.6677s / 611774.7640 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0037
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0037
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 1897.0,                last time consumption/overall running time: 1195.0752s / 612969.8392 s
env0_first_0:                 episode reward: -8.3000,                 loss: 0.0038
env0_second_0:                 episode reward: 8.3000,                 loss: 0.0040
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 2046.25,                last time consumption/overall running time: 1289.1888s / 614259.0281 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0039
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0043
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 2184.7,                last time consumption/overall running time: 1376.1024s / 615635.1304 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0034
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0038
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 1962.0,                last time consumption/overall running time: 1236.3913s / 616871.5218 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.0039
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0043
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 1874.65,                last time consumption/overall running time: 1181.5534s / 618053.0752 s
env0_first_0:                 episode reward: 2.5000,                 loss: 0.0037
env0_second_0:                 episode reward: -2.5000,                 loss: 0.0039
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 1781.4,                last time consumption/overall running time: 1119.7744s / 619172.8496 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.0031
env0_second_0:                 episode reward: 1.9500,                 loss: 0.0031
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 1802.85,                last time consumption/overall running time: 1125.4445s / 620298.2941 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.0028
env0_second_0:                 episode reward: -0.2500,                 loss: 0.0031
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 1779.85,                last time consumption/overall running time: 1050.0772s / 621348.3712 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.0025
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0029
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 1917.1,                last time consumption/overall running time: 1131.6297s / 622480.0009 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0030
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0033
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 2071.75,                last time consumption/overall running time: 1224.0175s / 623704.0184 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0033
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0037
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 2293.05,                last time consumption/overall running time: 1350.3432s / 625054.3616 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0032
env0_second_0:                 episode reward: 3.1500,                 loss: 0.0034
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 2362.4,                last time consumption/overall running time: 1395.0578s / 626449.4194 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0030
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0031
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 2184.75,                last time consumption/overall running time: 1287.6878s / 627737.1072 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0035
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0038
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 1968.65,                last time consumption/overall running time: 1163.7045s / 628900.8117 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0037
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0039
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 1995.3,                last time consumption/overall running time: 1177.4753s / 630078.2870 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0029
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0033
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 2246.55,                last time consumption/overall running time: 1324.2808s / 631402.5678 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0034
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0035
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 2258.85,                last time consumption/overall running time: 1294.3626s / 632696.9305 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0036
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0040
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 1929.15,                last time consumption/overall running time: 1061.0935s / 633758.0240 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0030
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0033
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 2220.4,                last time consumption/overall running time: 1214.6588s / 634972.6828 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.0033
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0037
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 1852.3,                last time consumption/overall running time: 1014.4520s / 635987.1348 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0036
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0039
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 1862.65,                last time consumption/overall running time: 1019.6760s / 637006.8108 s
env0_first_0:                 episode reward: 1.0500,                 loss: 0.0027
env0_second_0:                 episode reward: -1.0500,                 loss: 0.0029
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 2416.35,                last time consumption/overall running time: 1266.1802s / 638272.9909 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0021
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0025
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 2258.8,                last time consumption/overall running time: 1145.7068s / 639418.6978 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0028
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0035
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 2409.7,                last time consumption/overall running time: 1220.7236s / 640639.4214 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0027
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0034
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 2284.55,                last time consumption/overall running time: 1158.6920s / 641798.1134 s
env0_first_0:                 episode reward: -8.9500,                 loss: 0.0051
env0_second_0:                 episode reward: 8.9500,                 loss: 0.0039
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 2364.15,                last time consumption/overall running time: 1195.2882s / 642993.4015 s
env0_first_0:                 episode reward: -10.6000,                 loss: 0.0037
env0_second_0:                 episode reward: 10.6000,                 loss: 0.0043
env1_first_0:                 episode reward: -12.2500,                 loss: nan
env1_second_0:                 episode reward: 12.2500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 2210.45,                last time consumption/overall running time: 1117.5932s / 644110.9948 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0038
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0045
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 2435.75,                last time consumption/overall running time: 1229.9894s / 645340.9841 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0033
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0039
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 2539.75,                last time consumption/overall running time: 1279.1433s / 646620.1274 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0035
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0042
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 2430.95,                last time consumption/overall running time: 1227.5969s / 647847.7243 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0036
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0040
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 2426.55,                last time consumption/overall running time: 1218.3259s / 649066.0502 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0031
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0036
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 2638.45,                last time consumption/overall running time: 1326.9880s / 650393.0382 s
env0_first_0:                 episode reward: -9.3000,                 loss: 0.0033
env0_second_0:                 episode reward: 9.3000,                 loss: 0.0038
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 2515.6,                last time consumption/overall running time: 1263.8700s / 651656.9082 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0032
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0037
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 2853.25,                last time consumption/overall running time: 1431.2900s / 653088.1982 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0032
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0038
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 2828.2,                last time consumption/overall running time: 1420.5179s / 654508.7161 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0031
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0036
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 2921.9,                last time consumption/overall running time: 1467.9506s / 655976.6667 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0032
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0035
env1_first_0:                 episode reward: -8.3500,                 loss: nan
env1_second_0:                 episode reward: 8.3500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 2841.6,                last time consumption/overall running time: 1427.7689s / 657404.4357 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0031
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0035
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 2351.3,                last time consumption/overall running time: 1181.6770s / 658586.1127 s
env0_first_0:                 episode reward: -11.6500,                 loss: 0.0041
env0_second_0:                 episode reward: 11.6500,                 loss: 0.0042
env1_first_0:                 episode reward: -10.2000,                 loss: nan
env1_second_0:                 episode reward: 10.2000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 2500.3,                last time consumption/overall running time: 1257.4583s / 659843.5710 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0037
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0042
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 2598.05,                last time consumption/overall running time: 1304.7180s / 661148.2890 s
env0_first_0:                 episode reward: -1.8500,                 loss: 0.0029
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0033
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 2770.8,                last time consumption/overall running time: 1389.9105s / 662538.1995 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0027
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0031
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 2314.85,                last time consumption/overall running time: 1162.7102s / 663700.9097 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0027
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0030
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 2537.45,                last time consumption/overall running time: 1273.2735s / 664974.1832 s
env0_first_0:                 episode reward: -8.3000,                 loss: 0.0026
env0_second_0:                 episode reward: 8.3000,                 loss: 0.0030
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 2391.05,                last time consumption/overall running time: 1196.6551s / 666170.8383 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0027
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0032
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 2528.15,                last time consumption/overall running time: 1266.8521s / 667437.6904 s
env0_first_0:                 episode reward: -11.5500,                 loss: 0.0025
env0_second_0:                 episode reward: 11.5500,                 loss: 0.0033
env1_first_0:                 episode reward: -10.3500,                 loss: nan
env1_second_0:                 episode reward: 10.3500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 2758.1,                last time consumption/overall running time: 1381.0029s / 668818.6932 s
env0_first_0:                 episode reward: -8.2500,                 loss: 0.0027
env0_second_0:                 episode reward: 8.2500,                 loss: 0.0032
env1_first_0:                 episode reward: -10.2500,                 loss: nan
env1_second_0:                 episode reward: 10.2500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 2864.85,                last time consumption/overall running time: 1435.1125s / 670253.8057 s
env0_first_0:                 episode reward: -10.3000,                 loss: 0.0030
env0_second_0:                 episode reward: 10.3000,                 loss: 0.0034
env1_first_0:                 episode reward: -13.1000,                 loss: nan
env1_second_0:                 episode reward: 13.1000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 2289.8,                last time consumption/overall running time: 1149.1934s / 671402.9991 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0026
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0029
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 2443.25,                last time consumption/overall running time: 1224.9822s / 672627.9813 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0026
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0030
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 2384.0,                last time consumption/overall running time: 1193.6904s / 673821.6717 s
env0_first_0:                 episode reward: -5.6000,                 loss: 0.0027
env0_second_0:                 episode reward: 5.6000,                 loss: 0.0032
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 2629.8,                last time consumption/overall running time: 1313.9672s / 675135.6389 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0024
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0031
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 2623.85,                last time consumption/overall running time: 1314.7254s / 676450.3643 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0028
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0033
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 2552.25,                last time consumption/overall running time: 1279.1292s / 677729.4935 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.0035
env0_second_0:                 episode reward: 10.9000,                 loss: 0.0038
env1_first_0:                 episode reward: -12.3000,                 loss: nan
env1_second_0:                 episode reward: 12.3000,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 2536.7,                last time consumption/overall running time: 1271.2261s / 679000.7196 s
env0_first_0:                 episode reward: -9.8000,                 loss: 0.0043
env0_second_0:                 episode reward: 9.8000,                 loss: 0.0048
env1_first_0:                 episode reward: -10.6500,                 loss: nan
env1_second_0:                 episode reward: 10.6500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 2220.3,                last time consumption/overall running time: 1113.2678s / 680113.9874 s
env0_first_0:                 episode reward: -7.9000,                 loss: 0.0033
env0_second_0:                 episode reward: 7.9000,                 loss: 0.0034
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 2289.1,                last time consumption/overall running time: 1146.6103s / 681260.5977 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0033
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0034
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 2520.15,                last time consumption/overall running time: 1260.2791s / 682520.8768 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0030
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0034
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 2297.5,                last time consumption/overall running time: 1148.8086s / 683669.6854 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0029
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0037
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 2236.4,                last time consumption/overall running time: 1122.5520s / 684792.2374 s
env0_first_0:                 episode reward: -10.1500,                 loss: 0.0038
env0_second_0:                 episode reward: 10.1500,                 loss: 0.0038
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 2344.6,                last time consumption/overall running time: 1172.7802s / 685965.0176 s
env0_first_0:                 episode reward: -9.6000,                 loss: 0.0056
env0_second_0:                 episode reward: 9.6000,                 loss: 0.0041
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 2524.5,                last time consumption/overall running time: 1268.3925s / 687233.4102 s
env0_first_0:                 episode reward: -11.5500,                 loss: 0.0329
env0_second_0:                 episode reward: 11.5500,                 loss: 0.0046
env1_first_0:                 episode reward: -11.4500,                 loss: nan
env1_second_0:                 episode reward: 11.4500,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 2334.45,                last time consumption/overall running time: 1167.8395s / 688401.2496 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0435
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0040
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 2346.2,                last time consumption/overall running time: 1172.8505s / 689574.1001 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0272
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0039
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 2707.25,                last time consumption/overall running time: 1352.5697s / 690926.6698 s
env0_first_0:                 episode reward: -16.4000,                 loss: 0.0142
env0_second_0:                 episode reward: 16.4000,                 loss: 0.0038
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 2560.15,                last time consumption/overall running time: 1276.1742s / 692202.8440 s
env0_first_0:                 episode reward: -11.1500,                 loss: 0.0119
env0_second_0:                 episode reward: 11.1500,                 loss: 0.0045
env1_first_0:                 episode reward: -12.1000,                 loss: nan
env1_second_0:                 episode reward: 12.1000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 2566.15,                last time consumption/overall running time: 1286.7848s / 693489.6288 s
env0_first_0:                 episode reward: -2.6000,                 loss: 0.0072
env0_second_0:                 episode reward: 2.6000,                 loss: 0.0038
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 2152.25,                last time consumption/overall running time: 1080.9438s / 694570.5726 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0089
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0040
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 2498.6,                last time consumption/overall running time: 1257.3358s / 695827.9084 s
env0_first_0:                 episode reward: -9.6500,                 loss: 0.0077
env0_second_0:                 episode reward: 9.6500,                 loss: 0.0043
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 2462.6,                last time consumption/overall running time: 1229.0549s / 697056.9633 s
env0_first_0:                 episode reward: -16.7000,                 loss: 0.0086
env0_second_0:                 episode reward: 16.7000,                 loss: 0.0051
env1_first_0:                 episode reward: -17.0000,                 loss: nan
env1_second_0:                 episode reward: 17.0000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 2248.05,                last time consumption/overall running time: 1122.2266s / 698179.1899 s
env0_first_0:                 episode reward: -9.3500,                 loss: 0.0061
env0_second_0:                 episode reward: 9.3500,                 loss: 0.0056
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 2309.15,                last time consumption/overall running time: 1152.2184s / 699331.4083 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0043
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0043
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 2484.75,                last time consumption/overall running time: 1240.4209s / 700571.8292 s
env0_first_0:                 episode reward: -4.9000,                 loss: 0.0034
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0038
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 2543.7,                last time consumption/overall running time: 1271.8536s / 701843.6828 s
env0_first_0:                 episode reward: -13.4500,                 loss: 0.0042
env0_second_0:                 episode reward: 13.4500,                 loss: 0.0046
env1_first_0:                 episode reward: -12.7500,                 loss: nan
env1_second_0:                 episode reward: 12.7500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 2469.5,                last time consumption/overall running time: 1234.7465s / 703078.4293 s
env0_first_0:                 episode reward: -7.6000,                 loss: 0.0037
env0_second_0:                 episode reward: 7.6000,                 loss: 0.0043
env1_first_0:                 episode reward: -8.8000,                 loss: nan
env1_second_0:                 episode reward: 8.8000,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 2321.0,                last time consumption/overall running time: 1159.2536s / 704237.6829 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0035
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0039
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 2250.85,                last time consumption/overall running time: 1125.3065s / 705362.9893 s
env0_first_0:                 episode reward: -9.7000,                 loss: 0.0032
env0_second_0:                 episode reward: 9.7000,                 loss: 0.0036
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 2345.05,                last time consumption/overall running time: 1170.8816s / 706533.8709 s
env0_first_0:                 episode reward: -11.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 11.7500,                 loss: 0.0050
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 2379.55,                last time consumption/overall running time: 1189.8235s / 707723.6944 s
env0_first_0:                 episode reward: -10.0000,                 loss: 0.0053
env0_second_0:                 episode reward: 10.0000,                 loss: 0.0057
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 2336.05,                last time consumption/overall running time: 1165.8068s / 708889.5012 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0037
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0042
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 2152.7,                last time consumption/overall running time: 1076.0244s / 709965.5256 s
env0_first_0:                 episode reward: -10.6500,                 loss: 0.0037
env0_second_0:                 episode reward: 10.6500,                 loss: 0.0041
env1_first_0:                 episode reward: -8.2000,                 loss: nan
env1_second_0:                 episode reward: 8.2000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 2020.15,                last time consumption/overall running time: 1011.2043s / 710976.7299 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0033
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0037
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 2331.55,                last time consumption/overall running time: 1161.4223s / 712138.1522 s
env0_first_0:                 episode reward: -9.7500,                 loss: 0.0031
env0_second_0:                 episode reward: 9.7500,                 loss: 0.0037
env1_first_0:                 episode reward: -9.3500,                 loss: nan
env1_second_0:                 episode reward: 9.3500,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 2227.5,                last time consumption/overall running time: 989.7079s / 713127.8602 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0032
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0039
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 2516.1,                last time consumption/overall running time: 1118.4690s / 714246.3291 s
env0_first_0:                 episode reward: -13.6500,                 loss: 0.0035
env0_second_0:                 episode reward: 13.6500,                 loss: 0.0040
env1_first_0:                 episode reward: -14.3500,                 loss: nan
env1_second_0:                 episode reward: 14.3500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 2381.9,                last time consumption/overall running time: 1055.7853s / 715302.1145 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.0050
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0057
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 2566.8,                last time consumption/overall running time: 1138.4560s / 716440.5705 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0043
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0048
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 1907.7,                last time consumption/overall running time: 847.0881s / 717287.6586 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0036
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0039
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 2516.1,                last time consumption/overall running time: 1122.1326s / 718409.7912 s
env0_first_0:                 episode reward: -10.8500,                 loss: 0.0030
env0_second_0:                 episode reward: 10.8500,                 loss: 0.0034
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 2240.55,                last time consumption/overall running time: 994.6945s / 719404.4857 s
env0_first_0:                 episode reward: -16.1000,                 loss: 0.0048
env0_second_0:                 episode reward: 16.1000,                 loss: 0.0057
env1_first_0:                 episode reward: -15.4000,                 loss: nan
env1_second_0:                 episode reward: 15.4000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 2213.9,                last time consumption/overall running time: 984.4971s / 720388.9828 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0038
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0055
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 2051.65,                last time consumption/overall running time: 915.5367s / 721304.5195 s
env0_first_0:                 episode reward: 2.1500,                 loss: 0.0038
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0044
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 2262.25,                last time consumption/overall running time: 1007.9279s / 722312.4474 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.0034
env0_second_0:                 episode reward: -0.4500,                 loss: 0.0040
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 2494.7,                last time consumption/overall running time: 1114.0927s / 723426.5401 s
env0_first_0:                 episode reward: -10.3000,                 loss: 0.0034
env0_second_0:                 episode reward: 10.3000,                 loss: 0.0042
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 2292.35,                last time consumption/overall running time: 1027.7018s / 724454.2419 s
env0_first_0:                 episode reward: -10.9000,                 loss: 0.0081
env0_second_0:                 episode reward: 10.9000,                 loss: 0.0054
env1_first_0:                 episode reward: -13.5000,                 loss: nan
env1_second_0:                 episode reward: 13.5000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 2402.75,                last time consumption/overall running time: 1074.2418s / 725528.4837 s
env0_first_0:                 episode reward: -11.5000,                 loss: 0.0169
env0_second_0:                 episode reward: 11.5000,                 loss: 0.0054
env1_first_0:                 episode reward: -10.4000,                 loss: nan
env1_second_0:                 episode reward: 10.4000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 2311.65,                last time consumption/overall running time: 1033.0041s / 726561.4878 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0052
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0054
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 2056.0,                last time consumption/overall running time: 914.4651s / 727475.9529 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0045
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0043
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 2007.9,                last time consumption/overall running time: 897.4521s / 728373.4049 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0038
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0040
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 2053.05,                last time consumption/overall running time: 914.8540s / 729288.2589 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0034
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0037
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 2038.9,                last time consumption/overall running time: 912.5114s / 730200.7703 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0034
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0039
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 2024.85,                last time consumption/overall running time: 906.0066s / 731106.7769 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.0039
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0044
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 2203.9,                last time consumption/overall running time: 981.5233s / 732088.3002 s
env0_first_0:                 episode reward: -8.8500,                 loss: 0.0038
env0_second_0:                 episode reward: 8.8500,                 loss: 0.0044
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 2125.2,                last time consumption/overall running time: 950.1926s / 733038.4928 s
env0_first_0:                 episode reward: -8.2000,                 loss: 0.0034
env0_second_0:                 episode reward: 8.2000,                 loss: 0.0042
env1_first_0:                 episode reward: -11.0000,                 loss: nan
env1_second_0:                 episode reward: 11.0000,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 2780.65,                last time consumption/overall running time: 1306.2615s / 734344.7542 s
env0_first_0:                 episode reward: -10.7500,                 loss: 0.0036
env0_second_0:                 episode reward: 10.7500,                 loss: 0.0045
env1_first_0:                 episode reward: -12.2000,                 loss: nan
env1_second_0:                 episode reward: 12.2000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 2671.05,                last time consumption/overall running time: 1295.6558s / 735640.4101 s
env0_first_0:                 episode reward: -8.4500,                 loss: 0.0035
env0_second_0:                 episode reward: 8.4500,                 loss: 0.0044
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 2900.65,                last time consumption/overall running time: 1407.7188s / 737048.1289 s
env0_first_0:                 episode reward: -14.7000,                 loss: 0.0034
env0_second_0:                 episode reward: 14.7000,                 loss: 0.0041
env1_first_0:                 episode reward: -13.8000,                 loss: nan
env1_second_0:                 episode reward: 13.8000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 2016.3,                last time consumption/overall running time: 979.1393s / 738027.2683 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0037
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0043
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 2530.2,                last time consumption/overall running time: 1228.9253s / 739256.1935 s
env0_first_0:                 episode reward: -11.6000,                 loss: 0.0033
env0_second_0:                 episode reward: 11.6000,                 loss: 0.0043
env1_first_0:                 episode reward: -11.8500,                 loss: nan
env1_second_0:                 episode reward: 11.8500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 2040.65,                last time consumption/overall running time: 990.6189s / 740246.8125 s
env0_first_0:                 episode reward: -10.8000,                 loss: 0.0048
env0_second_0:                 episode reward: 10.8000,                 loss: 0.0057
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 2356.65,                last time consumption/overall running time: 1145.1665s / 741391.9789 s
env0_first_0:                 episode reward: -11.4500,                 loss: 0.0039
env0_second_0:                 episode reward: 11.4500,                 loss: 0.0046
env1_first_0:                 episode reward: -12.4500,                 loss: nan
env1_second_0:                 episode reward: 12.4500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 2909.8,                last time consumption/overall running time: 1413.6294s / 742805.6084 s
env0_first_0:                 episode reward: -23.2000,                 loss: 0.0042
env0_second_0:                 episode reward: 23.2000,                 loss: 0.0048
env1_first_0:                 episode reward: -24.1000,                 loss: nan
env1_second_0:                 episode reward: 24.1000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 3174.25,                last time consumption/overall running time: 1537.7169s / 744343.3252 s
env0_first_0:                 episode reward: -24.6000,                 loss: 0.0034
env0_second_0:                 episode reward: 24.6000,                 loss: 0.0040
env1_first_0:                 episode reward: -26.4500,                 loss: nan
env1_second_0:                 episode reward: 26.4500,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 2341.3,                last time consumption/overall running time: 1049.5621s / 745392.8873 s
env0_first_0:                 episode reward: -16.5500,                 loss: 0.0033
env0_second_0:                 episode reward: 16.5500,                 loss: 0.0039
env1_first_0:                 episode reward: -14.6000,                 loss: nan
env1_second_0:                 episode reward: 14.6000,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 2474.75,                last time consumption/overall running time: 1095.0868s / 746487.9741 s
env0_first_0:                 episode reward: -8.7500,                 loss: 0.0034
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0037
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 2256.8,                last time consumption/overall running time: 998.6469s / 747486.6210 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0038
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0043
env1_first_0:                 episode reward: -9.8500,                 loss: nan
env1_second_0:                 episode reward: 9.8500,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 2118.7,                last time consumption/overall running time: 940.2414s / 748426.8624 s
env0_first_0:                 episode reward: -10.5000,                 loss: 0.0034
env0_second_0:                 episode reward: 10.5000,                 loss: 0.0039
env1_first_0:                 episode reward: -8.7000,                 loss: nan
env1_second_0:                 episode reward: 8.7000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 2091.95,                last time consumption/overall running time: 924.4490s / 749351.3114 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0033
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0036
env1_first_0:                 episode reward: -10.3000,                 loss: nan
env1_second_0:                 episode reward: 10.3000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 2501.85,                last time consumption/overall running time: 1107.9212s / 750459.2326 s
env0_first_0:                 episode reward: -13.4000,                 loss: 0.0040
env0_second_0:                 episode reward: 13.4000,                 loss: 0.0042
env1_first_0:                 episode reward: -13.3500,                 loss: nan
env1_second_0:                 episode reward: 13.3500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 2213.45,                last time consumption/overall running time: 980.6731s / 751439.9057 s
env0_first_0:                 episode reward: -10.5000,                 loss: 0.0041
env0_second_0:                 episode reward: 10.5000,                 loss: 0.0045
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 2030.15,                last time consumption/overall running time: 833.0620s / 752272.9676 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0027
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0036
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 2128.85,                last time consumption/overall running time: 859.2891s / 753132.2568 s
env0_first_0:                 episode reward: -9.2500,                 loss: 0.0028
env0_second_0:                 episode reward: 9.2500,                 loss: 0.0036
env1_first_0:                 episode reward: -8.0000,                 loss: nan
env1_second_0:                 episode reward: 8.0000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 2091.55,                last time consumption/overall running time: 841.8129s / 753974.0696 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0032
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0038
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 2307.15,                last time consumption/overall running time: 930.8351s / 754904.9048 sLoad double_dunk_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load double_dunk_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: -4.1500,                 loss: 0.0029
env0_second_0:                 episode reward: 4.1500,                 loss: 0.0036
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 1865.25,                last time consumption/overall running time: 754.7571s / 755659.6619 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0031
env0_second_0:                 episode reward: 2.3000,                 loss: 0.0040
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 2066.25,                last time consumption/overall running time: 835.4898s / 756495.1517 s
env0_first_0:                 episode reward: -9.8500,                 loss: 0.0037
env0_second_0:                 episode reward: 9.8500,                 loss: 0.0044
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 1956.95,                last time consumption/overall running time: 791.2826s / 757286.4343 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0038
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0044
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 1985.05,                last time consumption/overall running time: 800.5503s / 758086.9846 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.0028
env0_second_0:                 episode reward: 2.4000,                 loss: 0.0034
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 2324.6,                last time consumption/overall running time: 938.2159s / 759025.2005 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0023
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0027
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 2161.1,                last time consumption/overall running time: 873.6770s / 759898.8776 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0032
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0035
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 2136.6,                last time consumption/overall running time: 864.3285s / 760763.2061 s
env0_first_0:                 episode reward: -7.7500,                 loss: 0.0036
env0_second_0:                 episode reward: 7.7500,                 loss: 0.0037
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 2069.15,                last time consumption/overall running time: 835.0018s / 761598.2079 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0039
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0040
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 1939.45,                last time consumption/overall running time: 786.7677s / 762384.9756 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0027
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0032
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 2312.8,                last time consumption/overall running time: 931.6290s / 763316.6046 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0029
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0034
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 2384.7,                last time consumption/overall running time: 963.1074s / 764279.7120 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0037
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0043
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
