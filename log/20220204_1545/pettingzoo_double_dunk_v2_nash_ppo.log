pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
double_dunk_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'double_dunk_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 30000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220204_1545/pettingzoo_double_dunk_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220204_1545/pettingzoo_double_dunk_v2_nash_ppo.
Episode: 1/30000 (0.0033%),                 avg. length: 4331.0,                last time consumption/overall running time: 25.3540s / 25.3540 s
env0_first_0:                 episode reward: -29.0000,                 loss: -0.0616
env0_second_0:                 episode reward: 29.0000,                 loss: -0.0598
env1_first_0:                 episode reward: -25.0000,                 loss: nan
env1_second_0:                 episode reward: 25.0000,                 loss: nan
Episode: 21/30000 (0.0700%),                 avg. length: 3945.65,                last time consumption/overall running time: 488.7447s / 514.0987 s
env0_first_0:                 episode reward: -26.2000,                 loss: -0.0363
env0_second_0:                 episode reward: 26.2000,                 loss: -0.0330
env1_first_0:                 episode reward: -29.2500,                 loss: nan
env1_second_0:                 episode reward: 29.2500,                 loss: nan
Episode: 41/30000 (0.1367%),                 avg. length: 3793.5,                last time consumption/overall running time: 470.3007s / 984.3995 s
env0_first_0:                 episode reward: -24.9000,                 loss: 0.0223
env0_second_0:                 episode reward: 24.9000,                 loss: 0.0257
env1_first_0:                 episode reward: -24.3000,                 loss: nan
env1_second_0:                 episode reward: 24.3000,                 loss: nan
Episode: 61/30000 (0.2033%),                 avg. length: 3776.4,                last time consumption/overall running time: 446.1071s / 1430.5065 s
env0_first_0:                 episode reward: -17.5000,                 loss: 0.0376
env0_second_0:                 episode reward: 17.5000,                 loss: 0.0362
env1_first_0:                 episode reward: -24.8000,                 loss: nan
env1_second_0:                 episode reward: 24.8000,                 loss: nan
Episode: 81/30000 (0.2700%),                 avg. length: 3778.25,                last time consumption/overall running time: 479.2045s / 1909.7111 s
env0_first_0:                 episode reward: -25.0000,                 loss: 0.0572
env0_second_0:                 episode reward: 25.0000,                 loss: 0.0586
env1_first_0:                 episode reward: -25.2000,                 loss: nan
env1_second_0:                 episode reward: 25.2000,                 loss: nan
Episode: 101/30000 (0.3367%),                 avg. length: 4139.25,                last time consumption/overall running time: 503.6592s / 2413.3703 s
env0_first_0:                 episode reward: -23.9000,                 loss: 0.0659
env0_second_0:                 episode reward: 23.9000,                 loss: 0.0698
env1_first_0:                 episode reward: -23.3500,                 loss: nan
env1_second_0:                 episode reward: 23.3500,                 loss: nan
Episode: 121/30000 (0.4033%),                 avg. length: 4073.1,                last time consumption/overall running time: 462.1391s / 2875.5093 s
env0_first_0:                 episode reward: -22.9500,                 loss: 0.0711
env0_second_0:                 episode reward: 22.9500,                 loss: 0.0742
env1_first_0:                 episode reward: -21.1500,                 loss: nan
env1_second_0:                 episode reward: 21.1500,                 loss: nan
Episode: 141/30000 (0.4700%),                 avg. length: 4071.9,                last time consumption/overall running time: 468.4880s / 3343.9973 s
env0_first_0:                 episode reward: -20.4500,                 loss: 0.0759
env0_second_0:                 episode reward: 20.4500,                 loss: 0.0838
env1_first_0:                 episode reward: -22.5500,                 loss: nan
env1_second_0:                 episode reward: 22.5500,                 loss: nan
Episode: 161/30000 (0.5367%),                 avg. length: 4199.1,                last time consumption/overall running time: 515.3768s / 3859.3741 s
env0_first_0:                 episode reward: -22.6500,                 loss: 0.0933
env0_second_0:                 episode reward: 22.6500,                 loss: 0.0943
env1_first_0:                 episode reward: -23.1000,                 loss: nan
env1_second_0:                 episode reward: 23.1000,                 loss: nan
Episode: 181/30000 (0.6033%),                 avg. length: 4061.55,                last time consumption/overall running time: 512.3635s / 4371.7376 s
env0_first_0:                 episode reward: -24.3500,                 loss: 0.0943
env0_second_0:                 episode reward: 24.3500,                 loss: 0.0852
env1_first_0:                 episode reward: -25.1000,                 loss: nan
env1_second_0:                 episode reward: 25.1000,                 loss: nan
Episode: 201/30000 (0.6700%),                 avg. length: 4394.4,                last time consumption/overall running time: 550.9981s / 4922.7357 s
env0_first_0:                 episode reward: -25.6500,                 loss: 0.0709
env0_second_0:                 episode reward: 25.6500,                 loss: 0.0649
env1_first_0:                 episode reward: -25.8500,                 loss: nan
env1_second_0:                 episode reward: 25.8500,                 loss: nan
Episode: 221/30000 (0.7367%),                 avg. length: 4367.1,                last time consumption/overall running time: 541.2058s / 5463.9415 s
env0_first_0:                 episode reward: -27.8500,                 loss: 0.0685
env0_second_0:                 episode reward: 27.8500,                 loss: 0.0677
env1_first_0:                 episode reward: -26.5500,                 loss: nan
env1_second_0:                 episode reward: 26.5500,                 loss: nan
Episode: 241/30000 (0.8033%),                 avg. length: 4164.5,                last time consumption/overall running time: 502.7117s / 5966.6533 s
env0_first_0:                 episode reward: -22.7000,                 loss: 0.0574
env0_second_0:                 episode reward: 22.7000,                 loss: 0.0538
env1_first_0:                 episode reward: -24.3500,                 loss: nan
env1_second_0:                 episode reward: 24.3500,                 loss: nan
Episode: 261/30000 (0.8700%),                 avg. length: 4697.45,                last time consumption/overall running time: 569.4095s / 6536.0628 s
env0_first_0:                 episode reward: -30.2500,                 loss: 0.0615
env0_second_0:                 episode reward: 30.2500,                 loss: 0.0547
env1_first_0:                 episode reward: -32.7000,                 loss: nan
env1_second_0:                 episode reward: 32.7000,                 loss: nan
Episode: 281/30000 (0.9367%),                 avg. length: 4433.95,                last time consumption/overall running time: 538.4987s / 7074.5614 s
env0_first_0:                 episode reward: -27.4500,                 loss: 0.0491
env0_second_0:                 episode reward: 27.4500,                 loss: 0.0473
env1_first_0:                 episode reward: -29.2000,                 loss: nan
env1_second_0:                 episode reward: 29.2000,                 loss: nan
Episode: 301/30000 (1.0033%),                 avg. length: 4932.2,                last time consumption/overall running time: 603.6742s / 7678.2356 s
env0_first_0:                 episode reward: -32.2000,                 loss: 0.0112
env0_second_0:                 episode reward: 32.2000,                 loss: 0.0085
env1_first_0:                 episode reward: -33.0500,                 loss: nan
env1_second_0:                 episode reward: 33.0500,                 loss: nan
Episode: 321/30000 (1.0700%),                 avg. length: 4479.35,                last time consumption/overall running time: 552.7431s / 8230.9787 s
env0_first_0:                 episode reward: -29.2000,                 loss: 0.0154
env0_second_0:                 episode reward: 29.2000,                 loss: 0.0206
env1_first_0:                 episode reward: -30.1500,                 loss: nan
env1_second_0:                 episode reward: 30.1500,                 loss: nan
Episode: 341/30000 (1.1367%),                 avg. length: 4177.5,                last time consumption/overall running time: 532.7284s / 8763.7071 s
env0_first_0:                 episode reward: -27.6500,                 loss: 0.0114
env0_second_0:                 episode reward: 27.6500,                 loss: 0.0117
env1_first_0:                 episode reward: -27.9000,                 loss: nan
env1_second_0:                 episode reward: 27.9000,                 loss: nan
Episode: 361/30000 (1.2033%),                 avg. length: 4495.45,                last time consumption/overall running time: 583.7611s / 9347.4682 s
env0_first_0:                 episode reward: -29.7000,                 loss: -0.0393
env0_second_0:                 episode reward: 29.7000,                 loss: -0.0372
env1_first_0:                 episode reward: -30.3500,                 loss: nan
env1_second_0:                 episode reward: 30.3500,                 loss: nan
Episode: 381/30000 (1.2700%),                 avg. length: 4161.35,                last time consumption/overall running time: 523.8614s / 9871.3296 s
env0_first_0:                 episode reward: -23.4000,                 loss: -0.1271
env0_second_0:                 episode reward: 23.4000,                 loss: -0.1267
env1_first_0:                 episode reward: -24.2000,                 loss: nan
env1_second_0:                 episode reward: 24.2000,                 loss: nan
Episode: 401/30000 (1.3367%),                 avg. length: 4198.55,                last time consumption/overall running time: 514.3966s / 10385.7262 s
env0_first_0:                 episode reward: -16.6500,                 loss: -0.2026
env0_second_0:                 episode reward: 16.6500,                 loss: -0.2016
env1_first_0:                 episode reward: -16.3000,                 loss: nan
env1_second_0:                 episode reward: 16.3000,                 loss: nan
Episode: 421/30000 (1.4033%),                 avg. length: 4189.7,                last time consumption/overall running time: 549.4157s / 10935.1418 s
env0_first_0:                 episode reward: -11.8000,                 loss: -0.1929
env0_second_0:                 episode reward: 11.8000,                 loss: -0.1849
env1_first_0:                 episode reward: -12.1500,                 loss: nan
env1_second_0:                 episode reward: 12.1500,                 loss: nan
Episode: 441/30000 (1.4700%),                 avg. length: 5357.6,                last time consumption/overall running time: 665.2516s / 11600.3934 s
env0_first_0:                 episode reward: -11.8000,                 loss: -0.2109
env0_second_0:                 episode reward: 11.8000,                 loss: -0.2065
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 461/30000 (1.5367%),                 avg. length: 6107.7,                last time consumption/overall running time: 741.9387s / 12342.3322 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.2236
env0_second_0:                 episode reward: 7.0000,                 loss: -0.2180
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 481/30000 (1.6033%),                 avg. length: 5685.0,                last time consumption/overall running time: 682.0195s / 13024.3517 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.2013
env0_second_0:                 episode reward: 3.9500,                 loss: -0.1978
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 501/30000 (1.6700%),                 avg. length: 5258.9,                last time consumption/overall running time: 645.5184s / 13669.8701 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1923
env0_second_0:                 episode reward: -1.2500,                 loss: -0.1838
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 521/30000 (1.7367%),                 avg. length: 4532.25,                last time consumption/overall running time: 548.4836s / 14218.3537 s
env0_first_0:                 episode reward: 8.4000,                 loss: -0.1915
env0_second_0:                 episode reward: -8.4000,                 loss: -0.1827
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 541/30000 (1.8033%),                 avg. length: 3441.55,                last time consumption/overall running time: 415.7965s / 14634.1501 s
env0_first_0:                 episode reward: 13.4000,                 loss: -0.1915
env0_second_0:                 episode reward: -13.4000,                 loss: -0.1836
env1_first_0:                 episode reward: 13.6500,                 loss: nan
env1_second_0:                 episode reward: -13.6500,                 loss: nan
Episode: 561/30000 (1.8700%),                 avg. length: 2889.15,                last time consumption/overall running time: 346.7505s / 14980.9006 s
env0_first_0:                 episode reward: 16.8000,                 loss: -0.1964
env0_second_0:                 episode reward: -16.8000,                 loss: -0.1842
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 581/30000 (1.9367%),                 avg. length: 2938.4,                last time consumption/overall running time: 363.9662s / 15344.8669 s
env0_first_0:                 episode reward: 13.6000,                 loss: -0.2077
env0_second_0:                 episode reward: -13.6000,                 loss: -0.1971
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 601/30000 (2.0033%),                 avg. length: 2706.4,                last time consumption/overall running time: 315.1645s / 15660.0314 s
env0_first_0:                 episode reward: 16.7500,                 loss: -0.2065
env0_second_0:                 episode reward: -16.7500,                 loss: -0.1991
env1_first_0:                 episode reward: 16.2500,                 loss: nan
env1_second_0:                 episode reward: -16.2500,                 loss: nan
Episode: 621/30000 (2.0700%),                 avg. length: 2327.75,                last time consumption/overall running time: 289.5294s / 15949.5608 s
env0_first_0:                 episode reward: 17.0500,                 loss: -0.1964
env0_second_0:                 episode reward: -17.0500,                 loss: -0.1903
env1_first_0:                 episode reward: 16.7500,                 loss: nan
env1_second_0:                 episode reward: -16.7500,                 loss: nan
Episode: 641/30000 (2.1367%),                 avg. length: 2262.95,                last time consumption/overall running time: 281.4688s / 16231.0295 s
env0_first_0:                 episode reward: 17.7500,                 loss: -0.2039
env0_second_0:                 episode reward: -17.7500,                 loss: -0.1952
env1_first_0:                 episode reward: 16.4500,                 loss: nan
env1_second_0:                 episode reward: -16.4500,                 loss: nan
Episode: 661/30000 (2.2033%),                 avg. length: 2259.2,                last time consumption/overall running time: 286.3637s / 16517.3932 s
env0_first_0:                 episode reward: 17.3500,                 loss: -0.2077
env0_second_0:                 episode reward: -17.3500,                 loss: -0.2032
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 681/30000 (2.2700%),                 avg. length: 2111.95,                last time consumption/overall running time: 266.5009s / 16783.8941 s
env0_first_0:                 episode reward: 16.4000,                 loss: -0.2025
env0_second_0:                 episode reward: -16.4000,                 loss: -0.1952
env1_first_0:                 episode reward: 17.2000,                 loss: nan
env1_second_0:                 episode reward: -17.2000,                 loss: nan
Episode: 701/30000 (2.3367%),                 avg. length: 1936.55,                last time consumption/overall running time: 231.3666s / 17015.2607 s
env0_first_0:                 episode reward: 16.2500,                 loss: -0.1816
env0_second_0:                 episode reward: -16.2500,                 loss: -0.1714
env1_first_0:                 episode reward: 16.7500,                 loss: nan
env1_second_0:                 episode reward: -16.7500,                 loss: nan
Episode: 721/30000 (2.4033%),                 avg. length: 1898.25,                last time consumption/overall running time: 237.7067s / 17252.9674 s
env0_first_0:                 episode reward: 18.4500,                 loss: -0.1626
env0_second_0:                 episode reward: -18.4500,                 loss: -0.1467
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
Episode: 741/30000 (2.4700%),                 avg. length: 1729.95,                last time consumption/overall running time: 204.7245s / 17457.6920 s
env0_first_0:                 episode reward: 14.5500,                 loss: -0.1130
env0_second_0:                 episode reward: -14.5500,                 loss: -0.0967
env1_first_0:                 episode reward: 15.0000,                 loss: nan
env1_second_0:                 episode reward: -15.0000,                 loss: nan
Episode: 761/30000 (2.5367%),                 avg. length: 1684.85,                last time consumption/overall running time: 209.1361s / 17666.8281 s
env0_first_0:                 episode reward: 10.3000,                 loss: -0.0778
env0_second_0:                 episode reward: -10.3000,                 loss: -0.0609
env1_first_0:                 episode reward: 12.2000,                 loss: nan
env1_second_0:                 episode reward: -12.2000,                 loss: nan
Episode: 781/30000 (2.6033%),                 avg. length: 1759.4,                last time consumption/overall running time: 212.6263s / 17879.4544 s
env0_first_0:                 episode reward: 9.2000,                 loss: -0.1138
env0_second_0:                 episode reward: -9.2000,                 loss: -0.0978
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 801/30000 (2.6700%),                 avg. length: 1729.1,                last time consumption/overall running time: 210.9354s / 18090.3898 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.1070
env0_second_0:                 episode reward: -8.8500,                 loss: -0.0900
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 821/30000 (2.7367%),                 avg. length: 1608.0,                last time consumption/overall running time: 193.8373s / 18284.2271 s
env0_first_0:                 episode reward: 11.2000,                 loss: -0.0853
env0_second_0:                 episode reward: -11.2000,                 loss: -0.0733
env1_first_0:                 episode reward: 12.5500,                 loss: nan
env1_second_0:                 episode reward: -12.5500,                 loss: nan
Episode: 841/30000 (2.8033%),                 avg. length: 2139.85,                last time consumption/overall running time: 273.0845s / 18557.3117 s
env0_first_0:                 episode reward: 12.7500,                 loss: -0.1054
env0_second_0:                 episode reward: -12.7500,                 loss: -0.0911
env1_first_0:                 episode reward: 13.8000,                 loss: nan
env1_second_0:                 episode reward: -13.8000,                 loss: nan
Episode: 861/30000 (2.8700%),                 avg. length: 2101.4,                last time consumption/overall running time: 255.4750s / 18812.7867 s
env0_first_0:                 episode reward: 12.4500,                 loss: -0.0995
env0_second_0:                 episode reward: -12.4500,                 loss: -0.0883
env1_first_0:                 episode reward: 12.6000,                 loss: nan
env1_second_0:                 episode reward: -12.6000,                 loss: nan
Episode: 881/30000 (2.9367%),                 avg. length: 1799.4,                last time consumption/overall running time: 231.9159s / 19044.7025 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0879
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0673
env1_first_0:                 episode reward: 11.4000,                 loss: nan
env1_second_0:                 episode reward: -11.4000,                 loss: nan
Episode: 901/30000 (3.0033%),                 avg. length: 1824.2,                last time consumption/overall running time: 227.6215s / 19272.3241 s
env0_first_0:                 episode reward: 11.9500,                 loss: -0.1083
env0_second_0:                 episode reward: -11.9500,                 loss: -0.0895
env1_first_0:                 episode reward: 11.9500,                 loss: nan
env1_second_0:                 episode reward: -11.9500,                 loss: nan
Episode: 921/30000 (3.0700%),                 avg. length: 1978.9,                last time consumption/overall running time: 251.1141s / 19523.4382 s
env0_first_0:                 episode reward: 10.5500,                 loss: -0.1073
env0_second_0:                 episode reward: -10.5500,                 loss: -0.0894
env1_first_0:                 episode reward: 10.5500,                 loss: nan
env1_second_0:                 episode reward: -10.5500,                 loss: nan
Episode: 941/30000 (3.1367%),                 avg. length: 1969.65,                last time consumption/overall running time: 247.2334s / 19770.6717 s
env0_first_0:                 episode reward: 11.8500,                 loss: -0.0975
env0_second_0:                 episode reward: -11.8500,                 loss: -0.0737
env1_first_0:                 episode reward: 10.5000,                 loss: nan
env1_second_0:                 episode reward: -10.5000,                 loss: nan
Episode: 961/30000 (3.2033%),                 avg. length: 1782.2,                last time consumption/overall running time: 223.2058s / 19993.8774 s
env0_first_0:                 episode reward: 10.5000,                 loss: -0.0922
env0_second_0:                 episode reward: -10.5000,                 loss: -0.0716
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 981/30000 (3.2700%),                 avg. length: 1770.75,                last time consumption/overall running time: 216.0032s / 20209.8806 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.1418
env0_second_0:                 episode reward: -9.5500,                 loss: -0.1217
env1_first_0:                 episode reward: 8.6500,                 loss: nan
env1_second_0:                 episode reward: -8.6500,                 loss: nan
Episode: 1001/30000 (3.3367%),                 avg. length: 1760.7,                last time consumption/overall running time: 229.8101s / 20439.6907 s
env0_first_0:                 episode reward: 8.0500,                 loss: -0.1479
env0_second_0:                 episode reward: -8.0500,                 loss: -0.1262
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 1021/30000 (3.4033%),                 avg. length: 1801.15,                last time consumption/overall running time: 223.0325s / 20662.7232 s
env0_first_0:                 episode reward: 7.7500,                 loss: -0.1767
env0_second_0:                 episode reward: -7.7500,                 loss: -0.1549
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 1041/30000 (3.4700%),                 avg. length: 1793.35,                last time consumption/overall running time: 224.5177s / 20887.2409 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.1413
env0_second_0:                 episode reward: -7.3500,                 loss: -0.1190
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 1061/30000 (3.5367%),                 avg. length: 1790.1,                last time consumption/overall running time: 226.4350s / 21113.6758 s
env0_first_0:                 episode reward: 8.4000,                 loss: -0.1275
env0_second_0:                 episode reward: -8.4000,                 loss: -0.0973
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 1081/30000 (3.6033%),                 avg. length: 2019.4,                last time consumption/overall running time: 255.6684s / 21369.3442 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.1417
env0_second_0:                 episode reward: -8.8000,                 loss: -0.1182
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 1101/30000 (3.6700%),                 avg. length: 2288.9,                last time consumption/overall running time: 289.5848s / 21658.9290 s
env0_first_0:                 episode reward: 10.3500,                 loss: -0.1187
env0_second_0:                 episode reward: -10.3500,                 loss: -0.1067
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 1121/30000 (3.7367%),                 avg. length: 2674.05,                last time consumption/overall running time: 330.4376s / 21989.3666 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.0578
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0396
env1_first_0:                 episode reward: 10.9500,                 loss: nan
env1_second_0:                 episode reward: -10.9500,                 loss: nan
Episode: 1141/30000 (3.8033%),                 avg. length: 2464.9,                last time consumption/overall running time: 290.6177s / 22279.9843 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0591
env0_second_0:                 episode reward: -9.4500,                 loss: -0.0364
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 1161/30000 (3.8700%),                 avg. length: 2814.1,                last time consumption/overall running time: 335.9502s / 22615.9345 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.0403
env0_second_0:                 episode reward: -9.0500,                 loss: -0.0257
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 1181/30000 (3.9367%),                 avg. length: 3240.6,                last time consumption/overall running time: 396.0343s / 23011.9688 s
env0_first_0:                 episode reward: 9.9500,                 loss: -0.0227
env0_second_0:                 episode reward: -9.9500,                 loss: -0.0058
env1_first_0:                 episode reward: 8.3000,                 loss: nan
env1_second_0:                 episode reward: -8.3000,                 loss: nan
Episode: 1201/30000 (4.0033%),                 avg. length: 2918.4,                last time consumption/overall running time: 362.9993s / 23374.9681 s
env0_first_0:                 episode reward: 10.1000,                 loss: -0.0268
env0_second_0:                 episode reward: -10.1000,                 loss: -0.0158
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 1221/30000 (4.0700%),                 avg. length: 2777.45,                last time consumption/overall running time: 337.9294s / 23712.8976 s
env0_first_0:                 episode reward: 9.9000,                 loss: -0.0678
env0_second_0:                 episode reward: -9.9000,                 loss: -0.0539
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 1241/30000 (4.1367%),                 avg. length: 2551.9,                last time consumption/overall running time: 320.6202s / 24033.5178 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0881
env0_second_0:                 episode reward: -5.8000,                 loss: -0.0702
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 1261/30000 (4.2033%),                 avg. length: 3004.9,                last time consumption/overall running time: 357.6965s / 24391.2142 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1065
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0866
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 1281/30000 (4.2700%),                 avg. length: 2817.05,                last time consumption/overall running time: 346.2358s / 24737.4500 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1022
env0_second_0:                 episode reward: -5.3000,                 loss: -0.0823
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 1301/30000 (4.3367%),                 avg. length: 3054.6,                last time consumption/overall running time: 371.5087s / 25108.9587 s
env0_first_0:                 episode reward: 9.1000,                 loss: -0.1014
env0_second_0:                 episode reward: -9.1000,                 loss: -0.0855
env1_first_0:                 episode reward: 10.3000,                 loss: nan
env1_second_0:                 episode reward: -10.3000,                 loss: nan
Episode: 1321/30000 (4.4033%),                 avg. length: 2956.15,                last time consumption/overall running time: 373.4047s / 25482.3634 s
env0_first_0:                 episode reward: 10.3500,                 loss: -0.1012
env0_second_0:                 episode reward: -10.3500,                 loss: -0.0844
env1_first_0:                 episode reward: 8.6000,                 loss: nan
env1_second_0:                 episode reward: -8.6000,                 loss: nan
Episode: 1341/30000 (4.4700%),                 avg. length: 2874.6,                last time consumption/overall running time: 352.1159s / 25834.4794 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.1041
env0_second_0:                 episode reward: -7.1500,                 loss: -0.0881
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 1361/30000 (4.5367%),                 avg. length: 2951.5,                last time consumption/overall running time: 353.4194s / 26187.8987 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.0976
env0_second_0:                 episode reward: -5.5000,                 loss: -0.0797
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 1381/30000 (4.6033%),                 avg. length: 3072.05,                last time consumption/overall running time: 366.5137s / 26554.4124 s
env0_first_0:                 episode reward: 7.8000,                 loss: -0.0390
env0_second_0:                 episode reward: -7.8000,                 loss: -0.0166
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 1401/30000 (4.6700%),                 avg. length: 3284.4,                last time consumption/overall running time: 408.7697s / 26963.1822 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0005
env0_second_0:                 episode reward: -7.1500,                 loss: 0.0143
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 1421/30000 (4.7367%),                 avg. length: 3308.45,                last time consumption/overall running time: 399.9916s / 27363.1738 s
env0_first_0:                 episode reward: 5.8500,                 loss: -0.0730
env0_second_0:                 episode reward: -5.8500,                 loss: -0.0510
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 1441/30000 (4.8033%),                 avg. length: 3405.35,                last time consumption/overall running time: 428.0031s / 27791.1769 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.0439
env0_second_0:                 episode reward: -6.4500,                 loss: -0.0286
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 1461/30000 (4.8700%),                 avg. length: 3132.75,                last time consumption/overall running time: 396.8687s / 28188.0456 s
env0_first_0:                 episode reward: 10.7500,                 loss: -0.0796
env0_second_0:                 episode reward: -10.7500,                 loss: -0.0555
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 1481/30000 (4.9367%),                 avg. length: 3018.7,                last time consumption/overall running time: 382.4976s / 28570.5432 s
env0_first_0:                 episode reward: 9.8000,                 loss: -0.0963
env0_second_0:                 episode reward: -9.8000,                 loss: -0.0774
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 1501/30000 (5.0033%),                 avg. length: 2286.2,                last time consumption/overall running time: 288.6165s / 28859.1597 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.0912
env0_second_0:                 episode reward: -9.6000,                 loss: -0.0688
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 1521/30000 (5.0700%),                 avg. length: 2324.7,                last time consumption/overall running time: 288.8988s / 29148.0585 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.1011
env0_second_0:                 episode reward: -9.6000,                 loss: -0.0703
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 1541/30000 (5.1367%),                 avg. length: 2348.8,                last time consumption/overall running time: 280.0940s / 29428.1525 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1055
env0_second_0:                 episode reward: -10.0000,                 loss: -0.0785
env1_first_0:                 episode reward: 10.1500,                 loss: nan
env1_second_0:                 episode reward: -10.1500,                 loss: nan
Episode: 1561/30000 (5.2033%),                 avg. length: 2369.05,                last time consumption/overall running time: 270.7585s / 29698.9109 s
env0_first_0:                 episode reward: 10.5000,                 loss: -0.1252
env0_second_0:                 episode reward: -10.5000,                 loss: -0.0969
env1_first_0:                 episode reward: 11.3500,                 loss: nan
env1_second_0:                 episode reward: -11.3500,                 loss: nan
Episode: 1581/30000 (5.2700%),                 avg. length: 2116.6,                last time consumption/overall running time: 247.8555s / 29946.7664 s
env0_first_0:                 episode reward: 11.2500,                 loss: -0.1170
env0_second_0:                 episode reward: -11.2500,                 loss: -0.0835
env1_first_0:                 episode reward: 10.6500,                 loss: nan
env1_second_0:                 episode reward: -10.6500,                 loss: nan
Episode: 1601/30000 (5.3367%),                 avg. length: 1955.95,                last time consumption/overall running time: 245.5555s / 30192.3219 s
env0_first_0:                 episode reward: 11.4000,                 loss: -0.1089
env0_second_0:                 episode reward: -11.4000,                 loss: -0.0750
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
Episode: 1621/30000 (5.4033%),                 avg. length: 2425.35,                last time consumption/overall running time: 301.4996s / 30493.8215 s
env0_first_0:                 episode reward: 10.9500,                 loss: -0.0706
env0_second_0:                 episode reward: -10.9500,                 loss: -0.0357
env1_first_0:                 episode reward: 11.0000,                 loss: nan
env1_second_0:                 episode reward: -11.0000,                 loss: nan
Episode: 1641/30000 (5.4700%),                 avg. length: 2028.15,                last time consumption/overall running time: 256.1128s / 30749.9343 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.1417
env0_second_0:                 episode reward: -9.3000,                 loss: -0.1140
env1_first_0:                 episode reward: 10.4000,                 loss: nan
env1_second_0:                 episode reward: -10.4000,                 loss: nan
Episode: 1661/30000 (5.5367%),                 avg. length: 2350.6,                last time consumption/overall running time: 291.5139s / 31041.4482 s
env0_first_0:                 episode reward: 10.8000,                 loss: -0.1440
env0_second_0:                 episode reward: -10.8000,                 loss: -0.1107
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 1681/30000 (5.6033%),                 avg. length: 2689.7,                last time consumption/overall running time: 335.2310s / 31376.6792 s
env0_first_0:                 episode reward: 19.2500,                 loss: -0.0166
env0_second_0:                 episode reward: -19.2500,                 loss: 0.0975
env1_first_0:                 episode reward: 19.7000,                 loss: nan
env1_second_0:                 episode reward: -19.7000,                 loss: nan
Episode: 1701/30000 (5.6700%),                 avg. length: 2353.95,                last time consumption/overall running time: 291.1824s / 31667.8616 s
env0_first_0:                 episode reward: 15.2000,                 loss: -0.1232
env0_second_0:                 episode reward: -15.2000,                 loss: -0.0752
env1_first_0:                 episode reward: 15.1000,                 loss: nan
env1_second_0:                 episode reward: -15.1000,                 loss: nan
Episode: 1721/30000 (5.7367%),                 avg. length: 2408.1,                last time consumption/overall running time: 296.8930s / 31964.7546 s
env0_first_0:                 episode reward: 14.3000,                 loss: -0.1573
env0_second_0:                 episode reward: -14.3000,                 loss: -0.1146
env1_first_0:                 episode reward: 14.1000,                 loss: nan
env1_second_0:                 episode reward: -14.1000,                 loss: nan
Episode: 1741/30000 (5.8033%),                 avg. length: 2152.3,                last time consumption/overall running time: 261.8649s / 32226.6195 s
env0_first_0:                 episode reward: 12.6000,                 loss: -0.1505
env0_second_0:                 episode reward: -12.6000,                 loss: -0.1014
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Episode: 1761/30000 (5.8700%),                 avg. length: 2281.2,                last time consumption/overall running time: 287.9925s / 32514.6120 s
env0_first_0:                 episode reward: 12.8000,                 loss: -0.1446
env0_second_0:                 episode reward: -12.8000,                 loss: -0.0957
env1_first_0:                 episode reward: 12.5000,                 loss: nan
env1_second_0:                 episode reward: -12.5000,                 loss: nan
Episode: 1781/30000 (5.9367%),                 avg. length: 2081.05,                last time consumption/overall running time: 263.9699s / 32778.5819 s
env0_first_0:                 episode reward: 12.5500,                 loss: -0.1409
env0_second_0:                 episode reward: -12.5500,                 loss: -0.0891
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 1801/30000 (6.0033%),                 avg. length: 1818.45,                last time consumption/overall running time: 230.3912s / 33008.9731 s
env0_first_0:                 episode reward: 9.0500,                 loss: -0.1324
env0_second_0:                 episode reward: -9.0500,                 loss: -0.0864
env1_first_0:                 episode reward: 9.8500,                 loss: nan
env1_second_0:                 episode reward: -9.8500,                 loss: nan
Episode: 1821/30000 (6.0700%),                 avg. length: 1915.75,                last time consumption/overall running time: 242.7286s / 33251.7017 s
env0_first_0:                 episode reward: 8.4500,                 loss: -0.1594
env0_second_0:                 episode reward: -8.4500,                 loss: -0.1222
env1_first_0:                 episode reward: 9.0500,                 loss: nan
env1_second_0:                 episode reward: -9.0500,                 loss: nan
Episode: 1841/30000 (6.1367%),                 avg. length: 2329.8,                last time consumption/overall running time: 291.3240s / 33543.0257 s
env0_first_0:                 episode reward: 11.0000,                 loss: -0.1582
env0_second_0:                 episode reward: -11.0000,                 loss: -0.1287
env1_first_0:                 episode reward: 10.7000,                 loss: nan
env1_second_0:                 episode reward: -10.7000,                 loss: nan
Episode: 1861/30000 (6.2033%),                 avg. length: 2096.65,                last time consumption/overall running time: 266.1670s / 33809.1927 s
env0_first_0:                 episode reward: 11.4500,                 loss: -0.1340
env0_second_0:                 episode reward: -11.4500,                 loss: 0.1003
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Episode: 1881/30000 (6.2700%),                 avg. length: 2007.75,                last time consumption/overall running time: 253.6150s / 34062.8076 s
env0_first_0:                 episode reward: 17.6500,                 loss: -0.1801
env0_second_0:                 episode reward: -17.6500,                 loss: -0.1370
env1_first_0:                 episode reward: 16.9500,                 loss: nan
env1_second_0:                 episode reward: -16.9500,                 loss: nan
Episode: 1901/30000 (6.3367%),                 avg. length: 1921.35,                last time consumption/overall running time: 244.6136s / 34307.4212 s
env0_first_0:                 episode reward: 18.0000,                 loss: -0.1636
env0_second_0:                 episode reward: -18.0000,                 loss: -0.1198
env1_first_0:                 episode reward: 18.4000,                 loss: nan
env1_second_0:                 episode reward: -18.4000,                 loss: nan
Episode: 1921/30000 (6.4033%),                 avg. length: 1956.35,                last time consumption/overall running time: 245.7225s / 34553.1437 s
env0_first_0:                 episode reward: 17.3000,                 loss: -0.1001
env0_second_0:                 episode reward: -17.3000,                 loss: -0.0645
env1_first_0:                 episode reward: 15.2000,                 loss: nan
env1_second_0:                 episode reward: -15.2000,                 loss: nan
Episode: 1941/30000 (6.4700%),                 avg. length: 2145.9,                last time consumption/overall running time: 272.2359s / 34825.3797 s
env0_first_0:                 episode reward: 16.3500,                 loss: -0.1384
env0_second_0:                 episode reward: -16.3500,                 loss: -0.0394
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 1961/30000 (6.5367%),                 avg. length: 2116.65,                last time consumption/overall running time: 267.8734s / 35093.2531 s
env0_first_0:                 episode reward: 15.9000,                 loss: -0.1314
env0_second_0:                 episode reward: -15.9000,                 loss: -0.0881
env1_first_0:                 episode reward: 17.8000,                 loss: nan
env1_second_0:                 episode reward: -17.8000,                 loss: nan
Episode: 1981/30000 (6.6033%),                 avg. length: 2039.5,                last time consumption/overall running time: 249.1577s / 35342.4108 s
env0_first_0:                 episode reward: 15.8500,                 loss: -0.1263
env0_second_0:                 episode reward: -15.8500,                 loss: -0.0977
env1_first_0:                 episode reward: 15.0500,                 loss: nan
env1_second_0:                 episode reward: -15.0500,                 loss: nan
Episode: 2001/30000 (6.6700%),                 avg. length: 2100.95,                last time consumption/overall running time: 261.8644s / 35604.2752 s
env0_first_0:                 episode reward: 15.1500,                 loss: -0.1026
env0_second_0:                 episode reward: -15.1500,                 loss: -0.0875
env1_first_0:                 episode reward: 16.6000,                 loss: nan
env1_second_0:                 episode reward: -16.6000,                 loss: nan
Episode: 2021/30000 (6.7367%),                 avg. length: 2107.2,                last time consumption/overall running time: 247.6231s / 35851.8984 s
env0_first_0:                 episode reward: 19.8000,                 loss: -0.1872
env0_second_0:                 episode reward: -19.8000,                 loss: -0.1689
env1_first_0:                 episode reward: 19.6000,                 loss: nan
env1_second_0:                 episode reward: -19.6000,                 loss: nan
Episode: 2041/30000 (6.8033%),                 avg. length: 2099.9,                last time consumption/overall running time: 266.4935s / 36118.3919 s
env0_first_0:                 episode reward: 16.9500,                 loss: -0.1582
env0_second_0:                 episode reward: -16.9500,                 loss: -0.1338
env1_first_0:                 episode reward: 17.6500,                 loss: nan
env1_second_0:                 episode reward: -17.6500,                 loss: nan
Episode: 2061/30000 (6.8700%),                 avg. length: 2049.35,                last time consumption/overall running time: 249.6726s / 36368.0645 s
env0_first_0:                 episode reward: 15.5500,                 loss: -0.1526
env0_second_0:                 episode reward: -15.5500,                 loss: -0.1234
env1_first_0:                 episode reward: 15.6500,                 loss: nan
env1_second_0:                 episode reward: -15.6500,                 loss: nan
Episode: 2081/30000 (6.9367%),                 avg. length: 2077.85,                last time consumption/overall running time: 260.7632s / 36628.8278 s
env0_first_0:                 episode reward: 19.5500,                 loss: -0.1777
env0_second_0:                 episode reward: -19.5500,                 loss: -0.1503
env1_first_0:                 episode reward: 18.7500,                 loss: nan
env1_second_0:                 episode reward: -18.7500,                 loss: nan
Episode: 2101/30000 (7.0033%),                 avg. length: 2219.7,                last time consumption/overall running time: 272.9294s / 36901.7571 s
env0_first_0:                 episode reward: 17.4000,                 loss: -0.1300
env0_second_0:                 episode reward: -17.4000,                 loss: -0.0503
env1_first_0:                 episode reward: 19.1500,                 loss: nan
env1_second_0:                 episode reward: -19.1500,                 loss: nan
Episode: 2121/30000 (7.0700%),                 avg. length: 2034.35,                last time consumption/overall running time: 244.8231s / 37146.5803 s
env0_first_0:                 episode reward: 16.2500,                 loss: -0.1281
env0_second_0:                 episode reward: -16.2500,                 loss: -0.1027
env1_first_0:                 episode reward: 15.8000,                 loss: nan
env1_second_0:                 episode reward: -15.8000,                 loss: nan
Episode: 2141/30000 (7.1367%),                 avg. length: 2306.65,                last time consumption/overall running time: 276.4696s / 37423.0499 s
env0_first_0:                 episode reward: 13.6500,                 loss: -0.0939
env0_second_0:                 episode reward: -13.6500,                 loss: -0.0630
env1_first_0:                 episode reward: 14.7500,                 loss: nan
env1_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 2161/30000 (7.2033%),                 avg. length: 2345.55,                last time consumption/overall running time: 294.3495s / 37717.3994 s
env0_first_0:                 episode reward: 14.7500,                 loss: -0.1195
env0_second_0:                 episode reward: -14.7500,                 loss: -0.0911
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 2181/30000 (7.2700%),                 avg. length: 2199.85,                last time consumption/overall running time: 270.8063s / 37988.2057 s
env0_first_0:                 episode reward: 13.8500,                 loss: -0.1368
env0_second_0:                 episode reward: -13.8500,                 loss: -0.1041
env1_first_0:                 episode reward: 15.4000,                 loss: nan
env1_second_0:                 episode reward: -15.4000,                 loss: nan
Episode: 2201/30000 (7.3367%),                 avg. length: 1965.65,                last time consumption/overall running time: 249.9213s / 38238.1269 s
env0_first_0:                 episode reward: 12.1000,                 loss: -0.1485
env0_second_0:                 episode reward: -12.1000,                 loss: -0.1180
env1_first_0:                 episode reward: 12.3000,                 loss: nan
env1_second_0:                 episode reward: -12.3000,                 loss: nan
Episode: 2221/30000 (7.4033%),                 avg. length: 2030.45,                last time consumption/overall running time: 253.6899s / 38491.8169 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.1503
env0_second_0:                 episode reward: -9.6000,                 loss: -0.0999
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 2241/30000 (7.4700%),                 avg. length: 2000.4,                last time consumption/overall running time: 251.2837s / 38743.1006 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.1489
env0_second_0:                 episode reward: -9.5500,                 loss: -0.1003
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 2261/30000 (7.5367%),                 avg. length: 1950.05,                last time consumption/overall running time: 245.4857s / 38988.5863 s
env0_first_0:                 episode reward: 7.8000,                 loss: -0.1422
env0_second_0:                 episode reward: -7.8000,                 loss: -0.1107
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 2281/30000 (7.6033%),                 avg. length: 1913.85,                last time consumption/overall running time: 242.0315s / 39230.6178 s
env0_first_0:                 episode reward: 8.3000,                 loss: -0.1493
env0_second_0:                 episode reward: -8.3000,                 loss: -0.1121
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 2301/30000 (7.6700%),                 avg. length: 1793.05,                last time consumption/overall running time: 227.2988s / 39457.9166 s
env0_first_0:                 episode reward: 8.7500,                 loss: -0.1309
env0_second_0:                 episode reward: -8.7500,                 loss: -0.1024
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 2321/30000 (7.7367%),                 avg. length: 1968.75,                last time consumption/overall running time: 239.6084s / 39697.5250 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.1533
env0_second_0:                 episode reward: -6.1500,                 loss: -0.1151
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 2341/30000 (7.8033%),                 avg. length: 1777.05,                last time consumption/overall running time: 227.6567s / 39925.1816 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.1425
env0_second_0:                 episode reward: -6.5000,                 loss: -0.1226
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2361/30000 (7.8700%),                 avg. length: 1735.65,                last time consumption/overall running time: 223.5342s / 40148.7159 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1611
env0_second_0:                 episode reward: -6.6000,                 loss: -0.1172
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2381/30000 (7.9367%),                 avg. length: 1728.85,                last time consumption/overall running time: 217.8931s / 40366.6089 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1637
env0_second_0:                 episode reward: -5.3000,                 loss: -0.1300
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 2401/30000 (8.0033%),                 avg. length: 1790.8,                last time consumption/overall running time: 219.7462s / 40586.3551 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1756
env0_second_0:                 episode reward: -4.5500,                 loss: -0.1350
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 2421/30000 (8.0700%),                 avg. length: 1766.0,                last time consumption/overall running time: 197.3956s / 40783.7507 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1747
env0_second_0:                 episode reward: -3.4000,                 loss: -0.1420
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 2441/30000 (8.1367%),                 avg. length: 1787.8,                last time consumption/overall running time: 221.8037s / 41005.5544 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.1361
env0_second_0:                 episode reward: -6.5000,                 loss: -0.0899
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 2461/30000 (8.2033%),                 avg. length: 1833.0,                last time consumption/overall running time: 212.7108s / 41218.2652 s
env0_first_0:                 episode reward: 8.6000,                 loss: -0.1333
env0_second_0:                 episode reward: -8.6000,                 loss: -0.0912
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 2481/30000 (8.2700%),                 avg. length: 1955.95,                last time consumption/overall running time: 224.4780s / 41442.7432 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.1500
env0_second_0:                 episode reward: -6.8000,                 loss: -0.0974
env1_first_0:                 episode reward: 8.4000,                 loss: nan
env1_second_0:                 episode reward: -8.4000,                 loss: nan
Episode: 2501/30000 (8.3367%),                 avg. length: 1879.9,                last time consumption/overall running time: 240.2791s / 41683.0223 s
env0_first_0:                 episode reward: 7.7000,                 loss: -0.1482
env0_second_0:                 episode reward: -7.7000,                 loss: -0.1062
env1_first_0:                 episode reward: 8.1000,                 loss: nan
env1_second_0:                 episode reward: -8.1000,                 loss: nan
Episode: 2521/30000 (8.4033%),                 avg. length: 1944.85,                last time consumption/overall running time: 246.4024s / 41929.4247 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1275
env0_second_0:                 episode reward: -6.6000,                 loss: -0.0569
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 2541/30000 (8.4700%),                 avg. length: 2012.55,                last time consumption/overall running time: 255.3246s / 42184.7493 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1521
env0_second_0:                 episode reward: -5.3000,                 loss: -0.0990
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 2561/30000 (8.5367%),                 avg. length: 1871.4,                last time consumption/overall running time: 234.3901s / 42419.1393 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.1583
env0_second_0:                 episode reward: -5.4500,                 loss: -0.1227
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 2581/30000 (8.6033%),                 avg. length: 1916.25,                last time consumption/overall running time: 240.2125s / 42659.3519 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.1458
env0_second_0:                 episode reward: -6.9500,                 loss: -0.1016
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 2601/30000 (8.6700%),                 avg. length: 2001.65,                last time consumption/overall running time: 253.3438s / 42912.6956 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.1321
env0_second_0:                 episode reward: -7.1500,                 loss: -0.0014
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 2621/30000 (8.7367%),                 avg. length: 2065.45,                last time consumption/overall running time: 261.5830s / 43174.2787 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1573
env0_second_0:                 episode reward: -7.0000,                 loss: -0.1052
env1_first_0:                 episode reward: 8.0500,                 loss: nan
env1_second_0:                 episode reward: -8.0500,                 loss: nan
Episode: 2641/30000 (8.8033%),                 avg. length: 2121.7,                last time consumption/overall running time: 267.1796s / 43441.4583 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.1799
env0_second_0:                 episode reward: -7.2500,                 loss: -0.1402
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 2661/30000 (8.8700%),                 avg. length: 4471.55,                last time consumption/overall running time: 556.3592s / 43997.8175 s
env0_first_0:                 episode reward: 99.3500,                 loss: 0.0931
env0_second_0:                 episode reward: -99.3500,                 loss: 0.1189
env1_first_0:                 episode reward: 99.6000,                 loss: nan
env1_second_0:                 episode reward: -99.6000,                 loss: nan
Episode: 2681/30000 (8.9367%),                 avg. length: 5348.85,                last time consumption/overall running time: 662.8993s / 44660.7168 s
env0_first_0:                 episode reward: 138.6000,                 loss: 0.2699
env0_second_0:                 episode reward: -138.6000,                 loss: 1.8274
env1_first_0:                 episode reward: 140.5500,                 loss: nan
env1_second_0:                 episode reward: -140.5500,                 loss: nan
Episode: 2701/30000 (9.0033%),                 avg. length: 1687.8,                last time consumption/overall running time: 212.6528s / 44873.3696 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.1776
env0_second_0:                 episode reward: -6.0000,                 loss: 0.4721
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 2721/30000 (9.0700%),                 avg. length: 1768.8,                last time consumption/overall running time: 226.3706s / 45099.7403 s
env0_first_0:                 episode reward: 6.3500,                 loss: -0.1765
env0_second_0:                 episode reward: -6.3500,                 loss: 0.0954
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 2741/30000 (9.1367%),                 avg. length: 1795.1,                last time consumption/overall running time: 224.8941s / 45324.6344 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.1873
env0_second_0:                 episode reward: -4.5000,                 loss: -0.0182
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 2761/30000 (9.2033%),                 avg. length: 1794.95,                last time consumption/overall running time: 227.2417s / 45551.8761 s
env0_first_0:                 episode reward: 6.3000,                 loss: -0.1736
env0_second_0:                 episode reward: -6.3000,                 loss: -0.0809
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 2781/30000 (9.2700%),                 avg. length: 1968.45,                last time consumption/overall running time: 250.2428s / 45802.1189 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.1746
env0_second_0:                 episode reward: -7.2500,                 loss: -0.1140
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 2801/30000 (9.3367%),                 avg. length: 1710.3,                last time consumption/overall running time: 215.0849s / 46017.2039 s
env0_first_0:                 episode reward: 6.2000,                 loss: -0.1478
env0_second_0:                 episode reward: -6.2000,                 loss: -0.0566
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 2821/30000 (9.4033%),                 avg. length: 2004.75,                last time consumption/overall running time: 250.9089s / 46268.1128 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.1282
env0_second_0:                 episode reward: -6.4500,                 loss: -0.0585
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 2841/30000 (9.4700%),                 avg. length: 1831.7,                last time consumption/overall running time: 230.8532s / 46498.9659 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.1476
env0_second_0:                 episode reward: -5.7500,                 loss: -0.0909
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 2861/30000 (9.5367%),                 avg. length: 1741.45,                last time consumption/overall running time: 221.3921s / 46720.3580 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1609
env0_second_0:                 episode reward: -5.1000,                 loss: -0.0386
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 2881/30000 (9.6033%),                 avg. length: 1679.0,                last time consumption/overall running time: 213.9080s / 46934.2660 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1123
env0_second_0:                 episode reward: -3.3500,                 loss: -0.0334
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 2901/30000 (9.6700%),                 avg. length: 1571.35,                last time consumption/overall running time: 199.3191s / 47133.5851 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1250
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0765
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 2921/30000 (9.7367%),                 avg. length: 1739.55,                last time consumption/overall running time: 217.1031s / 47350.6882 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.1517
env0_second_0:                 episode reward: -6.7500,                 loss: 0.1394
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 2941/30000 (9.8033%),                 avg. length: 1469.65,                last time consumption/overall running time: 187.5749s / 47538.2631 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2003
env0_second_0:                 episode reward: -2.1500,                 loss: -0.1328
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2961/30000 (9.8700%),                 avg. length: 1548.15,                last time consumption/overall running time: 192.0125s / 47730.2756 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.2140
env0_second_0:                 episode reward: -2.8000,                 loss: -0.1342
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 2981/30000 (9.9367%),                 avg. length: 1525.0,                last time consumption/overall running time: 191.6773s / 47921.9529 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1931
env0_second_0:                 episode reward: -3.1500,                 loss: -0.1013
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 3001/30000 (10.0033%),                 avg. length: 1890.65,                last time consumption/overall running time: 236.6576s / 48158.6106 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.1332
env0_second_0:                 episode reward: -4.9000,                 loss: -0.0713
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 3021/30000 (10.0700%),                 avg. length: 1748.75,                last time consumption/overall running time: 215.0531s / 48373.6637 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.1750
env0_second_0:                 episode reward: -4.3500,                 loss: -0.1264
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 3041/30000 (10.1367%),                 avg. length: 2187.1,                last time consumption/overall running time: 277.9103s / 48651.5740 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1066
env0_second_0:                 episode reward: -5.1000,                 loss: -0.0651
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3061/30000 (10.2033%),                 avg. length: 2029.75,                last time consumption/overall running time: 250.0172s / 48901.5912 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1738
env0_second_0:                 episode reward: -3.4500,                 loss: -0.1014
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 3081/30000 (10.2700%),                 avg. length: 2165.55,                last time consumption/overall running time: 268.9677s / 49170.5589 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1708
env0_second_0:                 episode reward: -3.9000,                 loss: -0.1099
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 3101/30000 (10.3367%),                 avg. length: 1924.2,                last time consumption/overall running time: 244.6597s / 49415.2185 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.1515
env0_second_0:                 episode reward: -6.8000,                 loss: -0.0802
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 3121/30000 (10.4033%),                 avg. length: 1777.75,                last time consumption/overall running time: 222.4285s / 49637.6470 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1567
env0_second_0:                 episode reward: -3.9000,                 loss: -0.0936
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 3141/30000 (10.4700%),                 avg. length: 1942.4,                last time consumption/overall running time: 238.0240s / 49875.6710 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1548
env0_second_0:                 episode reward: -1.4000,                 loss: -0.1059
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 3161/30000 (10.5367%),                 avg. length: 1724.7,                last time consumption/overall running time: 216.7897s / 50092.4607 s
env0_first_0:                 episode reward: 8.8500,                 loss: -0.0808
env0_second_0:                 episode reward: -8.8500,                 loss: -0.0389
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 3181/30000 (10.6033%),                 avg. length: 1677.55,                last time consumption/overall running time: 211.2207s / 50303.6815 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.1748
env0_second_0:                 episode reward: -5.9000,                 loss: -0.1098
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 3201/30000 (10.6700%),                 avg. length: 1655.45,                last time consumption/overall running time: 208.8175s / 50512.4990 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1803
env0_second_0:                 episode reward: -1.8500,                 loss: -0.1063
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 3221/30000 (10.7367%),                 avg. length: 1573.35,                last time consumption/overall running time: 188.0969s / 50700.5959 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1896
env0_second_0:                 episode reward: -2.9500,                 loss: -0.1116
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 3241/30000 (10.8033%),                 avg. length: 1737.1,                last time consumption/overall running time: 195.3009s / 50895.8968 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2077
env0_second_0:                 episode reward: -2.8500,                 loss: -0.1427
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3261/30000 (10.8700%),                 avg. length: 1585.6,                last time consumption/overall running time: 201.2144s / 51097.1113 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2226
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0963
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 3281/30000 (10.9367%),                 avg. length: 1562.85,                last time consumption/overall running time: 199.8846s / 51296.9959 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2088
env0_second_0:                 episode reward: -2.0500,                 loss: -0.0925
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3301/30000 (11.0033%),                 avg. length: 1534.35,                last time consumption/overall running time: 195.6967s / 51492.6925 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2025
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0888
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3321/30000 (11.0700%),                 avg. length: 1640.45,                last time consumption/overall running time: 205.5131s / 51698.2057 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1179
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0475
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 3341/30000 (11.1367%),                 avg. length: 1508.7,                last time consumption/overall running time: 189.2864s / 51887.4921 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0989
env0_second_0:                 episode reward: -3.3000,                 loss: -0.0341
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 3361/30000 (11.2033%),                 avg. length: 1612.85,                last time consumption/overall running time: 204.4330s / 52091.9251 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1293
env0_second_0:                 episode reward: -3.4000,                 loss: -0.0471
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 3381/30000 (11.2700%),                 avg. length: 1652.05,                last time consumption/overall running time: 194.0494s / 52285.9746 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.0999
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0160
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 3401/30000 (11.3367%),                 avg. length: 1678.4,                last time consumption/overall running time: 208.6252s / 52494.5997 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1134
env0_second_0:                 episode reward: -4.7000,                 loss: -0.0029
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 3421/30000 (11.4033%),                 avg. length: 2020.8,                last time consumption/overall running time: 260.1654s / 52754.7651 s
env0_first_0:                 episode reward: 8.6500,                 loss: -0.1573
env0_second_0:                 episode reward: -8.6500,                 loss: -0.0891
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 3441/30000 (11.4700%),                 avg. length: 2137.6,                last time consumption/overall running time: 258.9087s / 53013.6738 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1423
env0_second_0:                 episode reward: -4.9500,                 loss: -0.0954
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 3461/30000 (11.5367%),                 avg. length: 2247.45,                last time consumption/overall running time: 285.4884s / 53299.1622 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1030
env0_second_0:                 episode reward: -4.4000,                 loss: -0.0306
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 3481/30000 (11.6033%),                 avg. length: 2252.0,                last time consumption/overall running time: 285.3664s / 53584.5286 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.1539
env0_second_0:                 episode reward: -4.3000,                 loss: -0.0821
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 3501/30000 (11.6700%),                 avg. length: 1831.25,                last time consumption/overall running time: 219.4021s / 53803.9308 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.1383
env0_second_0:                 episode reward: -4.6500,                 loss: -0.0522
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 3521/30000 (11.7367%),                 avg. length: 1692.3,                last time consumption/overall running time: 215.0399s / 54018.9707 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.1776
env0_second_0:                 episode reward: -4.1000,                 loss: -0.0902
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 3541/30000 (11.8033%),                 avg. length: 1600.55,                last time consumption/overall running time: 203.0553s / 54222.0260 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1455
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0851
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 3561/30000 (11.8700%),                 avg. length: 1568.45,                last time consumption/overall running time: 199.3009s / 54421.3269 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1772
env0_second_0:                 episode reward: -4.1500,                 loss: -0.0881
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 3581/30000 (11.9367%),                 avg. length: 1728.65,                last time consumption/overall running time: 217.4936s / 54638.8204 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1899
env0_second_0:                 episode reward: -3.9000,                 loss: -0.1229
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 3601/30000 (12.0033%),                 avg. length: 1705.3,                last time consumption/overall running time: 217.1111s / 54855.9315 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1641
env0_second_0:                 episode reward: -5.0500,                 loss: -0.0999
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 3621/30000 (12.0700%),                 avg. length: 1798.3,                last time consumption/overall running time: 225.7752s / 55081.7067 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1694
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0949
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 3641/30000 (12.1367%),                 avg. length: 1553.1,                last time consumption/overall running time: 196.4849s / 55278.1917 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1569
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0224
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 3661/30000 (12.2033%),                 avg. length: 1562.4,                last time consumption/overall running time: 200.5313s / 55478.7230 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2035
env0_second_0:                 episode reward: -2.7000,                 loss: -0.1411
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 3681/30000 (12.2700%),                 avg. length: 1734.55,                last time consumption/overall running time: 218.8636s / 55697.5866 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.2053
env0_second_0:                 episode reward: -3.0500,                 loss: -0.1139
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 3701/30000 (12.3367%),                 avg. length: 1575.85,                last time consumption/overall running time: 201.6483s / 55899.2349 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2264
env0_second_0:                 episode reward: -3.3000,                 loss: -0.0899
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3721/30000 (12.4033%),                 avg. length: 1700.0,                last time consumption/overall running time: 208.4738s / 56107.7086 s
env0_first_0:                 episode reward: 7.2000,                 loss: -0.2046
env0_second_0:                 episode reward: -7.2000,                 loss: -0.1355
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 3741/30000 (12.4700%),                 avg. length: 1708.75,                last time consumption/overall running time: 214.3868s / 56322.0955 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.1854
env0_second_0:                 episode reward: -5.4500,                 loss: -0.1087
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3761/30000 (12.5367%),                 avg. length: 2012.25,                last time consumption/overall running time: 257.0627s / 56579.1582 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.1992
env0_second_0:                 episode reward: -5.9000,                 loss: -0.1205
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 3781/30000 (12.6033%),                 avg. length: 2027.4,                last time consumption/overall running time: 257.4202s / 56836.5784 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.2078
env0_second_0:                 episode reward: -5.0000,                 loss: -0.1555
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 3801/30000 (12.6700%),                 avg. length: 1709.0,                last time consumption/overall running time: 214.6483s / 57051.2268 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2151
env0_second_0:                 episode reward: -4.3000,                 loss: -0.1211
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 3821/30000 (12.7367%),                 avg. length: 1770.7,                last time consumption/overall running time: 225.0813s / 57276.3080 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.1894
env0_second_0:                 episode reward: -5.6000,                 loss: -0.0741
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 3841/30000 (12.8033%),                 avg. length: 1779.5,                last time consumption/overall running time: 219.1399s / 57495.4479 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1849
env0_second_0:                 episode reward: -5.0500,                 loss: -0.1391
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 3861/30000 (12.8700%),                 avg. length: 1728.7,                last time consumption/overall running time: 218.7342s / 57714.1822 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1954
env0_second_0:                 episode reward: -3.8500,                 loss: -0.0993
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 3881/30000 (12.9367%),                 avg. length: 1728.75,                last time consumption/overall running time: 208.5702s / 57922.7524 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.2137
env0_second_0:                 episode reward: -3.0000,                 loss: -0.1443
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 3901/30000 (13.0033%),                 avg. length: 1556.05,                last time consumption/overall running time: 197.3879s / 58120.1403 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2450
env0_second_0:                 episode reward: -2.1500,                 loss: -0.1591
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 3921/30000 (13.0700%),                 avg. length: 1639.7,                last time consumption/overall running time: 190.3976s / 58310.5379 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2135
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0167
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 3941/30000 (13.1367%),                 avg. length: 1730.85,                last time consumption/overall running time: 218.1113s / 58528.6492 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1910
env0_second_0:                 episode reward: -3.0500,                 loss: -0.0955
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 3961/30000 (13.2033%),                 avg. length: 2053.85,                last time consumption/overall running time: 249.1527s / 58777.8019 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1695
env0_second_0:                 episode reward: -3.6500,                 loss: 0.0720
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 3981/30000 (13.2700%),                 avg. length: 1615.1,                last time consumption/overall running time: 190.4725s / 58968.2743 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.2342
env0_second_0:                 episode reward: -4.6500,                 loss: -0.1032
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 4001/30000 (13.3367%),                 avg. length: 1822.85,                last time consumption/overall running time: 224.0918s / 59192.3661 s
env0_first_0:                 episode reward: 12.2000,                 loss: -0.1346
env0_second_0:                 episode reward: -12.2000,                 loss: -0.0373
env1_first_0:                 episode reward: 10.8500,                 loss: nan
env1_second_0:                 episode reward: -10.8500,                 loss: nan
Episode: 4021/30000 (13.4033%),                 avg. length: 1614.9,                last time consumption/overall running time: 198.0739s / 59390.4401 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.2152
env0_second_0:                 episode reward: -6.6500,                 loss: -0.1365
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 4041/30000 (13.4700%),                 avg. length: 1572.8,                last time consumption/overall running time: 197.0724s / 59587.5125 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.2214
env0_second_0:                 episode reward: -2.9000,                 loss: -0.1675
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4061/30000 (13.5367%),                 avg. length: 1515.65,                last time consumption/overall running time: 197.3670s / 59784.8795 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2454
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1707
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 4081/30000 (13.6033%),                 avg. length: 1654.95,                last time consumption/overall running time: 207.6545s / 59992.5339 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1937
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0862
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4101/30000 (13.6700%),                 avg. length: 1483.85,                last time consumption/overall running time: 186.3718s / 60178.9057 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2190
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1338
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 4121/30000 (13.7367%),                 avg. length: 1629.45,                last time consumption/overall running time: 200.9427s / 60379.8484 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2221
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0892
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4141/30000 (13.8033%),                 avg. length: 1519.8,                last time consumption/overall running time: 189.9206s / 60569.7690 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2184
env0_second_0:                 episode reward: -2.2500,                 loss: -0.1205
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 4161/30000 (13.8700%),                 avg. length: 1880.0,                last time consumption/overall running time: 238.2944s / 60808.0634 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2347
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0949
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 4181/30000 (13.9367%),                 avg. length: 1691.95,                last time consumption/overall running time: 212.0179s / 61020.0813 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2306
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0929
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 4201/30000 (14.0033%),                 avg. length: 1658.85,                last time consumption/overall running time: 208.6508s / 61228.7321 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2412
env0_second_0:                 episode reward: -2.0000,                 loss: -0.1306
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4221/30000 (14.0700%),                 avg. length: 2368.9,                last time consumption/overall running time: 293.1677s / 61521.8999 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.2042
env0_second_0:                 episode reward: 2.6000,                 loss: -0.0983
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 4241/30000 (14.1367%),                 avg. length: 1541.85,                last time consumption/overall running time: 196.8131s / 61718.7129 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2288
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0667
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4261/30000 (14.2033%),                 avg. length: 1557.05,                last time consumption/overall running time: 176.5861s / 61895.2990 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2546
env0_second_0:                 episode reward: -2.0000,                 loss: -0.1184
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 4281/30000 (14.2700%),                 avg. length: 1596.0,                last time consumption/overall running time: 201.9248s / 62097.2238 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2178
env0_second_0:                 episode reward: -2.7000,                 loss: -0.1062
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 4301/30000 (14.3367%),                 avg. length: 1634.75,                last time consumption/overall running time: 207.4931s / 62304.7168 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2549
env0_second_0:                 episode reward: -2.6500,                 loss: -0.1775
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 4321/30000 (14.4033%),                 avg. length: 1585.0,                last time consumption/overall running time: 200.2665s / 62504.9833 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2542
env0_second_0:                 episode reward: -2.6500,                 loss: -0.1650
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4341/30000 (14.4700%),                 avg. length: 1536.6,                last time consumption/overall running time: 194.1681s / 62699.1514 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1915
env0_second_0:                 episode reward: -2.7000,                 loss: -0.1161
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 4361/30000 (14.5367%),                 avg. length: 1475.1,                last time consumption/overall running time: 183.0131s / 62882.1645 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.2320
env0_second_0:                 episode reward: -2.5000,                 loss: -0.1368
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 4381/30000 (14.6033%),                 avg. length: 1504.55,                last time consumption/overall running time: 190.7277s / 63072.8923 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2101
env0_second_0:                 episode reward: -1.8500,                 loss: -0.1225
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 4401/30000 (14.6700%),                 avg. length: 1568.35,                last time consumption/overall running time: 194.2570s / 63267.1492 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.2208
env0_second_0:                 episode reward: -3.0500,                 loss: -0.1522
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 4421/30000 (14.7367%),                 avg. length: 1502.65,                last time consumption/overall running time: 191.8795s / 63459.0287 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2291
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1621
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 4441/30000 (14.8033%),                 avg. length: 1524.45,                last time consumption/overall running time: 189.7757s / 63648.8044 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2183
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1457
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 4461/30000 (14.8700%),                 avg. length: 1543.8,                last time consumption/overall running time: 186.1371s / 63834.9415 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2726
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2090
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 4481/30000 (14.9367%),                 avg. length: 1607.75,                last time consumption/overall running time: 203.9422s / 64038.8837 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2495
env0_second_0:                 episode reward: -2.7500,                 loss: -0.1910
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 4501/30000 (15.0033%),                 avg. length: 1522.9,                last time consumption/overall running time: 185.6229s / 64224.5065 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2176
env0_second_0:                 episode reward: -1.7000,                 loss: -0.1513
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4521/30000 (15.0700%),                 avg. length: 1626.95,                last time consumption/overall running time: 197.5319s / 64422.0384 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2117
env0_second_0:                 episode reward: -1.8000,                 loss: -0.1409
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4541/30000 (15.1367%),                 avg. length: 1572.4,                last time consumption/overall running time: 189.1520s / 64611.1904 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2413
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1675
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 4561/30000 (15.2033%),                 avg. length: 1611.7,                last time consumption/overall running time: 194.0064s / 64805.1968 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2277
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0690
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 4581/30000 (15.2700%),                 avg. length: 1657.2,                last time consumption/overall running time: 207.3596s / 65012.5564 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2474
env0_second_0:                 episode reward: -1.8000,                 loss: -0.1643
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 4601/30000 (15.3367%),                 avg. length: 1615.4,                last time consumption/overall running time: 197.8015s / 65210.3579 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2371
env0_second_0:                 episode reward: -1.5000,                 loss: -0.1601
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4621/30000 (15.4033%),                 avg. length: 1542.7,                last time consumption/overall running time: 196.4339s / 65406.7918 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2512
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1180
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 4641/30000 (15.4700%),                 avg. length: 1573.35,                last time consumption/overall running time: 201.3915s / 65608.1834 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2639
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1471
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4661/30000 (15.5367%),                 avg. length: 1505.2,                last time consumption/overall running time: 190.9171s / 65799.1004 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2526
env0_second_0:                 episode reward: -1.6500,                 loss: 0.0343
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 4681/30000 (15.6033%),                 avg. length: 1520.7,                last time consumption/overall running time: 189.5755s / 65988.6759 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2248
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0844
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 4701/30000 (15.6700%),                 avg. length: 1518.1,                last time consumption/overall running time: 184.9006s / 66173.5765 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2282
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0993
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 4721/30000 (15.7367%),                 avg. length: 1460.75,                last time consumption/overall running time: 186.2032s / 66359.7796 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2296
env0_second_0:                 episode reward: -1.0000,                 loss: -0.1125
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 4741/30000 (15.8033%),                 avg. length: 1658.0,                last time consumption/overall running time: 204.3924s / 66564.1720 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2029
env0_second_0:                 episode reward: -2.1000,                 loss: -0.0589
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 4761/30000 (15.8700%),                 avg. length: 1830.1,                last time consumption/overall running time: 227.8016s / 66791.9737 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1930
env0_second_0:                 episode reward: -2.5000,                 loss: -0.0997
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 4781/30000 (15.9367%),                 avg. length: 1544.8,                last time consumption/overall running time: 192.2069s / 66984.1806 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.2673
env0_second_0:                 episode reward: -2.5000,                 loss: -0.1567
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4801/30000 (16.0033%),                 avg. length: 1536.4,                last time consumption/overall running time: 195.7619s / 67179.9424 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.2513
env0_second_0:                 episode reward: -2.2000,                 loss: 0.0533
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4821/30000 (16.0700%),                 avg. length: 1621.45,                last time consumption/overall running time: 202.3865s / 67382.3290 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2463
env0_second_0:                 episode reward: -2.3000,                 loss: -0.1176
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 4841/30000 (16.1367%),                 avg. length: 1692.15,                last time consumption/overall running time: 213.5197s / 67595.8487 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2065
env0_second_0:                 episode reward: -1.4500,                 loss: -0.0849
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 4861/30000 (16.2033%),                 avg. length: 1825.1,                last time consumption/overall running time: 223.3195s / 67819.1681 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2120
env0_second_0:                 episode reward: -3.3500,                 loss: -0.0903
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 4881/30000 (16.2700%),                 avg. length: 1687.3,                last time consumption/overall running time: 211.3624s / 68030.5305 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2018
env0_second_0:                 episode reward: -1.8000,                 loss: -0.0585
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 4901/30000 (16.3367%),                 avg. length: 1590.3,                last time consumption/overall running time: 199.5617s / 68230.0922 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2172
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0815
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 4921/30000 (16.4033%),                 avg. length: 1543.8,                last time consumption/overall running time: 194.1964s / 68424.2887 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1756
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0142
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4941/30000 (16.4700%),                 avg. length: 1635.65,                last time consumption/overall running time: 203.7323s / 68628.0210 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2022
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0705
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4961/30000 (16.5367%),                 avg. length: 1672.6,                last time consumption/overall running time: 211.8619s / 68839.8829 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2100
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0980
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 4981/30000 (16.6033%),                 avg. length: 1599.2,                last time consumption/overall running time: 186.2846s / 69026.1675 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2067
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0895
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 5001/30000 (16.6700%),                 avg. length: 1744.65,                last time consumption/overall running time: 216.7369s / 69242.9044 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.2887
env0_second_0:                 episode reward: -5.0500,                 loss: 0.0321
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 5021/30000 (16.7367%),                 avg. length: 1790.45,                last time consumption/overall running time: 210.5148s / 69453.4193 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.1790
env0_second_0:                 episode reward: -9.7000,                 loss: 0.1142
env1_first_0:                 episode reward: 10.0000,                 loss: nan
env1_second_0:                 episode reward: -10.0000,                 loss: nan
Episode: 5041/30000 (16.8033%),                 avg. length: 1902.15,                last time consumption/overall running time: 239.5525s / 69692.9717 s
env0_first_0:                 episode reward: 6.8000,                 loss: -0.1634
env0_second_0:                 episode reward: -6.8000,                 loss: 0.0552
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 5061/30000 (16.8700%),                 avg. length: 1638.05,                last time consumption/overall running time: 206.2285s / 69899.2002 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1969
env0_second_0:                 episode reward: -2.9500,                 loss: -0.0656
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 5081/30000 (16.9367%),                 avg. length: 1499.8,                last time consumption/overall running time: 189.8865s / 70089.0867 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2202
env0_second_0:                 episode reward: -2.3500,                 loss: -0.1269
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 5101/30000 (17.0033%),                 avg. length: 1603.1,                last time consumption/overall running time: 192.3577s / 70281.4444 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2320
env0_second_0:                 episode reward: -2.1500,                 loss: -0.1569
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 5121/30000 (17.0700%),                 avg. length: 1487.4,                last time consumption/overall running time: 185.3305s / 70466.7749 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.2546
env0_second_0:                 episode reward: -2.5000,                 loss: -0.1286
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 5141/30000 (17.1367%),                 avg. length: 1491.5,                last time consumption/overall running time: 180.8020s / 70647.5769 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2437
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0286
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5161/30000 (17.2033%),                 avg. length: 1469.6,                last time consumption/overall running time: 182.5297s / 70830.1066 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2534
env0_second_0:                 episode reward: -2.7500,                 loss: -0.1129
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5181/30000 (17.2700%),                 avg. length: 1445.95,                last time consumption/overall running time: 184.5206s / 71014.6272 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2294
env0_second_0:                 episode reward: -1.2500,                 loss: -0.1482
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 5201/30000 (17.3367%),                 avg. length: 1482.3,                last time consumption/overall running time: 186.7442s / 71201.3714 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1681
env0_second_0:                 episode reward: -0.6500,                 loss: -0.0567
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5221/30000 (17.4033%),                 avg. length: 1496.55,                last time consumption/overall running time: 190.5286s / 71391.9000 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1922
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0947
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 5241/30000 (17.4700%),                 avg. length: 1474.65,                last time consumption/overall running time: 186.9001s / 71578.8001 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2496
env0_second_0:                 episode reward: -2.6000,                 loss: 0.2280
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5261/30000 (17.5367%),                 avg. length: 1479.6,                last time consumption/overall running time: 191.5765s / 71770.3766 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2646
env0_second_0:                 episode reward: -1.9000,                 loss: -0.1011
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 5281/30000 (17.6033%),                 avg. length: 1460.65,                last time consumption/overall running time: 188.7438s / 71959.1205 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2436
env0_second_0:                 episode reward: -1.7500,                 loss: -0.1278
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5301/30000 (17.6700%),                 avg. length: 1467.95,                last time consumption/overall running time: 176.2828s / 72135.4032 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2376
env0_second_0:                 episode reward: -1.8500,                 loss: -0.1219
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 5321/30000 (17.7367%),                 avg. length: 1480.55,                last time consumption/overall running time: 191.4368s / 72326.8400 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2677
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0634
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 5341/30000 (17.8033%),                 avg. length: 1441.75,                last time consumption/overall running time: 183.2384s / 72510.0784 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.2030
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0260
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5361/30000 (17.8700%),                 avg. length: 1488.9,                last time consumption/overall running time: 190.2785s / 72700.3569 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2354
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0009
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 5381/30000 (17.9367%),                 avg. length: 1482.25,                last time consumption/overall running time: 181.4616s / 72881.8185 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2151
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0436
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 5401/30000 (18.0033%),                 avg. length: 1615.8,                last time consumption/overall running time: 208.7458s / 73090.5644 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2306
env0_second_0:                 episode reward: -4.4000,                 loss: 0.3342
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 5421/30000 (18.0700%),                 avg. length: 1545.05,                last time consumption/overall running time: 199.5134s / 73290.0778 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2381
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0868
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5441/30000 (18.1367%),                 avg. length: 1492.3,                last time consumption/overall running time: 194.0273s / 73484.1050 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2085
env0_second_0:                 episode reward: -0.4500,                 loss: 0.3133
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5461/30000 (18.2033%),                 avg. length: 1450.9,                last time consumption/overall running time: 182.2085s / 73666.3136 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1561
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0133
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 5481/30000 (18.2700%),                 avg. length: 1461.55,                last time consumption/overall running time: 189.9402s / 73856.2538 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1925
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0993
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 5501/30000 (18.3367%),                 avg. length: 1422.55,                last time consumption/overall running time: 176.4735s / 74032.7273 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1490
env0_second_0:                 episode reward: 1.8000,                 loss: 0.1936
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 5521/30000 (18.4033%),                 avg. length: 1434.45,                last time consumption/overall running time: 182.1903s / 74214.9176 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1559
env0_second_0:                 episode reward: 0.2500,                 loss: 0.0840
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 5541/30000 (18.4700%),                 avg. length: 1623.9,                last time consumption/overall running time: 204.1201s / 74419.0377 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1623
env0_second_0:                 episode reward: -4.6000,                 loss: 0.2003
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 5561/30000 (18.5367%),                 avg. length: 1476.65,                last time consumption/overall running time: 177.9589s / 74596.9966 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2411
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0332
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 5581/30000 (18.6033%),                 avg. length: 1434.45,                last time consumption/overall running time: 182.6200s / 74779.6166 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2258
env0_second_0:                 episode reward: -2.8500,                 loss: 0.1065
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 5601/30000 (18.6700%),                 avg. length: 1433.3,                last time consumption/overall running time: 177.4987s / 74957.1154 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2204
env0_second_0:                 episode reward: -1.1000,                 loss: 0.0406
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 5621/30000 (18.7367%),                 avg. length: 1451.6,                last time consumption/overall running time: 184.2410s / 75141.3564 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2775
env0_second_0:                 episode reward: -1.3500,                 loss: -0.1010
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 5641/30000 (18.8033%),                 avg. length: 1438.9,                last time consumption/overall running time: 182.8638s / 75324.2202 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2071
env0_second_0:                 episode reward: -1.6500,                 loss: -0.0517
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 5661/30000 (18.8700%),                 avg. length: 1654.1,                last time consumption/overall running time: 200.4140s / 75524.6342 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1848
env0_second_0:                 episode reward: -2.2500,                 loss: 0.1091
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 5681/30000 (18.9367%),                 avg. length: 1614.1,                last time consumption/overall running time: 205.2802s / 75729.9144 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1690
env0_second_0:                 episode reward: -1.2000,                 loss: 0.1117
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 5701/30000 (19.0033%),                 avg. length: 1484.8,                last time consumption/overall running time: 190.3025s / 75920.2168 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2046
env0_second_0:                 episode reward: -0.6500,                 loss: 0.1057
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 5721/30000 (19.0700%),                 avg. length: 1503.25,                last time consumption/overall running time: 190.4388s / 76110.6556 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1983
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0032
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 5741/30000 (19.1367%),                 avg. length: 1505.6,                last time consumption/overall running time: 193.3237s / 76303.9793 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.2454
env0_second_0:                 episode reward: -2.5500,                 loss: -0.0106
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 5761/30000 (19.2033%),                 avg. length: 1474.55,                last time consumption/overall running time: 159.7097s / 76463.6889 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2143
env0_second_0:                 episode reward: -1.4500,                 loss: -0.0703
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 5781/30000 (19.2700%),                 avg. length: 1463.3,                last time consumption/overall running time: 188.8627s / 76652.5516 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2326
env0_second_0:                 episode reward: -2.4000,                 loss: -0.0991
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 5801/30000 (19.3367%),                 avg. length: 1433.05,                last time consumption/overall running time: 172.5575s / 76825.1091 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2425
env0_second_0:                 episode reward: -1.3500,                 loss: -0.1270
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 5821/30000 (19.4033%),                 avg. length: 1454.1,                last time consumption/overall running time: 186.0060s / 77011.1150 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2290
env0_second_0:                 episode reward: -1.7500,                 loss: -0.0017
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 5841/30000 (19.4700%),                 avg. length: 1433.8,                last time consumption/overall running time: 162.1639s / 77173.2789 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2398
env0_second_0:                 episode reward: -1.5500,                 loss: 0.3737
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 5861/30000 (19.5367%),                 avg. length: 1426.7,                last time consumption/overall running time: 168.9682s / 77342.2471 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2414
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0263
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 5881/30000 (19.6033%),                 avg. length: 1477.6,                last time consumption/overall running time: 178.1983s / 77520.4455 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2229
env0_second_0:                 episode reward: -1.3500,                 loss: -0.0179
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 5901/30000 (19.6700%),                 avg. length: 1426.65,                last time consumption/overall running time: 183.1036s / 77703.5491 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2409
env0_second_0:                 episode reward: -1.9500,                 loss: -0.0513
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5921/30000 (19.7367%),                 avg. length: 1515.25,                last time consumption/overall running time: 193.4773s / 77897.0263 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1590
env0_second_0:                 episode reward: -0.8000,                 loss: 0.1256
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 5941/30000 (19.8033%),                 avg. length: 1523.4,                last time consumption/overall running time: 189.5360s / 78086.5623 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1841
env0_second_0:                 episode reward: -3.1500,                 loss: 0.0445
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 5961/30000 (19.8700%),                 avg. length: 1479.45,                last time consumption/overall running time: 187.7913s / 78274.3536 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2493
env0_second_0:                 episode reward: -2.7500,                 loss: -0.0799
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5981/30000 (19.9367%),                 avg. length: 1445.15,                last time consumption/overall running time: 183.3076s / 78457.6613 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2579
env0_second_0:                 episode reward: -2.3500,                 loss: 0.1216
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 6001/30000 (20.0033%),                 avg. length: 1426.35,                last time consumption/overall running time: 182.1273s / 78639.7886 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2249
env0_second_0:                 episode reward: -2.3000,                 loss: -0.0783
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 6021/30000 (20.0700%),                 avg. length: 1416.2,                last time consumption/overall running time: 180.2144s / 78820.0030 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2192
env0_second_0:                 episode reward: -1.9000,                 loss: -0.0759
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 6041/30000 (20.1367%),                 avg. length: 1425.7,                last time consumption/overall running time: 182.6049s / 79002.6078 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2291
env0_second_0:                 episode reward: -1.9500,                 loss: 0.0258
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 6061/30000 (20.2033%),                 avg. length: 1460.9,                last time consumption/overall running time: 186.8151s / 79189.4229 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.1390
env0_second_0:                 episode reward: -5.5500,                 loss: 0.0532
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 6081/30000 (20.2700%),                 avg. length: 1423.2,                last time consumption/overall running time: 181.1458s / 79370.5688 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1931
env0_second_0:                 episode reward: -2.4500,                 loss: -0.0605
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6101/30000 (20.3367%),                 avg. length: 1427.25,                last time consumption/overall running time: 180.6841s / 79551.2528 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2227
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0719
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6121/30000 (20.4033%),                 avg. length: 1604.8,                last time consumption/overall running time: 178.0594s / 79729.3123 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.1485
env0_second_0:                 episode reward: -7.6500,                 loss: 0.0441
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 6141/30000 (20.4700%),                 avg. length: 1733.5,                last time consumption/overall running time: 217.2916s / 79946.6039 s
env0_first_0:                 episode reward: 12.6500,                 loss: -0.1905
env0_second_0:                 episode reward: -12.6500,                 loss: 0.1481
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
Episode: 6161/30000 (20.5367%),                 avg. length: 1503.7,                last time consumption/overall running time: 189.8982s / 80136.5021 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.2212
env0_second_0:                 episode reward: -5.1000,                 loss: 0.0070
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 6181/30000 (20.6033%),                 avg. length: 1474.4,                last time consumption/overall running time: 184.5078s / 80321.0099 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.2408
env0_second_0:                 episode reward: -2.8000,                 loss: -0.0004
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 6201/30000 (20.6700%),                 avg. length: 1450.0,                last time consumption/overall running time: 183.7106s / 80504.7204 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2524
env0_second_0:                 episode reward: -2.0000,                 loss: -0.1033
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 6221/30000 (20.7367%),                 avg. length: 1433.0,                last time consumption/overall running time: 177.2136s / 80681.9340 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1860
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0845
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 6241/30000 (20.8033%),                 avg. length: 1491.45,                last time consumption/overall running time: 188.3333s / 80870.2673 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.2445
env0_second_0:                 episode reward: -3.1500,                 loss: 0.1924
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 6261/30000 (20.8700%),                 avg. length: 1487.3,                last time consumption/overall running time: 189.9432s / 81060.2105 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2938
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1498
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 6281/30000 (20.9367%),                 avg. length: 1442.8,                last time consumption/overall running time: 184.1977s / 81244.4082 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2481
env0_second_0:                 episode reward: -1.2500,                 loss: -0.1387
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 6301/30000 (21.0033%),                 avg. length: 1507.0,                last time consumption/overall running time: 185.8974s / 81430.3056 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1930
env0_second_0:                 episode reward: -2.7500,                 loss: 0.0408
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6321/30000 (21.0700%),                 avg. length: 1605.25,                last time consumption/overall running time: 191.6886s / 81621.9942 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.2239
env0_second_0:                 episode reward: -3.1000,                 loss: 0.0398
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 6341/30000 (21.1367%),                 avg. length: 1480.3,                last time consumption/overall running time: 188.7697s / 81810.7639 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2470
env0_second_0:                 episode reward: -1.5000,                 loss: -0.0050
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6361/30000 (21.2033%),                 avg. length: 1472.15,                last time consumption/overall running time: 186.1404s / 81996.9043 s
env0_first_0:                 episode reward: 10.1000,                 loss: -0.1116
env0_second_0:                 episode reward: -10.1000,                 loss: 0.1299
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 6381/30000 (21.2700%),                 avg. length: 1443.2,                last time consumption/overall running time: 181.6683s / 82178.5726 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1990
env0_second_0:                 episode reward: -1.6000,                 loss: -0.0293
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 6401/30000 (21.3367%),                 avg. length: 1438.35,                last time consumption/overall running time: 179.9950s / 82358.5677 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1858
env0_second_0:                 episode reward: -3.9000,                 loss: -0.0133
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 6421/30000 (21.4033%),                 avg. length: 1444.1,                last time consumption/overall running time: 178.8347s / 82537.4023 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1673
env0_second_0:                 episode reward: -1.8000,                 loss: 0.1108
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6441/30000 (21.4700%),                 avg. length: 1436.0,                last time consumption/overall running time: 185.9600s / 82723.3623 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2048
env0_second_0:                 episode reward: -1.3000,                 loss: 0.1176
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 6461/30000 (21.5367%),                 avg. length: 1486.05,                last time consumption/overall running time: 187.5308s / 82910.8931 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1532
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0874
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 6481/30000 (21.6033%),                 avg. length: 1460.55,                last time consumption/overall running time: 179.2420s / 83090.1351 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2000
env0_second_0:                 episode reward: -2.3500,                 loss: 0.1614
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 6501/30000 (21.6700%),                 avg. length: 1744.9,                last time consumption/overall running time: 216.9401s / 83307.0752 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.0093
env0_second_0:                 episode reward: 4.3500,                 loss: 0.3509
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 6521/30000 (21.7367%),                 avg. length: 1579.4,                last time consumption/overall running time: 201.0139s / 83508.0891 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.0750
env0_second_0:                 episode reward: -7.3500,                 loss: 0.2814
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 6541/30000 (21.8033%),                 avg. length: 1442.55,                last time consumption/overall running time: 181.5442s / 83689.6332 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2373
env0_second_0:                 episode reward: -0.9500,                 loss: 0.1036
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 6561/30000 (21.8700%),                 avg. length: 1433.1,                last time consumption/overall running time: 182.6124s / 83872.2456 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2551
env0_second_0:                 episode reward: -2.6500,                 loss: 0.1261
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 6581/30000 (21.9367%),                 avg. length: 1444.15,                last time consumption/overall running time: 175.7957s / 84048.0413 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2504
env0_second_0:                 episode reward: -2.2500,                 loss: 0.1768
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 6601/30000 (22.0033%),                 avg. length: 1486.55,                last time consumption/overall running time: 186.1873s / 84234.2286 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2231
env0_second_0:                 episode reward: -0.4000,                 loss: 3.8248
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 6621/30000 (22.0700%),                 avg. length: 1438.25,                last time consumption/overall running time: 177.0234s / 84411.2520 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2579
env0_second_0:                 episode reward: -1.7500,                 loss: 0.2759
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 6641/30000 (22.1367%),                 avg. length: 1477.05,                last time consumption/overall running time: 182.5393s / 84593.7913 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1700
env0_second_0:                 episode reward: 1.8500,                 loss: 0.3626
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 6661/30000 (22.2033%),                 avg. length: 1455.95,                last time consumption/overall running time: 183.8985s / 84777.6899 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1766
env0_second_0:                 episode reward: 1.3000,                 loss: 0.1700
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 6681/30000 (22.2700%),                 avg. length: 1445.55,                last time consumption/overall running time: 181.3754s / 84959.0652 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1152
env0_second_0:                 episode reward: 2.9000,                 loss: 0.5730
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 6701/30000 (22.3367%),                 avg. length: 1632.7,                last time consumption/overall running time: 208.8499s / 85167.9151 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.2233
env0_second_0:                 episode reward: 0.7000,                 loss: 0.1435
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 6721/30000 (22.4033%),                 avg. length: 1532.4,                last time consumption/overall running time: 193.2530s / 85361.1682 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2066
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0613
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 6741/30000 (22.4700%),                 avg. length: 1420.9,                last time consumption/overall running time: 183.0999s / 85544.2680 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2470
env0_second_0:                 episode reward: -2.6000,                 loss: 0.2624
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 6761/30000 (22.5367%),                 avg. length: 1448.55,                last time consumption/overall running time: 183.3158s / 85727.5838 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2413
env0_second_0:                 episode reward: -2.8500,                 loss: -0.0426
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 6781/30000 (22.6033%),                 avg. length: 1515.25,                last time consumption/overall running time: 192.2613s / 85919.8451 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1582
env0_second_0:                 episode reward: -2.9000,                 loss: 0.1146
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 6801/30000 (22.6700%),                 avg. length: 1492.3,                last time consumption/overall running time: 179.0852s / 86098.9304 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1675
env0_second_0:                 episode reward: -2.7500,                 loss: 0.5114
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 6821/30000 (22.7367%),                 avg. length: 1448.95,                last time consumption/overall running time: 182.2499s / 86281.1802 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2518
env0_second_0:                 episode reward: -2.0500,                 loss: 0.1487
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 6841/30000 (22.8033%),                 avg. length: 1552.0,                last time consumption/overall running time: 198.3882s / 86479.5685 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1929
env0_second_0:                 episode reward: -2.8000,                 loss: 0.2140
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 6861/30000 (22.8700%),                 avg. length: 1466.15,                last time consumption/overall running time: 186.4276s / 86665.9961 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.2130
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0554
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 6881/30000 (22.9367%),                 avg. length: 1472.7,                last time consumption/overall running time: 187.3530s / 86853.3491 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2287
env0_second_0:                 episode reward: -2.1500,                 loss: 0.0282
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 6901/30000 (23.0033%),                 avg. length: 1485.8,                last time consumption/overall running time: 189.8486s / 87043.1976 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2270
env0_second_0:                 episode reward: -1.9500,                 loss: 0.1579
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 6921/30000 (23.0700%),                 avg. length: 1474.3,                last time consumption/overall running time: 187.6146s / 87230.8122 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2221
env0_second_0:                 episode reward: -1.9000,                 loss: 0.0273
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 6941/30000 (23.1367%),                 avg. length: 1630.65,                last time consumption/overall running time: 196.9178s / 87427.7300 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1721
env0_second_0:                 episode reward: -2.7500,                 loss: 0.1281
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 6961/30000 (23.2033%),                 avg. length: 1726.3,                last time consumption/overall running time: 198.9374s / 87626.6674 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1655
env0_second_0:                 episode reward: -0.0500,                 loss: 0.6323
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 6981/30000 (23.2700%),                 avg. length: 1826.3,                last time consumption/overall running time: 220.8128s / 87847.4803 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1120
env0_second_0:                 episode reward: -2.6500,                 loss: 0.4017
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 7001/30000 (23.3367%),                 avg. length: 1708.15,                last time consumption/overall running time: 216.9304s / 88064.4107 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.1591
env0_second_0:                 episode reward: -4.8500,                 loss: 0.5334
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 7021/30000 (23.4033%),                 avg. length: 1716.45,                last time consumption/overall running time: 217.6414s / 88282.0520 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1318
env0_second_0:                 episode reward: -2.4500,                 loss: 0.2530
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 7041/30000 (23.4700%),                 avg. length: 1554.3,                last time consumption/overall running time: 175.4885s / 88457.5405 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1634
env0_second_0:                 episode reward: -0.9000,                 loss: 0.0948
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 7061/30000 (23.5367%),                 avg. length: 1488.05,                last time consumption/overall running time: 189.5882s / 88647.1287 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1869
env0_second_0:                 episode reward: -4.6000,                 loss: 0.0532
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7081/30000 (23.6033%),                 avg. length: 1454.8,                last time consumption/overall running time: 175.0412s / 88822.1699 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2405
env0_second_0:                 episode reward: -2.0500,                 loss: -0.0540
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7101/30000 (23.6700%),                 avg. length: 1437.4,                last time consumption/overall running time: 170.9524s / 88993.1223 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2665
env0_second_0:                 episode reward: -1.7500,                 loss: -0.1186
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7121/30000 (23.7367%),                 avg. length: 1631.95,                last time consumption/overall running time: 198.0175s / 89191.1398 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1975
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0261
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 7141/30000 (23.8033%),                 avg. length: 1657.75,                last time consumption/overall running time: 209.6978s / 89400.8376 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1668
env0_second_0:                 episode reward: -4.5500,                 loss: 0.0317
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 7161/30000 (23.8700%),                 avg. length: 1824.95,                last time consumption/overall running time: 227.2928s / 89628.1304 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.1471
env0_second_0:                 episode reward: -3.3000,                 loss: 0.0777
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 7181/30000 (23.9367%),                 avg. length: 1696.55,                last time consumption/overall running time: 214.0659s / 89842.1964 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1563
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0454
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7201/30000 (24.0033%),                 avg. length: 1666.35,                last time consumption/overall running time: 209.7184s / 90051.9148 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1808
env0_second_0:                 episode reward: -2.3500,                 loss: -0.0287
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 7221/30000 (24.0700%),                 avg. length: 1709.85,                last time consumption/overall running time: 207.5913s / 90259.5061 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1256
env0_second_0:                 episode reward: -3.4500,                 loss: 0.1221
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 7241/30000 (24.1367%),                 avg. length: 1531.65,                last time consumption/overall running time: 189.8852s / 90449.3913 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1959
env0_second_0:                 episode reward: -1.2500,                 loss: 0.7207
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7261/30000 (24.2033%),                 avg. length: 1525.65,                last time consumption/overall running time: 171.6475s / 90621.0388 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2600
env0_second_0:                 episode reward: -1.8000,                 loss: 0.4207
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 7281/30000 (24.2700%),                 avg. length: 1684.3,                last time consumption/overall running time: 213.9297s / 90834.9685 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1891
env0_second_0:                 episode reward: -0.8000,                 loss: 0.4133
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 7301/30000 (24.3367%),                 avg. length: 1600.45,                last time consumption/overall running time: 198.4639s / 91033.4324 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2157
env0_second_0:                 episode reward: -2.4000,                 loss: 0.2429
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 7321/30000 (24.4033%),                 avg. length: 1659.4,                last time consumption/overall running time: 208.8895s / 91242.3219 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2003
env0_second_0:                 episode reward: -2.0000,                 loss: 0.4258
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 7341/30000 (24.4700%),                 avg. length: 1624.05,                last time consumption/overall running time: 210.1684s / 91452.4903 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1996
env0_second_0:                 episode reward: -2.2000,                 loss: 0.3980
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 7361/30000 (24.5367%),                 avg. length: 1606.25,                last time consumption/overall running time: 202.4863s / 91654.9766 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.1632
env0_second_0:                 episode reward: 1.6500,                 loss: 0.2849
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 7381/30000 (24.6033%),                 avg. length: 1881.95,                last time consumption/overall running time: 234.6558s / 91889.6324 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1719
env0_second_0:                 episode reward: 0.8500,                 loss: 0.5897
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 7401/30000 (24.6700%),                 avg. length: 1962.65,                last time consumption/overall running time: 247.1696s / 92136.8019 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2224
env0_second_0:                 episode reward: 0.1500,                 loss: 0.0831
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 7421/30000 (24.7367%),                 avg. length: 1700.95,                last time consumption/overall running time: 212.9813s / 92349.7832 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.2208
env0_second_0:                 episode reward: -3.4500,                 loss: -0.0452
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 7441/30000 (24.8033%),                 avg. length: 1644.1,                last time consumption/overall running time: 211.1528s / 92560.9360 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2008
env0_second_0:                 episode reward: -4.3000,                 loss: 0.0336
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 7461/30000 (24.8700%),                 avg. length: 1520.3,                last time consumption/overall running time: 193.0883s / 92754.0244 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.2259
env0_second_0:                 episode reward: -3.6000,                 loss: -0.0171
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 7481/30000 (24.9367%),                 avg. length: 1457.8,                last time consumption/overall running time: 182.1077s / 92936.1321 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2561
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0755
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 7501/30000 (25.0033%),                 avg. length: 1482.4,                last time consumption/overall running time: 170.9464s / 93107.0785 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1833
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0518
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 7521/30000 (25.0700%),                 avg. length: 1488.5,                last time consumption/overall running time: 156.9946s / 93264.0731 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2056
env0_second_0:                 episode reward: -1.7000,                 loss: 0.1074
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 7541/30000 (25.1367%),                 avg. length: 1632.1,                last time consumption/overall running time: 206.2337s / 93470.3068 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1373
env0_second_0:                 episode reward: -3.0000,                 loss: 0.1221
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 7561/30000 (25.2033%),                 avg. length: 1766.8,                last time consumption/overall running time: 204.3169s / 93674.6237 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1825
env0_second_0:                 episode reward: -3.2500,                 loss: 0.2638
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 7581/30000 (25.2700%),                 avg. length: 1919.45,                last time consumption/overall running time: 228.1691s / 93902.7928 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1473
env0_second_0:                 episode reward: -5.1500,                 loss: 0.1462
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 7601/30000 (25.3367%),                 avg. length: 1875.65,                last time consumption/overall running time: 237.2663s / 94140.0592 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1319
env0_second_0:                 episode reward: -4.7000,                 loss: 0.1274
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 7621/30000 (25.4033%),                 avg. length: 1995.35,                last time consumption/overall running time: 248.7473s / 94388.8065 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1433
env0_second_0:                 episode reward: -4.1500,                 loss: 0.1827
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 7641/30000 (25.4700%),                 avg. length: 1753.05,                last time consumption/overall running time: 217.8672s / 94606.6737 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1282
env0_second_0:                 episode reward: -4.5500,                 loss: 0.1422
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 7661/30000 (25.5367%),                 avg. length: 2061.35,                last time consumption/overall running time: 256.5397s / 94863.2134 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.1389
env0_second_0:                 episode reward: -5.5500,                 loss: 0.7712
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 7681/30000 (25.6033%),                 avg. length: 2321.15,                last time consumption/overall running time: 291.8496s / 95155.0630 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1454
env0_second_0:                 episode reward: -2.3500,                 loss: 0.1851
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 7701/30000 (25.6700%),                 avg. length: 2105.75,                last time consumption/overall running time: 257.0438s / 95412.1068 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1571
env0_second_0:                 episode reward: -3.0500,                 loss: 0.2356
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 7721/30000 (25.7367%),                 avg. length: 1940.7,                last time consumption/overall running time: 231.8162s / 95643.9230 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1900
env0_second_0:                 episode reward: -3.4000,                 loss: 0.0446
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 7741/30000 (25.8033%),                 avg. length: 2090.95,                last time consumption/overall running time: 253.3635s / 95897.2865 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1745
env0_second_0:                 episode reward: -4.7000,                 loss: -0.0140
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 7761/30000 (25.8700%),                 avg. length: 1849.95,                last time consumption/overall running time: 233.3030s / 96130.5895 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.2109
env0_second_0:                 episode reward: -2.9000,                 loss: -0.0470
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 7781/30000 (25.9367%),                 avg. length: 1898.2,                last time consumption/overall running time: 233.1147s / 96363.7042 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2054
env0_second_0:                 episode reward: -2.3000,                 loss: 0.1598
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 7801/30000 (26.0033%),                 avg. length: 1933.6,                last time consumption/overall running time: 239.2068s / 96602.9111 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1883
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0281
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 7821/30000 (26.0700%),                 avg. length: 1527.05,                last time consumption/overall running time: 192.1707s / 96795.0817 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2583
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0418
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 7841/30000 (26.1367%),                 avg. length: 1588.6,                last time consumption/overall running time: 199.0117s / 96994.0934 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2642
env0_second_0:                 episode reward: -2.0500,                 loss: 0.0116
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 7861/30000 (26.2033%),                 avg. length: 1964.45,                last time consumption/overall running time: 254.7849s / 97248.8783 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1970
env0_second_0:                 episode reward: -2.7000,                 loss: 0.0367
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 7881/30000 (26.2700%),                 avg. length: 1609.05,                last time consumption/overall running time: 221.9077s / 97470.7861 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2310
env0_second_0:                 episode reward: -1.0000,                 loss: -0.0336
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 7901/30000 (26.3367%),                 avg. length: 1610.7,                last time consumption/overall running time: 199.2490s / 97670.0351 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2203
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0283
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 7921/30000 (26.4033%),                 avg. length: 1596.15,                last time consumption/overall running time: 198.5818s / 97868.6169 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1860
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0471
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 7941/30000 (26.4700%),                 avg. length: 1712.85,                last time consumption/overall running time: 216.6611s / 98085.2780 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1940
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0224
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 7961/30000 (26.5367%),                 avg. length: 1599.95,                last time consumption/overall running time: 199.8766s / 98285.1546 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.2596
env0_second_0:                 episode reward: -3.5000,                 loss: 0.1319
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 7981/30000 (26.6033%),                 avg. length: 1498.85,                last time consumption/overall running time: 188.7441s / 98473.8987 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2797
env0_second_0:                 episode reward: -1.0500,                 loss: 0.1286
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 8001/30000 (26.6700%),                 avg. length: 1502.95,                last time consumption/overall running time: 189.9544s / 98663.8530 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2568
env0_second_0:                 episode reward: -1.5500,                 loss: 0.1360
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 8021/30000 (26.7367%),                 avg. length: 1463.45,                last time consumption/overall running time: 182.1413s / 98845.9943 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2312
env0_second_0:                 episode reward: -0.6500,                 loss: 0.2968
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 8041/30000 (26.8033%),                 avg. length: 1490.85,                last time consumption/overall running time: 192.1744s / 99038.1687 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2512
env0_second_0:                 episode reward: -1.5500,                 loss: 0.0389
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 8061/30000 (26.8700%),                 avg. length: 1620.25,                last time consumption/overall running time: 209.1591s / 99247.3277 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1277
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2846
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 8081/30000 (26.9367%),                 avg. length: 1632.8,                last time consumption/overall running time: 242.7307s / 99490.0585 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1490
env0_second_0:                 episode reward: 0.0000,                 loss: 0.1629
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8101/30000 (27.0033%),                 avg. length: 1544.6,                last time consumption/overall running time: 198.8661s / 99688.9246 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2063
env0_second_0:                 episode reward: -1.1500,                 loss: 0.0638
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 8121/30000 (27.0700%),                 avg. length: 1519.5,                last time consumption/overall running time: 198.7207s / 99887.6453 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2321
env0_second_0:                 episode reward: -1.7000,                 loss: 0.1589
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 8141/30000 (27.1367%),                 avg. length: 1504.85,                last time consumption/overall running time: 197.5165s / 100085.1617 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.1208
env0_second_0:                 episode reward: 1.6500,                 loss: 0.3998
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 8161/30000 (27.2033%),                 avg. length: 1576.95,                last time consumption/overall running time: 206.8244s / 100291.9861 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.1116
env0_second_0:                 episode reward: 4.0000,                 loss: 0.2261
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 8181/30000 (27.2700%),                 avg. length: 1460.65,                last time consumption/overall running time: 193.4466s / 100485.4327 s
env0_first_0:                 episode reward: -3.0000,                 loss: -0.1214
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1755
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8201/30000 (27.3367%),                 avg. length: 1509.85,                last time consumption/overall running time: 196.1228s / 100681.5555 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2624
env0_second_0:                 episode reward: -0.4000,                 loss: -0.0502
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 8221/30000 (27.4033%),                 avg. length: 1482.6,                last time consumption/overall running time: 209.3262s / 100890.8816 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.1462
env0_second_0:                 episode reward: 2.9500,                 loss: 0.2561
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 8241/30000 (27.4700%),                 avg. length: 1672.85,                last time consumption/overall running time: 216.8887s / 101107.7704 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0798
env0_second_0:                 episode reward: 4.6000,                 loss: 0.3460
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 8261/30000 (27.5367%),                 avg. length: 1582.25,                last time consumption/overall running time: 214.4909s / 101322.2613 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1354
env0_second_0:                 episode reward: 0.0500,                 loss: 0.1473
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 8281/30000 (27.6033%),                 avg. length: 1485.2,                last time consumption/overall running time: 200.8601s / 101523.1214 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2053
env0_second_0:                 episode reward: -0.5500,                 loss: 0.1425
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 8301/30000 (27.6700%),                 avg. length: 1626.3,                last time consumption/overall running time: 189.3524s / 101712.4738 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1476
env0_second_0:                 episode reward: 0.8500,                 loss: 0.1725
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 8321/30000 (27.7367%),                 avg. length: 1500.2,                last time consumption/overall running time: 187.5668s / 101900.0406 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.1326
env0_second_0:                 episode reward: 4.4000,                 loss: 0.1989
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 8341/30000 (27.8033%),                 avg. length: 1523.35,                last time consumption/overall running time: 189.1678s / 102089.2084 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.1339
env0_second_0:                 episode reward: 7.0000,                 loss: 0.1860
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 8361/30000 (27.8700%),                 avg. length: 1560.85,                last time consumption/overall running time: 185.6394s / 102274.8478 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1601
env0_second_0:                 episode reward: 2.4000,                 loss: 0.3468
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 8381/30000 (27.9367%),                 avg. length: 1425.9,                last time consumption/overall running time: 181.7051s / 102456.5529 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.1464
env0_second_0:                 episode reward: 3.6500,                 loss: 0.3445
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 8401/30000 (28.0033%),                 avg. length: 1442.6,                last time consumption/overall running time: 185.4577s / 102642.0106 s
env0_first_0:                 episode reward: -5.1500,                 loss: -0.1064
env0_second_0:                 episode reward: 5.1500,                 loss: 0.3628
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 8421/30000 (28.0700%),                 avg. length: 1483.55,                last time consumption/overall running time: 189.2398s / 102831.2504 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.1236
env0_second_0:                 episode reward: 1.6500,                 loss: 0.1880
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 8441/30000 (28.1367%),                 avg. length: 1563.2,                last time consumption/overall running time: 192.7259s / 103023.9763 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.1522
env0_second_0:                 episode reward: 1.4000,                 loss: 0.0967
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8461/30000 (28.2033%),                 avg. length: 1660.7,                last time consumption/overall running time: 201.1041s / 103225.0804 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1551
env0_second_0:                 episode reward: 1.2000,                 loss: 0.1486
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 8481/30000 (28.2700%),                 avg. length: 1562.45,                last time consumption/overall running time: 174.5846s / 103399.6650 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.0924
env0_second_0:                 episode reward: 1.6500,                 loss: 0.2954
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 8501/30000 (28.3367%),                 avg. length: 1476.2,                last time consumption/overall running time: 177.5880s / 103577.2530 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1189
env0_second_0:                 episode reward: 1.2000,                 loss: 0.2027
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 8521/30000 (28.4033%),                 avg. length: 1567.9,                last time consumption/overall running time: 183.8746s / 103761.1276 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1374
env0_second_0:                 episode reward: 1.1000,                 loss: 0.2034
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 8541/30000 (28.4700%),                 avg. length: 1548.4,                last time consumption/overall running time: 192.3725s / 103953.5001 s
env0_first_0:                 episode reward: -5.3500,                 loss: -0.0877
env0_second_0:                 episode reward: 5.3500,                 loss: 0.2817
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8561/30000 (28.5367%),                 avg. length: 1495.1,                last time consumption/overall running time: 194.6886s / 104148.1887 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1192
env0_second_0:                 episode reward: 3.8000,                 loss: 0.4607
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 8581/30000 (28.6033%),                 avg. length: 1464.6,                last time consumption/overall running time: 162.9633s / 104311.1520 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1680
env0_second_0:                 episode reward: -0.2000,                 loss: 0.2195
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 8601/30000 (28.6700%),                 avg. length: 1476.9,                last time consumption/overall running time: 186.9058s / 104498.0578 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1874
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0824
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 8621/30000 (28.7367%),                 avg. length: 1453.95,                last time consumption/overall running time: 183.1939s / 104681.2517 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.1511
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0809
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 8641/30000 (28.8033%),                 avg. length: 1544.95,                last time consumption/overall running time: 190.3733s / 104871.6250 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1905
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0750
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 8661/30000 (28.8700%),                 avg. length: 1497.65,                last time consumption/overall running time: 184.8100s / 105056.4350 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1556
env0_second_0:                 episode reward: 1.8500,                 loss: 0.0546
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 8681/30000 (28.9367%),                 avg. length: 1536.15,                last time consumption/overall running time: 183.7358s / 105240.1709 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1438
env0_second_0:                 episode reward: 1.2500,                 loss: 0.1243
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 8701/30000 (29.0033%),                 avg. length: 1469.7,                last time consumption/overall running time: 180.1011s / 105420.2720 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1546
env0_second_0:                 episode reward: 1.0000,                 loss: 0.0499
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 8721/30000 (29.0700%),                 avg. length: 1521.05,                last time consumption/overall running time: 180.1172s / 105600.3892 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1912
env0_second_0:                 episode reward: 1.0500,                 loss: 0.0305
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 8741/30000 (29.1367%),                 avg. length: 1479.2,                last time consumption/overall running time: 188.3289s / 105788.7181 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.1430
env0_second_0:                 episode reward: 2.5000,                 loss: 0.1892
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 8761/30000 (29.2033%),                 avg. length: 1484.35,                last time consumption/overall running time: 183.2759s / 105971.9940 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1782
env0_second_0:                 episode reward: 0.3500,                 loss: 0.1196
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 8781/30000 (29.2700%),                 avg. length: 1456.0,                last time consumption/overall running time: 184.5156s / 106156.5096 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0749
env0_second_0:                 episode reward: 3.9000,                 loss: 0.2810
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 8801/30000 (29.3367%),                 avg. length: 1503.9,                last time consumption/overall running time: 191.8677s / 106348.3773 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1668
env0_second_0:                 episode reward: -0.5000,                 loss: 0.1458
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 8821/30000 (29.4033%),                 avg. length: 1511.25,                last time consumption/overall running time: 192.6857s / 106541.0630 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1671
env0_second_0:                 episode reward: -1.3500,                 loss: 0.1860
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 8841/30000 (29.4700%),                 avg. length: 1611.95,                last time consumption/overall running time: 206.0143s / 106747.0773 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1537
env0_second_0:                 episode reward: -2.8500,                 loss: 0.2388
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 8861/30000 (29.5367%),                 avg. length: 1478.6,                last time consumption/overall running time: 189.4020s / 106936.4793 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1749
env0_second_0:                 episode reward: -0.5000,                 loss: 0.2167
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 8881/30000 (29.6033%),                 avg. length: 1478.35,                last time consumption/overall running time: 189.1907s / 107125.6700 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.1898
env0_second_0:                 episode reward: 1.6000,                 loss: 0.1564
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 8901/30000 (29.6700%),                 avg. length: 1525.45,                last time consumption/overall running time: 194.4339s / 107320.1039 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1660
env0_second_0:                 episode reward: 1.4500,                 loss: 0.2266
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 8921/30000 (29.7367%),                 avg. length: 1447.7,                last time consumption/overall running time: 183.5174s / 107503.6213 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.1018
env0_second_0:                 episode reward: 6.1500,                 loss: 0.2412
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 8941/30000 (29.8033%),                 avg. length: 1440.8,                last time consumption/overall running time: 176.0637s / 107679.6850 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1012
env0_second_0:                 episode reward: 3.5000,                 loss: 3.3274
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 8961/30000 (29.8700%),                 avg. length: 1514.15,                last time consumption/overall running time: 181.2666s / 107860.9516 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.1206
env0_second_0:                 episode reward: 1.6000,                 loss: 1.8688
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 8981/30000 (29.9367%),                 avg. length: 1517.05,                last time consumption/overall running time: 196.9400s / 108057.8916 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1345
env0_second_0:                 episode reward: 2.0500,                 loss: 1.1807
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9001/30000 (30.0033%),                 avg. length: 1695.75,                last time consumption/overall running time: 206.5849s / 108264.4765 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1161
env0_second_0:                 episode reward: 1.2500,                 loss: 4.4287
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9021/30000 (30.0700%),                 avg. length: 1622.85,                last time consumption/overall running time: 206.0211s / 108470.4976 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.0735
env0_second_0:                 episode reward: 3.4500,                 loss: 1.2632
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 9041/30000 (30.1367%),                 avg. length: 1537.85,                last time consumption/overall running time: 194.0817s / 108664.5793 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1167
env0_second_0:                 episode reward: 1.8000,                 loss: 0.9761
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9061/30000 (30.2033%),                 avg. length: 1516.4,                last time consumption/overall running time: 187.0769s / 108851.6562 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1940
env0_second_0:                 episode reward: -1.1500,                 loss: 0.8986
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 9081/30000 (30.2700%),                 avg. length: 1534.55,                last time consumption/overall running time: 188.0144s / 109039.6707 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1838
env0_second_0:                 episode reward: 0.4000,                 loss: 1.9183
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9101/30000 (30.3367%),                 avg. length: 1445.15,                last time consumption/overall running time: 183.5086s / 109223.1793 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1948
env0_second_0:                 episode reward: 0.9000,                 loss: 1.0637
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9121/30000 (30.4033%),                 avg. length: 1475.25,                last time consumption/overall running time: 186.6435s / 109409.8228 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2371
env0_second_0:                 episode reward: -1.3500,                 loss: 0.6989
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 9141/30000 (30.4700%),                 avg. length: 1484.9,                last time consumption/overall running time: 185.6135s / 109595.4363 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1907
env0_second_0:                 episode reward: 0.6000,                 loss: 0.6644
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 9161/30000 (30.5367%),                 avg. length: 1406.8,                last time consumption/overall running time: 176.2273s / 109771.6636 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.0996
env0_second_0:                 episode reward: 4.0000,                 loss: 0.7929
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 9181/30000 (30.6033%),                 avg. length: 1456.9,                last time consumption/overall running time: 174.3593s / 109946.0229 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1182
env0_second_0:                 episode reward: 4.5000,                 loss: 0.5652
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 9201/30000 (30.6700%),                 avg. length: 1553.8,                last time consumption/overall running time: 195.9287s / 110141.9516 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.1918
env0_second_0:                 episode reward: 1.4000,                 loss: 0.3254
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 9221/30000 (30.7367%),                 avg. length: 1540.15,                last time consumption/overall running time: 197.7715s / 110339.7231 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1964
env0_second_0:                 episode reward: 0.1000,                 loss: 0.3931
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 9241/30000 (30.8033%),                 avg. length: 1460.55,                last time consumption/overall running time: 174.5816s / 110514.3047 s
env0_first_0:                 episode reward: -2.8000,                 loss: -0.1672
env0_second_0:                 episode reward: 2.8000,                 loss: 0.2852
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 9261/30000 (30.8700%),                 avg. length: 1406.25,                last time consumption/overall running time: 179.0945s / 110693.3992 s
env0_first_0:                 episode reward: -4.5500,                 loss: -0.1652
env0_second_0:                 episode reward: 4.5500,                 loss: 0.6352
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 9281/30000 (30.9367%),                 avg. length: 1445.6,                last time consumption/overall running time: 184.0375s / 110877.4367 s
env0_first_0:                 episode reward: -5.8000,                 loss: -0.0945
env0_second_0:                 episode reward: 5.8000,                 loss: 0.4506
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 9301/30000 (31.0033%),                 avg. length: 1315.05,                last time consumption/overall running time: 149.8199s / 111027.2566 s
env0_first_0:                 episode reward: -7.5500,                 loss: -0.0599
env0_second_0:                 episode reward: 7.5500,                 loss: 0.4593
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 9321/30000 (31.0700%),                 avg. length: 1513.35,                last time consumption/overall running time: 164.5873s / 111191.8439 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1564
env0_second_0:                 episode reward: 0.3000,                 loss: 0.2672
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9341/30000 (31.1367%),                 avg. length: 1484.35,                last time consumption/overall running time: 167.0056s / 111358.8495 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2145
env0_second_0:                 episode reward: 0.5000,                 loss: 0.1104
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 9361/30000 (31.2033%),                 avg. length: 1561.0,                last time consumption/overall running time: 174.5434s / 111533.3929 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2377
env0_second_0:                 episode reward: 0.5000,                 loss: 0.0581
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9381/30000 (31.2700%),                 avg. length: 1636.5,                last time consumption/overall running time: 191.8210s / 111725.2139 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.2014
env0_second_0:                 episode reward: 0.7500,                 loss: 0.2910
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 9401/30000 (31.3367%),                 avg. length: 1533.5,                last time consumption/overall running time: 170.6381s / 111895.8520 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1969
env0_second_0:                 episode reward: 1.9000,                 loss: 0.0961
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 9421/30000 (31.4033%),                 avg. length: 1526.0,                last time consumption/overall running time: 167.3332s / 112063.1852 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2358
env0_second_0:                 episode reward: -0.5000,                 loss: 0.4938
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 9441/30000 (31.4700%),                 avg. length: 1658.75,                last time consumption/overall running time: 189.3238s / 112252.5090 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2158
env0_second_0:                 episode reward: -0.5000,                 loss: 0.4612
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 9461/30000 (31.5367%),                 avg. length: 1568.05,                last time consumption/overall running time: 190.2772s / 112442.7861 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2431
env0_second_0:                 episode reward: -2.6500,                 loss: 0.1643
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 9481/30000 (31.6033%),                 avg. length: 2183.4,                last time consumption/overall running time: 257.4047s / 112700.1908 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1740
env0_second_0:                 episode reward: -2.5500,                 loss: 0.1536
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 9501/30000 (31.6700%),                 avg. length: 2148.65,                last time consumption/overall running time: 268.3690s / 112968.5599 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1734
env0_second_0:                 episode reward: -3.7500,                 loss: 0.0560
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 9521/30000 (31.7367%),                 avg. length: 2202.7,                last time consumption/overall running time: 276.6845s / 113245.2444 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1855
env0_second_0:                 episode reward: -2.7500,                 loss: 0.1078
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 9541/30000 (31.8033%),                 avg. length: 2342.4,                last time consumption/overall running time: 266.8935s / 113512.1379 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1841
env0_second_0:                 episode reward: -3.2000,                 loss: 0.2692
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 9561/30000 (31.8700%),                 avg. length: 2289.6,                last time consumption/overall running time: 273.7332s / 113785.8711 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1913
env0_second_0:                 episode reward: -4.5500,                 loss: 0.0770
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 9581/30000 (31.9367%),                 avg. length: 2244.0,                last time consumption/overall running time: 272.7221s / 114058.5933 s
env0_first_0:                 episode reward: 7.7000,                 loss: -0.1890
env0_second_0:                 episode reward: -7.7000,                 loss: 0.0389
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 9601/30000 (32.0033%),                 avg. length: 2213.6,                last time consumption/overall running time: 279.1090s / 114337.7023 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.2088
env0_second_0:                 episode reward: -5.8000,                 loss: -0.0250
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 9621/30000 (32.0700%),                 avg. length: 2166.55,                last time consumption/overall running time: 271.6805s / 114609.3829 s
env0_first_0:                 episode reward: 9.0000,                 loss: -0.1843
env0_second_0:                 episode reward: -9.0000,                 loss: 0.0986
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 9641/30000 (32.1367%),                 avg. length: 2009.3,                last time consumption/overall running time: 252.4259s / 114861.8088 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.1794
env0_second_0:                 episode reward: -5.3000,                 loss: 0.1003
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 9661/30000 (32.2033%),                 avg. length: 1613.0,                last time consumption/overall running time: 204.5588s / 115066.3675 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2215
env0_second_0:                 episode reward: 0.4500,                 loss: 0.0121
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 9681/30000 (32.2700%),                 avg. length: 1575.15,                last time consumption/overall running time: 200.9805s / 115267.3481 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1718
env0_second_0:                 episode reward: -1.3000,                 loss: 0.3701
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9701/30000 (32.3367%),                 avg. length: 1472.55,                last time consumption/overall running time: 178.5963s / 115445.9444 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1767
env0_second_0:                 episode reward: 1.1500,                 loss: 0.5577
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9721/30000 (32.4033%),                 avg. length: 1438.95,                last time consumption/overall running time: 153.6428s / 115599.5871 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.1512
env0_second_0:                 episode reward: 2.2500,                 loss: 0.9320
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 9741/30000 (32.4700%),                 avg. length: 1417.95,                last time consumption/overall running time: 148.7543s / 115748.3415 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.1214
env0_second_0:                 episode reward: 3.0500,                 loss: 0.6299
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 9761/30000 (32.5367%),                 avg. length: 1487.4,                last time consumption/overall running time: 189.6108s / 115937.9522 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.1670
env0_second_0:                 episode reward: 2.3000,                 loss: 0.3242
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 9781/30000 (32.6033%),                 avg. length: 1545.95,                last time consumption/overall running time: 185.2159s / 116123.1681 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1121
env0_second_0:                 episode reward: 2.4500,                 loss: 0.5185
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 9801/30000 (32.6700%),                 avg. length: 1519.25,                last time consumption/overall running time: 157.7869s / 116280.9550 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.0510
env0_second_0:                 episode reward: 5.5500,                 loss: 1.1823
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 9821/30000 (32.7367%),                 avg. length: 1826.85,                last time consumption/overall running time: 179.8140s / 116460.7690 s
env0_first_0:                 episode reward: -6.0000,                 loss: -0.0284
env0_second_0:                 episode reward: 6.0000,                 loss: 1.0650
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 9841/30000 (32.8033%),                 avg. length: 1521.15,                last time consumption/overall running time: 171.8994s / 116632.6684 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1043
env0_second_0:                 episode reward: -1.2500,                 loss: 0.3243
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 9861/30000 (32.8700%),                 avg. length: 1763.75,                last time consumption/overall running time: 196.5568s / 116829.2252 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1755
env0_second_0:                 episode reward: -1.5500,                 loss: 0.3699
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 9881/30000 (32.9367%),                 avg. length: 1717.45,                last time consumption/overall running time: 185.2570s / 117014.4821 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1420
env0_second_0:                 episode reward: -1.2000,                 loss: 0.4111
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 9901/30000 (33.0033%),                 avg. length: 1703.3,                last time consumption/overall running time: 204.2066s / 117218.6887 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1471
env0_second_0:                 episode reward: -2.9500,                 loss: 0.3171
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 9921/30000 (33.0700%),                 avg. length: 1529.4,                last time consumption/overall running time: 197.6283s / 117416.3170 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1890
env0_second_0:                 episode reward: -1.6000,                 loss: 0.2906
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 9941/30000 (33.1367%),                 avg. length: 1515.2,                last time consumption/overall running time: 191.5328s / 117607.8498 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1950
env0_second_0:                 episode reward: -0.9500,                 loss: 0.2703
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 9961/30000 (33.2033%),                 avg. length: 1493.9,                last time consumption/overall running time: 185.6803s / 117793.5301 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1958
env0_second_0:                 episode reward: -2.3500,                 loss: 0.2557
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9981/30000 (33.2700%),                 avg. length: 1505.35,                last time consumption/overall running time: 190.8373s / 117984.3674 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2052
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3610
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 10001/30000 (33.3367%),                 avg. length: 1429.25,                last time consumption/overall running time: 182.0062s / 118166.3736 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2077
env0_second_0:                 episode reward: -0.8500,                 loss: 0.4673
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 10021/30000 (33.4033%),                 avg. length: 1489.75,                last time consumption/overall running time: 186.6400s / 118353.0136 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2019
env0_second_0:                 episode reward: -1.8500,                 loss: 0.4171
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 10041/30000 (33.4700%),                 avg. length: 1589.05,                last time consumption/overall running time: 200.7250s / 118553.7386 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1449
env0_second_0:                 episode reward: -0.7500,                 loss: 0.3397
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 10061/30000 (33.5367%),                 avg. length: 2086.85,                last time consumption/overall running time: 257.5167s / 118811.2554 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.1367
env0_second_0:                 episode reward: 1.7500,                 loss: 0.5648
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 10081/30000 (33.6033%),                 avg. length: 2132.9,                last time consumption/overall running time: 268.9761s / 119080.2314 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1246
env0_second_0:                 episode reward: 0.4500,                 loss: 0.6905
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 10101/30000 (33.6700%),                 avg. length: 1501.65,                last time consumption/overall running time: 191.9847s / 119272.2161 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1102
env0_second_0:                 episode reward: -3.5500,                 loss: 0.3335
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 10121/30000 (33.7367%),                 avg. length: 1517.45,                last time consumption/overall running time: 194.2567s / 119466.4729 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.1100
env0_second_0:                 episode reward: -4.6500,                 loss: 0.5119
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 10141/30000 (33.8033%),                 avg. length: 1872.35,                last time consumption/overall running time: 237.9436s / 119704.4164 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1081
env0_second_0:                 episode reward: 0.2500,                 loss: 0.4729
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 10161/30000 (33.8700%),                 avg. length: 2396.85,                last time consumption/overall running time: 302.7774s / 120007.1938 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.1140
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2626
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10181/30000 (33.9367%),                 avg. length: 2842.25,                last time consumption/overall running time: 352.1170s / 120359.3109 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1173
env0_second_0:                 episode reward: -2.6500,                 loss: 0.5907
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 10201/30000 (34.0033%),                 avg. length: 2625.7,                last time consumption/overall running time: 323.3126s / 120682.6235 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.1070
env0_second_0:                 episode reward: -4.0000,                 loss: 0.4445
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 10221/30000 (34.0700%),                 avg. length: 2857.85,                last time consumption/overall running time: 368.8985s / 121051.5220 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1071
env0_second_0:                 episode reward: -3.5500,                 loss: 0.5779
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 10241/30000 (34.1367%),                 avg. length: 2431.35,                last time consumption/overall running time: 312.5043s / 121364.0263 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1065
env0_second_0:                 episode reward: -3.9000,                 loss: 0.8430
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 10261/30000 (34.2033%),                 avg. length: 2558.35,                last time consumption/overall running time: 318.2331s / 121682.2594 s
env0_first_0:                 episode reward: 8.8000,                 loss: -0.1229
env0_second_0:                 episode reward: -8.8000,                 loss: 0.7643
env1_first_0:                 episode reward: 7.3000,                 loss: nan
env1_second_0:                 episode reward: -7.3000,                 loss: nan
Episode: 10281/30000 (34.2700%),                 avg. length: 2467.1,                last time consumption/overall running time: 304.9387s / 121987.1980 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.0860
env0_second_0:                 episode reward: -5.9000,                 loss: 0.5735
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 10301/30000 (34.3367%),                 avg. length: 2763.95,                last time consumption/overall running time: 349.1113s / 122336.3093 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.0610
env0_second_0:                 episode reward: 1.5000,                 loss: 0.4945
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 10321/30000 (34.4033%),                 avg. length: 2659.0,                last time consumption/overall running time: 338.9290s / 122675.2384 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0778
env0_second_0:                 episode reward: -4.6500,                 loss: 0.4553
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 10341/30000 (34.4700%),                 avg. length: 2105.25,                last time consumption/overall running time: 266.1259s / 122941.3643 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.0732
env0_second_0:                 episode reward: -2.8500,                 loss: 0.7939
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 10361/30000 (34.5367%),                 avg. length: 2158.75,                last time consumption/overall running time: 273.5036s / 123214.8678 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.0351
env0_second_0:                 episode reward: -5.1500,                 loss: 0.9772
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 10381/30000 (34.6033%),                 avg. length: 2739.85,                last time consumption/overall running time: 341.6182s / 123556.4861 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1034
env0_second_0:                 episode reward: -4.6000,                 loss: 0.6911
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 10401/30000 (34.6700%),                 avg. length: 2346.65,                last time consumption/overall running time: 293.5828s / 123850.0688 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.0917
env0_second_0:                 episode reward: -5.3000,                 loss: 0.4285
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 10421/30000 (34.7367%),                 avg. length: 2668.6,                last time consumption/overall running time: 339.0787s / 124189.1475 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.0841
env0_second_0:                 episode reward: -2.0000,                 loss: 0.7433
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 10441/30000 (34.8033%),                 avg. length: 2798.9,                last time consumption/overall running time: 359.7084s / 124548.8559 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.0767
env0_second_0:                 episode reward: -4.2500,                 loss: 0.5041
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 10461/30000 (34.8700%),                 avg. length: 2407.2,                last time consumption/overall running time: 298.3748s / 124847.2307 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.0992
env0_second_0:                 episode reward: -4.8000,                 loss: 0.4893
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 10481/30000 (34.9367%),                 avg. length: 2131.1,                last time consumption/overall running time: 269.6963s / 125116.9270 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0584
env0_second_0:                 episode reward: -0.9500,                 loss: 0.7571
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10501/30000 (35.0033%),                 avg. length: 1876.25,                last time consumption/overall running time: 234.8776s / 125351.8046 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.0821
env0_second_0:                 episode reward: -4.7000,                 loss: 0.8793
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 10521/30000 (35.0700%),                 avg. length: 2004.55,                last time consumption/overall running time: 248.3188s / 125600.1234 s
env0_first_0:                 episode reward: 9.4500,                 loss: -0.0932
env0_second_0:                 episode reward: -9.4500,                 loss: 0.6052
env1_first_0:                 episode reward: 11.4500,                 loss: nan
env1_second_0:                 episode reward: -11.4500,                 loss: nan
Episode: 10541/30000 (35.1367%),                 avg. length: 1816.55,                last time consumption/overall running time: 232.7711s / 125832.8945 s
env0_first_0:                 episode reward: 8.9500,                 loss: -0.0771
env0_second_0:                 episode reward: -8.9500,                 loss: 0.8063
env1_first_0:                 episode reward: 8.5500,                 loss: nan
env1_second_0:                 episode reward: -8.5500,                 loss: nan
Episode: 10561/30000 (35.2033%),                 avg. length: 1720.05,                last time consumption/overall running time: 217.7825s / 126050.6770 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.0807
env0_second_0:                 episode reward: -5.1000,                 loss: 0.7419
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 10581/30000 (35.2700%),                 avg. length: 1536.7,                last time consumption/overall running time: 196.0540s / 126246.7310 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1741
env0_second_0:                 episode reward: -4.8000,                 loss: 0.5226
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 10601/30000 (35.3367%),                 avg. length: 1680.9,                last time consumption/overall running time: 213.0563s / 126459.7873 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.0741
env0_second_0:                 episode reward: -4.0000,                 loss: 0.6886
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 10621/30000 (35.4033%),                 avg. length: 1568.9,                last time consumption/overall running time: 200.7824s / 126660.5697 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0940
env0_second_0:                 episode reward: -5.6500,                 loss: 0.7338
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 10641/30000 (35.4700%),                 avg. length: 1610.75,                last time consumption/overall running time: 205.2724s / 126865.8421 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1011
env0_second_0:                 episode reward: -3.2500,                 loss: 0.8730
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 10661/30000 (35.5367%),                 avg. length: 1569.15,                last time consumption/overall running time: 195.5531s / 127061.3951 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.1533
env0_second_0:                 episode reward: -4.0500,                 loss: 0.9858
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 10681/30000 (35.6033%),                 avg. length: 1647.55,                last time consumption/overall running time: 206.1247s / 127267.5198 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1334
env0_second_0:                 episode reward: -4.8000,                 loss: 0.6871
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 10701/30000 (35.6700%),                 avg. length: 1449.05,                last time consumption/overall running time: 185.0595s / 127452.5793 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2299
env0_second_0:                 episode reward: -1.9500,                 loss: 0.6800
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 10721/30000 (35.7367%),                 avg. length: 1500.95,                last time consumption/overall running time: 190.4707s / 127643.0500 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.2125
env0_second_0:                 episode reward: -3.2000,                 loss: 0.7903
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 10741/30000 (35.8033%),                 avg. length: 1497.8,                last time consumption/overall running time: 190.5747s / 127833.6247 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2273
env0_second_0:                 episode reward: -3.3500,                 loss: 1.0276
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 10761/30000 (35.8700%),                 avg. length: 1496.7,                last time consumption/overall running time: 192.8963s / 128026.5210 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2238
env0_second_0:                 episode reward: -2.3000,                 loss: 0.8551
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 10781/30000 (35.9367%),                 avg. length: 1461.55,                last time consumption/overall running time: 186.9503s / 128213.4714 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2306
env0_second_0:                 episode reward: -2.4000,                 loss: 0.6104
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 10801/30000 (36.0033%),                 avg. length: 1468.15,                last time consumption/overall running time: 187.3327s / 128400.8040 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2264
env0_second_0:                 episode reward: -2.0000,                 loss: 1.0044
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 10821/30000 (36.0700%),                 avg. length: 1445.35,                last time consumption/overall running time: 187.8722s / 128588.6763 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2484
env0_second_0:                 episode reward: -1.2500,                 loss: 0.5621
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 10841/30000 (36.1367%),                 avg. length: 1497.95,                last time consumption/overall running time: 194.2739s / 128782.9502 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2262
env0_second_0:                 episode reward: -0.9000,                 loss: 0.6840
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 10861/30000 (36.2033%),                 avg. length: 1459.1,                last time consumption/overall running time: 187.9080s / 128970.8582 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1758
env0_second_0:                 episode reward: -3.1000,                 loss: 0.9055
env1_first_0:                 episode reward: 4.8000,                 loss: nan
env1_second_0:                 episode reward: -4.8000,                 loss: nan
Episode: 10881/30000 (36.2700%),                 avg. length: 1472.05,                last time consumption/overall running time: 183.3477s / 129154.2058 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2216
env0_second_0:                 episode reward: -2.3500,                 loss: 0.7209
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 10901/30000 (36.3367%),                 avg. length: 1482.4,                last time consumption/overall running time: 182.0019s / 129336.2077 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1539
env0_second_0:                 episode reward: -1.9500,                 loss: 1.1303
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 10921/30000 (36.4033%),                 avg. length: 1496.9,                last time consumption/overall running time: 155.9871s / 129492.1949 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1910
env0_second_0:                 episode reward: -1.6500,                 loss: 0.6006
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 10941/30000 (36.4700%),                 avg. length: 1592.95,                last time consumption/overall running time: 175.2508s / 129667.4457 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1740
env0_second_0:                 episode reward: -1.1000,                 loss: 0.6931
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 10961/30000 (36.5367%),                 avg. length: 1618.55,                last time consumption/overall running time: 189.6735s / 129857.1192 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1944
env0_second_0:                 episode reward: 0.0000,                 loss: 0.5778
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 10981/30000 (36.6033%),                 avg. length: 1502.9,                last time consumption/overall running time: 166.7466s / 130023.8658 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1939
env0_second_0:                 episode reward: -1.6500,                 loss: 0.5887
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 11001/30000 (36.6700%),                 avg. length: 1727.65,                last time consumption/overall running time: 189.5513s / 130213.4171 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1764
env0_second_0:                 episode reward: -3.4000,                 loss: 0.9968
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 11021/30000 (36.7367%),                 avg. length: 2261.15,                last time consumption/overall running time: 245.2178s / 130458.6348 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.1008
env0_second_0:                 episode reward: -5.7000,                 loss: 1.3130
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 11041/30000 (36.8033%),                 avg. length: 2397.15,                last time consumption/overall running time: 249.7347s / 130708.3695 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1371
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7444
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 11061/30000 (36.8700%),                 avg. length: 2412.7,                last time consumption/overall running time: 265.2044s / 130973.5740 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0581
env0_second_0:                 episode reward: -5.4000,                 loss: 0.6393
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 11081/30000 (36.9367%),                 avg. length: 2029.1,                last time consumption/overall running time: 237.1071s / 131210.6811 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.0906
env0_second_0:                 episode reward: -3.0500,                 loss: 1.1135
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 11101/30000 (37.0033%),                 avg. length: 3694.1,                last time consumption/overall running time: 408.8987s / 131619.5798 s
env0_first_0:                 episode reward: -41.2000,                 loss: 0.1499
env0_second_0:                 episode reward: 41.2000,                 loss: 0.8709
env1_first_0:                 episode reward: -37.1000,                 loss: nan
env1_second_0:                 episode reward: 37.1000,                 loss: nan
Episode: 11121/30000 (37.0700%),                 avg. length: 3688.1,                last time consumption/overall running time: 420.3639s / 132039.9437 s
env0_first_0:                 episode reward: -5.5500,                 loss: -0.0624
env0_second_0:                 episode reward: 5.5500,                 loss: 0.5508
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 11141/30000 (37.1367%),                 avg. length: 2253.4,                last time consumption/overall running time: 230.5003s / 132270.4440 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1448
env0_second_0:                 episode reward: -2.3000,                 loss: 0.6193
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 11161/30000 (37.2033%),                 avg. length: 1972.1,                last time consumption/overall running time: 203.0712s / 132473.5152 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1308
env0_second_0:                 episode reward: -1.1500,                 loss: 1.0730
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 11181/30000 (37.2700%),                 avg. length: 2396.2,                last time consumption/overall running time: 265.6255s / 132739.1407 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1126
env0_second_0:                 episode reward: -3.7000,                 loss: 0.6143
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 11201/30000 (37.3367%),                 avg. length: 2104.6,                last time consumption/overall running time: 233.0001s / 132972.1408 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.0608
env0_second_0:                 episode reward: -9.1500,                 loss: 1.0792
env1_first_0:                 episode reward: 9.1000,                 loss: nan
env1_second_0:                 episode reward: -9.1000,                 loss: nan
Episode: 11221/30000 (37.4033%),                 avg. length: 1859.4,                last time consumption/overall running time: 190.2873s / 133162.4281 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.1325
env0_second_0:                 episode reward: 0.9500,                 loss: 0.8673
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 11241/30000 (37.4700%),                 avg. length: 1474.9,                last time consumption/overall running time: 156.7836s / 133319.2117 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2386
env0_second_0:                 episode reward: -2.6000,                 loss: 0.7682
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 11261/30000 (37.5367%),                 avg. length: 1467.75,                last time consumption/overall running time: 169.6625s / 133488.8742 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2590
env0_second_0:                 episode reward: -2.7000,                 loss: 0.9688
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 11281/30000 (37.6033%),                 avg. length: 1513.75,                last time consumption/overall running time: 184.5591s / 133673.4333 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2258
env0_second_0:                 episode reward: -1.6000,                 loss: 0.4898
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 11301/30000 (37.6700%),                 avg. length: 1684.1,                last time consumption/overall running time: 188.8821s / 133862.3154 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1373
env0_second_0:                 episode reward: 1.3000,                 loss: 1.2902
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 11321/30000 (37.7367%),                 avg. length: 1459.25,                last time consumption/overall running time: 170.0867s / 134032.4021 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.0944
env0_second_0:                 episode reward: 1.5000,                 loss: 1.1922
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 11341/30000 (37.8033%),                 avg. length: 1488.05,                last time consumption/overall running time: 168.8401s / 134201.2422 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1986
env0_second_0:                 episode reward: -1.9000,                 loss: 1.1447
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 11361/30000 (37.8700%),                 avg. length: 1462.0,                last time consumption/overall running time: 159.7290s / 134360.9712 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2008
env0_second_0:                 episode reward: -0.8500,                 loss: 0.6715
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 11381/30000 (37.9367%),                 avg. length: 1469.95,                last time consumption/overall running time: 156.4195s / 134517.3907 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2165
env0_second_0:                 episode reward: -1.3500,                 loss: 0.6377
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 11401/30000 (38.0033%),                 avg. length: 1476.5,                last time consumption/overall running time: 176.0185s / 134693.4092 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1797
env0_second_0:                 episode reward: -1.2500,                 loss: 0.7606
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 11421/30000 (38.0700%),                 avg. length: 1493.05,                last time consumption/overall running time: 180.4196s / 134873.8288 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2023
env0_second_0:                 episode reward: -1.3000,                 loss: 0.7984
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 11441/30000 (38.1367%),                 avg. length: 1457.7,                last time consumption/overall running time: 157.8266s / 135031.6553 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2037
env0_second_0:                 episode reward: -1.7500,                 loss: 0.5259
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 11461/30000 (38.2033%),                 avg. length: 1523.45,                last time consumption/overall running time: 166.7413s / 135198.3967 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2201
env0_second_0:                 episode reward: -1.6000,                 loss: 0.4331
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 11481/30000 (38.2700%),                 avg. length: 1498.35,                last time consumption/overall running time: 173.2352s / 135371.6319 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2000
env0_second_0:                 episode reward: -1.4500,                 loss: 0.7830
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 11501/30000 (38.3367%),                 avg. length: 1493.5,                last time consumption/overall running time: 167.2958s / 135538.9277 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2124
env0_second_0:                 episode reward: -1.5000,                 loss: 0.4343
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 11521/30000 (38.4033%),                 avg. length: 1507.2,                last time consumption/overall running time: 166.3661s / 135705.2938 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1832
env0_second_0:                 episode reward: -0.3500,                 loss: 0.5273
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 11541/30000 (38.4700%),                 avg. length: 1504.5,                last time consumption/overall running time: 178.6594s / 135883.9532 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2048
env0_second_0:                 episode reward: -1.4500,                 loss: 0.2840
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 11561/30000 (38.5367%),                 avg. length: 1511.7,                last time consumption/overall running time: 160.4271s / 136044.3803 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1845
env0_second_0:                 episode reward: -2.8500,                 loss: 0.3826
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 11581/30000 (38.6033%),                 avg. length: 1506.5,                last time consumption/overall running time: 183.1104s / 136227.4907 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2036
env0_second_0:                 episode reward: -0.1500,                 loss: 0.5168
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 11601/30000 (38.6700%),                 avg. length: 1456.65,                last time consumption/overall running time: 168.5991s / 136396.0899 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1872
env0_second_0:                 episode reward: 0.4000,                 loss: 0.4336
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 11621/30000 (38.7367%),                 avg. length: 1456.85,                last time consumption/overall running time: 177.0369s / 136573.1268 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.1669
env0_second_0:                 episode reward: 1.9500,                 loss: 0.3213
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 11641/30000 (38.8033%),                 avg. length: 1463.35,                last time consumption/overall running time: 181.6879s / 136754.8147 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1800
env0_second_0:                 episode reward: -1.4000,                 loss: 0.5837
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 11661/30000 (38.8700%),                 avg. length: 1457.35,                last time consumption/overall running time: 157.6028s / 136912.4175 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1935
env0_second_0:                 episode reward: -1.5500,                 loss: 0.6482
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 11681/30000 (38.9367%),                 avg. length: 1632.15,                last time consumption/overall running time: 173.9342s / 137086.3517 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.2087
env0_second_0:                 episode reward: -2.5500,                 loss: 0.5934
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 11701/30000 (39.0033%),                 avg. length: 1503.0,                last time consumption/overall running time: 157.1767s / 137243.5284 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2609
env0_second_0:                 episode reward: -1.7500,                 loss: 0.6816
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 11721/30000 (39.0700%),                 avg. length: 1524.25,                last time consumption/overall running time: 169.9860s / 137413.5144 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2217
env0_second_0:                 episode reward: -0.6000,                 loss: 0.8319
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 11741/30000 (39.1367%),                 avg. length: 1725.1,                last time consumption/overall running time: 194.0317s / 137607.5461 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2056
env0_second_0:                 episode reward: -1.5000,                 loss: 0.8577
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 11761/30000 (39.2033%),                 avg. length: 1668.3,                last time consumption/overall running time: 181.4448s / 137788.9909 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.2273
env0_second_0:                 episode reward: -2.9500,                 loss: 0.7465
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 11781/30000 (39.2700%),                 avg. length: 1696.15,                last time consumption/overall running time: 199.9260s / 137988.9170 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2251
env0_second_0:                 episode reward: -2.6000,                 loss: 0.5185
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 11801/30000 (39.3367%),                 avg. length: 2055.65,                last time consumption/overall running time: 234.6500s / 138223.5670 s
env0_first_0:                 episode reward: 9.5000,                 loss: -0.1283
env0_second_0:                 episode reward: -9.5000,                 loss: 0.7395
env1_first_0:                 episode reward: 9.5500,                 loss: nan
env1_second_0:                 episode reward: -9.5500,                 loss: nan
Episode: 11821/30000 (39.4033%),                 avg. length: 1863.45,                last time consumption/overall running time: 221.7505s / 138445.3175 s
env0_first_0:                 episode reward: 12.2000,                 loss: -0.1576
env0_second_0:                 episode reward: -12.2000,                 loss: 0.4758
env1_first_0:                 episode reward: 12.4000,                 loss: nan
env1_second_0:                 episode reward: -12.4000,                 loss: nan
Episode: 11841/30000 (39.4700%),                 avg. length: 1598.9,                last time consumption/overall running time: 173.1797s / 138618.4972 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.2348
env0_second_0:                 episode reward: -4.8000,                 loss: 0.4376
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 11861/30000 (39.5367%),                 avg. length: 1537.1,                last time consumption/overall running time: 167.8791s / 138786.3764 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2706
env0_second_0:                 episode reward: -1.6500,                 loss: 0.3527
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 11881/30000 (39.6033%),                 avg. length: 1571.45,                last time consumption/overall running time: 176.8565s / 138963.2329 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2239
env0_second_0:                 episode reward: -1.9000,                 loss: 0.2244
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 11901/30000 (39.6700%),                 avg. length: 1566.15,                last time consumption/overall running time: 169.2888s / 139132.5216 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2637
env0_second_0:                 episode reward: -2.4000,                 loss: 0.2387
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 11921/30000 (39.7367%),                 avg. length: 1958.95,                last time consumption/overall running time: 202.0396s / 139334.5612 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.2027
env0_second_0:                 episode reward: -4.7000,                 loss: 0.6218
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 11941/30000 (39.8033%),                 avg. length: 1630.0,                last time consumption/overall running time: 186.6186s / 139521.1798 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.2625
env0_second_0:                 episode reward: -4.3500,                 loss: 0.3390
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 11961/30000 (39.8700%),                 avg. length: 1544.0,                last time consumption/overall running time: 168.8230s / 139690.0029 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.2460
env0_second_0:                 episode reward: -3.9000,                 loss: 0.1810
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 11981/30000 (39.9367%),                 avg. length: 1569.25,                last time consumption/overall running time: 172.7391s / 139862.7420 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.2475
env0_second_0:                 episode reward: -4.6000,                 loss: 0.1570
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 12001/30000 (40.0033%),                 avg. length: 1688.9,                last time consumption/overall running time: 189.7037s / 140052.4457 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.2249
env0_second_0:                 episode reward: -4.9000,                 loss: 0.3150
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 12021/30000 (40.0700%),                 avg. length: 1635.3,                last time consumption/overall running time: 177.1481s / 140229.5939 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.2402
env0_second_0:                 episode reward: -5.4500,                 loss: 0.3286
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 12041/30000 (40.1367%),                 avg. length: 1718.55,                last time consumption/overall running time: 177.4218s / 140407.0156 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2401
env0_second_0:                 episode reward: -2.0500,                 loss: 0.3597
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 12061/30000 (40.2033%),                 avg. length: 1802.7,                last time consumption/overall running time: 204.7986s / 140611.8142 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2496
env0_second_0:                 episode reward: -1.6000,                 loss: 0.2420
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 12081/30000 (40.2700%),                 avg. length: 1722.0,                last time consumption/overall running time: 194.7888s / 140806.6030 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2428
env0_second_0:                 episode reward: -2.0000,                 loss: 0.2772
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 12101/30000 (40.3367%),                 avg. length: 1709.6,                last time consumption/overall running time: 193.9308s / 141000.5338 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1899
env0_second_0:                 episode reward: -3.8000,                 loss: 0.4012
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 12121/30000 (40.4033%),                 avg. length: 1588.45,                last time consumption/overall running time: 168.1212s / 141168.6550 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2254
env0_second_0:                 episode reward: -4.3000,                 loss: 0.3313
env1_first_0:                 episode reward: 4.4000,                 loss: nan
env1_second_0:                 episode reward: -4.4000,                 loss: nan
Episode: 12141/30000 (40.4700%),                 avg. length: 1473.45,                last time consumption/overall running time: 163.2260s / 141331.8809 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2398
env0_second_0:                 episode reward: -3.3000,                 loss: 0.3383
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 12161/30000 (40.5367%),                 avg. length: 1548.5,                last time consumption/overall running time: 167.0549s / 141498.9359 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2486
env0_second_0:                 episode reward: -2.3500,                 loss: 0.4244
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 12181/30000 (40.6033%),                 avg. length: 1501.85,                last time consumption/overall running time: 182.3584s / 141681.2943 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2732
env0_second_0:                 episode reward: -2.3000,                 loss: 0.3210
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 12201/30000 (40.6700%),                 avg. length: 1552.0,                last time consumption/overall running time: 176.0609s / 141857.3552 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2670
env0_second_0:                 episode reward: -2.1000,                 loss: 0.2214
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 12221/30000 (40.7367%),                 avg. length: 1524.65,                last time consumption/overall running time: 176.9527s / 142034.3079 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2778
env0_second_0:                 episode reward: -2.4000,                 loss: 0.3445
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 12241/30000 (40.8033%),                 avg. length: 1613.1,                last time consumption/overall running time: 187.8202s / 142222.1281 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1843
env0_second_0:                 episode reward: -0.5500,                 loss: 0.6791
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 12261/30000 (40.8700%),                 avg. length: 1580.05,                last time consumption/overall running time: 179.0886s / 142401.2167 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.2039
env0_second_0:                 episode reward: 2.6500,                 loss: 0.5196
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 12281/30000 (40.9367%),                 avg. length: 1780.5,                last time consumption/overall running time: 207.5827s / 142608.7993 s
env0_first_0:                 episode reward: -4.1000,                 loss: -0.1586
env0_second_0:                 episode reward: 4.1000,                 loss: 0.3895
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 12301/30000 (41.0033%),                 avg. length: 1751.1,                last time consumption/overall running time: 200.8468s / 142809.6462 s
env0_first_0:                 episode reward: -5.2000,                 loss: -0.1303
env0_second_0:                 episode reward: 5.2000,                 loss: 0.9841
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 12321/30000 (41.0700%),                 avg. length: 1588.6,                last time consumption/overall running time: 187.5103s / 142997.1565 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.1677
env0_second_0:                 episode reward: 2.6000,                 loss: 1.1834
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 12341/30000 (41.1367%),                 avg. length: 1482.7,                last time consumption/overall running time: 173.8344s / 143170.9909 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.1637
env0_second_0:                 episode reward: 1.7500,                 loss: 1.0290
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 12361/30000 (41.2033%),                 avg. length: 1607.75,                last time consumption/overall running time: 174.1494s / 143345.1403 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1597
env0_second_0:                 episode reward: 1.4500,                 loss: 1.4013
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 12381/30000 (41.2700%),                 avg. length: 1727.2,                last time consumption/overall running time: 189.4299s / 143534.5703 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1107
env0_second_0:                 episode reward: -1.2500,                 loss: 1.9878
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 12401/30000 (41.3367%),                 avg. length: 1808.3,                last time consumption/overall running time: 204.0598s / 143738.6300 s
env0_first_0:                 episode reward: -6.1500,                 loss: -0.0804
env0_second_0:                 episode reward: 6.1500,                 loss: 1.3992
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 12421/30000 (41.4033%),                 avg. length: 1544.35,                last time consumption/overall running time: 179.5665s / 143918.1965 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1896
env0_second_0:                 episode reward: -1.3500,                 loss: 0.6742
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 12441/30000 (41.4700%),                 avg. length: 1506.55,                last time consumption/overall running time: 181.2392s / 144099.4357 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1739
env0_second_0:                 episode reward: 0.0500,                 loss: 0.5586
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 12461/30000 (41.5367%),                 avg. length: 1487.2,                last time consumption/overall running time: 163.1293s / 144262.5650 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1783
env0_second_0:                 episode reward: -0.5500,                 loss: 0.9294
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 12481/30000 (41.6033%),                 avg. length: 1701.35,                last time consumption/overall running time: 182.7422s / 144445.3073 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.0503
env0_second_0:                 episode reward: -0.2500,                 loss: 1.0225
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 12501/30000 (41.6700%),                 avg. length: 1561.8,                last time consumption/overall running time: 183.4079s / 144628.7152 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.1400
env0_second_0:                 episode reward: 2.9000,                 loss: 1.3053
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 12521/30000 (41.7367%),                 avg. length: 1745.65,                last time consumption/overall running time: 192.8968s / 144821.6119 s
env0_first_0:                 episode reward: -2.7500,                 loss: -0.0931
env0_second_0:                 episode reward: 2.7500,                 loss: 1.0739
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 12541/30000 (41.8033%),                 avg. length: 1614.95,                last time consumption/overall running time: 212.2774s / 145033.8893 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1453
env0_second_0:                 episode reward: -1.7500,                 loss: 0.6104
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 12561/30000 (41.8700%),                 avg. length: 1623.2,                last time consumption/overall running time: 189.6732s / 145223.5626 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1224
env0_second_0:                 episode reward: -0.7500,                 loss: 1.0068
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 12581/30000 (41.9367%),                 avg. length: 1931.05,                last time consumption/overall running time: 213.5568s / 145437.1194 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.0248
env0_second_0:                 episode reward: 0.9500,                 loss: 0.7840
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 12601/30000 (42.0033%),                 avg. length: 1969.5,                last time consumption/overall running time: 210.1442s / 145647.2636 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.0530
env0_second_0:                 episode reward: -0.1000,                 loss: 0.5053
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 12621/30000 (42.0700%),                 avg. length: 1655.25,                last time consumption/overall running time: 195.9293s / 145843.1929 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1400
env0_second_0:                 episode reward: -2.6500,                 loss: 0.5142
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 12641/30000 (42.1367%),                 avg. length: 1620.95,                last time consumption/overall running time: 172.5669s / 146015.7598 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1343
env0_second_0:                 episode reward: -1.0000,                 loss: 0.7552
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 12661/30000 (42.2033%),                 avg. length: 1578.65,                last time consumption/overall running time: 169.6908s / 146185.4506 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1088
env0_second_0:                 episode reward: -1.7500,                 loss: 1.1238
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 12681/30000 (42.2700%),                 avg. length: 1504.4,                last time consumption/overall running time: 174.9035s / 146360.3540 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1557
env0_second_0:                 episode reward: -1.7000,                 loss: 0.9892
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 12701/30000 (42.3367%),                 avg. length: 1553.05,                last time consumption/overall running time: 182.9117s / 146543.2658 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1284
env0_second_0:                 episode reward: -1.8500,                 loss: 0.5466
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 12721/30000 (42.4033%),                 avg. length: 1524.75,                last time consumption/overall running time: 169.4881s / 146712.7539 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1270
env0_second_0:                 episode reward: -1.4500,                 loss: 0.5742
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 12741/30000 (42.4700%),                 avg. length: 1513.5,                last time consumption/overall running time: 157.3597s / 146870.1136 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1568
env0_second_0:                 episode reward: -3.1000,                 loss: 0.6667
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 12761/30000 (42.5367%),                 avg. length: 1460.5,                last time consumption/overall running time: 158.3055s / 147028.4191 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1761
env0_second_0:                 episode reward: -1.6000,                 loss: 0.8180
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 12781/30000 (42.6033%),                 avg. length: 1465.35,                last time consumption/overall running time: 164.9122s / 147193.3314 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0889
env0_second_0:                 episode reward: 1.2000,                 loss: 0.5165
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 12801/30000 (42.6700%),                 avg. length: 1406.95,                last time consumption/overall running time: 165.8234s / 147359.1548 s
env0_first_0:                 episode reward: -4.2000,                 loss: -0.1278
env0_second_0:                 episode reward: 4.2000,                 loss: 2.1082
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 12821/30000 (42.7367%),                 avg. length: 1490.65,                last time consumption/overall running time: 166.1255s / 147525.2804 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.1215
env0_second_0:                 episode reward: 1.6000,                 loss: 1.1407
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 12841/30000 (42.8033%),                 avg. length: 1516.35,                last time consumption/overall running time: 168.5190s / 147693.7993 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1785
env0_second_0:                 episode reward: -1.2000,                 loss: 1.1697
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 12861/30000 (42.8700%),                 avg. length: 1460.95,                last time consumption/overall running time: 155.2802s / 147849.0795 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2005
env0_second_0:                 episode reward: 0.5000,                 loss: 0.7691
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 12881/30000 (42.9367%),                 avg. length: 1555.9,                last time consumption/overall running time: 158.4551s / 148007.5346 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.1313
env0_second_0:                 episode reward: 3.5500,                 loss: 1.0272
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 12901/30000 (43.0033%),                 avg. length: 1538.2,                last time consumption/overall running time: 166.7544s / 148174.2890 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1950
env0_second_0:                 episode reward: 1.2500,                 loss: 0.7947
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 12921/30000 (43.0700%),                 avg. length: 1502.3,                last time consumption/overall running time: 171.3023s / 148345.5914 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2044
env0_second_0:                 episode reward: -0.7000,                 loss: 2.4086
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 12941/30000 (43.1367%),                 avg. length: 1484.15,                last time consumption/overall running time: 165.1113s / 148510.7027 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2022
env0_second_0:                 episode reward: -0.6500,                 loss: 0.8544
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 12961/30000 (43.2033%),                 avg. length: 1535.7,                last time consumption/overall running time: 172.6764s / 148683.3791 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2344
env0_second_0:                 episode reward: -2.0500,                 loss: 0.8133
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 12981/30000 (43.2700%),                 avg. length: 1532.25,                last time consumption/overall running time: 173.0649s / 148856.4440 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2011
env0_second_0:                 episode reward: -1.9500,                 loss: 1.2688
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 13001/30000 (43.3367%),                 avg. length: 1598.0,                last time consumption/overall running time: 167.5193s / 149023.9633 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1785
env0_second_0:                 episode reward: -1.6000,                 loss: 1.2919
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 13021/30000 (43.4033%),                 avg. length: 1865.8,                last time consumption/overall running time: 196.1251s / 149220.0884 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1610
env0_second_0:                 episode reward: -0.0500,                 loss: 1.1369
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 13041/30000 (43.4700%),                 avg. length: 1674.9,                last time consumption/overall running time: 206.2276s / 149426.3160 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1407
env0_second_0:                 episode reward: -4.7000,                 loss: 1.3417
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 13061/30000 (43.5367%),                 avg. length: 1580.3,                last time consumption/overall running time: 169.3329s / 149595.6488 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.1130
env0_second_0:                 episode reward: -5.4000,                 loss: 0.9451
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 13081/30000 (43.6033%),                 avg. length: 2062.05,                last time consumption/overall running time: 207.2537s / 149802.9026 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1280
env0_second_0:                 episode reward: -1.6000,                 loss: 0.7335
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 13101/30000 (43.6700%),                 avg. length: 2374.0,                last time consumption/overall running time: 250.3793s / 150053.2818 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1396
env0_second_0:                 episode reward: -1.2000,                 loss: 0.6079
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 13121/30000 (43.7367%),                 avg. length: 1941.2,                last time consumption/overall running time: 213.1071s / 150266.3890 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1203
env0_second_0:                 episode reward: 0.2000,                 loss: 0.5491
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 13141/30000 (43.8033%),                 avg. length: 1790.35,                last time consumption/overall running time: 198.0904s / 150464.4794 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1488
env0_second_0:                 episode reward: 0.0000,                 loss: 0.5017
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 13161/30000 (43.8700%),                 avg. length: 2273.6,                last time consumption/overall running time: 256.6402s / 150721.1196 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1509
env0_second_0:                 episode reward: -0.5000,                 loss: 0.6767
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 13181/30000 (43.9367%),                 avg. length: 1938.9,                last time consumption/overall running time: 218.7715s / 150939.8911 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1522
env0_second_0:                 episode reward: -1.2000,                 loss: 0.6656
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 13201/30000 (44.0033%),                 avg. length: 1747.5,                last time consumption/overall running time: 198.7664s / 151138.6575 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1533
env0_second_0:                 episode reward: -1.0500,                 loss: 0.8417
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13221/30000 (44.0700%),                 avg. length: 2226.7,                last time consumption/overall running time: 248.7592s / 151387.4167 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1721
env0_second_0:                 episode reward: -0.6000,                 loss: 0.4391
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 13241/30000 (44.1367%),                 avg. length: 2317.25,                last time consumption/overall running time: 254.4975s / 151641.9143 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1480
env0_second_0:                 episode reward: -2.8500,                 loss: 0.5453
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 13261/30000 (44.2033%),                 avg. length: 2258.8,                last time consumption/overall running time: 253.4640s / 151895.3782 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1684
env0_second_0:                 episode reward: -3.0000,                 loss: 0.4190
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 13281/30000 (44.2700%),                 avg. length: 1892.5,                last time consumption/overall running time: 212.1113s / 152107.4895 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1294
env0_second_0:                 episode reward: -0.6000,                 loss: 1.4990
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 13301/30000 (44.3367%),                 avg. length: 1940.75,                last time consumption/overall running time: 217.6618s / 152325.1513 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1378
env0_second_0:                 episode reward: -1.3500,                 loss: 1.4616
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 13321/30000 (44.4033%),                 avg. length: 2035.9,                last time consumption/overall running time: 228.3618s / 152553.5131 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1292
env0_second_0:                 episode reward: 2.1500,                 loss: 1.3412
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 13341/30000 (44.4700%),                 avg. length: 1867.05,                last time consumption/overall running time: 195.9770s / 152749.4902 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1577
env0_second_0:                 episode reward: -1.1500,                 loss: 1.0682
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 13361/30000 (44.5367%),                 avg. length: 1913.85,                last time consumption/overall running time: 203.3364s / 152952.8266 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1089
env0_second_0:                 episode reward: -4.6000,                 loss: 0.9082
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 13381/30000 (44.6033%),                 avg. length: 2023.95,                last time consumption/overall running time: 222.3011s / 153175.1277 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1583
env0_second_0:                 episode reward: -3.6500,                 loss: 0.7273
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 13401/30000 (44.6700%),                 avg. length: 1642.65,                last time consumption/overall running time: 183.6970s / 153358.8248 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1172
env0_second_0:                 episode reward: -2.7500,                 loss: 1.3321
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 13421/30000 (44.7367%),                 avg. length: 1623.9,                last time consumption/overall running time: 176.3445s / 153535.1693 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1100
env0_second_0:                 episode reward: -7.0000,                 loss: 0.9581
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 13441/30000 (44.8033%),                 avg. length: 1679.0,                last time consumption/overall running time: 186.9371s / 153722.1064 s
env0_first_0:                 episode reward: 8.5000,                 loss: -0.1128
env0_second_0:                 episode reward: -8.5000,                 loss: 0.7610
env1_first_0:                 episode reward: 7.7000,                 loss: nan
env1_second_0:                 episode reward: -7.7000,                 loss: nan
Episode: 13461/30000 (44.8700%),                 avg. length: 1648.65,                last time consumption/overall running time: 176.1642s / 153898.2705 s
env0_first_0:                 episode reward: 8.4500,                 loss: -0.1142
env0_second_0:                 episode reward: -8.4500,                 loss: 1.0644
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 13481/30000 (44.9367%),                 avg. length: 1627.2,                last time consumption/overall running time: 179.5303s / 154077.8008 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0798
env0_second_0:                 episode reward: -9.7000,                 loss: 1.3205
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 13501/30000 (45.0033%),                 avg. length: 1720.7,                last time consumption/overall running time: 200.6003s / 154278.4011 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.0835
env0_second_0:                 episode reward: -7.1500,                 loss: 1.3807
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 13521/30000 (45.0700%),                 avg. length: 1926.65,                last time consumption/overall running time: 202.6344s / 154481.0355 s
env0_first_0:                 episode reward: 6.4000,                 loss: -0.0569
env0_second_0:                 episode reward: -6.4000,                 loss: 1.7448
env1_first_0:                 episode reward: 9.1500,                 loss: nan
env1_second_0:                 episode reward: -9.1500,                 loss: nan
Episode: 13541/30000 (45.1367%),                 avg. length: 2202.0,                last time consumption/overall running time: 233.5140s / 154714.5494 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.0805
env0_second_0:                 episode reward: -5.7000,                 loss: 1.3599
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 13561/30000 (45.2033%),                 avg. length: 2206.35,                last time consumption/overall running time: 260.2656s / 154974.8150 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0748
env0_second_0:                 episode reward: -4.8500,                 loss: 1.1047
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 13581/30000 (45.2700%),                 avg. length: 2044.65,                last time consumption/overall running time: 232.5376s / 155207.3526 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.0400
env0_second_0:                 episode reward: -7.3500,                 loss: 2.1130
env1_first_0:                 episode reward: 7.9500,                 loss: nan
env1_second_0:                 episode reward: -7.9500,                 loss: nan
Episode: 13601/30000 (45.3367%),                 avg. length: 2055.3,                last time consumption/overall running time: 222.3390s / 155429.6915 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1106
env0_second_0:                 episode reward: -4.8000,                 loss: 1.3178
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 13621/30000 (45.4033%),                 avg. length: 2040.4,                last time consumption/overall running time: 235.0147s / 155664.7062 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.0483
env0_second_0:                 episode reward: -0.4500,                 loss: 1.2313
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 13641/30000 (45.4700%),                 avg. length: 2447.4,                last time consumption/overall running time: 263.5839s / 155928.2901 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.0895
env0_second_0:                 episode reward: -3.3000,                 loss: 1.1972
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 13661/30000 (45.5367%),                 avg. length: 1889.85,                last time consumption/overall running time: 199.9385s / 156128.2286 s
env0_first_0:                 episode reward: 8.1500,                 loss: -0.0774
env0_second_0:                 episode reward: -8.1500,                 loss: 1.0890
env1_first_0:                 episode reward: 6.7500,                 loss: nan
env1_second_0:                 episode reward: -6.7500,                 loss: nan
Episode: 13681/30000 (45.6033%),                 avg. length: 1949.5,                last time consumption/overall running time: 209.6623s / 156337.8909 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.0051
env0_second_0:                 episode reward: -4.4000,                 loss: 1.0508
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 13701/30000 (45.6700%),                 avg. length: 1746.65,                last time consumption/overall running time: 201.5833s / 156539.4742 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.0183
env0_second_0:                 episode reward: -8.7000,                 loss: 1.3421
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 13721/30000 (45.7367%),                 avg. length: 1811.4,                last time consumption/overall running time: 215.7595s / 156755.2338 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.0296
env0_second_0:                 episode reward: -4.2500,                 loss: 2.0030
env1_first_0:                 episode reward: 5.9000,                 loss: nan
env1_second_0:                 episode reward: -5.9000,                 loss: nan
Episode: 13741/30000 (45.8033%),                 avg. length: 1660.3,                last time consumption/overall running time: 189.5815s / 156944.8153 s
env0_first_0:                 episode reward: 9.5500,                 loss: -0.0581
env0_second_0:                 episode reward: -9.5500,                 loss: 0.9907
env1_first_0:                 episode reward: 8.8000,                 loss: nan
env1_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 13761/30000 (45.8700%),                 avg. length: 1711.6,                last time consumption/overall running time: 205.6801s / 157150.4954 s
env0_first_0:                 episode reward: 10.9500,                 loss: -0.0954
env0_second_0:                 episode reward: -10.9500,                 loss: 0.8721
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 13781/30000 (45.9367%),                 avg. length: 1773.4,                last time consumption/overall running time: 197.3035s / 157347.7989 s
env0_first_0:                 episode reward: 8.4000,                 loss: -0.0471
env0_second_0:                 episode reward: -8.4000,                 loss: 0.8242
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 13801/30000 (46.0033%),                 avg. length: 2053.1,                last time consumption/overall running time: 235.9555s / 157583.7544 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.0311
env0_second_0:                 episode reward: -3.9000,                 loss: 0.9564
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 13821/30000 (46.0700%),                 avg. length: 2286.15,                last time consumption/overall running time: 259.0573s / 157842.8117 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0087
env0_second_0:                 episode reward: -7.0500,                 loss: 0.9655
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 13841/30000 (46.1367%),                 avg. length: 2345.7,                last time consumption/overall running time: 253.1306s / 158095.9424 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.0177
env0_second_0:                 episode reward: 1.5000,                 loss: 0.7503
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 13861/30000 (46.2033%),                 avg. length: 2222.85,                last time consumption/overall running time: 237.4474s / 158333.3898 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.0399
env0_second_0:                 episode reward: 2.3000,                 loss: 0.8631
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 13881/30000 (46.2700%),                 avg. length: 2131.5,                last time consumption/overall running time: 239.6971s / 158573.0869 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.0642
env0_second_0:                 episode reward: 1.3500,                 loss: 0.7138
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 13901/30000 (46.3367%),                 avg. length: 1780.7,                last time consumption/overall running time: 207.7529s / 158780.8398 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.0753
env0_second_0:                 episode reward: -3.9500,                 loss: 1.0284
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 13921/30000 (46.4033%),                 avg. length: 2011.25,                last time consumption/overall running time: 205.0344s / 158985.8742 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.0116
env0_second_0:                 episode reward: -6.7500,                 loss: 1.0426
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 13941/30000 (46.4700%),                 avg. length: 1666.55,                last time consumption/overall running time: 197.7064s / 159183.5806 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.0315
env0_second_0:                 episode reward: -8.7000,                 loss: 0.9318
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 13961/30000 (46.5367%),                 avg. length: 1562.35,                last time consumption/overall running time: 180.8647s / 159364.4453 s
env0_first_0:                 episode reward: 9.7000,                 loss: -0.0748
env0_second_0:                 episode reward: -9.7000,                 loss: 1.1940
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 13981/30000 (46.6033%),                 avg. length: 1538.45,                last time consumption/overall running time: 177.1687s / 159541.6140 s
env0_first_0:                 episode reward: 7.8500,                 loss: -0.1018
env0_second_0:                 episode reward: -7.8500,                 loss: 1.0617
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 14001/30000 (46.6700%),                 avg. length: 2031.3,                last time consumption/overall running time: 218.4660s / 159760.0800 s
env0_first_0:                 episode reward: 10.4500,                 loss: -0.0686
env0_second_0:                 episode reward: -10.4500,                 loss: 1.6317
env1_first_0:                 episode reward: 9.2000,                 loss: nan
env1_second_0:                 episode reward: -9.2000,                 loss: nan
Episode: 14021/30000 (46.7367%),                 avg. length: 1864.75,                last time consumption/overall running time: 213.2200s / 159973.3000 s
env0_first_0:                 episode reward: 5.3500,                 loss: -0.1298
env0_second_0:                 episode reward: -5.3500,                 loss: 0.9576
env1_first_0:                 episode reward: 4.7000,                 loss: nan
env1_second_0:                 episode reward: -4.7000,                 loss: nan
Episode: 14041/30000 (46.8033%),                 avg. length: 1934.65,                last time consumption/overall running time: 215.5083s / 160188.8084 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1493
env0_second_0:                 episode reward: -4.9500,                 loss: 0.9619
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 14061/30000 (46.8700%),                 avg. length: 1744.35,                last time consumption/overall running time: 192.0393s / 160380.8476 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.1444
env0_second_0:                 episode reward: -6.6000,                 loss: 0.8280
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 14081/30000 (46.9367%),                 avg. length: 1545.2,                last time consumption/overall running time: 176.8442s / 160557.6919 s
env0_first_0:                 episode reward: 7.0500,                 loss: -0.1456
env0_second_0:                 episode reward: -7.0500,                 loss: 0.7294
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 14101/30000 (47.0033%),                 avg. length: 1754.7,                last time consumption/overall running time: 192.8147s / 160750.5066 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.1567
env0_second_0:                 episode reward: -4.8500,                 loss: 0.8144
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 14121/30000 (47.0700%),                 avg. length: 2217.95,                last time consumption/overall running time: 252.5089s / 161003.0155 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1197
env0_second_0:                 episode reward: -1.1000,                 loss: 0.9882
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 14141/30000 (47.1367%),                 avg. length: 1901.55,                last time consumption/overall running time: 217.7339s / 161220.7494 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1321
env0_second_0:                 episode reward: -2.7500,                 loss: 0.7894
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 14161/30000 (47.2033%),                 avg. length: 1852.8,                last time consumption/overall running time: 208.8293s / 161429.5787 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.1256
env0_second_0:                 episode reward: -5.7500,                 loss: 1.3418
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 14181/30000 (47.2700%),                 avg. length: 2135.5,                last time consumption/overall running time: 233.8492s / 161663.4279 s
env0_first_0:                 episode reward: 8.5500,                 loss: -0.1010
env0_second_0:                 episode reward: -8.5500,                 loss: 1.0614
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 14201/30000 (47.3367%),                 avg. length: 1916.2,                last time consumption/overall running time: 209.1427s / 161872.5706 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1327
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7816
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 14221/30000 (47.4033%),                 avg. length: 1630.85,                last time consumption/overall running time: 188.0690s / 162060.6396 s
env0_first_0:                 episode reward: 8.7000,                 loss: -0.1216
env0_second_0:                 episode reward: -8.7000,                 loss: 0.9130
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 14241/30000 (47.4700%),                 avg. length: 3610.2,                last time consumption/overall running time: 387.9995s / 162448.6391 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.1830
env0_second_0:                 episode reward: 0.5500,                 loss: 1.1425
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 14261/30000 (47.5367%),                 avg. length: 2556.8,                last time consumption/overall running time: 279.8441s / 162728.4832 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.1515
env0_second_0:                 episode reward: -6.0000,                 loss: 1.0160
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 14281/30000 (47.6033%),                 avg. length: 2690.95,                last time consumption/overall running time: 311.2661s / 163039.7493 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1308
env0_second_0:                 episode reward: -3.2500,                 loss: 0.7059
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 14301/30000 (47.6700%),                 avg. length: 4777.75,                last time consumption/overall running time: 518.2339s / 163557.9832 s
env0_first_0:                 episode reward: 6.4500,                 loss: -0.1837
env0_second_0:                 episode reward: -6.4500,                 loss: 0.6799
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 14321/30000 (47.7367%),                 avg. length: 6723.75,                last time consumption/overall running time: 712.4477s / 164270.4308 s
env0_first_0:                 episode reward: 14.6000,                 loss: -0.1633
env0_second_0:                 episode reward: -14.6000,                 loss: 0.6177
env1_first_0:                 episode reward: 14.6000,                 loss: nan
env1_second_0:                 episode reward: -14.6000,                 loss: nan
Episode: 14341/30000 (47.8033%),                 avg. length: 3222.95,                last time consumption/overall running time: 354.2806s / 164624.7114 s
env0_first_0:                 episode reward: 7.2000,                 loss: -0.1598
env0_second_0:                 episode reward: -7.2000,                 loss: 0.6927
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 14361/30000 (47.8700%),                 avg. length: 2820.45,                last time consumption/overall running time: 312.4271s / 164937.1385 s
env0_first_0:                 episode reward: 6.1000,                 loss: -0.1665
env0_second_0:                 episode reward: -6.1000,                 loss: 0.6239
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 14381/30000 (47.9367%),                 avg. length: 1847.25,                last time consumption/overall running time: 232.7036s / 165169.8421 s
env0_first_0:                 episode reward: 7.8000,                 loss: -0.1581
env0_second_0:                 episode reward: -7.8000,                 loss: 0.9049
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 14401/30000 (48.0033%),                 avg. length: 3275.5,                last time consumption/overall running time: 348.2241s / 165518.0662 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1666
env0_second_0:                 episode reward: -5.2500,                 loss: 0.6996
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 14421/30000 (48.0700%),                 avg. length: 2846.65,                last time consumption/overall running time: 309.6881s / 165827.7542 s
env0_first_0:                 episode reward: 7.1500,                 loss: -0.1357
env0_second_0:                 episode reward: -7.1500,                 loss: 0.6424
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 14441/30000 (48.1367%),                 avg. length: 2291.85,                last time consumption/overall running time: 268.8426s / 166096.5969 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.1392
env0_second_0:                 episode reward: -5.2000,                 loss: 1.2235
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 14461/30000 (48.2033%),                 avg. length: 2090.95,                last time consumption/overall running time: 228.5885s / 166325.1854 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1484
env0_second_0:                 episode reward: -4.4000,                 loss: 0.7061
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 14481/30000 (48.2700%),                 avg. length: 1837.25,                last time consumption/overall running time: 204.2160s / 166529.4014 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1079
env0_second_0:                 episode reward: -4.6000,                 loss: 1.7032
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 14501/30000 (48.3367%),                 avg. length: 1988.45,                last time consumption/overall running time: 237.7577s / 166767.1590 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.0737
env0_second_0:                 episode reward: -2.2000,                 loss: 1.4990
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 14521/30000 (48.4033%),                 avg. length: 2150.25,                last time consumption/overall running time: 239.8014s / 167006.9604 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.0657
env0_second_0:                 episode reward: 4.6000,                 loss: 1.3326
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 14541/30000 (48.4700%),                 avg. length: 2073.0,                last time consumption/overall running time: 240.7600s / 167247.7204 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.0525
env0_second_0:                 episode reward: 6.2500,                 loss: 1.4478
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 14561/30000 (48.5367%),                 avg. length: 1775.55,                last time consumption/overall running time: 203.1998s / 167450.9202 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.0696
env0_second_0:                 episode reward: -3.2000,                 loss: 1.0188
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 14581/30000 (48.6033%),                 avg. length: 1696.5,                last time consumption/overall running time: 186.1526s / 167637.0728 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.1199
env0_second_0:                 episode reward: -4.6500,                 loss: 1.3730
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 14601/30000 (48.6700%),                 avg. length: 1968.1,                last time consumption/overall running time: 220.1896s / 167857.2624 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1731
env0_second_0:                 episode reward: -2.0500,                 loss: 1.0437
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 14621/30000 (48.7367%),                 avg. length: 1769.7,                last time consumption/overall running time: 205.6011s / 168062.8635 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2109
env0_second_0:                 episode reward: -1.8000,                 loss: 0.9326
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 14641/30000 (48.8033%),                 avg. length: 1942.55,                last time consumption/overall running time: 234.9043s / 168297.7678 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1603
env0_second_0:                 episode reward: 2.1500,                 loss: 0.8163
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 14661/30000 (48.8700%),                 avg. length: 2367.0,                last time consumption/overall running time: 266.2507s / 168564.0186 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1726
env0_second_0:                 episode reward: 1.3000,                 loss: 0.7507
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 14681/30000 (48.9367%),                 avg. length: 2510.95,                last time consumption/overall running time: 292.3949s / 168856.4135 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1827
env0_second_0:                 episode reward: 0.8000,                 loss: 0.5826
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 14701/30000 (49.0033%),                 avg. length: 2421.3,                last time consumption/overall running time: 272.3837s / 169128.7972 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.1942
env0_second_0:                 episode reward: 2.0000,                 loss: 1.1830
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 14721/30000 (49.0700%),                 avg. length: 2129.05,                last time consumption/overall running time: 248.8890s / 169377.6862 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1821
env0_second_0:                 episode reward: -2.5500,                 loss: 0.7455
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 14741/30000 (49.1367%),                 avg. length: 1957.85,                last time consumption/overall running time: 226.6881s / 169604.3743 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.1395
env0_second_0:                 episode reward: -4.5000,                 loss: 0.9641
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 14761/30000 (49.2033%),                 avg. length: 2151.9,                last time consumption/overall running time: 242.1596s / 169846.5339 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.0865
env0_second_0:                 episode reward: -4.1500,                 loss: 0.9741
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 14781/30000 (49.2700%),                 avg. length: 1533.9,                last time consumption/overall running time: 173.0706s / 170019.6045 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1897
env0_second_0:                 episode reward: -2.9500,                 loss: 0.9663
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 14801/30000 (49.3367%),                 avg. length: 1818.95,                last time consumption/overall running time: 201.1861s / 170220.7906 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1786
env0_second_0:                 episode reward: -2.1000,                 loss: 0.8502
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 14821/30000 (49.4033%),                 avg. length: 1632.2,                last time consumption/overall running time: 179.0449s / 170399.8355 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1424
env0_second_0:                 episode reward: -3.2000,                 loss: 1.6699
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 14841/30000 (49.4700%),                 avg. length: 1829.5,                last time consumption/overall running time: 217.0463s / 170616.8818 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.0912
env0_second_0:                 episode reward: -2.0500,                 loss: 1.1067
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 14861/30000 (49.5367%),                 avg. length: 2064.85,                last time consumption/overall running time: 225.7272s / 170842.6090 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.0687
env0_second_0:                 episode reward: 2.0500,                 loss: 1.2044
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 14881/30000 (49.6033%),                 avg. length: 1951.15,                last time consumption/overall running time: 213.5039s / 171056.1129 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.0904
env0_second_0:                 episode reward: 2.5500,                 loss: 1.3690
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 14901/30000 (49.6700%),                 avg. length: 1839.3,                last time consumption/overall running time: 208.5353s / 171264.6482 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1783
env0_second_0:                 episode reward: -2.5500,                 loss: 1.1769
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 14921/30000 (49.7367%),                 avg. length: 2294.05,                last time consumption/overall running time: 249.7486s / 171514.3968 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.1221
env0_second_0:                 episode reward: 2.5500,                 loss: 1.0959
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 14941/30000 (49.8033%),                 avg. length: 2177.6,                last time consumption/overall running time: 237.6873s / 171752.0841 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.0840
env0_second_0:                 episode reward: -1.2000,                 loss: 1.3892
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 14961/30000 (49.8700%),                 avg. length: 2655.95,                last time consumption/overall running time: 289.4746s / 172041.5586 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0522
env0_second_0:                 episode reward: -0.8000,                 loss: 1.2667
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 14981/30000 (49.9367%),                 avg. length: 4186.8,                last time consumption/overall running time: 461.1455s / 172502.7041 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.0992
env0_second_0:                 episode reward: -1.0500,                 loss: 0.8974
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 15001/30000 (50.0033%),                 avg. length: 2507.9,                last time consumption/overall running time: 282.6888s / 172785.3930 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.0724
env0_second_0:                 episode reward: -3.3500,                 loss: 1.2452
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 15021/30000 (50.0700%),                 avg. length: 2180.0,                last time consumption/overall running time: 259.5442s / 173044.9371 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0841
env0_second_0:                 episode reward: -1.6500,                 loss: 1.1616
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 15041/30000 (50.1367%),                 avg. length: 2187.8,                last time consumption/overall running time: 253.3123s / 173298.2494 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1018
env0_second_0:                 episode reward: -0.3500,                 loss: 1.2619
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 15061/30000 (50.2033%),                 avg. length: 1809.35,                last time consumption/overall running time: 201.6426s / 173499.8920 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0700
env0_second_0:                 episode reward: -4.5500,                 loss: 1.4051
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 15081/30000 (50.2700%),                 avg. length: 1717.85,                last time consumption/overall running time: 191.9279s / 173691.8199 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0938
env0_second_0:                 episode reward: -2.6000,                 loss: 1.3806
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 15101/30000 (50.3367%),                 avg. length: 6662.55,                last time consumption/overall running time: 708.1826s / 174400.0025 s
env0_first_0:                 episode reward: -189.8500,                 loss: 0.3738
env0_second_0:                 episode reward: 189.8500,                 loss: 1.7045
env1_first_0:                 episode reward: -187.9000,                 loss: nan
env1_second_0:                 episode reward: 187.9000,                 loss: nan
Episode: 15121/30000 (50.4033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.5571s / 175450.5596 s
env0_first_0:                 episode reward: -320.8500,                 loss: 0.5051
env0_second_0:                 episode reward: 320.8500,                 loss: 0.7932
env1_first_0:                 episode reward: -317.9500,                 loss: nan
env1_second_0:                 episode reward: 317.9500,                 loss: nan
Episode: 15141/30000 (50.4700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1059.0081s / 176509.5677 s
env0_first_0:                 episode reward: -317.7500,                 loss: 0.4857
env0_second_0:                 episode reward: 317.7500,                 loss: 0.6968
env1_first_0:                 episode reward: -319.3500,                 loss: nan
env1_second_0:                 episode reward: 319.3500,                 loss: nan
Episode: 15161/30000 (50.5367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1088.2673s / 177597.8351 s
env0_first_0:                 episode reward: -322.6000,                 loss: 0.4752
env0_second_0:                 episode reward: 322.6000,                 loss: 0.6089
env1_first_0:                 episode reward: -318.7000,                 loss: nan
env1_second_0:                 episode reward: 318.7000,                 loss: nan
Episode: 15181/30000 (50.6033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1049.8177s / 178647.6528 s
env0_first_0:                 episode reward: -321.6000,                 loss: 0.4674
env0_second_0:                 episode reward: 321.6000,                 loss: 0.5741
env1_first_0:                 episode reward: -321.7000,                 loss: nan
env1_second_0:                 episode reward: 321.7000,                 loss: nan
Episode: 15201/30000 (50.6700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1059.9290s / 179707.5817 s
env0_first_0:                 episode reward: -322.3000,                 loss: 0.4682
env0_second_0:                 episode reward: 322.3000,                 loss: 0.5216
env1_first_0:                 episode reward: -322.6000,                 loss: nan
env1_second_0:                 episode reward: 322.6000,                 loss: nan
Episode: 15221/30000 (50.7367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1085.7277s / 180793.3094 s
env0_first_0:                 episode reward: -320.7500,                 loss: 0.4796
env0_second_0:                 episode reward: 320.7500,                 loss: 0.5130
env1_first_0:                 episode reward: -320.9500,                 loss: nan
env1_second_0:                 episode reward: 320.9500,                 loss: nan
Episode: 15241/30000 (50.8033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1065.9870s / 181859.2964 s
env0_first_0:                 episode reward: -165.8000,                 loss: 0.1588
env0_second_0:                 episode reward: 165.8000,                 loss: 0.3664
env1_first_0:                 episode reward: -172.5000,                 loss: nan
env1_second_0:                 episode reward: 172.5000,                 loss: nan
Episode: 15261/30000 (50.8700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1084.4881s / 182943.7845 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2076
env0_second_0:                 episode reward: 0.5000,                 loss: 0.4465
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 15281/30000 (50.9367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.8991s / 183989.6836 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.2114
env0_second_0:                 episode reward: 1.3000,                 loss: 0.0985
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 15301/30000 (51.0033%),                 avg. length: 9999.0,                last time consumption/overall running time: 1100.5045s / 185090.1882 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.2182
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1307
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 15321/30000 (51.0700%),                 avg. length: 9999.0,                last time consumption/overall running time: 1087.8842s / 186178.0724 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.2242
env0_second_0:                 episode reward: 0.7000,                 loss: -0.0860
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 15341/30000 (51.1367%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.8493s / 187232.9216 s
env0_first_0:                 episode reward: -173.5500,                 loss: 0.2449
env0_second_0:                 episode reward: 173.5500,                 loss: 0.4891
env1_first_0:                 episode reward: -206.9500,                 loss: nan
env1_second_0:                 episode reward: 206.9500,                 loss: nan
Episode: 15361/30000 (51.2033%),                 avg. length: 2640.65,                last time consumption/overall running time: 317.0621s / 187549.9837 s
env0_first_0:                 episode reward: -27.9500,                 loss: 0.0764
env0_second_0:                 episode reward: 27.9500,                 loss: 1.0314
env1_first_0:                 episode reward: -28.3000,                 loss: nan
env1_second_0:                 episode reward: 28.3000,                 loss: nan
Episode: 15381/30000 (51.2700%),                 avg. length: 1585.7,                last time consumption/overall running time: 182.3683s / 187732.3520 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1489
env0_second_0:                 episode reward: -1.8000,                 loss: 1.0122
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 15401/30000 (51.3367%),                 avg. length: 1608.0,                last time consumption/overall running time: 173.9572s / 187906.3093 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1182
env0_second_0:                 episode reward: -1.5000,                 loss: 1.0461
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 15421/30000 (51.4033%),                 avg. length: 1643.3,                last time consumption/overall running time: 187.7361s / 188094.0454 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.1151
env0_second_0:                 episode reward: -5.7000,                 loss: 1.1353
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 15441/30000 (51.4700%),                 avg. length: 1764.15,                last time consumption/overall running time: 191.8398s / 188285.8852 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.0901
env0_second_0:                 episode reward: -6.9500,                 loss: 1.4573
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 15461/30000 (51.5367%),                 avg. length: 1681.6,                last time consumption/overall running time: 178.3939s / 188464.2791 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.0548
env0_second_0:                 episode reward: -4.6000,                 loss: 1.5044
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 15481/30000 (51.6033%),                 avg. length: 1549.35,                last time consumption/overall running time: 172.8227s / 188637.1018 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0834
env0_second_0:                 episode reward: 1.2000,                 loss: 1.3536
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 15501/30000 (51.6700%),                 avg. length: 1479.8,                last time consumption/overall running time: 177.2694s / 188814.3712 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1238
env0_second_0:                 episode reward: -1.3000,                 loss: 0.8696
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 15521/30000 (51.7367%),                 avg. length: 1500.25,                last time consumption/overall running time: 167.9372s / 188982.3084 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1990
env0_second_0:                 episode reward: -2.9000,                 loss: 2.2414
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 15541/30000 (51.8033%),                 avg. length: 1583.65,                last time consumption/overall running time: 192.5032s / 189174.8116 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.1622
env0_second_0:                 episode reward: -4.2500,                 loss: 1.5358
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 15561/30000 (51.8700%),                 avg. length: 1507.85,                last time consumption/overall running time: 167.2892s / 189342.1008 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.1812
env0_second_0:                 episode reward: -4.0000,                 loss: 2.2519
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 15581/30000 (51.9367%),                 avg. length: 1476.2,                last time consumption/overall running time: 165.0651s / 189507.1659 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1782
env0_second_0:                 episode reward: -2.7000,                 loss: 1.7306
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 15601/30000 (52.0033%),                 avg. length: 1459.15,                last time consumption/overall running time: 166.2933s / 189673.4593 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1687
env0_second_0:                 episode reward: -2.2500,                 loss: 1.3366
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 15621/30000 (52.0700%),                 avg. length: 1723.25,                last time consumption/overall running time: 187.0662s / 189860.5254 s
env0_first_0:                 episode reward: 3.0500,                 loss: -0.1178
env0_second_0:                 episode reward: -3.0500,                 loss: 1.7001
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 15641/30000 (52.1367%),                 avg. length: 1841.45,                last time consumption/overall running time: 199.7568s / 190060.2822 s
env0_first_0:                 episode reward: 6.0000,                 loss: -0.0828
env0_second_0:                 episode reward: -6.0000,                 loss: 2.2457
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 15661/30000 (52.2033%),                 avg. length: 1880.3,                last time consumption/overall running time: 201.6762s / 190261.9584 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.0973
env0_second_0:                 episode reward: -5.6500,                 loss: 1.5556
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 15681/30000 (52.2700%),                 avg. length: 1900.55,                last time consumption/overall running time: 212.5832s / 190474.5417 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1035
env0_second_0:                 episode reward: -1.5500,                 loss: 1.3325
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 15701/30000 (52.3367%),                 avg. length: 1646.45,                last time consumption/overall running time: 179.5758s / 190654.1175 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.1233
env0_second_0:                 episode reward: -2.5500,                 loss: 1.0572
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 15721/30000 (52.4033%),                 avg. length: 1796.9,                last time consumption/overall running time: 200.5251s / 190854.6426 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.0444
env0_second_0:                 episode reward: 0.6500,                 loss: 1.2483
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 15741/30000 (52.4700%),                 avg. length: 1847.4,                last time consumption/overall running time: 200.3838s / 191055.0264 s
env0_first_0:                 episode reward: 7.0500,                 loss: -0.0861
env0_second_0:                 episode reward: -7.0500,                 loss: 1.1840
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 15761/30000 (52.5367%),                 avg. length: 1937.05,                last time consumption/overall running time: 206.7822s / 191261.8086 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1153
env0_second_0:                 episode reward: -5.2500,                 loss: 0.8658
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 15781/30000 (52.6033%),                 avg. length: 1793.85,                last time consumption/overall running time: 201.0398s / 191462.8485 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.1417
env0_second_0:                 episode reward: -4.3000,                 loss: 0.9114
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 15801/30000 (52.6700%),                 avg. length: 2252.45,                last time consumption/overall running time: 250.1084s / 191712.9568 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1195
env0_second_0:                 episode reward: -3.7000,                 loss: 0.8843
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 15821/30000 (52.7367%),                 avg. length: 2188.15,                last time consumption/overall running time: 242.9009s / 191955.8578 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.0748
env0_second_0:                 episode reward: -1.9500,                 loss: 1.0002
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 15841/30000 (52.8033%),                 avg. length: 2297.45,                last time consumption/overall running time: 256.4237s / 192212.2815 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0421
env0_second_0:                 episode reward: -0.3000,                 loss: 1.2568
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 15861/30000 (52.8700%),                 avg. length: 2389.8,                last time consumption/overall running time: 282.0075s / 192494.2890 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0000
env0_second_0:                 episode reward: 3.0500,                 loss: 1.4304
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 15881/30000 (52.9367%),                 avg. length: 2158.75,                last time consumption/overall running time: 243.8333s / 192738.1223 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.0074
env0_second_0:                 episode reward: 5.9500,                 loss: 1.5935
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 15901/30000 (53.0033%),                 avg. length: 2117.15,                last time consumption/overall running time: 226.0031s / 192964.1254 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.0520
env0_second_0:                 episode reward: 2.6000,                 loss: 1.3474
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 15921/30000 (53.0700%),                 avg. length: 1845.8,                last time consumption/overall running time: 197.0360s / 193161.1614 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.0820
env0_second_0:                 episode reward: 2.4500,                 loss: 1.5501
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 15941/30000 (53.1367%),                 avg. length: 1642.05,                last time consumption/overall running time: 175.8833s / 193337.0447 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1556
env0_second_0:                 episode reward: 0.1000,                 loss: 1.2960
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15961/30000 (53.2033%),                 avg. length: 1632.35,                last time consumption/overall running time: 181.1568s / 193518.2015 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2305
env0_second_0:                 episode reward: -0.9500,                 loss: 1.3065
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 15981/30000 (53.2700%),                 avg. length: 1878.1,                last time consumption/overall running time: 203.7587s / 193721.9602 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1668
env0_second_0:                 episode reward: -1.3500,                 loss: 1.4971
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16001/30000 (53.3367%),                 avg. length: 1869.3,                last time consumption/overall running time: 201.6895s / 193923.6497 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.1311
env0_second_0:                 episode reward: 3.3500,                 loss: 1.0868
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 16021/30000 (53.4033%),                 avg. length: 1988.5,                last time consumption/overall running time: 208.6794s / 194132.3291 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.0776
env0_second_0:                 episode reward: 2.5000,                 loss: 1.3261
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 16041/30000 (53.4700%),                 avg. length: 1814.75,                last time consumption/overall running time: 199.4049s / 194331.7340 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.0911
env0_second_0:                 episode reward: 4.9000,                 loss: 0.9858
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 16061/30000 (53.5367%),                 avg. length: 1808.75,                last time consumption/overall running time: 206.9688s / 194538.7028 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0398
env0_second_0:                 episode reward: 3.7000,                 loss: 1.0703
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 16081/30000 (53.6033%),                 avg. length: 1806.75,                last time consumption/overall running time: 195.8743s / 194734.5771 s
env0_first_0:                 episode reward: -3.2000,                 loss: -0.0946
env0_second_0:                 episode reward: 3.2000,                 loss: 0.9301
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 16101/30000 (53.6700%),                 avg. length: 1887.3,                last time consumption/overall running time: 217.8750s / 194952.4521 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.1332
env0_second_0:                 episode reward: 1.8000,                 loss: 1.1701
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 16121/30000 (53.7367%),                 avg. length: 1721.75,                last time consumption/overall running time: 186.8048s / 195139.2569 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1252
env0_second_0:                 episode reward: -2.6500,                 loss: 1.4501
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 16141/30000 (53.8033%),                 avg. length: 1937.5,                last time consumption/overall running time: 205.5590s / 195344.8159 s
env0_first_0:                 episode reward: 5.7500,                 loss: -0.0628
env0_second_0:                 episode reward: -5.7500,                 loss: 1.5568
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 16161/30000 (53.8700%),                 avg. length: 2226.35,                last time consumption/overall running time: 237.2260s / 195582.0419 s
env0_first_0:                 episode reward: 5.8000,                 loss: -0.0881
env0_second_0:                 episode reward: -5.8000,                 loss: 1.4771
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 16181/30000 (53.9367%),                 avg. length: 1838.5,                last time consumption/overall running time: 209.1601s / 195791.2021 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1233
env0_second_0:                 episode reward: -4.1500,                 loss: 1.1206
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 16201/30000 (54.0033%),                 avg. length: 1781.95,                last time consumption/overall running time: 199.6796s / 195990.8816 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1120
env0_second_0:                 episode reward: -4.8000,                 loss: 1.1461
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 16221/30000 (54.0700%),                 avg. length: 1960.25,                last time consumption/overall running time: 209.5739s / 196200.4555 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1041
env0_second_0:                 episode reward: -2.0000,                 loss: 1.0660
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 16241/30000 (54.1367%),                 avg. length: 2126.15,                last time consumption/overall running time: 229.4231s / 196429.8786 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1319
env0_second_0:                 episode reward: -2.1000,                 loss: 0.7493
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 16261/30000 (54.2033%),                 avg. length: 1937.75,                last time consumption/overall running time: 214.9950s / 196644.8735 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1453
env0_second_0:                 episode reward: -1.3500,                 loss: 0.8155
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 16281/30000 (54.2700%),                 avg. length: 1810.05,                last time consumption/overall running time: 200.8722s / 196845.7457 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1310
env0_second_0:                 episode reward: -0.0500,                 loss: 0.7181
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 16301/30000 (54.3367%),                 avg. length: 1826.65,                last time consumption/overall running time: 206.0015s / 197051.7472 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1432
env0_second_0:                 episode reward: -1.7000,                 loss: 0.8408
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 16321/30000 (54.4033%),                 avg. length: 2108.0,                last time consumption/overall running time: 236.9241s / 197288.6713 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.0404
env0_second_0:                 episode reward: -3.8500,                 loss: 0.9526
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 16341/30000 (54.4700%),                 avg. length: 2357.7,                last time consumption/overall running time: 256.4970s / 197545.1683 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1295
env0_second_0:                 episode reward: -1.4000,                 loss: 0.7268
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 16361/30000 (54.5367%),                 avg. length: 2184.15,                last time consumption/overall running time: 243.4599s / 197788.6282 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.0758
env0_second_0:                 episode reward: -2.4000,                 loss: 1.1969
env1_first_0:                 episode reward: 3.2500,                 loss: nan
env1_second_0:                 episode reward: -3.2500,                 loss: nan
Episode: 16381/30000 (54.6033%),                 avg. length: 2726.25,                last time consumption/overall running time: 303.8619s / 198092.4901 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.0918
env0_second_0:                 episode reward: -0.9500,                 loss: 0.6574
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 16401/30000 (54.6700%),                 avg. length: 1957.35,                last time consumption/overall running time: 224.6202s / 198317.1103 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0921
env0_second_0:                 episode reward: -4.8500,                 loss: 0.9342
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 16421/30000 (54.7367%),                 avg. length: 1964.95,                last time consumption/overall running time: 209.4735s / 198526.5839 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0208
env0_second_0:                 episode reward: -0.4000,                 loss: 1.1974
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 16441/30000 (54.8033%),                 avg. length: 3407.6,                last time consumption/overall running time: 377.8922s / 198904.4760 s
env0_first_0:                 episode reward: -43.5500,                 loss: 0.1324
env0_second_0:                 episode reward: 43.5500,                 loss: 1.4622
env1_first_0:                 episode reward: -41.3000,                 loss: nan
env1_second_0:                 episode reward: 41.3000,                 loss: nan
Episode: 16461/30000 (54.8700%),                 avg. length: 2115.35,                last time consumption/overall running time: 247.4130s / 199151.8890 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1135
env0_second_0:                 episode reward: -2.4000,                 loss: 0.9645
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 16481/30000 (54.9367%),                 avg. length: 2099.0,                last time consumption/overall running time: 232.1361s / 199384.0251 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.0860
env0_second_0:                 episode reward: -1.7500,                 loss: 1.1053
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 16501/30000 (55.0033%),                 avg. length: 2031.7,                last time consumption/overall running time: 218.5120s / 199602.5371 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.0523
env0_second_0:                 episode reward: -1.1000,                 loss: 0.9394
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 16521/30000 (55.0700%),                 avg. length: 1724.15,                last time consumption/overall running time: 190.5579s / 199793.0950 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.1218
env0_second_0:                 episode reward: 1.7000,                 loss: 0.7094
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 16541/30000 (55.1367%),                 avg. length: 2015.6,                last time consumption/overall running time: 223.3433s / 200016.4383 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.1361
env0_second_0:                 episode reward: -0.3000,                 loss: 0.7440
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 16561/30000 (55.2033%),                 avg. length: 1769.75,                last time consumption/overall running time: 197.0610s / 200213.4993 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1448
env0_second_0:                 episode reward: 0.2000,                 loss: 0.6234
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 16581/30000 (55.2700%),                 avg. length: 1580.65,                last time consumption/overall running time: 174.1069s / 200387.6063 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2186
env0_second_0:                 episode reward: -2.3000,                 loss: 1.2409
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 16601/30000 (55.3367%),                 avg. length: 1728.65,                last time consumption/overall running time: 187.2784s / 200574.8846 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1698
env0_second_0:                 episode reward: -3.0000,                 loss: 0.8893
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 16621/30000 (55.4033%),                 avg. length: 1780.65,                last time consumption/overall running time: 194.1566s / 200769.0413 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1768
env0_second_0:                 episode reward: -2.0000,                 loss: 0.5296
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 16641/30000 (55.4700%),                 avg. length: 1743.8,                last time consumption/overall running time: 189.0967s / 200958.1379 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.1622
env0_second_0:                 episode reward: 3.2500,                 loss: 0.4016
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 16661/30000 (55.5367%),                 avg. length: 1811.3,                last time consumption/overall running time: 198.9315s / 201157.0695 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1975
env0_second_0:                 episode reward: -1.0500,                 loss: 0.5207
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 16681/30000 (55.6033%),                 avg. length: 1712.65,                last time consumption/overall running time: 189.9436s / 201347.0131 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.1574
env0_second_0:                 episode reward: 2.2000,                 loss: 0.7887
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 16701/30000 (55.6700%),                 avg. length: 1765.7,                last time consumption/overall running time: 194.8566s / 201541.8697 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.1841
env0_second_0:                 episode reward: 0.7500,                 loss: 0.7933
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 16721/30000 (55.7367%),                 avg. length: 1666.35,                last time consumption/overall running time: 191.5377s / 201733.4075 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2123
env0_second_0:                 episode reward: -1.7000,                 loss: 0.5456
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 16741/30000 (55.8033%),                 avg. length: 1564.95,                last time consumption/overall running time: 188.4711s / 201921.8786 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1739
env0_second_0:                 episode reward: 0.3500,                 loss: 1.0857
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 16761/30000 (55.8700%),                 avg. length: 1509.95,                last time consumption/overall running time: 166.8793s / 202088.7578 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2238
env0_second_0:                 episode reward: 0.1000,                 loss: 0.5918
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 16781/30000 (55.9367%),                 avg. length: 1543.85,                last time consumption/overall running time: 168.2643s / 202257.0221 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1726
env0_second_0:                 episode reward: -0.2000,                 loss: 0.6475
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 16801/30000 (56.0033%),                 avg. length: 1513.75,                last time consumption/overall running time: 167.2185s / 202424.2407 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2472
env0_second_0:                 episode reward: -1.8000,                 loss: 0.6447
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 16821/30000 (56.0700%),                 avg. length: 1500.2,                last time consumption/overall running time: 167.5784s / 202591.8190 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2684
env0_second_0:                 episode reward: -1.7000,                 loss: 0.5658
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 16841/30000 (56.1367%),                 avg. length: 1531.05,                last time consumption/overall running time: 172.5152s / 202764.3342 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2474
env0_second_0:                 episode reward: -1.5000,                 loss: 0.5452
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 16861/30000 (56.2033%),                 avg. length: 1549.5,                last time consumption/overall running time: 173.9338s / 202938.2680 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2370
env0_second_0:                 episode reward: -0.5500,                 loss: 0.3097
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16881/30000 (56.2700%),                 avg. length: 1489.05,                last time consumption/overall running time: 173.0659s / 203111.3340 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2473
env0_second_0:                 episode reward: -1.5500,                 loss: 0.5206
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 16901/30000 (56.3367%),                 avg. length: 1452.0,                last time consumption/overall running time: 163.3883s / 203274.7222 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.2773
env0_second_0:                 episode reward: -2.2000,                 loss: 0.3273
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 16921/30000 (56.4033%),                 avg. length: 1557.5,                last time consumption/overall running time: 196.0257s / 203470.7479 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2230
env0_second_0:                 episode reward: -2.4000,                 loss: 0.5602
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 16941/30000 (56.4700%),                 avg. length: 1912.35,                last time consumption/overall running time: 204.6042s / 203675.3521 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1574
env0_second_0:                 episode reward: -2.5000,                 loss: 0.7380
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 16961/30000 (56.5367%),                 avg. length: 1940.6,                last time consumption/overall running time: 216.3452s / 203891.6973 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1800
env0_second_0:                 episode reward: -1.5000,                 loss: 0.7081
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 16981/30000 (56.6033%),                 avg. length: 1890.05,                last time consumption/overall running time: 204.0745s / 204095.7718 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1898
env0_second_0:                 episode reward: -0.8500,                 loss: 0.5100
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 17001/30000 (56.6700%),                 avg. length: 1892.75,                last time consumption/overall running time: 209.2952s / 204305.0670 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1537
env0_second_0:                 episode reward: -2.3500,                 loss: 0.7920
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 17021/30000 (56.7367%),                 avg. length: 1611.35,                last time consumption/overall running time: 178.6167s / 204483.6837 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1940
env0_second_0:                 episode reward: -2.2000,                 loss: 0.6819
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 17041/30000 (56.8033%),                 avg. length: 1558.85,                last time consumption/overall running time: 185.2496s / 204668.9333 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.2051
env0_second_0:                 episode reward: -2.7000,                 loss: 0.5029
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 17061/30000 (56.8700%),                 avg. length: 1438.8,                last time consumption/overall running time: 167.7692s / 204836.7025 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2300
env0_second_0:                 episode reward: -2.3000,                 loss: 0.5461
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 17081/30000 (56.9367%),                 avg. length: 1779.3,                last time consumption/overall running time: 192.9923s / 205029.6948 s
env0_first_0:                 episode reward: 5.6000,                 loss: -0.1359
env0_second_0:                 episode reward: -5.6000,                 loss: 3.3813
env1_first_0:                 episode reward: 6.2500,                 loss: nan
env1_second_0:                 episode reward: -6.2500,                 loss: nan
Episode: 17101/30000 (57.0033%),                 avg. length: 1762.25,                last time consumption/overall running time: 184.6724s / 205214.3672 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.1917
env0_second_0:                 episode reward: -4.9000,                 loss: 1.8394
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 17121/30000 (57.0700%),                 avg. length: 1658.05,                last time consumption/overall running time: 174.9164s / 205389.2836 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1722
env0_second_0:                 episode reward: -3.9000,                 loss: 1.2966
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 17141/30000 (57.1367%),                 avg. length: 1793.4,                last time consumption/overall running time: 194.7484s / 205584.0321 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.1187
env0_second_0:                 episode reward: -4.5000,                 loss: 1.0320
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 17161/30000 (57.2033%),                 avg. length: 1726.45,                last time consumption/overall running time: 187.7881s / 205771.8201 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.1145
env0_second_0:                 episode reward: -4.4500,                 loss: 1.0111
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 17181/30000 (57.2700%),                 avg. length: 1584.25,                last time consumption/overall running time: 175.9182s / 205947.7384 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1874
env0_second_0:                 episode reward: -1.7500,                 loss: 0.8506
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 17201/30000 (57.3367%),                 avg. length: 1589.4,                last time consumption/overall running time: 182.6006s / 206130.3389 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2042
env0_second_0:                 episode reward: -2.0000,                 loss: 0.7074
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 17221/30000 (57.4033%),                 avg. length: 1822.35,                last time consumption/overall running time: 203.1015s / 206333.4404 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1769
env0_second_0:                 episode reward: -1.3500,                 loss: 1.0721
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 17241/30000 (57.4700%),                 avg. length: 2157.0,                last time consumption/overall running time: 233.9482s / 206567.3886 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.0634
env0_second_0:                 episode reward: -1.4500,                 loss: 0.9403
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 17261/30000 (57.5367%),                 avg. length: 2174.4,                last time consumption/overall running time: 230.7283s / 206798.1169 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.0784
env0_second_0:                 episode reward: -3.1000,                 loss: 1.0723
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 17281/30000 (57.6033%),                 avg. length: 2142.8,                last time consumption/overall running time: 223.1707s / 207021.2876 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.0977
env0_second_0:                 episode reward: -2.5500,                 loss: 0.9755
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 17301/30000 (57.6700%),                 avg. length: 2214.8,                last time consumption/overall running time: 245.7403s / 207267.0279 s
env0_first_0:                 episode reward: -7.4000,                 loss: -0.0812
env0_second_0:                 episode reward: 7.4000,                 loss: 1.0140
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 17321/30000 (57.7367%),                 avg. length: 2078.25,                last time consumption/overall running time: 232.8179s / 207499.8457 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1189
env0_second_0:                 episode reward: -3.1500,                 loss: 1.0664
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 17341/30000 (57.8033%),                 avg. length: 1687.35,                last time consumption/overall running time: 184.6402s / 207684.4859 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.0698
env0_second_0:                 episode reward: -3.6500,                 loss: 1.2641
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 17361/30000 (57.8700%),                 avg. length: 1849.8,                last time consumption/overall running time: 217.8408s / 207902.3267 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1509
env0_second_0:                 episode reward: -1.4500,                 loss: 1.0259
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 17381/30000 (57.9367%),                 avg. length: 9196.35,                last time consumption/overall running time: 1011.0808s / 208913.4076 s
env0_first_0:                 episode reward: -288.8500,                 loss: 0.5211
env0_second_0:                 episode reward: 288.8500,                 loss: 1.0314
env1_first_0:                 episode reward: -288.6500,                 loss: nan
env1_second_0:                 episode reward: 288.6500,                 loss: nan
Episode: 17401/30000 (58.0033%),                 avg. length: 7481.25,                last time consumption/overall running time: 792.0368s / 209705.4444 s
env0_first_0:                 episode reward: -196.2000,                 loss: 0.3975
env0_second_0:                 episode reward: 196.2000,                 loss: 0.9034
env1_first_0:                 episode reward: -195.1500,                 loss: nan
env1_second_0:                 episode reward: 195.1500,                 loss: nan
Episode: 17421/30000 (58.0700%),                 avg. length: 1932.3,                last time consumption/overall running time: 220.6358s / 209926.0802 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.0619
env0_second_0:                 episode reward: -1.8500,                 loss: 0.7982
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 17441/30000 (58.1367%),                 avg. length: 1653.55,                last time consumption/overall running time: 184.0423s / 210110.1224 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1711
env0_second_0:                 episode reward: -3.2500,                 loss: 0.6549
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 17461/30000 (58.2033%),                 avg. length: 2001.8,                last time consumption/overall running time: 215.9736s / 210326.0961 s
env0_first_0:                 episode reward: -11.4000,                 loss: -0.1392
env0_second_0:                 episode reward: 11.4000,                 loss: 0.9195
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 17481/30000 (58.2700%),                 avg. length: 1725.4,                last time consumption/overall running time: 197.0310s / 210523.1270 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0669
env0_second_0:                 episode reward: 0.8500,                 loss: 0.9081
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 17501/30000 (58.3367%),                 avg. length: 1673.4,                last time consumption/overall running time: 194.7047s / 210717.8317 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1564
env0_second_0:                 episode reward: -2.7500,                 loss: 0.6339
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 17521/30000 (58.4033%),                 avg. length: 1578.55,                last time consumption/overall running time: 179.9414s / 210897.7731 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1857
env0_second_0:                 episode reward: -3.0000,                 loss: 0.4839
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 17541/30000 (58.4700%),                 avg. length: 1581.3,                last time consumption/overall running time: 177.8613s / 211075.6344 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1900
env0_second_0:                 episode reward: -0.6500,                 loss: 0.4723
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 17561/30000 (58.5367%),                 avg. length: 1563.2,                last time consumption/overall running time: 180.1480s / 211255.7824 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1440
env0_second_0:                 episode reward: -4.8000,                 loss: 0.5770
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 17581/30000 (58.6033%),                 avg. length: 1614.1,                last time consumption/overall running time: 174.4863s / 211430.2687 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.0884
env0_second_0:                 episode reward: -4.8500,                 loss: 1.1777
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 17601/30000 (58.6700%),                 avg. length: 1933.0,                last time consumption/overall running time: 208.5037s / 211638.7724 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.1340
env0_second_0:                 episode reward: -2.9500,                 loss: 0.9154
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 17621/30000 (58.7367%),                 avg. length: 2203.65,                last time consumption/overall running time: 240.8433s / 211879.6157 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1335
env0_second_0:                 episode reward: -3.0000,                 loss: 0.7814
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 17641/30000 (58.8033%),                 avg. length: 1874.85,                last time consumption/overall running time: 204.5013s / 212084.1170 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.1438
env0_second_0:                 episode reward: -4.2000,                 loss: 4.9505
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 17661/30000 (58.8700%),                 avg. length: 1653.35,                last time consumption/overall running time: 175.2539s / 212259.3709 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1853
env0_second_0:                 episode reward: -3.9000,                 loss: 1.5699
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 17681/30000 (58.9367%),                 avg. length: 1483.15,                last time consumption/overall running time: 170.1795s / 212429.5504 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.2005
env0_second_0:                 episode reward: -3.2000,                 loss: 1.6098
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 17701/30000 (59.0033%),                 avg. length: 1555.35,                last time consumption/overall running time: 169.3670s / 212598.9174 s
env0_first_0:                 episode reward: 7.6500,                 loss: -0.0997
env0_second_0:                 episode reward: -7.6500,                 loss: 2.1430
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 17721/30000 (59.0700%),                 avg. length: 1554.05,                last time consumption/overall running time: 171.9185s / 212770.8359 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1299
env0_second_0:                 episode reward: -4.7000,                 loss: 3.3858
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 17741/30000 (59.1367%),                 avg. length: 1828.95,                last time consumption/overall running time: 199.9589s / 212970.7949 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1437
env0_second_0:                 episode reward: -3.4000,                 loss: 1.7423
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 17761/30000 (59.2033%),                 avg. length: 1636.6,                last time consumption/overall running time: 183.5428s / 213154.3377 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1233
env0_second_0:                 episode reward: -1.5000,                 loss: 1.4285
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 17781/30000 (59.2700%),                 avg. length: 1652.9,                last time consumption/overall running time: 181.9971s / 213336.3348 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1403
env0_second_0:                 episode reward: -1.3000,                 loss: 1.0628
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 17801/30000 (59.3367%),                 avg. length: 1724.55,                last time consumption/overall running time: 184.8857s / 213521.2205 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1432
env0_second_0:                 episode reward: -3.4500,                 loss: 0.8322
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 17821/30000 (59.4033%),                 avg. length: 1782.25,                last time consumption/overall running time: 195.5743s / 213716.7948 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1099
env0_second_0:                 episode reward: -3.3500,                 loss: 1.0919
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 17841/30000 (59.4700%),                 avg. length: 1820.9,                last time consumption/overall running time: 198.6205s / 213915.4153 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1289
env0_second_0:                 episode reward: -2.5000,                 loss: 1.0522
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 17861/30000 (59.5367%),                 avg. length: 1574.55,                last time consumption/overall running time: 173.3560s / 214088.7713 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.1218
env0_second_0:                 episode reward: -3.5000,                 loss: 1.4781
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 17881/30000 (59.6033%),                 avg. length: 1647.8,                last time consumption/overall running time: 176.0280s / 214264.7993 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1483
env0_second_0:                 episode reward: -1.1500,                 loss: 1.1703
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 17901/30000 (59.6700%),                 avg. length: 1602.25,                last time consumption/overall running time: 182.6641s / 214447.4634 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.1647
env0_second_0:                 episode reward: -4.0000,                 loss: 0.9532
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 17921/30000 (59.7367%),                 avg. length: 1728.0,                last time consumption/overall running time: 186.0235s / 214633.4869 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1594
env0_second_0:                 episode reward: -4.8000,                 loss: 1.5890
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 17941/30000 (59.8033%),                 avg. length: 2138.9,                last time consumption/overall running time: 231.9204s / 214865.4074 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.1356
env0_second_0:                 episode reward: 2.0000,                 loss: 1.0964
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 17961/30000 (59.8700%),                 avg. length: 1842.45,                last time consumption/overall running time: 213.4586s / 215078.8660 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1128
env0_second_0:                 episode reward: -1.0500,                 loss: 1.0390
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 17981/30000 (59.9367%),                 avg. length: 1763.05,                last time consumption/overall running time: 195.9313s / 215274.7973 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1104
env0_second_0:                 episode reward: -4.8000,                 loss: 1.0870
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 18001/30000 (60.0033%),                 avg. length: 1603.2,                last time consumption/overall running time: 181.2356s / 215456.0329 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2219
env0_second_0:                 episode reward: 0.1000,                 loss: 0.6133
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 18021/30000 (60.0700%),                 avg. length: 1562.1,                last time consumption/overall running time: 171.9101s / 215627.9431 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2550
env0_second_0:                 episode reward: -1.2500,                 loss: 0.5925
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 18041/30000 (60.1367%),                 avg. length: 1751.5,                last time consumption/overall running time: 193.6919s / 215821.6350 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1821
env0_second_0:                 episode reward: -1.9000,                 loss: 0.8591
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 18061/30000 (60.2033%),                 avg. length: 1644.45,                last time consumption/overall running time: 182.2025s / 216003.8375 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2223
env0_second_0:                 episode reward: -2.2500,                 loss: 0.6308
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 18081/30000 (60.2700%),                 avg. length: 1492.8,                last time consumption/overall running time: 159.8211s / 216163.6587 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3014
env0_second_0:                 episode reward: -2.3000,                 loss: 0.4372
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 18101/30000 (60.3367%),                 avg. length: 1531.1,                last time consumption/overall running time: 172.8276s / 216336.4863 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2839
env0_second_0:                 episode reward: -1.8500,                 loss: 0.4643
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 18121/30000 (60.4033%),                 avg. length: 1819.45,                last time consumption/overall running time: 205.8401s / 216542.3264 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1623
env0_second_0:                 episode reward: -2.2000,                 loss: 0.9046
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 18141/30000 (60.4700%),                 avg. length: 1999.75,                last time consumption/overall running time: 219.7321s / 216762.0586 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1424
env0_second_0:                 episode reward: 0.0000,                 loss: 1.0774
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 18161/30000 (60.5367%),                 avg. length: 1801.35,                last time consumption/overall running time: 198.4842s / 216960.5427 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1534
env0_second_0:                 episode reward: 0.5000,                 loss: 0.7783
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18181/30000 (60.6033%),                 avg. length: 1677.5,                last time consumption/overall running time: 188.4759s / 217149.0186 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1711
env0_second_0:                 episode reward: -1.6500,                 loss: 0.5942
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 18201/30000 (60.6700%),                 avg. length: 1695.55,                last time consumption/overall running time: 188.6565s / 217337.6751 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1911
env0_second_0:                 episode reward: -2.1500,                 loss: 0.6944
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 18221/30000 (60.7367%),                 avg. length: 1821.55,                last time consumption/overall running time: 200.3434s / 217538.0185 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1684
env0_second_0:                 episode reward: -3.7500,                 loss: 0.7619
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 18241/30000 (60.8033%),                 avg. length: 1925.2,                last time consumption/overall running time: 206.6212s / 217744.6397 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1415
env0_second_0:                 episode reward: -4.4000,                 loss: 0.7208
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 18261/30000 (60.8700%),                 avg. length: 1834.5,                last time consumption/overall running time: 196.0675s / 217940.7072 s
env0_first_0:                 episode reward: 10.2500,                 loss: -0.1288
env0_second_0:                 episode reward: -10.2500,                 loss: 1.2088
env1_first_0:                 episode reward: 8.7500,                 loss: nan
env1_second_0:                 episode reward: -8.7500,                 loss: nan
Episode: 18281/30000 (60.9367%),                 avg. length: 1840.35,                last time consumption/overall running time: 202.2702s / 218142.9774 s
env0_first_0:                 episode reward: 5.2000,                 loss: -0.1424
env0_second_0:                 episode reward: -5.2000,                 loss: 0.9856
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 18301/30000 (61.0033%),                 avg. length: 1700.15,                last time consumption/overall running time: 193.2711s / 218336.2484 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1967
env0_second_0:                 episode reward: -2.2500,                 loss: 0.8867
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 18321/30000 (61.0700%),                 avg. length: 1557.75,                last time consumption/overall running time: 174.1366s / 218510.3850 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1902
env0_second_0:                 episode reward: -2.2000,                 loss: 0.6845
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 18341/30000 (61.1367%),                 avg. length: 1534.7,                last time consumption/overall running time: 167.2686s / 218677.6536 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1891
env0_second_0:                 episode reward: -1.5500,                 loss: 1.0851
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 18361/30000 (61.2033%),                 avg. length: 1512.2,                last time consumption/overall running time: 164.9350s / 218842.5885 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2316
env0_second_0:                 episode reward: -2.6000,                 loss: 1.0976
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 18381/30000 (61.2700%),                 avg. length: 1613.15,                last time consumption/overall running time: 175.4588s / 219018.0473 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1395
env0_second_0:                 episode reward: -5.1000,                 loss: 0.8015
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 18401/30000 (61.3367%),                 avg. length: 1663.55,                last time consumption/overall running time: 179.8914s / 219197.9387 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.1213
env0_second_0:                 episode reward: -7.1000,                 loss: 1.5916
env1_first_0:                 episode reward: 7.0500,                 loss: nan
env1_second_0:                 episode reward: -7.0500,                 loss: nan
Episode: 18421/30000 (61.4033%),                 avg. length: 2299.55,                last time consumption/overall running time: 245.5601s / 219443.4988 s
env0_first_0:                 episode reward: -10.8500,                 loss: -0.0402
env0_second_0:                 episode reward: 10.8500,                 loss: 1.8822
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 18441/30000 (61.4700%),                 avg. length: 1714.8,                last time consumption/overall running time: 193.7865s / 219637.2853 s
env0_first_0:                 episode reward: 9.4000,                 loss: -0.1188
env0_second_0:                 episode reward: -9.4000,                 loss: 1.3780
env1_first_0:                 episode reward: 7.2500,                 loss: nan
env1_second_0:                 episode reward: -7.2500,                 loss: nan
Episode: 18461/30000 (61.5367%),                 avg. length: 1806.35,                last time consumption/overall running time: 194.4914s / 219831.7767 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.1555
env0_second_0:                 episode reward: -3.9500,                 loss: 1.1741
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 18481/30000 (61.6033%),                 avg. length: 1462.85,                last time consumption/overall running time: 157.8293s / 219989.6060 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.2353
env0_second_0:                 episode reward: -1.4000,                 loss: 1.2973
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 18501/30000 (61.6700%),                 avg. length: 1684.65,                last time consumption/overall running time: 178.3301s / 220167.9361 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1527
env0_second_0:                 episode reward: -1.9000,                 loss: 1.0421
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 18521/30000 (61.7367%),                 avg. length: 1561.95,                last time consumption/overall running time: 173.0245s / 220340.9606 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2012
env0_second_0:                 episode reward: -0.5000,                 loss: 1.0989
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 18541/30000 (61.8033%),                 avg. length: 1509.7,                last time consumption/overall running time: 176.6907s / 220517.6514 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2192
env0_second_0:                 episode reward: -1.7500,                 loss: 2.1083
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 18561/30000 (61.8700%),                 avg. length: 1520.6,                last time consumption/overall running time: 187.2284s / 220704.8798 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1866
env0_second_0:                 episode reward: -2.0000,                 loss: 1.2271
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 18581/30000 (61.9367%),                 avg. length: 1504.75,                last time consumption/overall running time: 173.2149s / 220878.0947 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1995
env0_second_0:                 episode reward: -3.1000,                 loss: 1.0061
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 18601/30000 (62.0033%),                 avg. length: 1537.05,                last time consumption/overall running time: 177.0749s / 221055.1696 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2182
env0_second_0:                 episode reward: -2.3500,                 loss: 1.0628
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 18621/30000 (62.0700%),                 avg. length: 1557.9,                last time consumption/overall running time: 174.2294s / 221229.3990 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.2005
env0_second_0:                 episode reward: -2.7500,                 loss: 1.0815
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 18641/30000 (62.1367%),                 avg. length: 1530.15,                last time consumption/overall running time: 172.0860s / 221401.4850 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.1815
env0_second_0:                 episode reward: -5.4000,                 loss: 0.7588
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 18661/30000 (62.2033%),                 avg. length: 1828.45,                last time consumption/overall running time: 202.1535s / 221603.6385 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1484
env0_second_0:                 episode reward: -3.0000,                 loss: 0.6887
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 18681/30000 (62.2700%),                 avg. length: 1993.0,                last time consumption/overall running time: 219.5380s / 221823.1764 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1046
env0_second_0:                 episode reward: -1.9500,                 loss: 0.9839
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 18701/30000 (62.3367%),                 avg. length: 1892.4,                last time consumption/overall running time: 215.7684s / 222038.9449 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.1017
env0_second_0:                 episode reward: -2.6000,                 loss: 0.6732
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 18721/30000 (62.4033%),                 avg. length: 2100.2,                last time consumption/overall running time: 229.0994s / 222268.0443 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.0883
env0_second_0:                 episode reward: -0.7000,                 loss: 0.8978
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 18741/30000 (62.4700%),                 avg. length: 1842.0,                last time consumption/overall running time: 206.7276s / 222474.7719 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.2024
env0_second_0:                 episode reward: -2.9500,                 loss: 0.8976
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 18761/30000 (62.5367%),                 avg. length: 1718.1,                last time consumption/overall running time: 189.3507s / 222664.1226 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.1707
env0_second_0:                 episode reward: -3.5000,                 loss: 1.5271
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 18781/30000 (62.6033%),                 avg. length: 1548.35,                last time consumption/overall running time: 174.7811s / 222838.9036 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2284
env0_second_0:                 episode reward: -2.0500,                 loss: 1.0113
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 18801/30000 (62.6700%),                 avg. length: 1550.15,                last time consumption/overall running time: 164.6200s / 223003.5236 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2339
env0_second_0:                 episode reward: -1.0500,                 loss: 0.9468
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 18821/30000 (62.7367%),                 avg. length: 1507.0,                last time consumption/overall running time: 162.2920s / 223165.8156 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2609
env0_second_0:                 episode reward: -2.1500,                 loss: 1.0178
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 18841/30000 (62.8033%),                 avg. length: 1591.35,                last time consumption/overall running time: 177.0175s / 223342.8331 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2333
env0_second_0:                 episode reward: -1.7500,                 loss: 0.9213
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 18861/30000 (62.8700%),                 avg. length: 1825.25,                last time consumption/overall running time: 202.2540s / 223545.0871 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1628
env0_second_0:                 episode reward: -3.6500,                 loss: 0.7635
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 18881/30000 (62.9367%),                 avg. length: 1894.2,                last time consumption/overall running time: 207.0011s / 223752.0881 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.1679
env0_second_0:                 episode reward: -3.1000,                 loss: 0.5623
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 18901/30000 (63.0033%),                 avg. length: 1987.35,                last time consumption/overall running time: 220.2216s / 223972.3097 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.1615
env0_second_0:                 episode reward: -4.3500,                 loss: 0.7452
env1_first_0:                 episode reward: 4.0500,                 loss: nan
env1_second_0:                 episode reward: -4.0500,                 loss: nan
Episode: 18921/30000 (63.0700%),                 avg. length: 1975.95,                last time consumption/overall running time: 215.3984s / 224187.7081 s
env0_first_0:                 episode reward: 4.6000,                 loss: -0.1098
env0_second_0:                 episode reward: -4.6000,                 loss: 0.9530
env1_first_0:                 episode reward: 5.1500,                 loss: nan
env1_second_0:                 episode reward: -5.1500,                 loss: nan
Episode: 18941/30000 (63.1367%),                 avg. length: 1943.25,                last time consumption/overall running time: 236.4802s / 224424.1883 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.1601
env0_second_0:                 episode reward: -7.3000,                 loss: 1.5300
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 18961/30000 (63.2033%),                 avg. length: 1840.65,                last time consumption/overall running time: 197.8571s / 224622.0454 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1527
env0_second_0:                 episode reward: -4.8000,                 loss: 1.4287
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 18981/30000 (63.2700%),                 avg. length: 1830.05,                last time consumption/overall running time: 201.0104s / 224823.0558 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1111
env0_second_0:                 episode reward: -0.7000,                 loss: 1.4159
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 19001/30000 (63.3367%),                 avg. length: 1651.95,                last time consumption/overall running time: 179.2200s / 225002.2758 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1685
env0_second_0:                 episode reward: -2.1000,                 loss: 1.1029
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 19021/30000 (63.4033%),                 avg. length: 2066.35,                last time consumption/overall running time: 230.2297s / 225232.5055 s
env0_first_0:                 episode reward: -19.8000,                 loss: -0.0461
env0_second_0:                 episode reward: 19.8000,                 loss: 1.2460
env1_first_0:                 episode reward: -18.7500,                 loss: nan
env1_second_0:                 episode reward: 18.7500,                 loss: nan
Episode: 19041/30000 (63.4700%),                 avg. length: 1507.9,                last time consumption/overall running time: 168.1096s / 225400.6151 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2229
env0_second_0:                 episode reward: -1.7000,                 loss: 0.6944
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 19061/30000 (63.5367%),                 avg. length: 1662.65,                last time consumption/overall running time: 182.4326s / 225583.0477 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2053
env0_second_0:                 episode reward: -2.1000,                 loss: 1.1315
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 19081/30000 (63.6033%),                 avg. length: 2141.45,                last time consumption/overall running time: 230.3000s / 225813.3477 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1030
env0_second_0:                 episode reward: -4.8000,                 loss: 1.1508
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 19101/30000 (63.6700%),                 avg. length: 2080.65,                last time consumption/overall running time: 226.3765s / 226039.7241 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.1410
env0_second_0:                 episode reward: -7.3000,                 loss: 1.0500
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 19121/30000 (63.7367%),                 avg. length: 2351.0,                last time consumption/overall running time: 259.0746s / 226298.7987 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0611
env0_second_0:                 episode reward: -2.6000,                 loss: 1.3500
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 19141/30000 (63.8033%),                 avg. length: 2116.1,                last time consumption/overall running time: 230.9470s / 226529.7458 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1360
env0_second_0:                 episode reward: 2.8500,                 loss: 0.7790
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 19161/30000 (63.8700%),                 avg. length: 2248.65,                last time consumption/overall running time: 257.2001s / 226786.9459 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.1181
env0_second_0:                 episode reward: 1.6000,                 loss: 1.1328
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 19181/30000 (63.9367%),                 avg. length: 1978.8,                last time consumption/overall running time: 221.8946s / 227008.8405 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1499
env0_second_0:                 episode reward: 1.3000,                 loss: 0.8253
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 19201/30000 (64.0033%),                 avg. length: 2064.75,                last time consumption/overall running time: 237.8246s / 227246.6651 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1800
env0_second_0:                 episode reward: 0.1500,                 loss: 1.4918
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 19221/30000 (64.0700%),                 avg. length: 1680.9,                last time consumption/overall running time: 193.0467s / 227439.7118 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1812
env0_second_0:                 episode reward: 0.9000,                 loss: 0.8660
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 19241/30000 (64.1367%),                 avg. length: 1778.8,                last time consumption/overall running time: 195.6392s / 227635.3509 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1403
env0_second_0:                 episode reward: 1.1000,                 loss: 0.8207
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 19261/30000 (64.2033%),                 avg. length: 1981.55,                last time consumption/overall running time: 221.4112s / 227856.7622 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1410
env0_second_0:                 episode reward: -1.3000,                 loss: 0.7056
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 19281/30000 (64.2700%),                 avg. length: 1809.7,                last time consumption/overall running time: 198.2779s / 228055.0401 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.1468
env0_second_0:                 episode reward: -0.4500,                 loss: 0.7621
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 19301/30000 (64.3367%),                 avg. length: 1964.2,                last time consumption/overall running time: 210.4551s / 228265.4952 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0997
env0_second_0:                 episode reward: -2.6000,                 loss: 0.9781
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 19321/30000 (64.4033%),                 avg. length: 1947.7,                last time consumption/overall running time: 207.4717s / 228472.9668 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.0518
env0_second_0:                 episode reward: 2.2500,                 loss: 1.1310
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 19341/30000 (64.4700%),                 avg. length: 2015.8,                last time consumption/overall running time: 224.9422s / 228697.9090 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.0085
env0_second_0:                 episode reward: 8.3000,                 loss: 1.0391
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 19361/30000 (64.5367%),                 avg. length: 1635.2,                last time consumption/overall running time: 187.0622s / 228884.9712 s
env0_first_0:                 episode reward: -1.9500,                 loss: -0.0813
env0_second_0:                 episode reward: 1.9500,                 loss: 1.1112
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 19381/30000 (64.6033%),                 avg. length: 1498.9,                last time consumption/overall running time: 197.7444s / 229082.7156 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1314
env0_second_0:                 episode reward: -1.6000,                 loss: 1.1298
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 19401/30000 (64.6700%),                 avg. length: 1652.5,                last time consumption/overall running time: 189.6681s / 229272.3837 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.1223
env0_second_0:                 episode reward: 1.9000,                 loss: 0.9232
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 19421/30000 (64.7367%),                 avg. length: 1541.05,                last time consumption/overall running time: 168.2384s / 229440.6222 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.0504
env0_second_0:                 episode reward: -4.5500,                 loss: 2.4044
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 19441/30000 (64.8033%),                 avg. length: 1588.75,                last time consumption/overall running time: 179.5717s / 229620.1938 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0706
env0_second_0:                 episode reward: -4.6500,                 loss: 2.4305
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 19461/30000 (64.8700%),                 avg. length: 1554.35,                last time consumption/overall running time: 168.1338s / 229788.3276 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.0993
env0_second_0:                 episode reward: -4.7500,                 loss: 1.7231
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 19481/30000 (64.9367%),                 avg. length: 1499.65,                last time consumption/overall running time: 161.3104s / 229949.6381 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1824
env0_second_0:                 episode reward: -2.7500,                 loss: 1.1199
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 19501/30000 (65.0033%),                 avg. length: 1659.6,                last time consumption/overall running time: 182.1878s / 230131.8259 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.1025
env0_second_0:                 episode reward: -4.4500,                 loss: 1.6603
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 19521/30000 (65.0700%),                 avg. length: 1462.75,                last time consumption/overall running time: 164.5720s / 230296.3979 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1745
env0_second_0:                 episode reward: -2.0000,                 loss: 1.4414
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 19541/30000 (65.1367%),                 avg. length: 1793.45,                last time consumption/overall running time: 211.5784s / 230507.9763 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1498
env0_second_0:                 episode reward: -1.5000,                 loss: 1.6442
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 19561/30000 (65.2033%),                 avg. length: 1529.4,                last time consumption/overall running time: 181.7051s / 230689.6814 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1289
env0_second_0:                 episode reward: -1.9500,                 loss: 1.2693
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 19581/30000 (65.2700%),                 avg. length: 1488.45,                last time consumption/overall running time: 160.4533s / 230850.1347 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1968
env0_second_0:                 episode reward: -1.4000,                 loss: 1.2010
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 19601/30000 (65.3367%),                 avg. length: 1523.4,                last time consumption/overall running time: 167.8905s / 231018.0252 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1467
env0_second_0:                 episode reward: -1.7500,                 loss: 1.3668
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 19621/30000 (65.4033%),                 avg. length: 1676.6,                last time consumption/overall running time: 189.2226s / 231207.2478 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.0933
env0_second_0:                 episode reward: -4.4500,                 loss: 1.6462
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 19641/30000 (65.4700%),                 avg. length: 1607.35,                last time consumption/overall running time: 177.6661s / 231384.9139 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1656
env0_second_0:                 episode reward: -1.8000,                 loss: 1.4698
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 19661/30000 (65.5367%),                 avg. length: 1589.35,                last time consumption/overall running time: 173.2084s / 231558.1223 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1572
env0_second_0:                 episode reward: -2.9000,                 loss: 1.4116
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 19681/30000 (65.6033%),                 avg. length: 1631.4,                last time consumption/overall running time: 175.5582s / 231733.6805 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1458
env0_second_0:                 episode reward: -3.4000,                 loss: 1.3062
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 19701/30000 (65.6700%),                 avg. length: 1676.2,                last time consumption/overall running time: 183.8492s / 231917.5297 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.1026
env0_second_0:                 episode reward: -5.5500,                 loss: 1.7290
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 19721/30000 (65.7367%),                 avg. length: 1740.9,                last time consumption/overall running time: 194.1846s / 232111.7143 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.0813
env0_second_0:                 episode reward: -4.9500,                 loss: 1.2597
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 19741/30000 (65.8033%),                 avg. length: 2068.45,                last time consumption/overall running time: 221.6149s / 232333.3292 s
env0_first_0:                 episode reward: 10.5000,                 loss: -0.0999
env0_second_0:                 episode reward: -10.5000,                 loss: 1.2660
env1_first_0:                 episode reward: 12.5500,                 loss: nan
env1_second_0:                 episode reward: -12.5500,                 loss: nan
Episode: 19761/30000 (65.8700%),                 avg. length: 1908.0,                last time consumption/overall running time: 207.2980s / 232540.6272 s
env0_first_0:                 episode reward: 12.1000,                 loss: -0.0507
env0_second_0:                 episode reward: -12.1000,                 loss: 1.6207
env1_first_0:                 episode reward: 11.0500,                 loss: nan
env1_second_0:                 episode reward: -11.0500,                 loss: nan
Episode: 19781/30000 (65.9367%),                 avg. length: 2240.5,                last time consumption/overall running time: 241.4009s / 232782.0281 s
env0_first_0:                 episode reward: 13.3000,                 loss: -0.0425
env0_second_0:                 episode reward: -13.3000,                 loss: 1.5731
env1_first_0:                 episode reward: 12.4500,                 loss: nan
env1_second_0:                 episode reward: -12.4500,                 loss: nan
Episode: 19801/30000 (66.0033%),                 avg. length: 1621.3,                last time consumption/overall running time: 171.8998s / 232953.9278 s
env0_first_0:                 episode reward: 6.2500,                 loss: -0.1167
env0_second_0:                 episode reward: -6.2500,                 loss: 1.2283
env1_first_0:                 episode reward: 6.1500,                 loss: nan
env1_second_0:                 episode reward: -6.1500,                 loss: nan
Episode: 19821/30000 (66.0700%),                 avg. length: 1713.55,                last time consumption/overall running time: 186.2810s / 233140.2089 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1621
env0_second_0:                 episode reward: -0.6000,                 loss: 1.1138
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 19841/30000 (66.1367%),                 avg. length: 1767.0,                last time consumption/overall running time: 191.7386s / 233331.9475 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.0980
env0_second_0:                 episode reward: 0.5000,                 loss: 0.6883
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 19861/30000 (66.2033%),                 avg. length: 1573.2,                last time consumption/overall running time: 168.9583s / 233500.9058 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.1611
env0_second_0:                 episode reward: 4.0000,                 loss: 0.7127
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 19881/30000 (66.2700%),                 avg. length: 1456.7,                last time consumption/overall running time: 164.5269s / 233665.4327 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1890
env0_second_0:                 episode reward: 0.1000,                 loss: 0.4658
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 19901/30000 (66.3367%),                 avg. length: 1466.1,                last time consumption/overall running time: 161.5821s / 233827.0148 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1676
env0_second_0:                 episode reward: 0.7000,                 loss: 0.5681
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 19921/30000 (66.4033%),                 avg. length: 2410.4,                last time consumption/overall running time: 271.3313s / 234098.3461 s
env0_first_0:                 episode reward: 31.9500,                 loss: -0.0532
env0_second_0:                 episode reward: -31.9500,                 loss: 1.2332
env1_first_0:                 episode reward: 30.8000,                 loss: nan
env1_second_0:                 episode reward: -30.8000,                 loss: nan
Episode: 19941/30000 (66.4700%),                 avg. length: 1634.45,                last time consumption/overall running time: 191.4423s / 234289.7884 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.0560
env0_second_0:                 episode reward: -4.3000,                 loss: 1.2927
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 19961/30000 (66.5367%),                 avg. length: 1531.25,                last time consumption/overall running time: 182.3467s / 234472.1351 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1552
env0_second_0:                 episode reward: -0.5000,                 loss: 1.0266
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 19981/30000 (66.6033%),                 avg. length: 1472.95,                last time consumption/overall running time: 168.7823s / 234640.9174 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1813
env0_second_0:                 episode reward: -1.5000,                 loss: 0.7985
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 20001/30000 (66.6700%),                 avg. length: 1590.65,                last time consumption/overall running time: 177.4184s / 234818.3358 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.1104
env0_second_0:                 episode reward: 2.2000,                 loss: 0.6814
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 20021/30000 (66.7367%),                 avg. length: 1959.4,                last time consumption/overall running time: 218.8007s / 235037.1365 s
env0_first_0:                 episode reward: -19.5000,                 loss: 0.2231
env0_second_0:                 episode reward: 19.5000,                 loss: 1.3166
env1_first_0:                 episode reward: -18.2500,                 loss: nan
env1_second_0:                 episode reward: 18.2500,                 loss: nan
Episode: 20041/30000 (66.8033%),                 avg. length: 1778.75,                last time consumption/overall running time: 193.3182s / 235230.4547 s
env0_first_0:                 episode reward: -10.6500,                 loss: 0.0138
env0_second_0:                 episode reward: 10.6500,                 loss: 1.3646
env1_first_0:                 episode reward: -11.4000,                 loss: nan
env1_second_0:                 episode reward: 11.4000,                 loss: nan
Episode: 20061/30000 (66.8700%),                 avg. length: 1777.1,                last time consumption/overall running time: 190.2808s / 235420.7356 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0582
env0_second_0:                 episode reward: 7.6500,                 loss: 1.0782
env1_first_0:                 episode reward: -8.5000,                 loss: nan
env1_second_0:                 episode reward: 8.5000,                 loss: nan
Episode: 20081/30000 (66.9367%),                 avg. length: 1816.45,                last time consumption/overall running time: 194.1866s / 235614.9221 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0398
env0_second_0:                 episode reward: 6.8000,                 loss: 1.1717
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 20101/30000 (67.0033%),                 avg. length: 1606.55,                last time consumption/overall running time: 171.4484s / 235786.3705 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1342
env0_second_0:                 episode reward: 1.5500,                 loss: 1.0042
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 20121/30000 (67.0700%),                 avg. length: 1491.15,                last time consumption/overall running time: 162.9264s / 235949.2969 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1715
env0_second_0:                 episode reward: 0.7000,                 loss: 0.8658
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 20141/30000 (67.1367%),                 avg. length: 1795.3,                last time consumption/overall running time: 206.7490s / 236156.0459 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0903
env0_second_0:                 episode reward: -0.1500,                 loss: 1.0266
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 20161/30000 (67.2033%),                 avg. length: 1642.65,                last time consumption/overall running time: 183.8833s / 236339.9292 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1496
env0_second_0:                 episode reward: -1.2000,                 loss: 1.1760
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 20181/30000 (67.2700%),                 avg. length: 1656.55,                last time consumption/overall running time: 182.7678s / 236522.6971 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0068
env0_second_0:                 episode reward: 4.2000,                 loss: 1.1123
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 20201/30000 (67.3367%),                 avg. length: 1703.45,                last time consumption/overall running time: 181.7553s / 236704.4523 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.0755
env0_second_0:                 episode reward: -1.9000,                 loss: 1.5200
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 20221/30000 (67.4033%),                 avg. length: 1824.55,                last time consumption/overall running time: 208.8526s / 236913.3050 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1612
env0_second_0:                 episode reward: -0.7500,                 loss: 1.2275
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 20241/30000 (67.4700%),                 avg. length: 2062.7,                last time consumption/overall running time: 227.2341s / 237140.5391 s
env0_first_0:                 episode reward: 7.7000,                 loss: -0.0627
env0_second_0:                 episode reward: -7.7000,                 loss: 1.5012
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 20261/30000 (67.5367%),                 avg. length: 2020.45,                last time consumption/overall running time: 217.4411s / 237357.9801 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.0603
env0_second_0:                 episode reward: -7.1000,                 loss: 1.2254
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 20281/30000 (67.6033%),                 avg. length: 2491.2,                last time consumption/overall running time: 271.3041s / 237629.2842 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.1281
env0_second_0:                 episode reward: -4.6500,                 loss: 0.7562
env1_first_0:                 episode reward: 4.9500,                 loss: nan
env1_second_0:                 episode reward: -4.9500,                 loss: nan
Episode: 20301/30000 (67.6700%),                 avg. length: 2204.2,                last time consumption/overall running time: 238.2910s / 237867.5752 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.0796
env0_second_0:                 episode reward: -0.3000,                 loss: 1.1245
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 20321/30000 (67.7367%),                 avg. length: 2035.45,                last time consumption/overall running time: 218.7414s / 238086.3166 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.0588
env0_second_0:                 episode reward: -3.8000,                 loss: 1.2424
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 20341/30000 (67.8033%),                 avg. length: 2010.4,                last time consumption/overall running time: 252.5810s / 238338.8976 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.0765
env0_second_0:                 episode reward: -2.2500,                 loss: 1.1163
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 20361/30000 (67.8700%),                 avg. length: 2101.35,                last time consumption/overall running time: 226.0947s / 238564.9922 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.0967
env0_second_0:                 episode reward: -2.9500,                 loss: 0.9975
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 20381/30000 (67.9367%),                 avg. length: 2327.25,                last time consumption/overall running time: 249.9849s / 238814.9771 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.0681
env0_second_0:                 episode reward: -1.6500,                 loss: 0.9468
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 20401/30000 (68.0033%),                 avg. length: 2927.45,                last time consumption/overall running time: 315.0792s / 239130.0564 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0916
env0_second_0:                 episode reward: -3.2500,                 loss: 0.9756
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 20421/30000 (68.0700%),                 avg. length: 4049.45,                last time consumption/overall running time: 436.0946s / 239566.1509 s
env0_first_0:                 episode reward: 10.0000,                 loss: -0.1371
env0_second_0:                 episode reward: -10.0000,                 loss: 0.9370
env1_first_0:                 episode reward: 7.7500,                 loss: nan
env1_second_0:                 episode reward: -7.7500,                 loss: nan
Episode: 20441/30000 (68.1367%),                 avg. length: 8136.75,                last time consumption/overall running time: 859.8526s / 240426.0036 s
env0_first_0:                 episode reward: 8.9000,                 loss: -0.1653
env0_second_0:                 episode reward: -8.9000,                 loss: 0.8560
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 20461/30000 (68.2033%),                 avg. length: 6620.15,                last time consumption/overall running time: 691.6235s / 241117.6271 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.1558
env0_second_0:                 episode reward: -4.9500,                 loss: 0.6256
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 20481/30000 (68.2700%),                 avg. length: 4000.05,                last time consumption/overall running time: 423.9671s / 241541.5942 s
env0_first_0:                 episode reward: 5.4500,                 loss: -0.0691
env0_second_0:                 episode reward: -5.4500,                 loss: 0.9763
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 20501/30000 (68.3367%),                 avg. length: 1648.05,                last time consumption/overall running time: 177.3032s / 241718.8974 s
env0_first_0:                 episode reward: 4.4000,                 loss: 0.0167
env0_second_0:                 episode reward: -4.4000,                 loss: 1.8000
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 20521/30000 (68.4033%),                 avg. length: 1845.15,                last time consumption/overall running time: 199.5384s / 241918.4357 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.0971
env0_second_0:                 episode reward: -0.1500,                 loss: 1.6933
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 20541/30000 (68.4700%),                 avg. length: 2196.1,                last time consumption/overall running time: 233.5197s / 242151.9555 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1150
env0_second_0:                 episode reward: -0.8500,                 loss: 1.6808
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 20561/30000 (68.5367%),                 avg. length: 1737.45,                last time consumption/overall running time: 189.9227s / 242341.8782 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1548
env0_second_0:                 episode reward: -1.6000,                 loss: 1.4243
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 20581/30000 (68.6033%),                 avg. length: 1634.8,                last time consumption/overall running time: 182.7760s / 242524.6542 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1461
env0_second_0:                 episode reward: -0.6000,                 loss: 1.7731
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 20601/30000 (68.6700%),                 avg. length: 1648.1,                last time consumption/overall running time: 182.4664s / 242707.1206 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.1201
env0_second_0:                 episode reward: -2.4000,                 loss: 1.7716
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 20621/30000 (68.7367%),                 avg. length: 2152.75,                last time consumption/overall running time: 233.6115s / 242940.7321 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1532
env0_second_0:                 episode reward: -2.9000,                 loss: 1.2210
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 20641/30000 (68.8033%),                 avg. length: 2003.4,                last time consumption/overall running time: 223.7199s / 243164.4520 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1596
env0_second_0:                 episode reward: 0.0000,                 loss: 0.9946
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20661/30000 (68.8700%),                 avg. length: 1450.15,                last time consumption/overall running time: 160.5795s / 243325.0315 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1978
env0_second_0:                 episode reward: -1.7500,                 loss: 0.8696
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 20681/30000 (68.9367%),                 avg. length: 1843.45,                last time consumption/overall running time: 204.7008s / 243529.7323 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1125
env0_second_0:                 episode reward: -1.2000,                 loss: 1.2449
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 20701/30000 (69.0033%),                 avg. length: 1952.05,                last time consumption/overall running time: 210.9296s / 243740.6618 s
env0_first_0:                 episode reward: 5.1000,                 loss: -0.1230
env0_second_0:                 episode reward: -5.1000,                 loss: 1.5750
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 20721/30000 (69.0700%),                 avg. length: 1731.35,                last time consumption/overall running time: 190.4822s / 243931.1441 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1591
env0_second_0:                 episode reward: -1.1500,                 loss: 1.1544
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 20741/30000 (69.1367%),                 avg. length: 2248.5,                last time consumption/overall running time: 241.3562s / 244172.5003 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1372
env0_second_0:                 episode reward: -1.7000,                 loss: 1.2017
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 20761/30000 (69.2033%),                 avg. length: 2185.25,                last time consumption/overall running time: 240.3919s / 244412.8922 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.1597
env0_second_0:                 episode reward: 3.3500,                 loss: 1.1257
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 20781/30000 (69.2700%),                 avg. length: 2182.7,                last time consumption/overall running time: 243.9565s / 244656.8488 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1532
env0_second_0:                 episode reward: 4.5000,                 loss: 1.2261
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 20801/30000 (69.3367%),                 avg. length: 2389.85,                last time consumption/overall running time: 263.8089s / 244920.6577 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1484
env0_second_0:                 episode reward: -0.5500,                 loss: 1.1477
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 20821/30000 (69.4033%),                 avg. length: 1901.55,                last time consumption/overall running time: 213.6941s / 245134.3517 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1571
env0_second_0:                 episode reward: -2.7000,                 loss: 1.1846
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 20841/30000 (69.4700%),                 avg. length: 1513.1,                last time consumption/overall running time: 166.2565s / 245300.6082 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1946
env0_second_0:                 episode reward: -1.8500,                 loss: 1.2490
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 20861/30000 (69.5367%),                 avg. length: 2115.5,                last time consumption/overall running time: 230.7143s / 245531.3225 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1800
env0_second_0:                 episode reward: -0.5500,                 loss: 1.2572
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 20881/30000 (69.6033%),                 avg. length: 2370.4,                last time consumption/overall running time: 253.6519s / 245784.9744 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1820
env0_second_0:                 episode reward: -0.9000,                 loss: 1.4217
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 20901/30000 (69.6700%),                 avg. length: 2500.2,                last time consumption/overall running time: 267.6714s / 246052.6458 s
env0_first_0:                 episode reward: 4.5000,                 loss: -0.1641
env0_second_0:                 episode reward: -4.5000,                 loss: 1.9326
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 20921/30000 (69.7367%),                 avg. length: 3144.5,                last time consumption/overall running time: 344.2229s / 246396.8687 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.1911
env0_second_0:                 episode reward: -3.5000,                 loss: 1.8982
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 20941/30000 (69.8033%),                 avg. length: 3170.6,                last time consumption/overall running time: 347.0055s / 246743.8742 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2057
env0_second_0:                 episode reward: -3.8000,                 loss: 1.4745
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 20961/30000 (69.8700%),                 avg. length: 2435.0,                last time consumption/overall running time: 264.0185s / 247007.8927 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1592
env0_second_0:                 episode reward: 1.2500,                 loss: 1.5663
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 20981/30000 (69.9367%),                 avg. length: 2618.2,                last time consumption/overall running time: 295.9036s / 247303.7963 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1746
env0_second_0:                 episode reward: -0.6000,                 loss: 1.3582
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 21001/30000 (70.0033%),                 avg. length: 2764.8,                last time consumption/overall running time: 295.9146s / 247599.7109 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1927
env0_second_0:                 episode reward: -5.1500,                 loss: 1.6617
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 21021/30000 (70.0700%),                 avg. length: 2246.7,                last time consumption/overall running time: 237.6173s / 247837.3282 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1831
env0_second_0:                 episode reward: -2.4500,                 loss: 1.1838
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 21041/30000 (70.1367%),                 avg. length: 1769.3,                last time consumption/overall running time: 188.9539s / 248026.2821 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1734
env0_second_0:                 episode reward: -3.6000,                 loss: 1.2978
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 21061/30000 (70.2033%),                 avg. length: 1648.6,                last time consumption/overall running time: 184.1280s / 248210.4101 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1930
env0_second_0:                 episode reward: -1.3500,                 loss: 0.9901
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21081/30000 (70.2700%),                 avg. length: 1709.0,                last time consumption/overall running time: 194.8068s / 248405.2169 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1802
env0_second_0:                 episode reward: 2.1500,                 loss: 1.5788
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 21101/30000 (70.3367%),                 avg. length: 1758.75,                last time consumption/overall running time: 202.1620s / 248607.3789 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.1557
env0_second_0:                 episode reward: 1.2000,                 loss: 1.3097
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 21121/30000 (70.4033%),                 avg. length: 1551.55,                last time consumption/overall running time: 179.0316s / 248786.4105 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.1678
env0_second_0:                 episode reward: -3.6500,                 loss: 1.8651
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 21141/30000 (70.4700%),                 avg. length: 1666.95,                last time consumption/overall running time: 185.4745s / 248971.8850 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1778
env0_second_0:                 episode reward: -2.0000,                 loss: 1.3310
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 21161/30000 (70.5367%),                 avg. length: 2061.85,                last time consumption/overall running time: 229.0471s / 249200.9321 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1661
env0_second_0:                 episode reward: -2.2500,                 loss: 1.2461
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 21181/30000 (70.6033%),                 avg. length: 1515.3,                last time consumption/overall running time: 172.5148s / 249373.4470 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.2280
env0_second_0:                 episode reward: -3.1500,                 loss: 1.4945
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 21201/30000 (70.6700%),                 avg. length: 1497.75,                last time consumption/overall running time: 167.0363s / 249540.4833 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1652
env0_second_0:                 episode reward: -3.8000,                 loss: 1.0036
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 21221/30000 (70.7367%),                 avg. length: 1477.95,                last time consumption/overall running time: 164.6826s / 249705.1659 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.2253
env0_second_0:                 episode reward: -1.4000,                 loss: 0.9747
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 21241/30000 (70.8033%),                 avg. length: 1580.95,                last time consumption/overall running time: 171.7743s / 249876.9401 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1734
env0_second_0:                 episode reward: -1.4000,                 loss: 0.9872
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 21261/30000 (70.8700%),                 avg. length: 1700.85,                last time consumption/overall running time: 192.6775s / 250069.6176 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2113
env0_second_0:                 episode reward: -1.5500,                 loss: 1.0576
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 21281/30000 (70.9367%),                 avg. length: 1621.25,                last time consumption/overall running time: 179.0246s / 250248.6422 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2383
env0_second_0:                 episode reward: -0.5000,                 loss: 0.7607
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 21301/30000 (71.0033%),                 avg. length: 1644.15,                last time consumption/overall running time: 189.7115s / 250438.3537 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1910
env0_second_0:                 episode reward: -2.2500,                 loss: 1.2212
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 21321/30000 (71.0700%),                 avg. length: 1460.55,                last time consumption/overall running time: 161.4879s / 250599.8416 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2570
env0_second_0:                 episode reward: -2.6500,                 loss: 0.9647
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 21341/30000 (71.1367%),                 avg. length: 1492.75,                last time consumption/overall running time: 167.4949s / 250767.3365 s
env0_first_0:                 episode reward: 4.3500,                 loss: -0.2408
env0_second_0:                 episode reward: -4.3500,                 loss: 0.8715
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 21361/30000 (71.2033%),                 avg. length: 1474.8,                last time consumption/overall running time: 161.2802s / 250928.6167 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2440
env0_second_0:                 episode reward: -2.0500,                 loss: 0.6873
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 21381/30000 (71.2700%),                 avg. length: 1777.0,                last time consumption/overall running time: 199.4018s / 251128.0185 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1974
env0_second_0:                 episode reward: 2.0500,                 loss: 0.8457
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 21401/30000 (71.3367%),                 avg. length: 1777.1,                last time consumption/overall running time: 200.6256s / 251328.6441 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2305
env0_second_0:                 episode reward: -1.5000,                 loss: 0.6982
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 21421/30000 (71.4033%),                 avg. length: 1918.15,                last time consumption/overall running time: 213.9294s / 251542.5735 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2293
env0_second_0:                 episode reward: -1.8500,                 loss: 0.9267
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 21441/30000 (71.4700%),                 avg. length: 1681.0,                last time consumption/overall running time: 189.7516s / 251732.3251 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2288
env0_second_0:                 episode reward: -1.4500,                 loss: 0.9203
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 21461/30000 (71.5367%),                 avg. length: 1777.65,                last time consumption/overall running time: 192.9161s / 251925.2412 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.2102
env0_second_0:                 episode reward: -3.2500,                 loss: 0.9964
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 21481/30000 (71.6033%),                 avg. length: 1655.45,                last time consumption/overall running time: 182.8619s / 252108.1032 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2243
env0_second_0:                 episode reward: -1.8500,                 loss: 1.6315
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 21501/30000 (71.6700%),                 avg. length: 2062.25,                last time consumption/overall running time: 231.0745s / 252339.1777 s
env0_first_0:                 episode reward: -16.9500,                 loss: -0.0962
env0_second_0:                 episode reward: 16.9500,                 loss: 3.3631
env1_first_0:                 episode reward: -17.1000,                 loss: nan
env1_second_0:                 episode reward: 17.1000,                 loss: nan
Episode: 21521/30000 (71.7367%),                 avg. length: 1465.7,                last time consumption/overall running time: 169.1796s / 252508.3572 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2411
env0_second_0:                 episode reward: -0.3500,                 loss: 1.0678
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 21541/30000 (71.8033%),                 avg. length: 1496.1,                last time consumption/overall running time: 164.0008s / 252672.3580 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2230
env0_second_0:                 episode reward: -0.6500,                 loss: 1.0397
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 21561/30000 (71.8700%),                 avg. length: 1494.6,                last time consumption/overall running time: 163.2227s / 252835.5807 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2706
env0_second_0:                 episode reward: -1.2500,                 loss: 2.8738
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 21581/30000 (71.9367%),                 avg. length: 1584.05,                last time consumption/overall running time: 179.0205s / 253014.6012 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2808
env0_second_0:                 episode reward: -1.3500,                 loss: 1.4232
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 21601/30000 (72.0033%),                 avg. length: 1556.9,                last time consumption/overall running time: 173.2129s / 253187.8142 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2417
env0_second_0:                 episode reward: -2.1500,                 loss: 1.8615
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 21621/30000 (72.0700%),                 avg. length: 1750.5,                last time consumption/overall running time: 186.8783s / 253374.6924 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2544
env0_second_0:                 episode reward: -1.3000,                 loss: 1.2630
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 21641/30000 (72.1367%),                 avg. length: 1710.7,                last time consumption/overall running time: 184.4584s / 253559.1508 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2361
env0_second_0:                 episode reward: -2.0500,                 loss: 1.1928
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 21661/30000 (72.2033%),                 avg. length: 1852.4,                last time consumption/overall running time: 208.8023s / 253767.9531 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2031
env0_second_0:                 episode reward: -1.7000,                 loss: 1.3134
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 21681/30000 (72.2700%),                 avg. length: 1789.9,                last time consumption/overall running time: 192.7440s / 253960.6971 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2083
env0_second_0:                 episode reward: -1.4500,                 loss: 1.0589
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21701/30000 (72.3367%),                 avg. length: 1841.9,                last time consumption/overall running time: 205.2609s / 254165.9580 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.1790
env0_second_0:                 episode reward: 1.7500,                 loss: 1.5763
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 21721/30000 (72.4033%),                 avg. length: 2196.2,                last time consumption/overall running time: 244.1999s / 254410.1579 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.1476
env0_second_0:                 episode reward: 3.8500,                 loss: 1.4414
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 21741/30000 (72.4700%),                 avg. length: 2376.85,                last time consumption/overall running time: 264.2609s / 254674.4187 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.0735
env0_second_0:                 episode reward: 0.8500,                 loss: 1.5255
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 21761/30000 (72.5367%),                 avg. length: 2247.05,                last time consumption/overall running time: 266.1249s / 254940.5436 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0579
env0_second_0:                 episode reward: 0.2500,                 loss: 2.1034
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 21781/30000 (72.6033%),                 avg. length: 1706.5,                last time consumption/overall running time: 181.9716s / 255122.5152 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.0307
env0_second_0:                 episode reward: -3.2500,                 loss: 2.1928
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 21801/30000 (72.6700%),                 avg. length: 1546.75,                last time consumption/overall running time: 165.4867s / 255288.0019 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1398
env0_second_0:                 episode reward: -0.8000,                 loss: 1.5577
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21821/30000 (72.7367%),                 avg. length: 1604.85,                last time consumption/overall running time: 174.7946s / 255462.7965 s
env0_first_0:                 episode reward: 5.9500,                 loss: -0.1515
env0_second_0:                 episode reward: -5.9500,                 loss: 1.6613
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 21841/30000 (72.8033%),                 avg. length: 1615.5,                last time consumption/overall running time: 176.4411s / 255639.2376 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1904
env0_second_0:                 episode reward: -3.2500,                 loss: 1.4268
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 21861/30000 (72.8700%),                 avg. length: 1828.8,                last time consumption/overall running time: 199.8452s / 255839.0827 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1601
env0_second_0:                 episode reward: -0.9000,                 loss: 1.2646
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 21881/30000 (72.9367%),                 avg. length: 1649.7,                last time consumption/overall running time: 178.2320s / 256017.3147 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1922
env0_second_0:                 episode reward: -2.0500,                 loss: 1.2740
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 21901/30000 (73.0033%),                 avg. length: 1939.35,                last time consumption/overall running time: 220.2320s / 256237.5467 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.1085
env0_second_0:                 episode reward: -2.7500,                 loss: 1.6174
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 21921/30000 (73.0700%),                 avg. length: 2217.3,                last time consumption/overall running time: 239.3322s / 256476.8789 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.1388
env0_second_0:                 episode reward: 3.4500,                 loss: 1.3774
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 21941/30000 (73.1367%),                 avg. length: 2762.9,                last time consumption/overall running time: 298.6983s / 256775.5772 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1024
env0_second_0:                 episode reward: 0.1000,                 loss: 1.0878
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 21961/30000 (73.2033%),                 avg. length: 1589.6,                last time consumption/overall running time: 191.2892s / 256966.8664 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2381
env0_second_0:                 episode reward: -0.8500,                 loss: 0.6174
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 21981/30000 (73.2700%),                 avg. length: 1810.8,                last time consumption/overall running time: 202.6328s / 257169.4992 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1953
env0_second_0:                 episode reward: -1.6000,                 loss: 0.6996
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 22001/30000 (73.3367%),                 avg. length: 1693.45,                last time consumption/overall running time: 189.4359s / 257358.9350 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.1869
env0_second_0:                 episode reward: -2.4500,                 loss: 1.0413
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 22021/30000 (73.4033%),                 avg. length: 1733.7,                last time consumption/overall running time: 191.8233s / 257550.7584 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1357
env0_second_0:                 episode reward: -3.9000,                 loss: 1.3384
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 22041/30000 (73.4700%),                 avg. length: 2022.5,                last time consumption/overall running time: 226.0802s / 257776.8386 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1237
env0_second_0:                 episode reward: -4.4000,                 loss: 1.5783
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 22061/30000 (73.5367%),                 avg. length: 1821.25,                last time consumption/overall running time: 196.0517s / 257972.8903 s
env0_first_0:                 episode reward: 6.1500,                 loss: -0.0919
env0_second_0:                 episode reward: -6.1500,                 loss: 1.1644
env1_first_0:                 episode reward: 6.0000,                 loss: nan
env1_second_0:                 episode reward: -6.0000,                 loss: nan
Episode: 22081/30000 (73.6033%),                 avg. length: 1946.95,                last time consumption/overall running time: 211.5382s / 258184.4285 s
env0_first_0:                 episode reward: 5.4000,                 loss: -0.0743
env0_second_0:                 episode reward: -5.4000,                 loss: 1.1619
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 22101/30000 (73.6700%),                 avg. length: 1884.5,                last time consumption/overall running time: 215.0448s / 258399.4733 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.0824
env0_second_0:                 episode reward: -5.9000,                 loss: 1.2621
env1_first_0:                 episode reward: 6.5500,                 loss: nan
env1_second_0:                 episode reward: -6.5500,                 loss: nan
Episode: 22121/30000 (73.7367%),                 avg. length: 1867.9,                last time consumption/overall running time: 211.0332s / 258610.5065 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.1038
env0_second_0:                 episode reward: -5.9000,                 loss: 1.0828
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 22141/30000 (73.8033%),                 avg. length: 2011.2,                last time consumption/overall running time: 236.7980s / 258847.3045 s
env0_first_0:                 episode reward: 3.6000,                 loss: -0.1025
env0_second_0:                 episode reward: -3.6000,                 loss: 0.9840
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 22161/30000 (73.8700%),                 avg. length: 2877.2,                last time consumption/overall running time: 357.1600s / 259204.4645 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1354
env0_second_0:                 episode reward: 2.0500,                 loss: 1.0139
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 22181/30000 (73.9367%),                 avg. length: 2313.5,                last time consumption/overall running time: 280.1893s / 259484.6538 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1441
env0_second_0:                 episode reward: 2.8500,                 loss: 0.9503
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 22201/30000 (74.0033%),                 avg. length: 2110.5,                last time consumption/overall running time: 232.3023s / 259716.9561 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1642
env0_second_0:                 episode reward: 0.1500,                 loss: 0.7345
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 22221/30000 (74.0700%),                 avg. length: 1879.05,                last time consumption/overall running time: 205.5979s / 259922.5540 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1925
env0_second_0:                 episode reward: 0.7000,                 loss: 1.1986
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 22241/30000 (74.1367%),                 avg. length: 1977.85,                last time consumption/overall running time: 234.1258s / 260156.6798 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1349
env0_second_0:                 episode reward: -2.5000,                 loss: 1.6490
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 22261/30000 (74.2033%),                 avg. length: 1694.9,                last time consumption/overall running time: 182.6334s / 260339.3132 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1888
env0_second_0:                 episode reward: -0.9000,                 loss: 1.1008
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 22281/30000 (74.2700%),                 avg. length: 1831.8,                last time consumption/overall running time: 206.6719s / 260545.9851 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1948
env0_second_0:                 episode reward: -1.7500,                 loss: 1.1257
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 22301/30000 (74.3367%),                 avg. length: 2246.4,                last time consumption/overall running time: 246.8182s / 260792.8033 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1787
env0_second_0:                 episode reward: -3.2000,                 loss: 1.2597
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 22321/30000 (74.4033%),                 avg. length: 2305.9,                last time consumption/overall running time: 247.0417s / 261039.8450 s
env0_first_0:                 episode reward: 5.2500,                 loss: -0.1080
env0_second_0:                 episode reward: -5.2500,                 loss: 1.0482
env1_first_0:                 episode reward: 4.6000,                 loss: nan
env1_second_0:                 episode reward: -4.6000,                 loss: nan
Episode: 22341/30000 (74.4700%),                 avg. length: 2171.2,                last time consumption/overall running time: 258.1294s / 261297.9743 s
env0_first_0:                 episode reward: 6.2000,                 loss: -0.1020
env0_second_0:                 episode reward: -6.2000,                 loss: 1.1615
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 22361/30000 (74.5367%),                 avg. length: 2425.65,                last time consumption/overall running time: 266.9674s / 261564.9417 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1113
env0_second_0:                 episode reward: -7.0000,                 loss: 1.1078
env1_first_0:                 episode reward: 7.2000,                 loss: nan
env1_second_0:                 episode reward: -7.2000,                 loss: nan
Episode: 22381/30000 (74.6033%),                 avg. length: 2497.85,                last time consumption/overall running time: 280.9514s / 261845.8930 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1657
env0_second_0:                 episode reward: -2.2000,                 loss: 1.0048
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 22401/30000 (74.6700%),                 avg. length: 2120.75,                last time consumption/overall running time: 235.3393s / 262081.2324 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1192
env0_second_0:                 episode reward: -1.4500,                 loss: 1.1134
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 22421/30000 (74.7367%),                 avg. length: 2040.25,                last time consumption/overall running time: 238.4304s / 262319.6628 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1556
env0_second_0:                 episode reward: -0.8500,                 loss: 1.3577
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 22441/30000 (74.8033%),                 avg. length: 2011.9,                last time consumption/overall running time: 219.2352s / 262538.8980 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1473
env0_second_0:                 episode reward: -0.1500,                 loss: 1.8944
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 22461/30000 (74.8700%),                 avg. length: 1755.7,                last time consumption/overall running time: 207.5792s / 262746.4772 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.1817
env0_second_0:                 episode reward: -5.7000,                 loss: 1.2921
env1_first_0:                 episode reward: 5.6500,                 loss: nan
env1_second_0:                 episode reward: -5.6500,                 loss: nan
Episode: 22481/30000 (74.9367%),                 avg. length: 1894.45,                last time consumption/overall running time: 220.6256s / 262967.1029 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1899
env0_second_0:                 episode reward: 2.3500,                 loss: 0.8606
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 22501/30000 (75.0033%),                 avg. length: 1616.25,                last time consumption/overall running time: 176.1170s / 263143.2199 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1886
env0_second_0:                 episode reward: 0.4000,                 loss: 0.8671
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 22521/30000 (75.0700%),                 avg. length: 1652.05,                last time consumption/overall running time: 187.2726s / 263330.4925 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1944
env0_second_0:                 episode reward: -1.9000,                 loss: 1.0602
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 22541/30000 (75.1367%),                 avg. length: 2080.35,                last time consumption/overall running time: 230.9400s / 263561.4324 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1760
env0_second_0:                 episode reward: -1.2500,                 loss: 1.2643
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 22561/30000 (75.2033%),                 avg. length: 2263.75,                last time consumption/overall running time: 244.0528s / 263805.4853 s
env0_first_0:                 episode reward: 3.2000,                 loss: -0.1747
env0_second_0:                 episode reward: -3.2000,                 loss: 1.1761
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 22581/30000 (75.2700%),                 avg. length: 1981.95,                last time consumption/overall running time: 225.1043s / 264030.5896 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1530
env0_second_0:                 episode reward: -3.0000,                 loss: 0.9635
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 22601/30000 (75.3367%),                 avg. length: 1714.75,                last time consumption/overall running time: 196.0476s / 264226.6372 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.1206
env0_second_0:                 episode reward: -2.2000,                 loss: 1.2382
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 22621/30000 (75.4033%),                 avg. length: 1747.75,                last time consumption/overall running time: 199.6489s / 264426.2861 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1421
env0_second_0:                 episode reward: -1.4500,                 loss: 1.5289
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 22641/30000 (75.4700%),                 avg. length: 1590.95,                last time consumption/overall running time: 174.6697s / 264600.9557 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.1837
env0_second_0:                 episode reward: -0.4500,                 loss: 1.0540
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 22661/30000 (75.5367%),                 avg. length: 1675.0,                last time consumption/overall running time: 186.6474s / 264787.6032 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1758
env0_second_0:                 episode reward: -1.5000,                 loss: 1.0747
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 22681/30000 (75.6033%),                 avg. length: 1577.25,                last time consumption/overall running time: 180.5409s / 264968.1441 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.1607
env0_second_0:                 episode reward: 0.7500,                 loss: 1.2728
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 22701/30000 (75.6700%),                 avg. length: 1812.6,                last time consumption/overall running time: 199.0605s / 265167.2046 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1544
env0_second_0:                 episode reward: 0.2500,                 loss: 1.0203
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 22721/30000 (75.7367%),                 avg. length: 1675.55,                last time consumption/overall running time: 184.6411s / 265351.8456 s
env0_first_0:                 episode reward: 3.7500,                 loss: -0.1346
env0_second_0:                 episode reward: -3.7500,                 loss: 1.1540
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 22741/30000 (75.8033%),                 avg. length: 1801.7,                last time consumption/overall running time: 200.2393s / 265552.0850 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1014
env0_second_0:                 episode reward: -1.2500,                 loss: 1.1052
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 22761/30000 (75.8700%),                 avg. length: 1753.2,                last time consumption/overall running time: 193.3448s / 265745.4298 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.0812
env0_second_0:                 episode reward: -0.0500,                 loss: 1.0406
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 22781/30000 (75.9367%),                 avg. length: 2092.25,                last time consumption/overall running time: 231.1815s / 265976.6113 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.0473
env0_second_0:                 episode reward: 1.2000,                 loss: 1.1552
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 22801/30000 (76.0033%),                 avg. length: 2260.35,                last time consumption/overall running time: 240.1467s / 266216.7580 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.0959
env0_second_0:                 episode reward: 2.3500,                 loss: 1.7235
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 22821/30000 (76.0700%),                 avg. length: 1865.95,                last time consumption/overall running time: 209.4799s / 266426.2379 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.0539
env0_second_0:                 episode reward: 4.9500,                 loss: 1.2939
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 22841/30000 (76.1367%),                 avg. length: 1739.85,                last time consumption/overall running time: 195.7844s / 266622.0223 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.0845
env0_second_0:                 episode reward: -1.5000,                 loss: 1.1299
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 22861/30000 (76.2033%),                 avg. length: 2110.3,                last time consumption/overall running time: 235.1621s / 266857.1844 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.1021
env0_second_0:                 episode reward: 2.3500,                 loss: 1.2243
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 22881/30000 (76.2700%),                 avg. length: 1907.75,                last time consumption/overall running time: 221.6339s / 267078.8183 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0001
env0_second_0:                 episode reward: 5.4000,                 loss: 1.2670
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 22901/30000 (76.3367%),                 avg. length: 1754.05,                last time consumption/overall running time: 209.0098s / 267287.8281 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.0441
env0_second_0:                 episode reward: 1.7500,                 loss: 1.4343
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 22921/30000 (76.4033%),                 avg. length: 1602.7,                last time consumption/overall running time: 182.7480s / 267470.5762 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1767
env0_second_0:                 episode reward: -0.5000,                 loss: 1.0038
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 22941/30000 (76.4700%),                 avg. length: 1642.45,                last time consumption/overall running time: 187.0355s / 267657.6117 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1458
env0_second_0:                 episode reward: 1.2500,                 loss: 0.9757
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 22961/30000 (76.5367%),                 avg. length: 1660.9,                last time consumption/overall running time: 180.2137s / 267837.8254 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.1712
env0_second_0:                 episode reward: -0.6500,                 loss: 0.9852
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 22981/30000 (76.6033%),                 avg. length: 1549.25,                last time consumption/overall running time: 186.7204s / 268024.5458 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1479
env0_second_0:                 episode reward: 0.8000,                 loss: 0.9440
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 23001/30000 (76.6700%),                 avg. length: 1801.45,                last time consumption/overall running time: 195.3329s / 268219.8787 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.1243
env0_second_0:                 episode reward: 2.2000,                 loss: 1.0336
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 23021/30000 (76.7367%),                 avg. length: 1929.0,                last time consumption/overall running time: 216.7480s / 268436.6267 s
env0_first_0:                 episode reward: -5.0500,                 loss: -0.0003
env0_second_0:                 episode reward: 5.0500,                 loss: 1.7170
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 23041/30000 (76.8033%),                 avg. length: 2010.65,                last time consumption/overall running time: 223.9018s / 268660.5285 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.0706
env0_second_0:                 episode reward: 0.7500,                 loss: 1.5928
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 23061/30000 (76.8700%),                 avg. length: 1730.5,                last time consumption/overall running time: 198.5020s / 268859.0305 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1437
env0_second_0:                 episode reward: -2.3000,                 loss: 1.5357
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 23081/30000 (76.9367%),                 avg. length: 1888.75,                last time consumption/overall running time: 209.0849s / 269068.1155 s
env0_first_0:                 episode reward: 5.0500,                 loss: -0.1718
env0_second_0:                 episode reward: -5.0500,                 loss: 1.4573
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 23101/30000 (77.0033%),                 avg. length: 2066.6,                last time consumption/overall running time: 236.4841s / 269304.5995 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.1599
env0_second_0:                 episode reward: -3.9500,                 loss: 1.4348
env1_first_0:                 episode reward: 3.1000,                 loss: nan
env1_second_0:                 episode reward: -3.1000,                 loss: nan
Episode: 23121/30000 (77.0700%),                 avg. length: 1634.6,                last time consumption/overall running time: 181.6983s / 269486.2978 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2041
env0_second_0:                 episode reward: -3.3000,                 loss: 1.0653
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 23141/30000 (77.1367%),                 avg. length: 1465.4,                last time consumption/overall running time: 166.9375s / 269653.2354 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2764
env0_second_0:                 episode reward: -1.0500,                 loss: 0.6393
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 23161/30000 (77.2033%),                 avg. length: 1534.75,                last time consumption/overall running time: 174.0383s / 269827.2737 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2521
env0_second_0:                 episode reward: -1.8500,                 loss: 0.6563
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 23181/30000 (77.2700%),                 avg. length: 1481.35,                last time consumption/overall running time: 171.1017s / 269998.3754 s
env0_first_0:                 episode reward: 2.2000,                 loss: -0.2799
env0_second_0:                 episode reward: -2.2000,                 loss: 1.0092
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 23201/30000 (77.3367%),                 avg. length: 1475.15,                last time consumption/overall running time: 167.8908s / 270166.2661 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2882
env0_second_0:                 episode reward: -1.7000,                 loss: 0.9796
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 23221/30000 (77.4033%),                 avg. length: 1467.7,                last time consumption/overall running time: 167.4903s / 270333.7564 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2753
env0_second_0:                 episode reward: -1.1000,                 loss: 0.7955
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 23241/30000 (77.4700%),                 avg. length: 1468.8,                last time consumption/overall running time: 177.3640s / 270511.1204 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2772
env0_second_0:                 episode reward: -1.8000,                 loss: 0.9003
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 23261/30000 (77.5367%),                 avg. length: 1499.75,                last time consumption/overall running time: 168.2308s / 270679.3512 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2517
env0_second_0:                 episode reward: -1.0000,                 loss: 0.6498
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 23281/30000 (77.6033%),                 avg. length: 1507.15,                last time consumption/overall running time: 172.2870s / 270851.6382 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2075
env0_second_0:                 episode reward: 0.4000,                 loss: 0.6828
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 23301/30000 (77.6700%),                 avg. length: 1478.4,                last time consumption/overall running time: 166.4412s / 271018.0794 s
env0_first_0:                 episode reward: -6.9000,                 loss: -0.1334
env0_second_0:                 episode reward: 6.9000,                 loss: 0.9968
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 23321/30000 (77.7367%),                 avg. length: 1515.0,                last time consumption/overall running time: 169.9977s / 271188.0772 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.1700
env0_second_0:                 episode reward: 2.2000,                 loss: 0.6859
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 23341/30000 (77.8033%),                 avg. length: 1767.95,                last time consumption/overall running time: 188.9207s / 271376.9978 s
env0_first_0:                 episode reward: -2.0500,                 loss: -0.1591
env0_second_0:                 episode reward: 2.0500,                 loss: 1.1913
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 23361/30000 (77.8700%),                 avg. length: 1565.5,                last time consumption/overall running time: 167.2975s / 271544.2953 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1264
env0_second_0:                 episode reward: -3.4000,                 loss: 1.4787
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 23381/30000 (77.9367%),                 avg. length: 1473.8,                last time consumption/overall running time: 164.3807s / 271708.6760 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1997
env0_second_0:                 episode reward: -2.8500,                 loss: 1.2804
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 23401/30000 (78.0033%),                 avg. length: 1506.8,                last time consumption/overall running time: 167.0263s / 271875.7023 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1831
env0_second_0:                 episode reward: 0.3000,                 loss: 1.1984
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 23421/30000 (78.0700%),                 avg. length: 1439.9,                last time consumption/overall running time: 173.6495s / 272049.3517 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2417
env0_second_0:                 episode reward: -2.6500,                 loss: 1.2568
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 23441/30000 (78.1367%),                 avg. length: 1607.1,                last time consumption/overall running time: 179.2177s / 272228.5694 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.1969
env0_second_0:                 episode reward: -3.1500,                 loss: 1.9969
env1_first_0:                 episode reward: 4.1000,                 loss: nan
env1_second_0:                 episode reward: -4.1000,                 loss: nan
Episode: 23461/30000 (78.2033%),                 avg. length: 1470.45,                last time consumption/overall running time: 165.2315s / 272393.8009 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2493
env0_second_0:                 episode reward: -2.4500,                 loss: 1.4733
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 23481/30000 (78.2700%),                 avg. length: 1703.9,                last time consumption/overall running time: 189.6203s / 272583.4212 s
env0_first_0:                 episode reward: 4.8000,                 loss: -0.1907
env0_second_0:                 episode reward: -4.8000,                 loss: 1.4029
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 23501/30000 (78.3367%),                 avg. length: 1713.05,                last time consumption/overall running time: 191.3076s / 272774.7287 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.1747
env0_second_0:                 episode reward: -4.1000,                 loss: 1.4306
env1_first_0:                 episode reward: 5.2000,                 loss: nan
env1_second_0:                 episode reward: -5.2000,                 loss: nan
Episode: 23521/30000 (78.4033%),                 avg. length: 1572.5,                last time consumption/overall running time: 187.5677s / 272962.2965 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2440
env0_second_0:                 episode reward: -2.3500,                 loss: 0.7969
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 23541/30000 (78.4700%),                 avg. length: 1511.25,                last time consumption/overall running time: 170.5202s / 273132.8167 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2621
env0_second_0:                 episode reward: -1.8000,                 loss: 1.0341
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 23561/30000 (78.5367%),                 avg. length: 1504.1,                last time consumption/overall running time: 170.2103s / 273303.0270 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2523
env0_second_0:                 episode reward: -1.8500,                 loss: 0.6775
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 23581/30000 (78.6033%),                 avg. length: 1511.5,                last time consumption/overall running time: 173.0115s / 273476.0385 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2644
env0_second_0:                 episode reward: -2.0000,                 loss: 0.7337
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 23601/30000 (78.6700%),                 avg. length: 1497.0,                last time consumption/overall running time: 164.1932s / 273640.2317 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2259
env0_second_0:                 episode reward: -2.2500,                 loss: 5.8190
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 23621/30000 (78.7367%),                 avg. length: 1467.1,                last time consumption/overall running time: 161.4817s / 273801.7134 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2798
env0_second_0:                 episode reward: -1.5000,                 loss: 0.8643
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 23641/30000 (78.8033%),                 avg. length: 1487.0,                last time consumption/overall running time: 166.9674s / 273968.6809 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1994
env0_second_0:                 episode reward: -1.2500,                 loss: 1.4956
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 23661/30000 (78.8700%),                 avg. length: 1643.2,                last time consumption/overall running time: 181.7498s / 274150.4306 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1659
env0_second_0:                 episode reward: -1.4500,                 loss: 1.2762
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 23681/30000 (78.9367%),                 avg. length: 1788.65,                last time consumption/overall running time: 199.8110s / 274350.2416 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.0492
env0_second_0:                 episode reward: -6.9500,                 loss: 1.5653
env1_first_0:                 episode reward: 6.8500,                 loss: nan
env1_second_0:                 episode reward: -6.8500,                 loss: nan
Episode: 23701/30000 (79.0033%),                 avg. length: 1509.3,                last time consumption/overall running time: 166.3035s / 274516.5451 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2201
env0_second_0:                 episode reward: -2.1500,                 loss: 0.9247
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 23721/30000 (79.0700%),                 avg. length: 1656.25,                last time consumption/overall running time: 186.5728s / 274703.1179 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1823
env0_second_0:                 episode reward: -1.2000,                 loss: 1.1320
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 23741/30000 (79.1367%),                 avg. length: 1705.4,                last time consumption/overall running time: 185.0233s / 274888.1412 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.1798
env0_second_0:                 episode reward: -2.3500,                 loss: 0.9389
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 23761/30000 (79.2033%),                 avg. length: 1566.5,                last time consumption/overall running time: 180.1097s / 275068.2509 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2148
env0_second_0:                 episode reward: -2.6000,                 loss: 0.8359
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 23781/30000 (79.2700%),                 avg. length: 1695.6,                last time consumption/overall running time: 190.0727s / 275258.3236 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.1604
env0_second_0:                 episode reward: -2.8500,                 loss: 1.2096
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 23801/30000 (79.3367%),                 avg. length: 1547.65,                last time consumption/overall running time: 183.4479s / 275441.7716 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2273
env0_second_0:                 episode reward: -2.2500,                 loss: 0.7497
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 23821/30000 (79.4033%),                 avg. length: 1505.65,                last time consumption/overall running time: 171.5504s / 275613.3220 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2085
env0_second_0:                 episode reward: -1.0500,                 loss: 0.9156
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 23841/30000 (79.4700%),                 avg. length: 1564.0,                last time consumption/overall running time: 171.6251s / 275784.9470 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2434
env0_second_0:                 episode reward: -1.0000,                 loss: 0.7336
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 23861/30000 (79.5367%),                 avg. length: 1547.65,                last time consumption/overall running time: 166.7996s / 275951.7466 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2684
env0_second_0:                 episode reward: -1.3000,                 loss: 0.6264
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 23881/30000 (79.6033%),                 avg. length: 1472.4,                last time consumption/overall running time: 157.2544s / 276109.0010 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2562
env0_second_0:                 episode reward: -1.8500,                 loss: 0.5960
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 23901/30000 (79.6700%),                 avg. length: 1529.95,                last time consumption/overall running time: 161.7889s / 276270.7899 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.1743
env0_second_0:                 episode reward: -3.9500,                 loss: 1.0837
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 23921/30000 (79.7367%),                 avg. length: 1517.4,                last time consumption/overall running time: 162.8297s / 276433.6196 s
env0_first_0:                 episode reward: 3.5000,                 loss: -0.1534
env0_second_0:                 episode reward: -3.5000,                 loss: 1.0396
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 23941/30000 (79.8033%),                 avg. length: 1469.7,                last time consumption/overall running time: 158.9028s / 276592.5224 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2075
env0_second_0:                 episode reward: -0.6000,                 loss: 0.9899
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 23961/30000 (79.8700%),                 avg. length: 1537.8,                last time consumption/overall running time: 174.5869s / 276767.1093 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.1807
env0_second_0:                 episode reward: -5.0000,                 loss: 1.2717
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 23981/30000 (79.9367%),                 avg. length: 1472.5,                last time consumption/overall running time: 165.8932s / 276933.0025 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2391
env0_second_0:                 episode reward: -2.0500,                 loss: 1.0164
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 24001/30000 (80.0033%),                 avg. length: 1508.35,                last time consumption/overall running time: 161.2300s / 277094.2325 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2540
env0_second_0:                 episode reward: -1.3500,                 loss: 0.7490
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 24021/30000 (80.0700%),                 avg. length: 1676.2,                last time consumption/overall running time: 193.3375s / 277287.5700 s
env0_first_0:                 episode reward: -3.5000,                 loss: -0.1940
env0_second_0:                 episode reward: 3.5000,                 loss: 1.0274
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 24041/30000 (80.1367%),                 avg. length: 1683.4,                last time consumption/overall running time: 180.0241s / 277467.5941 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2058
env0_second_0:                 episode reward: 0.3500,                 loss: 0.8581
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 24061/30000 (80.2033%),                 avg. length: 1632.9,                last time consumption/overall running time: 175.1369s / 277642.7310 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1368
env0_second_0:                 episode reward: 4.5000,                 loss: 0.9813
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 24081/30000 (80.2700%),                 avg. length: 1602.85,                last time consumption/overall running time: 173.0484s / 277815.7794 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.0810
env0_second_0:                 episode reward: -0.8000,                 loss: 1.1834
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 24101/30000 (80.3367%),                 avg. length: 1912.75,                last time consumption/overall running time: 210.4896s / 278026.2690 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.1419
env0_second_0:                 episode reward: 3.0500,                 loss: 1.3990
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 24121/30000 (80.4033%),                 avg. length: 1809.4,                last time consumption/overall running time: 192.3915s / 278218.6605 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.1469
env0_second_0:                 episode reward: 0.8500,                 loss: 1.2334
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 24141/30000 (80.4700%),                 avg. length: 1881.0,                last time consumption/overall running time: 202.7491s / 278421.4096 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.1597
env0_second_0:                 episode reward: 2.2500,                 loss: 1.0297
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 24161/30000 (80.5367%),                 avg. length: 2230.2,                last time consumption/overall running time: 245.8480s / 278667.2576 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.1430
env0_second_0:                 episode reward: 2.7000,                 loss: 0.8181
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 24181/30000 (80.6033%),                 avg. length: 2049.95,                last time consumption/overall running time: 223.7477s / 278891.0053 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2258
env0_second_0:                 episode reward: -0.9000,                 loss: 0.4529
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 24201/30000 (80.6700%),                 avg. length: 1793.65,                last time consumption/overall running time: 192.5224s / 279083.5277 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2200
env0_second_0:                 episode reward: -1.5000,                 loss: 1.4895
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 24221/30000 (80.7367%),                 avg. length: 1735.0,                last time consumption/overall running time: 190.1139s / 279273.6416 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2193
env0_second_0:                 episode reward: -2.6000,                 loss: 1.0139
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 24241/30000 (80.8033%),                 avg. length: 1641.55,                last time consumption/overall running time: 177.1482s / 279450.7898 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1924
env0_second_0:                 episode reward: -3.8000,                 loss: 0.8160
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 24261/30000 (80.8700%),                 avg. length: 1764.35,                last time consumption/overall running time: 189.4054s / 279640.1951 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1411
env0_second_0:                 episode reward: -1.2000,                 loss: 0.7508
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 24281/30000 (80.9367%),                 avg. length: 1838.95,                last time consumption/overall running time: 194.8301s / 279835.0253 s
env0_first_0:                 episode reward: 3.4500,                 loss: -0.1484
env0_second_0:                 episode reward: -3.4500,                 loss: 0.6766
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 24301/30000 (81.0033%),                 avg. length: 1507.05,                last time consumption/overall running time: 162.3561s / 279997.3813 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2302
env0_second_0:                 episode reward: -1.7500,                 loss: 0.8532
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 24321/30000 (81.0700%),                 avg. length: 1771.4,                last time consumption/overall running time: 200.5569s / 280197.9382 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.1898
env0_second_0:                 episode reward: -2.8000,                 loss: 0.7933
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 24341/30000 (81.1367%),                 avg. length: 1839.1,                last time consumption/overall running time: 198.9208s / 280396.8590 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1206
env0_second_0:                 episode reward: -2.5000,                 loss: 0.9113
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 24361/30000 (81.2033%),                 avg. length: 2003.8,                last time consumption/overall running time: 221.9204s / 280618.7794 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1675
env0_second_0:                 episode reward: 0.8000,                 loss: 0.6030
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 24381/30000 (81.2700%),                 avg. length: 1923.55,                last time consumption/overall running time: 221.4780s / 280840.2574 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2485
env0_second_0:                 episode reward: -0.8500,                 loss: 0.7374
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 24401/30000 (81.3367%),                 avg. length: 2348.1,                last time consumption/overall running time: 269.5642s / 281109.8216 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2494
env0_second_0:                 episode reward: -0.4000,                 loss: 0.5083
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 24421/30000 (81.4033%),                 avg. length: 3128.6,                last time consumption/overall running time: 338.4010s / 281448.2226 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2355
env0_second_0:                 episode reward: -3.5500,                 loss: 0.5459
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 24441/30000 (81.4700%),                 avg. length: 2576.35,                last time consumption/overall running time: 275.0599s / 281723.2826 s
env0_first_0:                 episode reward: 7.3000,                 loss: -0.1797
env0_second_0:                 episode reward: -7.3000,                 loss: 0.7477
env1_first_0:                 episode reward: 6.0500,                 loss: nan
env1_second_0:                 episode reward: -6.0500,                 loss: nan
Episode: 24461/30000 (81.5367%),                 avg. length: 1701.8,                last time consumption/overall running time: 183.2639s / 281906.5464 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.2256
env0_second_0:                 episode reward: -2.5500,                 loss: 0.8504
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 24481/30000 (81.6033%),                 avg. length: 1628.55,                last time consumption/overall running time: 186.2672s / 282092.8136 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2014
env0_second_0:                 episode reward: -0.9500,                 loss: 0.7569
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 24501/30000 (81.6700%),                 avg. length: 1602.2,                last time consumption/overall running time: 173.7443s / 282266.5580 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1989
env0_second_0:                 episode reward: 0.1500,                 loss: 0.7866
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 24521/30000 (81.7367%),                 avg. length: 1695.9,                last time consumption/overall running time: 179.3273s / 282445.8852 s
env0_first_0:                 episode reward: -6.2500,                 loss: -0.1505
env0_second_0:                 episode reward: 6.2500,                 loss: 0.8482
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 24541/30000 (81.8033%),                 avg. length: 1896.8,                last time consumption/overall running time: 201.6021s / 282647.4873 s
env0_first_0:                 episode reward: -8.0500,                 loss: -0.1389
env0_second_0:                 episode reward: 8.0500,                 loss: 1.5590
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 24561/30000 (81.8700%),                 avg. length: 1764.85,                last time consumption/overall running time: 194.0711s / 282841.5584 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.2055
env0_second_0:                 episode reward: 3.3000,                 loss: 0.7116
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 24581/30000 (81.9367%),                 avg. length: 1480.25,                last time consumption/overall running time: 172.7105s / 283014.2689 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2283
env0_second_0:                 episode reward: -1.1000,                 loss: 0.4800
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 24601/30000 (82.0033%),                 avg. length: 1563.45,                last time consumption/overall running time: 182.7883s / 283197.0572 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2336
env0_second_0:                 episode reward: -0.9000,                 loss: 0.5395
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 24621/30000 (82.0700%),                 avg. length: 1554.75,                last time consumption/overall running time: 165.0813s / 283362.1385 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2215
env0_second_0:                 episode reward: -2.1000,                 loss: 0.6949
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 24641/30000 (82.1367%),                 avg. length: 1535.55,                last time consumption/overall running time: 174.7270s / 283536.8655 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2210
env0_second_0:                 episode reward: -0.5500,                 loss: 0.6089
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 24661/30000 (82.2033%),                 avg. length: 1481.65,                last time consumption/overall running time: 159.6811s / 283696.5467 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2582
env0_second_0:                 episode reward: -0.9500,                 loss: 0.4854
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 24681/30000 (82.2700%),                 avg. length: 1507.2,                last time consumption/overall running time: 164.1153s / 283860.6620 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2324
env0_second_0:                 episode reward: -2.1500,                 loss: 0.5798
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 24701/30000 (82.3367%),                 avg. length: 1565.5,                last time consumption/overall running time: 183.0439s / 284043.7059 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2627
env0_second_0:                 episode reward: -1.7500,                 loss: 0.4824
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 24721/30000 (82.4033%),                 avg. length: 1559.5,                last time consumption/overall running time: 168.0412s / 284211.7470 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2579
env0_second_0:                 episode reward: -3.3000,                 loss: 0.7895
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 24741/30000 (82.4700%),                 avg. length: 1837.55,                last time consumption/overall running time: 211.0482s / 284422.7952 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1825
env0_second_0:                 episode reward: -2.9000,                 loss: 1.4026
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 24761/30000 (82.5367%),                 avg. length: 1843.55,                last time consumption/overall running time: 202.1291s / 284624.9244 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1197
env0_second_0:                 episode reward: -0.6000,                 loss: 1.3275
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 24781/30000 (82.6033%),                 avg. length: 2006.85,                last time consumption/overall running time: 211.4364s / 284836.3608 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.1150
env0_second_0:                 episode reward: 3.3500,                 loss: 0.9612
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 24801/30000 (82.6700%),                 avg. length: 1878.8,                last time consumption/overall running time: 214.1318s / 285050.4926 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.1460
env0_second_0:                 episode reward: 3.8000,                 loss: 0.6225
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 24821/30000 (82.7367%),                 avg. length: 1841.2,                last time consumption/overall running time: 197.6220s / 285248.1146 s
env0_first_0:                 episode reward: -5.0000,                 loss: -0.1398
env0_second_0:                 episode reward: 5.0000,                 loss: 0.9308
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 24841/30000 (82.8033%),                 avg. length: 1851.65,                last time consumption/overall running time: 195.8837s / 285443.9983 s
env0_first_0:                 episode reward: -2.9500,                 loss: -0.1405
env0_second_0:                 episode reward: 2.9500,                 loss: 1.1469
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 24861/30000 (82.8700%),                 avg. length: 1909.25,                last time consumption/overall running time: 200.5486s / 285644.5469 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1116
env0_second_0:                 episode reward: 0.3000,                 loss: 0.8350
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 24881/30000 (82.9367%),                 avg. length: 1736.4,                last time consumption/overall running time: 187.8268s / 285832.3737 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.1466
env0_second_0:                 episode reward: 3.3000,                 loss: 1.1517
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 24901/30000 (83.0033%),                 avg. length: 2835.65,                last time consumption/overall running time: 315.4897s / 286147.8633 s
env0_first_0:                 episode reward: 39.2500,                 loss: 0.0890
env0_second_0:                 episode reward: -39.2500,                 loss: 1.2064
env1_first_0:                 episode reward: 37.5000,                 loss: nan
env1_second_0:                 episode reward: -37.5000,                 loss: nan
Episode: 24921/30000 (83.0700%),                 avg. length: 1827.45,                last time consumption/overall running time: 193.6441s / 286341.5075 s
env0_first_0:                 episode reward: 8.3000,                 loss: -0.1424
env0_second_0:                 episode reward: -8.3000,                 loss: 0.9672
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 24941/30000 (83.1367%),                 avg. length: 1802.9,                last time consumption/overall running time: 191.3037s / 286532.8112 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1963
env0_second_0:                 episode reward: -0.2000,                 loss: 0.6546
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 24961/30000 (83.2033%),                 avg. length: 1821.35,                last time consumption/overall running time: 200.6477s / 286733.4589 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1629
env0_second_0:                 episode reward: 0.4500,                 loss: 4.4652
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 24981/30000 (83.2700%),                 avg. length: 1974.05,                last time consumption/overall running time: 223.3366s / 286956.7954 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1735
env0_second_0:                 episode reward: 0.2500,                 loss: 0.8046
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 25001/30000 (83.3367%),                 avg. length: 1574.0,                last time consumption/overall running time: 172.0248s / 287128.8203 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2530
env0_second_0:                 episode reward: -0.3000,                 loss: 0.6599
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 25021/30000 (83.4033%),                 avg. length: 1736.15,                last time consumption/overall running time: 202.0825s / 287330.9027 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1652
env0_second_0:                 episode reward: 0.4500,                 loss: 0.7428
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 25041/30000 (83.4700%),                 avg. length: 1581.9,                last time consumption/overall running time: 175.5356s / 287506.4383 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2362
env0_second_0:                 episode reward: -0.8500,                 loss: 0.6776
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 25061/30000 (83.5367%),                 avg. length: 1647.2,                last time consumption/overall running time: 174.1698s / 287680.6081 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2298
env0_second_0:                 episode reward: -2.3500,                 loss: 0.6079
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 25081/30000 (83.6033%),                 avg. length: 1695.75,                last time consumption/overall running time: 182.1831s / 287862.7912 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2335
env0_second_0:                 episode reward: -2.6000,                 loss: 0.5824
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 25101/30000 (83.6700%),                 avg. length: 1771.45,                last time consumption/overall running time: 194.6273s / 288057.4185 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.2305
env0_second_0:                 episode reward: -2.5000,                 loss: 0.8174
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 25121/30000 (83.7367%),                 avg. length: 1879.95,                last time consumption/overall running time: 203.7470s / 288261.1655 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2392
env0_second_0:                 episode reward: -1.5500,                 loss: 0.7503
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 25141/30000 (83.8033%),                 avg. length: 2031.8,                last time consumption/overall running time: 241.6451s / 288502.8106 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2162
env0_second_0:                 episode reward: -2.4500,                 loss: 0.5842
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 25161/30000 (83.8700%),                 avg. length: 1751.1,                last time consumption/overall running time: 202.5199s / 288705.3305 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2176
env0_second_0:                 episode reward: -1.5000,                 loss: 0.6771
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 25181/30000 (83.9367%),                 avg. length: 1689.75,                last time consumption/overall running time: 179.7968s / 288885.1273 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.2324
env0_second_0:                 episode reward: -3.1500,                 loss: 1.2095
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 25201/30000 (84.0033%),                 avg. length: 1667.95,                last time consumption/overall running time: 179.6374s / 289064.7646 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2573
env0_second_0:                 episode reward: -2.1000,                 loss: 0.6404
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 25221/30000 (84.0700%),                 avg. length: 1838.8,                last time consumption/overall running time: 197.2013s / 289261.9659 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1828
env0_second_0:                 episode reward: -1.8000,                 loss: 1.5003
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 25241/30000 (84.1367%),                 avg. length: 2215.4,                last time consumption/overall running time: 233.0727s / 289495.0386 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1880
env0_second_0:                 episode reward: -0.7000,                 loss: 1.6080
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 25261/30000 (84.2033%),                 avg. length: 2279.65,                last time consumption/overall running time: 237.8350s / 289732.8736 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2246
env0_second_0:                 episode reward: -1.0000,                 loss: 1.1286
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 25281/30000 (84.2700%),                 avg. length: 1953.95,                last time consumption/overall running time: 220.6072s / 289953.4809 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.1987
env0_second_0:                 episode reward: -3.3000,                 loss: 1.3064
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 25301/30000 (84.3367%),                 avg. length: 1525.7,                last time consumption/overall running time: 175.6690s / 290129.1498 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.2290
env0_second_0:                 episode reward: -1.4000,                 loss: 0.9746
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 25321/30000 (84.4033%),                 avg. length: 1481.5,                last time consumption/overall running time: 163.2379s / 290292.3878 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1080
env0_second_0:                 episode reward: 0.5500,                 loss: 0.9990
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 25341/30000 (84.4700%),                 avg. length: 1493.75,                last time consumption/overall running time: 163.1494s / 290455.5372 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.1518
env0_second_0:                 episode reward: 1.8500,                 loss: 1.2221
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 25361/30000 (84.5367%),                 avg. length: 1460.0,                last time consumption/overall running time: 165.6059s / 290621.1431 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.2586
env0_second_0:                 episode reward: -2.0500,                 loss: 1.1866
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 25381/30000 (84.6033%),                 avg. length: 1549.55,                last time consumption/overall running time: 167.0378s / 290788.1809 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2772
env0_second_0:                 episode reward: -1.6000,                 loss: 0.6325
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 25401/30000 (84.6700%),                 avg. length: 1513.85,                last time consumption/overall running time: 163.2462s / 290951.4271 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2736
env0_second_0:                 episode reward: -1.8500,                 loss: 0.8899
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 25421/30000 (84.7367%),                 avg. length: 1490.25,                last time consumption/overall running time: 159.7312s / 291111.1583 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2283
env0_second_0:                 episode reward: -2.8500,                 loss: 1.1921
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 25441/30000 (84.8033%),                 avg. length: 1521.7,                last time consumption/overall running time: 163.5459s / 291274.7042 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.1671
env0_second_0:                 episode reward: -1.2000,                 loss: 1.8897
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 25461/30000 (84.8700%),                 avg. length: 1466.2,                last time consumption/overall running time: 165.4145s / 291440.1187 s
env0_first_0:                 episode reward: -3.2500,                 loss: -0.1066
env0_second_0:                 episode reward: 3.2500,                 loss: 1.0197
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 25481/30000 (84.9367%),                 avg. length: 1476.95,                last time consumption/overall running time: 163.3660s / 291603.4847 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2294
env0_second_0:                 episode reward: -1.5000,                 loss: 0.7358
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 25501/30000 (85.0033%),                 avg. length: 1519.65,                last time consumption/overall running time: 163.3639s / 291766.8485 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2448
env0_second_0:                 episode reward: -1.6500,                 loss: 1.0220
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 25521/30000 (85.0700%),                 avg. length: 1484.45,                last time consumption/overall running time: 164.2726s / 291931.1212 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2523
env0_second_0:                 episode reward: -1.9000,                 loss: 1.0786
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 25541/30000 (85.1367%),                 avg. length: 1743.05,                last time consumption/overall running time: 188.3726s / 292119.4938 s
env0_first_0:                 episode reward: -3.0500,                 loss: -0.1305
env0_second_0:                 episode reward: 3.0500,                 loss: 1.3345
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 25561/30000 (85.2033%),                 avg. length: 1495.3,                last time consumption/overall running time: 162.6629s / 292282.1567 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2218
env0_second_0:                 episode reward: -2.2500,                 loss: 0.7759
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 25581/30000 (85.2700%),                 avg. length: 1848.05,                last time consumption/overall running time: 201.8866s / 292484.0433 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1975
env0_second_0:                 episode reward: 2.1500,                 loss: 1.1390
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 25601/30000 (85.3367%),                 avg. length: 3596.3,                last time consumption/overall running time: 399.6168s / 292883.6601 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1181
env0_second_0:                 episode reward: -1.6000,                 loss: 1.2681
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 25621/30000 (85.4033%),                 avg. length: 2997.4,                last time consumption/overall running time: 318.6346s / 293202.2947 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.1779
env0_second_0:                 episode reward: 1.5500,                 loss: 1.1196
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 25641/30000 (85.4700%),                 avg. length: 2011.35,                last time consumption/overall running time: 215.5078s / 293417.8025 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1686
env0_second_0:                 episode reward: -0.7500,                 loss: 0.9212
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 25661/30000 (85.5367%),                 avg. length: 1621.05,                last time consumption/overall running time: 176.3385s / 293594.1411 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1871
env0_second_0:                 episode reward: -1.5000,                 loss: 0.8926
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 25681/30000 (85.6033%),                 avg. length: 1706.05,                last time consumption/overall running time: 185.0987s / 293779.2398 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2274
env0_second_0:                 episode reward: -0.1500,                 loss: 0.9177
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 25701/30000 (85.6700%),                 avg. length: 1906.95,                last time consumption/overall running time: 204.1757s / 293983.4155 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2120
env0_second_0:                 episode reward: -1.8500,                 loss: 0.8459
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 25721/30000 (85.7367%),                 avg. length: 1507.25,                last time consumption/overall running time: 163.0701s / 294146.4855 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2397
env0_second_0:                 episode reward: -1.6000,                 loss: 0.5640
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 25741/30000 (85.8033%),                 avg. length: 1587.4,                last time consumption/overall running time: 171.5881s / 294318.0736 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2098
env0_second_0:                 episode reward: -1.8500,                 loss: 1.0474
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 25761/30000 (85.8700%),                 avg. length: 1570.55,                last time consumption/overall running time: 170.5477s / 294488.6213 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1923
env0_second_0:                 episode reward: 0.4500,                 loss: 1.0995
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 25781/30000 (85.9367%),                 avg. length: 1528.8,                last time consumption/overall running time: 164.7839s / 294653.4052 s
env0_first_0:                 episode reward: 4.3000,                 loss: -0.2150
env0_second_0:                 episode reward: -4.3000,                 loss: 1.1598
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 25801/30000 (86.0033%),                 avg. length: 1797.25,                last time consumption/overall running time: 191.7315s / 294845.1367 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.1647
env0_second_0:                 episode reward: -2.9000,                 loss: 0.9964
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 25821/30000 (86.0700%),                 avg. length: 2061.2,                last time consumption/overall running time: 219.0415s / 295064.1782 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.1147
env0_second_0:                 episode reward: -4.7000,                 loss: 1.2113
env1_first_0:                 episode reward: 5.9500,                 loss: nan
env1_second_0:                 episode reward: -5.9500,                 loss: nan
Episode: 25841/30000 (86.1367%),                 avg. length: 2920.15,                last time consumption/overall running time: 310.3772s / 295374.5554 s
env0_first_0:                 episode reward: 7.2500,                 loss: -0.1558
env0_second_0:                 episode reward: -7.2500,                 loss: 1.0011
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 25861/30000 (86.2033%),                 avg. length: 2399.25,                last time consumption/overall running time: 298.3141s / 295672.8694 s
env0_first_0:                 episode reward: 6.6500,                 loss: -0.1552
env0_second_0:                 episode reward: -6.6500,                 loss: 1.0149
env1_first_0:                 episode reward: 5.3500,                 loss: nan
env1_second_0:                 episode reward: -5.3500,                 loss: nan
Episode: 25881/30000 (86.2700%),                 avg. length: 2712.5,                last time consumption/overall running time: 335.6414s / 296008.5108 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.1565
env0_second_0:                 episode reward: -3.3500,                 loss: 0.9117
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 25901/30000 (86.3367%),                 avg. length: 2167.75,                last time consumption/overall running time: 233.0967s / 296241.6076 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1887
env0_second_0:                 episode reward: -3.0000,                 loss: 0.9647
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 25921/30000 (86.4033%),                 avg. length: 2037.15,                last time consumption/overall running time: 227.9364s / 296469.5440 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.1552
env0_second_0:                 episode reward: -3.8000,                 loss: 0.9233
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 25941/30000 (86.4700%),                 avg. length: 2156.35,                last time consumption/overall running time: 250.5116s / 296720.0556 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1452
env0_second_0:                 episode reward: -4.1500,                 loss: 1.1037
env1_first_0:                 episode reward: 5.5000,                 loss: nan
env1_second_0:                 episode reward: -5.5000,                 loss: nan
Episode: 25961/30000 (86.5367%),                 avg. length: 2243.75,                last time consumption/overall running time: 243.2528s / 296963.3084 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1277
env0_second_0:                 episode reward: -4.5500,                 loss: 1.2351
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 25981/30000 (86.6033%),                 avg. length: 2361.2,                last time consumption/overall running time: 270.0039s / 297233.3123 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1409
env0_second_0:                 episode reward: -3.9000,                 loss: 1.3006
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 26001/30000 (86.6700%),                 avg. length: 2386.4,                last time consumption/overall running time: 249.5573s / 297482.8695 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1180
env0_second_0:                 episode reward: -3.2500,                 loss: 1.1596
env1_first_0:                 episode reward: 3.6500,                 loss: nan
env1_second_0:                 episode reward: -3.6500,                 loss: nan
Episode: 26021/30000 (86.7367%),                 avg. length: 2417.95,                last time consumption/overall running time: 253.5648s / 297736.4344 s
env0_first_0:                 episode reward: 3.4000,                 loss: -0.1452
env0_second_0:                 episode reward: -3.4000,                 loss: 0.8788
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 26041/30000 (86.8033%),                 avg. length: 1981.7,                last time consumption/overall running time: 212.2661s / 297948.7005 s
env0_first_0:                 episode reward: 3.7000,                 loss: -0.1730
env0_second_0:                 episode reward: -3.7000,                 loss: 0.9237
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 26061/30000 (86.8700%),                 avg. length: 1736.1,                last time consumption/overall running time: 190.1485s / 298138.8489 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2317
env0_second_0:                 episode reward: -2.1500,                 loss: 0.6921
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 26081/30000 (86.9367%),                 avg. length: 1880.1,                last time consumption/overall running time: 209.5297s / 298348.3787 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1140
env0_second_0:                 episode reward: -3.9000,                 loss: 1.0788
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 26101/30000 (87.0033%),                 avg. length: 1796.1,                last time consumption/overall running time: 205.7351s / 298554.1138 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.1575
env0_second_0:                 episode reward: -3.2500,                 loss: 1.1259
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 26121/30000 (87.0700%),                 avg. length: 2029.95,                last time consumption/overall running time: 215.0476s / 298769.1614 s
env0_first_0:                 episode reward: 5.5000,                 loss: -0.1502
env0_second_0:                 episode reward: -5.5000,                 loss: 1.8890
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 26141/30000 (87.1367%),                 avg. length: 1505.0,                last time consumption/overall running time: 162.0115s / 298931.1729 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2170
env0_second_0:                 episode reward: -2.1500,                 loss: 0.6707
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 26161/30000 (87.2033%),                 avg. length: 1624.3,                last time consumption/overall running time: 170.8851s / 299102.0579 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2315
env0_second_0:                 episode reward: -2.0000,                 loss: 0.3708
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 26181/30000 (87.2700%),                 avg. length: 1587.8,                last time consumption/overall running time: 166.5571s / 299268.6151 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2362
env0_second_0:                 episode reward: -1.6000,                 loss: 0.5398
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 26201/30000 (87.3367%),                 avg. length: 1766.0,                last time consumption/overall running time: 186.3220s / 299454.9371 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.2315
env0_second_0:                 episode reward: -1.2000,                 loss: 0.7023
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 26221/30000 (87.4033%),                 avg. length: 2196.5,                last time consumption/overall running time: 229.1206s / 299684.0576 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2414
env0_second_0:                 episode reward: -1.1000,                 loss: 1.7013
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 26241/30000 (87.4700%),                 avg. length: 2805.1,                last time consumption/overall running time: 317.5578s / 300001.6155 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.2243
env0_second_0:                 episode reward: 2.4000,                 loss: 0.9640
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 26261/30000 (87.5367%),                 avg. length: 2430.45,                last time consumption/overall running time: 254.4951s / 300256.1106 s
env0_first_0:                 episode reward: -12.0500,                 loss: -0.0548
env0_second_0:                 episode reward: 12.0500,                 loss: 1.0461
env1_first_0:                 episode reward: -10.2000,                 loss: nan
env1_second_0:                 episode reward: 10.2000,                 loss: nan
Episode: 26281/30000 (87.6033%),                 avg. length: 1581.5,                last time consumption/overall running time: 167.8034s / 300423.9140 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.2432
env0_second_0:                 episode reward: -1.6500,                 loss: 0.5318
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 26301/30000 (87.6700%),                 avg. length: 2029.95,                last time consumption/overall running time: 213.2802s / 300637.1942 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.1814
env0_second_0:                 episode reward: 2.1500,                 loss: 1.0316
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 26321/30000 (87.7367%),                 avg. length: 2389.7,                last time consumption/overall running time: 250.5257s / 300887.7200 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1934
env0_second_0:                 episode reward: 0.6500,                 loss: 1.0042
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 26341/30000 (87.8033%),                 avg. length: 2553.5,                last time consumption/overall running time: 266.7984s / 301154.5183 s
env0_first_0:                 episode reward: -1.1500,                 loss: -0.1711
env0_second_0:                 episode reward: 1.1500,                 loss: 0.8424
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 26361/30000 (87.8700%),                 avg. length: 2826.6,                last time consumption/overall running time: 308.3469s / 301462.8653 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1482
env0_second_0:                 episode reward: 0.2000,                 loss: 0.8596
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 26381/30000 (87.9367%),                 avg. length: 2085.3,                last time consumption/overall running time: 218.1952s / 301681.0604 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.1376
env0_second_0:                 episode reward: -3.5500,                 loss: 0.9718
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 26401/30000 (88.0033%),                 avg. length: 2279.15,                last time consumption/overall running time: 236.8907s / 301917.9511 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1755
env0_second_0:                 episode reward: -1.5000,                 loss: 0.8850
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 26421/30000 (88.0700%),                 avg. length: 2515.7,                last time consumption/overall running time: 261.4296s / 302179.3807 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1554
env0_second_0:                 episode reward: -5.1500,                 loss: 1.0680
env1_first_0:                 episode reward: 5.2500,                 loss: nan
env1_second_0:                 episode reward: -5.2500,                 loss: nan
Episode: 26441/30000 (88.1367%),                 avg. length: 2127.05,                last time consumption/overall running time: 225.5405s / 302404.9211 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.0770
env0_second_0:                 episode reward: -1.7000,                 loss: 1.2688
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 26461/30000 (88.2033%),                 avg. length: 1986.15,                last time consumption/overall running time: 227.2680s / 302632.1891 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.0576
env0_second_0:                 episode reward: 4.5000,                 loss: 1.2011
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 26481/30000 (88.2700%),                 avg. length: 1701.25,                last time consumption/overall running time: 183.7939s / 302815.9830 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1424
env0_second_0:                 episode reward: -1.0500,                 loss: 0.8518
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 26501/30000 (88.3367%),                 avg. length: 1494.4,                last time consumption/overall running time: 162.6731s / 302978.6561 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2472
env0_second_0:                 episode reward: -1.7500,                 loss: 3.2554
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 26521/30000 (88.4033%),                 avg. length: 1684.8,                last time consumption/overall running time: 187.4283s / 303166.0844 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.2264
env0_second_0:                 episode reward: -2.4000,                 loss: 1.0038
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 26541/30000 (88.4700%),                 avg. length: 1508.0,                last time consumption/overall running time: 188.8197s / 303354.9041 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.2013
env0_second_0:                 episode reward: -3.0000,                 loss: 1.0491
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 26561/30000 (88.5367%),                 avg. length: 1487.4,                last time consumption/overall running time: 173.2358s / 303528.1399 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2546
env0_second_0:                 episode reward: -3.3000,                 loss: 0.6610
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 26581/30000 (88.6033%),                 avg. length: 1592.1,                last time consumption/overall running time: 169.0595s / 303697.1994 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.1804
env0_second_0:                 episode reward: -4.2000,                 loss: 1.0777
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 26601/30000 (88.6700%),                 avg. length: 1542.45,                last time consumption/overall running time: 188.6872s / 303885.8866 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1887
env0_second_0:                 episode reward: -0.3500,                 loss: 0.9310
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 26621/30000 (88.7367%),                 avg. length: 1462.1,                last time consumption/overall running time: 162.1041s / 304047.9906 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2579
env0_second_0:                 episode reward: -1.2500,                 loss: 0.8953
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 26641/30000 (88.8033%),                 avg. length: 1458.4,                last time consumption/overall running time: 155.9772s / 304203.9678 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.2031
env0_second_0:                 episode reward: -3.2500,                 loss: 0.9682
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 26661/30000 (88.8700%),                 avg. length: 1525.4,                last time consumption/overall running time: 171.7697s / 304375.7375 s
env0_first_0:                 episode reward: 7.5000,                 loss: -0.1442
env0_second_0:                 episode reward: -7.5000,                 loss: 2.9415
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 26681/30000 (88.9367%),                 avg. length: 1486.95,                last time consumption/overall running time: 194.3659s / 304570.1034 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1983
env0_second_0:                 episode reward: -3.9000,                 loss: 1.6744
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 26701/30000 (89.0033%),                 avg. length: 1477.55,                last time consumption/overall running time: 158.9285s / 304729.0319 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.2231
env0_second_0:                 episode reward: -1.9000,                 loss: 1.0229
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 26721/30000 (89.0700%),                 avg. length: 1614.75,                last time consumption/overall running time: 188.3863s / 304917.4182 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2433
env0_second_0:                 episode reward: -3.3500,                 loss: 0.8558
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 26741/30000 (89.1367%),                 avg. length: 1580.4,                last time consumption/overall running time: 182.0558s / 305099.4740 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2990
env0_second_0:                 episode reward: -1.8500,                 loss: 0.5854
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 26761/30000 (89.2033%),                 avg. length: 1676.1,                last time consumption/overall running time: 177.5085s / 305276.9825 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2698
env0_second_0:                 episode reward: -1.1000,                 loss: 0.6617
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 26781/30000 (89.2700%),                 avg. length: 1531.5,                last time consumption/overall running time: 168.1503s / 305445.1328 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2713
env0_second_0:                 episode reward: -1.3500,                 loss: 1.3595
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 26801/30000 (89.3367%),                 avg. length: 1471.1,                last time consumption/overall running time: 159.3156s / 305604.4485 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.2953
env0_second_0:                 episode reward: -2.4500,                 loss: 0.7109
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 26821/30000 (89.4033%),                 avg. length: 1489.9,                last time consumption/overall running time: 162.7782s / 305767.2266 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2734
env0_second_0:                 episode reward: -1.8500,                 loss: 0.6009
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 26841/30000 (89.4700%),                 avg. length: 1531.15,                last time consumption/overall running time: 163.0726s / 305930.2992 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.2248
env0_second_0:                 episode reward: -2.3500,                 loss: 0.5358
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 26861/30000 (89.5367%),                 avg. length: 1667.3,                last time consumption/overall running time: 176.2203s / 306106.5195 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.2287
env0_second_0:                 episode reward: -2.6000,                 loss: 0.8020
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 26881/30000 (89.6033%),                 avg. length: 1734.2,                last time consumption/overall running time: 197.3305s / 306303.8500 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1921
env0_second_0:                 episode reward: 0.1000,                 loss: 0.6928
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 26901/30000 (89.6700%),                 avg. length: 2002.95,                last time consumption/overall running time: 227.5450s / 306531.3950 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.1983
env0_second_0:                 episode reward: -1.9500,                 loss: 1.6927
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 26921/30000 (89.7367%),                 avg. length: 1908.25,                last time consumption/overall running time: 212.8069s / 306744.2019 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2018
env0_second_0:                 episode reward: -1.0500,                 loss: 0.7234
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 26941/30000 (89.8033%),                 avg. length: 1673.6,                last time consumption/overall running time: 181.5882s / 306925.7901 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2248
env0_second_0:                 episode reward: -1.5000,                 loss: 0.7397
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 26961/30000 (89.8700%),                 avg. length: 1563.25,                last time consumption/overall running time: 199.5012s / 307125.2912 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2405
env0_second_0:                 episode reward: -0.5000,                 loss: 0.4351
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 26981/30000 (89.9367%),                 avg. length: 1652.1,                last time consumption/overall running time: 199.2697s / 307324.5609 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2394
env0_second_0:                 episode reward: -1.2500,                 loss: 0.3158
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 27001/30000 (90.0033%),                 avg. length: 1753.05,                last time consumption/overall running time: 184.2226s / 307508.7835 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2286
env0_second_0:                 episode reward: -0.7000,                 loss: 0.5166
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 27021/30000 (90.0700%),                 avg. length: 1716.9,                last time consumption/overall running time: 189.2983s / 307698.0818 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1704
env0_second_0:                 episode reward: 0.8000,                 loss: 0.7591
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 27041/30000 (90.1367%),                 avg. length: 1867.75,                last time consumption/overall running time: 209.5799s / 307907.6617 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1820
env0_second_0:                 episode reward: 0.9000,                 loss: 1.1309
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 27061/30000 (90.2033%),                 avg. length: 1496.15,                last time consumption/overall running time: 166.8848s / 308074.5465 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2391
env0_second_0:                 episode reward: -1.0500,                 loss: 0.5495
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 27081/30000 (90.2700%),                 avg. length: 1694.4,                last time consumption/overall running time: 179.2699s / 308253.8164 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2101
env0_second_0:                 episode reward: -0.3500,                 loss: 0.5363
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 27101/30000 (90.3367%),                 avg. length: 2102.4,                last time consumption/overall running time: 220.4161s / 308474.2325 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.1250
env0_second_0:                 episode reward: 2.4000,                 loss: 1.8052
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 27121/30000 (90.4033%),                 avg. length: 2392.5,                last time consumption/overall running time: 248.9701s / 308723.2027 s
env0_first_0:                 episode reward: -11.4500,                 loss: -0.0179
env0_second_0:                 episode reward: 11.4500,                 loss: 1.1276
env1_first_0:                 episode reward: -14.4000,                 loss: nan
env1_second_0:                 episode reward: 14.4000,                 loss: nan
Episode: 27141/30000 (90.4700%),                 avg. length: 2460.95,                last time consumption/overall running time: 278.1003s / 309001.3029 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.0440
env0_second_0:                 episode reward: 8.2000,                 loss: 1.1920
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 27161/30000 (90.5367%),                 avg. length: 2642.1,                last time consumption/overall running time: 299.4323s / 309300.7352 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.0865
env0_second_0:                 episode reward: 1.8500,                 loss: 0.7999
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 27181/30000 (90.6033%),                 avg. length: 1483.3,                last time consumption/overall running time: 175.3739s / 309476.1091 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2029
env0_second_0:                 episode reward: -1.5500,                 loss: 0.4756
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 27201/30000 (90.6700%),                 avg. length: 1764.85,                last time consumption/overall running time: 197.8601s / 309673.9691 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1174
env0_second_0:                 episode reward: -0.0500,                 loss: 0.6531
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 27221/30000 (90.7367%),                 avg. length: 2055.25,                last time consumption/overall running time: 220.5225s / 309894.4916 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.0835
env0_second_0:                 episode reward: -4.9000,                 loss: 0.8533
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 27241/30000 (90.8033%),                 avg. length: 2052.7,                last time consumption/overall running time: 216.1910s / 310110.6826 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.0956
env0_second_0:                 episode reward: -5.1500,                 loss: 1.0491
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 27261/30000 (90.8700%),                 avg. length: 2048.35,                last time consumption/overall running time: 221.0064s / 310331.6890 s
env0_first_0:                 episode reward: 8.0500,                 loss: -0.1514
env0_second_0:                 episode reward: -8.0500,                 loss: 1.0117
env1_first_0:                 episode reward: 7.8500,                 loss: nan
env1_second_0:                 episode reward: -7.8500,                 loss: nan
Episode: 27281/30000 (90.9367%),                 avg. length: 1662.4,                last time consumption/overall running time: 178.1525s / 310509.8416 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.2467
env0_second_0:                 episode reward: -1.7000,                 loss: 0.5578
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 27301/30000 (91.0033%),                 avg. length: 1858.6,                last time consumption/overall running time: 215.9661s / 310725.8077 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2174
env0_second_0:                 episode reward: -2.0000,                 loss: 0.7006
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 27321/30000 (91.0700%),                 avg. length: 1702.75,                last time consumption/overall running time: 198.3803s / 310924.1880 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.1527
env0_second_0:                 episode reward: -2.0500,                 loss: 0.7630
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 27341/30000 (91.1367%),                 avg. length: 2182.75,                last time consumption/overall running time: 240.3422s / 311164.5301 s
env0_first_0:                 episode reward: -5.9000,                 loss: -0.1056
env0_second_0:                 episode reward: 5.9000,                 loss: 0.8231
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 27361/30000 (91.2033%),                 avg. length: 1816.4,                last time consumption/overall running time: 218.1470s / 311382.6771 s
env0_first_0:                 episode reward: -6.5000,                 loss: -0.1528
env0_second_0:                 episode reward: 6.5000,                 loss: 0.8801
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 27381/30000 (91.2700%),                 avg. length: 1736.6,                last time consumption/overall running time: 186.7272s / 311569.4043 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.1734
env0_second_0:                 episode reward: 1.2500,                 loss: 0.9665
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 27401/30000 (91.3367%),                 avg. length: 1711.4,                last time consumption/overall running time: 189.7197s / 311759.1241 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1386
env0_second_0:                 episode reward: -1.7500,                 loss: 0.6832
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 27421/30000 (91.4033%),                 avg. length: 1562.7,                last time consumption/overall running time: 169.1308s / 311928.2549 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1930
env0_second_0:                 episode reward: 0.2000,                 loss: 0.7275
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 27441/30000 (91.4700%),                 avg. length: 1503.6,                last time consumption/overall running time: 162.4261s / 312090.6810 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.2492
env0_second_0:                 episode reward: -1.6000,                 loss: 0.4640
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 27461/30000 (91.5367%),                 avg. length: 1616.1,                last time consumption/overall running time: 196.6495s / 312287.3305 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1785
env0_second_0:                 episode reward: -1.5500,                 loss: 0.4855
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 27481/30000 (91.6033%),                 avg. length: 1480.45,                last time consumption/overall running time: 168.2105s / 312455.5410 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1969
env0_second_0:                 episode reward: 0.5500,                 loss: 0.4989
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 27501/30000 (91.6700%),                 avg. length: 1588.35,                last time consumption/overall running time: 178.6926s / 312634.2336 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.1602
env0_second_0:                 episode reward: -1.2500,                 loss: 0.7213
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 27521/30000 (91.7367%),                 avg. length: 1755.25,                last time consumption/overall running time: 194.4738s / 312828.7074 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.1073
env0_second_0:                 episode reward: 4.5000,                 loss: 1.3570
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 27541/30000 (91.8033%),                 avg. length: 1600.45,                last time consumption/overall running time: 174.0354s / 313002.7428 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1917
env0_second_0:                 episode reward: -1.0000,                 loss: 0.9857
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 27561/30000 (91.8700%),                 avg. length: 1759.45,                last time consumption/overall running time: 192.0615s / 313194.8043 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.1934
env0_second_0:                 episode reward: -2.0000,                 loss: 2.1108
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 27581/30000 (91.9367%),                 avg. length: 2075.45,                last time consumption/overall running time: 220.9872s / 313415.7915 s
env0_first_0:                 episode reward: 4.5500,                 loss: -0.1064
env0_second_0:                 episode reward: -4.5500,                 loss: 1.6039
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 27601/30000 (92.0033%),                 avg. length: 1933.95,                last time consumption/overall running time: 209.9894s / 313625.7809 s
env0_first_0:                 episode reward: 5.6500,                 loss: -0.1245
env0_second_0:                 episode reward: -5.6500,                 loss: 1.2008
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 27621/30000 (92.0700%),                 avg. length: 2244.6,                last time consumption/overall running time: 241.2893s / 313867.0702 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.1362
env0_second_0:                 episode reward: -4.0500,                 loss: 1.5927
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 27641/30000 (92.1367%),                 avg. length: 2301.35,                last time consumption/overall running time: 252.1015s / 314119.1717 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1452
env0_second_0:                 episode reward: 0.4000,                 loss: 1.0474
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 27661/30000 (92.2033%),                 avg. length: 2097.05,                last time consumption/overall running time: 235.0471s / 314354.2188 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.0747
env0_second_0:                 episode reward: 8.6000,                 loss: 1.2399
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 27681/30000 (92.2700%),                 avg. length: 2207.75,                last time consumption/overall running time: 239.5119s / 314593.7308 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.0151
env0_second_0:                 episode reward: 1.6500,                 loss: 0.9645
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 27701/30000 (92.3367%),                 avg. length: 1644.3,                last time consumption/overall running time: 181.8374s / 314775.5681 s
env0_first_0:                 episode reward: -3.3500,                 loss: -0.1076
env0_second_0:                 episode reward: 3.3500,                 loss: 0.9035
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 27721/30000 (92.4033%),                 avg. length: 1593.0,                last time consumption/overall running time: 182.4954s / 314958.0635 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1437
env0_second_0:                 episode reward: 0.4500,                 loss: 0.9106
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 27741/30000 (92.4700%),                 avg. length: 1474.7,                last time consumption/overall running time: 161.1394s / 315119.2029 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1844
env0_second_0:                 episode reward: -1.8000,                 loss: 0.6843
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 27761/30000 (92.5367%),                 avg. length: 1466.7,                last time consumption/overall running time: 158.4080s / 315277.6108 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1696
env0_second_0:                 episode reward: -1.9000,                 loss: 0.9249
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 27781/30000 (92.6033%),                 avg. length: 1462.6,                last time consumption/overall running time: 160.7765s / 315438.3873 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2056
env0_second_0:                 episode reward: -1.1500,                 loss: 1.3050
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 27801/30000 (92.6700%),                 avg. length: 1460.75,                last time consumption/overall running time: 156.3741s / 315594.7614 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2047
env0_second_0:                 episode reward: -0.7500,                 loss: 0.8122
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 27821/30000 (92.7367%),                 avg. length: 1612.0,                last time consumption/overall running time: 189.1902s / 315783.9516 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1999
env0_second_0:                 episode reward: -1.1500,                 loss: 0.7535
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 27841/30000 (92.8033%),                 avg. length: 1925.05,                last time consumption/overall running time: 219.9129s / 316003.8646 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1674
env0_second_0:                 episode reward: -1.7500,                 loss: 1.2480
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 27861/30000 (92.8700%),                 avg. length: 1910.25,                last time consumption/overall running time: 205.7690s / 316209.6336 s
env0_first_0:                 episode reward: 4.2500,                 loss: -0.1499
env0_second_0:                 episode reward: -4.2500,                 loss: 0.9917
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 27881/30000 (92.9367%),                 avg. length: 1933.05,                last time consumption/overall running time: 212.3905s / 316422.0241 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1232
env0_second_0:                 episode reward: -1.1000,                 loss: 0.8472
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 27901/30000 (93.0033%),                 avg. length: 1708.05,                last time consumption/overall running time: 201.9791s / 316624.0032 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.1317
env0_second_0:                 episode reward: -3.9500,                 loss: 0.9115
env1_first_0:                 episode reward: 3.5000,                 loss: nan
env1_second_0:                 episode reward: -3.5000,                 loss: nan
Episode: 27921/30000 (93.0700%),                 avg. length: 1584.65,                last time consumption/overall running time: 173.9460s / 316797.9492 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.1347
env0_second_0:                 episode reward: 3.1000,                 loss: 0.7284
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 27941/30000 (93.1367%),                 avg. length: 1535.45,                last time consumption/overall running time: 162.5760s / 316960.5252 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1573
env0_second_0:                 episode reward: -1.4000,                 loss: 1.1349
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 27961/30000 (93.2033%),                 avg. length: 1979.9,                last time consumption/overall running time: 224.9826s / 317185.5078 s
env0_first_0:                 episode reward: -5.4000,                 loss: -0.0718
env0_second_0:                 episode reward: 5.4000,                 loss: 1.1159
env1_first_0:                 episode reward: -9.7000,                 loss: nan
env1_second_0:                 episode reward: 9.7000,                 loss: nan
Episode: 27981/30000 (93.2700%),                 avg. length: 1486.75,                last time consumption/overall running time: 159.0975s / 317344.6053 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1749
env0_second_0:                 episode reward: -1.3000,                 loss: 0.9096
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 28001/30000 (93.3367%),                 avg. length: 1559.75,                last time consumption/overall running time: 178.3641s / 317522.9694 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1701
env0_second_0:                 episode reward: -1.8000,                 loss: 0.9174
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 28021/30000 (93.4033%),                 avg. length: 1596.25,                last time consumption/overall running time: 178.7582s / 317701.7276 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1858
env0_second_0:                 episode reward: -1.8000,                 loss: 0.7043
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 28041/30000 (93.4700%),                 avg. length: 1508.75,                last time consumption/overall running time: 174.3547s / 317876.0823 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2339
env0_second_0:                 episode reward: -1.3000,                 loss: 0.7440
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 28061/30000 (93.5367%),                 avg. length: 1479.1,                last time consumption/overall running time: 166.0489s / 318042.1312 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1878
env0_second_0:                 episode reward: -0.5000,                 loss: 0.5531
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 28081/30000 (93.6033%),                 avg. length: 1445.35,                last time consumption/overall running time: 166.8974s / 318209.0286 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2303
env0_second_0:                 episode reward: -2.2500,                 loss: 2.6991
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 28101/30000 (93.6700%),                 avg. length: 1525.95,                last time consumption/overall running time: 160.3662s / 318369.3948 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1729
env0_second_0:                 episode reward: -1.7500,                 loss: 1.2067
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 28121/30000 (93.7367%),                 avg. length: 1460.95,                last time consumption/overall running time: 161.1435s / 318530.5384 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.2139
env0_second_0:                 episode reward: -1.4000,                 loss: 0.8403
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 28141/30000 (93.8033%),                 avg. length: 1453.6,                last time consumption/overall running time: 152.9733s / 318683.5116 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2122
env0_second_0:                 episode reward: -1.5000,                 loss: 0.7179
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 28161/30000 (93.8700%),                 avg. length: 1539.4,                last time consumption/overall running time: 162.3712s / 318845.8828 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1075
env0_second_0:                 episode reward: -1.0000,                 loss: 1.1561
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 28181/30000 (93.9367%),                 avg. length: 1483.35,                last time consumption/overall running time: 157.5333s / 319003.4160 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.1348
env0_second_0:                 episode reward: -2.3000,                 loss: 1.3111
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 28201/30000 (94.0033%),                 avg. length: 1444.25,                last time consumption/overall running time: 153.5414s / 319156.9574 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1988
env0_second_0:                 episode reward: -1.0000,                 loss: 0.9731
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 28221/30000 (94.0700%),                 avg. length: 1548.9,                last time consumption/overall running time: 174.9671s / 319331.9245 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.0991
env0_second_0:                 episode reward: 0.2500,                 loss: 1.2645
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 28241/30000 (94.1367%),                 avg. length: 1450.8,                last time consumption/overall running time: 168.2072s / 319500.1317 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.1942
env0_second_0:                 episode reward: -1.0500,                 loss: 1.1248
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 28261/30000 (94.2033%),                 avg. length: 1512.35,                last time consumption/overall running time: 169.5896s / 319669.7213 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1719
env0_second_0:                 episode reward: -1.4000,                 loss: 0.9755
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 28281/30000 (94.2700%),                 avg. length: 1520.75,                last time consumption/overall running time: 187.1619s / 319856.8832 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1759
env0_second_0:                 episode reward: -0.7000,                 loss: 1.4375
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 28301/30000 (94.3367%),                 avg. length: 1477.55,                last time consumption/overall running time: 186.0204s / 320042.9036 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1881
env0_second_0:                 episode reward: -0.8000,                 loss: 1.1324
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 28321/30000 (94.4033%),                 avg. length: 1466.6,                last time consumption/overall running time: 186.4696s / 320229.3732 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1332
env0_second_0:                 episode reward: -0.2000,                 loss: 1.0971
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 28341/30000 (94.4700%),                 avg. length: 1445.8,                last time consumption/overall running time: 165.2871s / 320394.6603 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1418
env0_second_0:                 episode reward: -1.6000,                 loss: 0.9953
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 28361/30000 (94.5367%),                 avg. length: 1512.55,                last time consumption/overall running time: 166.5886s / 320561.2489 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1144
env0_second_0:                 episode reward: 0.5500,                 loss: 12.3845
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 28381/30000 (94.6033%),                 avg. length: 1542.65,                last time consumption/overall running time: 179.9891s / 320741.2380 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.1324
env0_second_0:                 episode reward: 2.4500,                 loss: 1.2031
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 28401/30000 (94.6700%),                 avg. length: 1454.8,                last time consumption/overall running time: 156.4311s / 320897.6691 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.1627
env0_second_0:                 episode reward: -0.8500,                 loss: 0.8809
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 28421/30000 (94.7367%),                 avg. length: 1437.15,                last time consumption/overall running time: 151.9963s / 321049.6654 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1806
env0_second_0:                 episode reward: -1.1500,                 loss: 1.0202
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 28441/30000 (94.8033%),                 avg. length: 1456.75,                last time consumption/overall running time: 162.7107s / 321212.3761 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1561
env0_second_0:                 episode reward: -0.9500,                 loss: 1.1476
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 28461/30000 (94.8700%),                 avg. length: 1437.65,                last time consumption/overall running time: 162.6127s / 321374.9888 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2312
env0_second_0:                 episode reward: -1.5000,                 loss: 0.8294
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 28481/30000 (94.9367%),                 avg. length: 1451.25,                last time consumption/overall running time: 159.0229s / 321534.0117 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2131
env0_second_0:                 episode reward: -0.6500,                 loss: 0.8388
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 28501/30000 (95.0033%),                 avg. length: 2053.35,                last time consumption/overall running time: 220.3302s / 321754.3419 s
env0_first_0:                 episode reward: 4.6500,                 loss: -0.0890
env0_second_0:                 episode reward: -4.6500,                 loss: 2.1057
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 28521/30000 (95.0700%),                 avg. length: 1468.5,                last time consumption/overall running time: 161.3598s / 321915.7018 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1964
env0_second_0:                 episode reward: -0.7000,                 loss: 0.9619
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 28541/30000 (95.1367%),                 avg. length: 1457.85,                last time consumption/overall running time: 164.9660s / 322080.6678 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.2248
env0_second_0:                 episode reward: -1.8000,                 loss: 2.3442
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 28561/30000 (95.2033%),                 avg. length: 1438.85,                last time consumption/overall running time: 155.8988s / 322236.5666 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1950
env0_second_0:                 episode reward: -1.1500,                 loss: 1.2198
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 28581/30000 (95.2700%),                 avg. length: 1447.8,                last time consumption/overall running time: 157.1439s / 322393.7105 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2155
env0_second_0:                 episode reward: -1.5000,                 loss: 1.0689
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 28601/30000 (95.3367%),                 avg. length: 1988.5,                last time consumption/overall running time: 224.7295s / 322618.4400 s
env0_first_0:                 episode reward: -15.3500,                 loss: -0.0739
env0_second_0:                 episode reward: 15.3500,                 loss: 1.4427
env1_first_0:                 episode reward: -18.8500,                 loss: nan
env1_second_0:                 episode reward: 18.8500,                 loss: nan
Episode: 28621/30000 (95.4033%),                 avg. length: 1829.7,                last time consumption/overall running time: 212.9723s / 322831.4123 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0241
env0_second_0:                 episode reward: 3.7000,                 loss: 1.3846
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 28641/30000 (95.4700%),                 avg. length: 1571.3,                last time consumption/overall running time: 182.3810s / 323013.7933 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1874
env0_second_0:                 episode reward: 0.5000,                 loss: 1.0592
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 28661/30000 (95.5367%),                 avg. length: 1528.0,                last time consumption/overall running time: 169.9224s / 323183.7157 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.1714
env0_second_0:                 episode reward: -1.4500,                 loss: 1.1250
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 28681/30000 (95.6033%),                 avg. length: 1440.45,                last time consumption/overall running time: 178.3341s / 323362.0498 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.2453
env0_second_0:                 episode reward: -1.7500,                 loss: 0.9793
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 28701/30000 (95.6700%),                 avg. length: 1487.35,                last time consumption/overall running time: 169.3810s / 323531.4308 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2252
env0_second_0:                 episode reward: -0.9000,                 loss: 1.1803
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 28721/30000 (95.7367%),                 avg. length: 1479.05,                last time consumption/overall running time: 168.2327s / 323699.6636 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2038
env0_second_0:                 episode reward: -1.1000,                 loss: 0.9397
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 28741/30000 (95.8033%),                 avg. length: 1463.2,                last time consumption/overall running time: 159.5543s / 323859.2179 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.2373
env0_second_0:                 episode reward: -1.2500,                 loss: 0.6602
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 28761/30000 (95.8700%),                 avg. length: 1528.15,                last time consumption/overall running time: 183.1462s / 324042.3640 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2539
env0_second_0:                 episode reward: -1.8500,                 loss: 0.7887
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 28781/30000 (95.9367%),                 avg. length: 1548.05,                last time consumption/overall running time: 177.3610s / 324219.7251 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.2453
env0_second_0:                 episode reward: -1.3000,                 loss: 0.7364
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 28801/30000 (96.0033%),                 avg. length: 1541.35,                last time consumption/overall running time: 167.8015s / 324387.5266 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2295
env0_second_0:                 episode reward: -1.5500,                 loss: 1.0617
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 28821/30000 (96.0700%),                 avg. length: 1535.25,                last time consumption/overall running time: 165.9901s / 324553.5167 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.1541
env0_second_0:                 episode reward: -3.0000,                 loss: 1.2001
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 28841/30000 (96.1367%),                 avg. length: 1441.1,                last time consumption/overall running time: 153.4122s / 324706.9289 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.2574
env0_second_0:                 episode reward: -1.4500,                 loss: 0.7233
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 28861/30000 (96.2033%),                 avg. length: 1438.8,                last time consumption/overall running time: 153.9571s / 324860.8860 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.2584
env0_second_0:                 episode reward: -1.8500,                 loss: 0.8057
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 28881/30000 (96.2700%),                 avg. length: 1440.65,                last time consumption/overall running time: 151.3303s / 325012.2163 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.2640
env0_second_0:                 episode reward: -2.1000,                 loss: 0.4867
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 28901/30000 (96.3367%),                 avg. length: 1447.25,                last time consumption/overall running time: 156.5793s / 325168.7956 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1885
env0_second_0:                 episode reward: -2.1000,                 loss: 0.4482
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 28921/30000 (96.4033%),                 avg. length: 1556.85,                last time consumption/overall running time: 172.3240s / 325341.1197 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1916
env0_second_0:                 episode reward: -1.3000,                 loss: 0.8692
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 28941/30000 (96.4700%),                 avg. length: 1623.7,                last time consumption/overall running time: 193.6534s / 325534.7731 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1282
env0_second_0:                 episode reward: -1.1000,                 loss: 0.8344
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 28961/30000 (96.5367%),                 avg. length: 1674.45,                last time consumption/overall running time: 193.4294s / 325728.2025 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.1479
env0_second_0:                 episode reward: -3.3000,                 loss: 1.0121
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 28981/30000 (96.6033%),                 avg. length: 1456.75,                last time consumption/overall running time: 155.4687s / 325883.6712 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.1432
env0_second_0:                 episode reward: -3.8500,                 loss: 0.7521
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 29001/30000 (96.6700%),                 avg. length: 1565.25,                last time consumption/overall running time: 173.6783s / 326057.3495 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.1719
env0_second_0:                 episode reward: -2.1000,                 loss: 0.7400
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 29021/30000 (96.7367%),                 avg. length: 1893.8,                last time consumption/overall running time: 203.4514s / 326260.8008 s
env0_first_0:                 episode reward: 4.4500,                 loss: -0.1047
env0_second_0:                 episode reward: -4.4500,                 loss: 1.0361
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 29041/30000 (96.8033%),                 avg. length: 1626.4,                last time consumption/overall running time: 171.2243s / 326432.0251 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.1366
env0_second_0:                 episode reward: -1.9000,                 loss: 0.8678
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 29061/30000 (96.8700%),                 avg. length: 1760.45,                last time consumption/overall running time: 185.9907s / 326618.0159 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1465
env0_second_0:                 episode reward: -1.7500,                 loss: 0.7096
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 29081/30000 (96.9367%),                 avg. length: 1575.4,                last time consumption/overall running time: 174.8732s / 326792.8891 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1769
env0_second_0:                 episode reward: -1.3500,                 loss: 0.4454
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 29101/30000 (97.0033%),                 avg. length: 1742.5,                last time consumption/overall running time: 193.8806s / 326986.7697 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1483
env0_second_0:                 episode reward: 0.4500,                 loss: 0.7683
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 29121/30000 (97.0700%),                 avg. length: 1706.5,                last time consumption/overall running time: 190.4023s / 327177.1721 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1375
env0_second_0:                 episode reward: 0.4000,                 loss: 1.1351
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 29141/30000 (97.1367%),                 avg. length: 1705.4,                last time consumption/overall running time: 182.9270s / 327360.0991 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1210
env0_second_0:                 episode reward: 0.1500,                 loss: 0.9988
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 29161/30000 (97.2033%),                 avg. length: 1706.05,                last time consumption/overall running time: 190.4427s / 327550.5418 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1214
env0_second_0:                 episode reward: -0.7500,                 loss: 1.1555
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29181/30000 (97.2700%),                 avg. length: 1491.0,                last time consumption/overall running time: 169.9750s / 327720.5168 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1866
env0_second_0:                 episode reward: -0.9000,                 loss: 0.9124
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 29201/30000 (97.3367%),                 avg. length: 1511.0,                last time consumption/overall running time: 162.5765s / 327883.0933 s
env0_first_0:                 episode reward: 2.5000,                 loss: -0.1478
env0_second_0:                 episode reward: -2.5000,                 loss: 0.8284
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 29221/30000 (97.4033%),                 avg. length: 1458.2,                last time consumption/overall running time: 165.0636s / 328048.1569 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1735
env0_second_0:                 episode reward: -1.7500,                 loss: 0.9152
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 29241/30000 (97.4700%),                 avg. length: 1703.5,                last time consumption/overall running time: 188.8163s / 328236.9732 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1150
env0_second_0:                 episode reward: -0.7000,                 loss: 0.9967
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 29261/30000 (97.5367%),                 avg. length: 1680.35,                last time consumption/overall running time: 191.5062s / 328428.4793 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1428
env0_second_0:                 episode reward: -0.2000,                 loss: 0.7807
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 29281/30000 (97.6033%),                 avg. length: 1645.95,                last time consumption/overall running time: 175.9056s / 328604.3850 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1585
env0_second_0:                 episode reward: 1.3000,                 loss: 0.6431
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 29301/30000 (97.6700%),                 avg. length: 1542.65,                last time consumption/overall running time: 171.4347s / 328775.8197 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1811
env0_second_0:                 episode reward: -1.1000,                 loss: 0.7769
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29321/30000 (97.7367%),                 avg. length: 1585.25,                last time consumption/overall running time: 172.3982s / 328948.2179 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1614
env0_second_0:                 episode reward: 1.0000,                 loss: 0.7283
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 29341/30000 (97.8033%),                 avg. length: 1691.55,                last time consumption/overall running time: 202.2004s / 329150.4182 s
env0_first_0:                 episode reward: -3.8000,                 loss: -0.0941
env0_second_0:                 episode reward: 3.8000,                 loss: 1.0951
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 29361/30000 (97.8700%),                 avg. length: 1780.9,                last time consumption/overall running time: 193.2836s / 329343.7018 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.1119
env0_second_0:                 episode reward: 2.5500,                 loss: 1.1922
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 29381/30000 (97.9367%),                 avg. length: 2479.85,                last time consumption/overall running time: 275.2396s / 329618.9414 s
env0_first_0:                 episode reward: -11.9500,                 loss: -0.0742
env0_second_0:                 episode reward: 11.9500,                 loss: 1.3160
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 29401/30000 (98.0033%),                 avg. length: 2655.35,                last time consumption/overall running time: 284.2528s / 329903.1942 s
env0_first_0:                 episode reward: -10.0000,                 loss: -0.0656
env0_second_0:                 episode reward: 10.0000,                 loss: 1.3204
env1_first_0:                 episode reward: -11.3500,                 loss: nan
env1_second_0:                 episode reward: 11.3500,                 loss: nan
Episode: 29421/30000 (98.0700%),                 avg. length: 2816.35,                last time consumption/overall running time: 298.5675s / 330201.7617 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.1010
env0_second_0:                 episode reward: 4.9000,                 loss: 0.9856
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 29441/30000 (98.1367%),                 avg. length: 2073.75,                last time consumption/overall running time: 217.6548s / 330419.4166 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1334
env0_second_0:                 episode reward: 0.6500,                 loss: 0.9445
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 29461/30000 (98.2033%),                 avg. length: 2062.6,                last time consumption/overall running time: 232.5291s / 330651.9457 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1246
env0_second_0:                 episode reward: -0.7000,                 loss: 1.0212
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 29481/30000 (98.2700%),                 avg. length: 1909.75,                last time consumption/overall running time: 231.1471s / 330883.0928 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1520
env0_second_0:                 episode reward: -1.5000,                 loss: 0.8043
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 29501/30000 (98.3367%),                 avg. length: 1782.25,                last time consumption/overall running time: 192.1470s / 331075.2398 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1453
env0_second_0:                 episode reward: -1.1000,                 loss: 0.8340
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 29521/30000 (98.4033%),                 avg. length: 1506.85,                last time consumption/overall running time: 161.8857s / 331237.1255 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.1796
env0_second_0:                 episode reward: -1.7500,                 loss: 0.8040
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 29541/30000 (98.4700%),                 avg. length: 1573.5,                last time consumption/overall running time: 166.1672s / 331403.2927 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.1169
env0_second_0:                 episode reward: -6.5500,                 loss: 1.2462
env1_first_0:                 episode reward: 7.5000,                 loss: nan
env1_second_0:                 episode reward: -7.5000,                 loss: nan
Episode: 29561/30000 (98.5367%),                 avg. length: 1503.2,                last time consumption/overall running time: 161.9429s / 331565.2356 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.1805
env0_second_0:                 episode reward: -4.4000,                 loss: 1.4221
env1_first_0:                 episode reward: 3.8500,                 loss: nan
env1_second_0:                 episode reward: -3.8500,                 loss: nan
Episode: 29581/30000 (98.6033%),                 avg. length: 1482.55,                last time consumption/overall running time: 161.4869s / 331726.7225 s
env0_first_0:                 episode reward: 5.1500,                 loss: -0.1681
env0_second_0:                 episode reward: -5.1500,                 loss: 1.6460
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 29601/30000 (98.6700%),                 avg. length: 1496.75,                last time consumption/overall running time: 161.1240s / 331887.8465 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.1957
env0_second_0:                 episode reward: -2.7000,                 loss: 1.8668
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 29621/30000 (98.7367%),                 avg. length: 1457.35,                last time consumption/overall running time: 158.4952s / 332046.3416 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2200
env0_second_0:                 episode reward: -2.2500,                 loss: 0.8921
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 29641/30000 (98.8033%),                 avg. length: 1516.5,                last time consumption/overall running time: 165.6513s / 332211.9930 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1957
env0_second_0:                 episode reward: -0.9500,                 loss: 0.9065
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 29661/30000 (98.8700%),                 avg. length: 1453.9,                last time consumption/overall running time: 160.8636s / 332372.8566 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1760
env0_second_0:                 episode reward: -0.1000,                 loss: 0.9616
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 29681/30000 (98.9367%),                 avg. length: 1624.4,                last time consumption/overall running time: 179.5324s / 332552.3891 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2008
env0_second_0:                 episode reward: -0.4500,                 loss: 1.1667
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 29701/30000 (99.0033%),                 avg. length: 2162.35,                last time consumption/overall running time: 230.8620s / 332783.2511 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.0831
env0_second_0:                 episode reward: 1.7500,                 loss: 1.2497
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 29721/30000 (99.0700%),                 avg. length: 1892.65,                last time consumption/overall running time: 200.7856s / 332984.0366 s
env0_first_0:                 episode reward: 4.1500,                 loss: -0.1042
env0_second_0:                 episode reward: -4.1500,                 loss: 1.4009
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 29741/30000 (99.1367%),                 avg. length: 1964.95,                last time consumption/overall running time: 229.1833s / 333213.2199 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.1911
env0_second_0:                 episode reward: -2.6000,                 loss: 1.1686
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 29761/30000 (99.2033%),                 avg. length: 2276.65,                last time consumption/overall running time: 240.8525s / 333454.0725 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.1431
env0_second_0:                 episode reward: -3.3000,                 loss: 1.3313
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 29781/30000 (99.2700%),                 avg. length: 2423.0,                last time consumption/overall running time: 268.0314s / 333722.1038 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1150
env0_second_0:                 episode reward: 0.9000,                 loss: 1.0208
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 29801/30000 (99.3367%),                 avg. length: 2177.4,                last time consumption/overall running time: 250.4393s / 333972.5431 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.0727
env0_second_0:                 episode reward: -0.7500,                 loss: 1.2370
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 29821/30000 (99.4033%),                 avg. length: 1782.5,                last time consumption/overall running time: 213.5040s / 334186.0471 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1601
env0_second_0:                 episode reward: -2.2500,                 loss: 0.8923
env1_first_0:                 episode reward: 3.9500,                 loss: nan
env1_second_0:                 episode reward: -3.9500,                 loss: nan
Episode: 29841/30000 (99.4700%),                 avg. length: 1651.5,                last time consumption/overall running time: 201.4546s / 334387.5017 sLoad double_dunk_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load double_dunk_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rl/agents/nash_ppo.py:181: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  s,a,r,s_prime,prob_a,done_mask =    torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst).to(self.device), \
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: 0.3000,                 loss: -0.2226
env0_second_0:                 episode reward: -0.3000,                 loss: 0.5740
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 29861/30000 (99.5367%),                 avg. length: 1777.5,                last time consumption/overall running time: 190.7846s / 334578.2862 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.2279
env0_second_0:                 episode reward: -2.0000,                 loss: 0.6911
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 29881/30000 (99.6033%),                 avg. length: 1995.5,                last time consumption/overall running time: 216.0223s / 334794.3085 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2194
env0_second_0:                 episode reward: -2.1500,                 loss: 0.6075
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 29901/30000 (99.6700%),                 avg. length: 2454.25,                last time consumption/overall running time: 272.8997s / 335067.2082 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1801
env0_second_0:                 episode reward: -0.9500,                 loss: 0.9086
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 29921/30000 (99.7367%),                 avg. length: 3177.4,                last time consumption/overall running time: 347.0106s / 335414.2188 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.1976
env0_second_0:                 episode reward: -1.5000,                 loss: 1.1047
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 29941/30000 (99.8033%),                 avg. length: 2831.85,                last time consumption/overall running time: 311.3612s / 335725.5800 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.1631
env0_second_0:                 episode reward: -0.4000,                 loss: 0.9888
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 29961/30000 (99.8700%),                 avg. length: 3127.75,                last time consumption/overall running time: 331.6094s / 336057.1893 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.2109
env0_second_0:                 episode reward: -3.1500,                 loss: 0.7693
env1_first_0:                 episode reward: 4.3500,                 loss: nan
env1_second_0:                 episode reward: -4.3500,                 loss: nan
Episode: 29981/30000 (99.9367%),                 avg. length: 2836.75,                last time consumption/overall running time: 310.0577s / 336367.2471 s
env0_first_0:                 episode reward: 4.0000,                 loss: -0.1631
env0_second_0:                 episode reward: -4.0000,                 loss: 0.8800
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
