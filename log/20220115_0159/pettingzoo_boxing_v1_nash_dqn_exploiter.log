pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
boxing_v1 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [88, 58]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'boxing_v1', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQNExploiter', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'exploiter_update_itr': 1}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 0.1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn_exploiter', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220115_0159/pettingzoo_boxing_v1_nash_dqn_exploiter. 
 Save logs to: /home/zihan/research/MARS/data/log/20220115_0159/pettingzoo_boxing_v1_nash_dqn_exploiter.
Episode: 1/10000 (0.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 17.8876s / 17.8876 s
env0_first_0:                 episode reward: 2.0000,                 loss: 0.0150
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0117
env1_first_0:                 episode reward: -11.0000,                 loss: nan
env1_second_0:                 episode reward: 11.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 387.4275s / 405.3151 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.0146
env0_second_0:                 episode reward: -1.2500,                 loss: 0.0144
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 446.5877s / 851.9028 s
env0_first_0:                 episode reward: 1.4000,                 loss: 0.0142
env0_second_0:                 episode reward: -1.4000,                 loss: 0.0147
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 464.2013s / 1316.1041 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0136
env0_second_0:                 episode reward: 0.8500,                 loss: 0.0133
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 466.0575s / 1782.1616 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.0123
env0_second_0:                 episode reward: -0.7500,                 loss: 0.0122
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.2897s / 2244.4513 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0108
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0110
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.5116s / 2705.9629 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.0139
env0_second_0:                 episode reward: -0.0500,                 loss: 0.0137
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 469.5822s / 3175.5451 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.0127
env0_second_0:                 episode reward: -0.8000,                 loss: 0.0130
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 465.4135s / 3640.9586 s
env0_first_0:                 episode reward: 1.4500,                 loss: 0.0101
env0_second_0:                 episode reward: -1.4500,                 loss: 0.0100
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 466.5910s / 4107.5496 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0160
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0158
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 475.0814s / 4582.6310 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0177
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0180
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 466.5957s / 5049.2267 s
env0_first_0:                 episode reward: 4.0500,                 loss: 0.0146
env0_second_0:                 episode reward: -4.0500,                 loss: 0.0153
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 473.5246s / 5522.7513 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0127
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0126
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 466.1053s / 5988.8567 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0131
env0_second_0:                 episode reward: -4.9000,                 loss: 0.0129
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 465.7910s / 6454.6477 s
env0_first_0:                 episode reward: 2.3000,                 loss: 0.0125
env0_second_0:                 episode reward: -2.3000,                 loss: 0.0125
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.2829s / 6916.9306 s
env0_first_0:                 episode reward: 5.1000,                 loss: 0.0163
env0_second_0:                 episode reward: -5.1000,                 loss: 0.0163
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.0400s / 7377.9706 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.0160
env0_second_0:                 episode reward: 1.2500,                 loss: 0.0150
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.8693s / 7840.8400 s
env0_first_0:                 episode reward: 1.3000,                 loss: 0.0145
env0_second_0:                 episode reward: -1.3000,                 loss: 0.0136
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.9655s / 8302.8054 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0137
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0133
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 457.4717s / 8760.2771 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0110
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0101
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.5389s / 9219.8160 s
env0_first_0:                 episode reward: 3.8500,                 loss: 0.0104
env0_second_0:                 episode reward: -3.8500,                 loss: 0.0099
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.3665s / 9682.1825 s
env0_first_0:                 episode reward: 2.3500,                 loss: 0.0152
env0_second_0:                 episode reward: -2.3500,                 loss: 0.0145
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.2689s / 10144.4515 s
env0_first_0:                 episode reward: 2.0000,                 loss: 0.0139
env0_second_0:                 episode reward: -2.0000,                 loss: 0.0129
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 472.8362s / 10617.2876 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0138
env0_second_0:                 episode reward: -1.7500,                 loss: 0.0125
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 473.1729s / 11090.4605 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.0153
env0_second_0:                 episode reward: -0.9500,                 loss: 0.0138
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 472.0196s / 11562.4802 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0144
env0_second_0:                 episode reward: -2.5500,                 loss: 0.0136
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 472.2138s / 12034.6940 s
env0_first_0:                 episode reward: 2.6500,                 loss: 0.0128
env0_second_0:                 episode reward: -2.6500,                 loss: 0.0112
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 472.1450s / 12506.8390 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0141
env0_second_0:                 episode reward: -4.5500,                 loss: 0.0117
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 467.0367s / 12973.8757 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0186
env0_second_0:                 episode reward: -4.8000,                 loss: 0.0152
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.0458s / 13434.9215 s
env0_first_0:                 episode reward: 6.9000,                 loss: 0.0189
env0_second_0:                 episode reward: -6.9000,                 loss: 0.0152
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 465.8413s / 13900.7628 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0227
env0_second_0:                 episode reward: -3.5000,                 loss: 0.0190
env1_first_0:                 episode reward: 5.8000,                 loss: nan
env1_second_0:                 episode reward: -5.8000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 463.6340s / 14364.3968 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0223
env0_second_0:                 episode reward: -3.2500,                 loss: 0.0181
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.4236s / 14826.8204 s
env0_first_0:                 episode reward: 6.8500,                 loss: 0.0220
env0_second_0:                 episode reward: -6.8500,                 loss: 0.0188
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 460.3324s / 15287.1528 s
env0_first_0:                 episode reward: 3.3500,                 loss: 0.0186
env0_second_0:                 episode reward: -3.3500,                 loss: 0.0167
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 458.1473s / 15745.3001 s
env0_first_0:                 episode reward: 0.7000,                 loss: 0.0158
env0_second_0:                 episode reward: -0.7000,                 loss: 0.0138
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 473.6071s / 16218.9073 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0169
env0_second_0:                 episode reward: -4.2000,                 loss: 0.0134
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 475.4604s / 16694.3676 s
env0_first_0:                 episode reward: 8.8500,                 loss: 0.0281
env0_second_0:                 episode reward: -8.8500,                 loss: 0.0207
env1_first_0:                 episode reward: 8.7000,                 loss: nan
env1_second_0:                 episode reward: -8.7000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 472.0344s / 17166.4020 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.0367
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0261
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 475.1779s / 17641.5799 s
env0_first_0:                 episode reward: 2.9500,                 loss: 0.0180
env0_second_0:                 episode reward: -2.9500,                 loss: 0.0128
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 471.9121s / 18113.4920 s
env0_first_0:                 episode reward: 1.6000,                 loss: 0.0161
env0_second_0:                 episode reward: -1.6000,                 loss: 0.0140
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 470.8934s / 18584.3853 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.0140
env0_second_0:                 episode reward: 0.5500,                 loss: 0.0107
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 470.8647s / 19055.2500 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.0126
env0_second_0:                 episode reward: 0.0500,                 loss: 0.0101
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 473.3600s / 19528.6099 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.0226
env0_second_0:                 episode reward: 0.1000,                 loss: 0.0189
env1_first_0:                 episode reward: 4.9000,                 loss: nan
env1_second_0:                 episode reward: -4.9000,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 463.7196s / 19992.3296 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.0254
env0_second_0:                 episode reward: -0.4000,                 loss: 0.0209
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 465.3664s / 20457.6960 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.0257
env0_second_0:                 episode reward: 1.7000,                 loss: 0.0190
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 464.8000s / 20922.4959 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0405
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0271
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 463.4948s / 21385.9908 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0337
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0240
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 468.5912s / 21854.5820 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0269
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0197
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 463.0523s / 22317.6343 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.0314
env0_second_0:                 episode reward: 2.7000,                 loss: 0.0274
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.8635s / 22777.4978 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.0253
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0217
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.0288s / 23239.5266 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0308
env0_second_0:                 episode reward: 3.4500,                 loss: 0.0226
env1_first_0:                 episode reward: -10.1500,                 loss: nan
env1_second_0:                 episode reward: 10.1500,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 456.9589s / 23696.4855 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.0612
env0_second_0:                 episode reward: 0.3500,                 loss: 0.0423
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 466.2958s / 24162.7813 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0626
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0409
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.3317s / 24624.1130 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0383
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0276
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.9904s / 25086.1035 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0281
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0219
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 463.8977s / 25550.0012 s
env0_first_0:                 episode reward: -1.4500,                 loss: 0.0199
env0_second_0:                 episode reward: 1.4500,                 loss: 0.0132
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 458.2561s / 26008.2572 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0218
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0143
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 456.5184s / 26464.7757 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0263
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0188
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.5742s / 26926.3499 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.0146
env0_second_0:                 episode reward: 2.6500,                 loss: 0.0112
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.5434s / 27385.8934 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.0165
env0_second_0:                 episode reward: 0.9000,                 loss: 0.0111
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.6125s / 27848.5059 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.0204
env0_second_0:                 episode reward: 1.5500,                 loss: 0.0129
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 463.9180s / 28312.4239 s
env0_first_0:                 episode reward: -2.5000,                 loss: 0.0195
env0_second_0:                 episode reward: 2.5000,                 loss: 0.0150
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 462.6116s / 28775.0355 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0171
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0145
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.8076s / 29234.8431 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0197
env0_second_0:                 episode reward: 3.3500,                 loss: 0.0153
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 463.5803s / 29698.4234 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0156
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0145
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 465.5187s / 30163.9420 s
env0_first_0:                 episode reward: -2.2500,                 loss: 0.0155
env0_second_0:                 episode reward: 2.2500,                 loss: 0.0120
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 470.0779s / 30634.0199 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0175
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0140
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 468.1233s / 31102.1432 s
env0_first_0:                 episode reward: -4.7500,                 loss: 0.0181
env0_second_0:                 episode reward: 4.7500,                 loss: 0.0150
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 467.5253s / 31569.6685 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0167
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0120
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.8427s / 32029.5112 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.0201
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0141
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 463.6562s / 32493.1674 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0281
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0181
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.2902s / 32952.4576 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0242
env0_second_0:                 episode reward: 2.7500,                 loss: 0.0150
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 457.7281s / 33410.1857 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.0198
env0_second_0:                 episode reward: 4.2500,                 loss: 0.0132
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 464.2637s / 33874.4494 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0168
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0129
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.0783s / 34333.5277 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0159
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0122
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.2103s / 34792.7380 s
env0_first_0:                 episode reward: -16.5000,                 loss: 0.0210
env0_second_0:                 episode reward: 16.5000,                 loss: 0.0168
env1_first_0:                 episode reward: -16.3500,                 loss: nan
env1_second_0:                 episode reward: 16.3500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 456.1374s / 35248.8754 s
env0_first_0:                 episode reward: -19.8500,                 loss: 0.0299
env0_second_0:                 episode reward: 19.8500,                 loss: 0.0245
env1_first_0:                 episode reward: -22.5500,                 loss: nan
env1_second_0:                 episode reward: 22.5500,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 454.8106s / 35703.6861 s
env0_first_0:                 episode reward: -23.8500,                 loss: 0.0369
env0_second_0:                 episode reward: 23.8500,                 loss: 0.0323
env1_first_0:                 episode reward: -24.9000,                 loss: nan
env1_second_0:                 episode reward: 24.9000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 459.4911s / 36163.1772 s
env0_first_0:                 episode reward: -13.7000,                 loss: 0.0328
env0_second_0:                 episode reward: 13.7000,                 loss: 0.0277
env1_first_0:                 episode reward: -21.2500,                 loss: nan
env1_second_0:                 episode reward: 21.2500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 1784.0,                last time consumption/overall running time: 455.4517s / 36618.6289 s
env0_first_0:                 episode reward: -13.7000,                 loss: 0.0239
env0_second_0:                 episode reward: 13.7000,                 loss: 0.0208
env1_first_0:                 episode reward: -9.6500,                 loss: nan
env1_second_0:                 episode reward: 9.6500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.7966s / 37080.4255 s
env0_first_0:                 episode reward: -11.7000,                 loss: 0.0197
env0_second_0:                 episode reward: 11.7000,                 loss: 0.0159
env1_first_0:                 episode reward: -11.0000,                 loss: nan
env1_second_0:                 episode reward: 11.0000,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 1784.0,                last time consumption/overall running time: 454.6848s / 37535.1104 s
env0_first_0:                 episode reward: -10.6000,                 loss: 0.0152
env0_second_0:                 episode reward: 10.6000,                 loss: 0.0124
env1_first_0:                 episode reward: -15.3500,                 loss: nan
env1_second_0:                 episode reward: 15.3500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 1784.0,                last time consumption/overall running time: 469.9289s / 38005.0392 s
env0_first_0:                 episode reward: -14.3500,                 loss: 0.0172
env0_second_0:                 episode reward: 14.3500,                 loss: 0.0138
env1_first_0:                 episode reward: -11.9500,                 loss: nan
env1_second_0:                 episode reward: 11.9500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 469.3566s / 38474.3959 s
env0_first_0:                 episode reward: -14.6500,                 loss: 0.0201
env0_second_0:                 episode reward: 14.6500,                 loss: 0.0166
env1_first_0:                 episode reward: -13.6000,                 loss: nan
env1_second_0:                 episode reward: 13.6000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 1774.4,                last time consumption/overall running time: 461.1797s / 38935.5756 s
env0_first_0:                 episode reward: -24.7500,                 loss: 0.0283
env0_second_0:                 episode reward: 24.7500,                 loss: 0.0195
env1_first_0:                 episode reward: -18.7500,                 loss: nan
env1_second_0:                 episode reward: 18.7500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 1784.0,                last time consumption/overall running time: 461.3197s / 39396.8953 s
env0_first_0:                 episode reward: -16.5000,                 loss: 0.0270
env0_second_0:                 episode reward: 16.5000,                 loss: 0.0210
env1_first_0:                 episode reward: -18.8000,                 loss: nan
env1_second_0:                 episode reward: 18.8000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 1738.75,                last time consumption/overall running time: 454.1203s / 39851.0156 s
env0_first_0:                 episode reward: -34.5500,                 loss: 0.0321
env0_second_0:                 episode reward: 34.5500,                 loss: 0.0248
env1_first_0:                 episode reward: -27.5500,                 loss: nan
env1_second_0:                 episode reward: 27.5500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 1471.5,                last time consumption/overall running time: 386.4050s / 40237.4206 s
env0_first_0:                 episode reward: -40.5000,                 loss: 0.0469
env0_second_0:                 episode reward: 40.5000,                 loss: 0.0386
env1_first_0:                 episode reward: -39.9500,                 loss: nan
env1_second_0:                 episode reward: 39.9500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 1784.0,                last time consumption/overall running time: 446.0389s / 40683.4595 s
env0_first_0:                 episode reward: -30.4000,                 loss: 0.0482
env0_second_0:                 episode reward: 30.4000,                 loss: 0.0409
env1_first_0:                 episode reward: -22.1500,                 loss: nan
env1_second_0:                 episode reward: 22.1500,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 1531.75,                last time consumption/overall running time: 386.6989s / 41070.1584 s
env0_first_0:                 episode reward: -49.9500,                 loss: 0.0464
env0_second_0:                 episode reward: 49.9500,                 loss: 0.0380
env1_first_0:                 episode reward: -46.0000,                 loss: nan
env1_second_0:                 episode reward: 46.0000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 1466.7,                last time consumption/overall running time: 373.5013s / 41443.6598 s
env0_first_0:                 episode reward: -49.7500,                 loss: 0.0622
env0_second_0:                 episode reward: 49.7500,                 loss: 0.0523
env1_first_0:                 episode reward: -60.6000,                 loss: nan
env1_second_0:                 episode reward: 60.6000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 1285.8,                last time consumption/overall running time: 328.1127s / 41771.7724 s
env0_first_0:                 episode reward: -69.7000,                 loss: 0.0803
env0_second_0:                 episode reward: 69.7000,                 loss: 0.0714
env1_first_0:                 episode reward: -61.0000,                 loss: nan
env1_second_0:                 episode reward: 61.0000,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 1659.8,                last time consumption/overall running time: 422.8264s / 42194.5988 s
env0_first_0:                 episode reward: -58.1500,                 loss: 0.0988
env0_second_0:                 episode reward: 58.1500,                 loss: 0.0819
env1_first_0:                 episode reward: -64.6500,                 loss: nan
env1_second_0:                 episode reward: 64.6500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 1116.6,                last time consumption/overall running time: 288.1486s / 42482.7474 s
env0_first_0:                 episode reward: -60.8500,                 loss: 0.0854
env0_second_0:                 episode reward: 60.8500,                 loss: 0.0670
env1_first_0:                 episode reward: -57.8500,                 loss: nan
env1_second_0:                 episode reward: 57.8500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 384.15,                last time consumption/overall running time: 98.1784s / 42580.9258 s
env0_first_0:                 episode reward: -86.2500,                 loss: 0.0786
env0_second_0:                 episode reward: 86.2500,                 loss: 0.0672
env1_first_0:                 episode reward: -69.2500,                 loss: nan
env1_second_0:                 episode reward: 69.2500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 374.9,                last time consumption/overall running time: 94.3684s / 42675.2942 s
env0_first_0:                 episode reward: -74.7500,                 loss: 0.0856
env0_second_0:                 episode reward: 74.7500,                 loss: 0.0696
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 489.15,                last time consumption/overall running time: 126.7821s / 42802.0764 s
env0_first_0:                 episode reward: -70.5500,                 loss: 0.0956
env0_second_0:                 episode reward: 70.5500,                 loss: 0.0714
env1_first_0:                 episode reward: -73.5500,                 loss: nan
env1_second_0:                 episode reward: 73.5500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 531.65,                last time consumption/overall running time: 137.8796s / 42939.9560 s
env0_first_0:                 episode reward: -64.0000,                 loss: 0.1005
env0_second_0:                 episode reward: 64.0000,                 loss: 0.0795
env1_first_0:                 episode reward: -58.1000,                 loss: nan
env1_second_0:                 episode reward: 58.1000,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 669.85,                last time consumption/overall running time: 173.0277s / 43112.9837 s
env0_first_0:                 episode reward: -72.7000,                 loss: 0.1071
env0_second_0:                 episode reward: 72.7000,                 loss: 0.0817
env1_first_0:                 episode reward: -69.7500,                 loss: nan
env1_second_0:                 episode reward: 69.7500,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 529.3,                last time consumption/overall running time: 137.0243s / 43250.0080 s
env0_first_0:                 episode reward: -74.7500,                 loss: 0.1103
env0_second_0:                 episode reward: 74.7500,                 loss: 0.0825
env1_first_0:                 episode reward: -70.4000,                 loss: nan
env1_second_0:                 episode reward: 70.4000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 341.5,                last time consumption/overall running time: 90.1429s / 43340.1509 s
env0_first_0:                 episode reward: -59.2500,                 loss: 0.1084
env0_second_0:                 episode reward: 59.2500,                 loss: 0.0890
env1_first_0:                 episode reward: -86.3500,                 loss: nan
env1_second_0:                 episode reward: 86.3500,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 379.05,                last time consumption/overall running time: 100.3671s / 43440.5180 s
env0_first_0:                 episode reward: -73.7000,                 loss: 0.1023
env0_second_0:                 episode reward: 73.7000,                 loss: 0.0884
env1_first_0:                 episode reward: -70.4500,                 loss: nan
env1_second_0:                 episode reward: 70.4500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 324.9,                last time consumption/overall running time: 86.5369s / 43527.0549 s
env0_first_0:                 episode reward: -65.7500,                 loss: 0.1038
env0_second_0:                 episode reward: 65.7500,                 loss: 0.0869
env1_first_0:                 episode reward: -79.7500,                 loss: nan
env1_second_0:                 episode reward: 79.7500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 287.05,                last time consumption/overall running time: 74.4642s / 43601.5191 s
env0_first_0:                 episode reward: -78.8000,                 loss: 0.1208
env0_second_0:                 episode reward: 78.8000,                 loss: 0.0939
env1_first_0:                 episode reward: -64.7500,                 loss: nan
env1_second_0:                 episode reward: 64.7500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 307.75,                last time consumption/overall running time: 78.9811s / 43680.5002 s
env0_first_0:                 episode reward: -87.9000,                 loss: 0.1305
env0_second_0:                 episode reward: 87.9000,                 loss: 0.0956
env1_first_0:                 episode reward: -82.6000,                 loss: nan
env1_second_0:                 episode reward: 82.6000,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 399.8,                last time consumption/overall running time: 102.4110s / 43782.9112 s
env0_first_0:                 episode reward: -67.0500,                 loss: 0.1397
env0_second_0:                 episode reward: 67.0500,                 loss: 0.1125
env1_first_0:                 episode reward: -81.1500,                 loss: nan
env1_second_0:                 episode reward: 81.1500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 332.0,                last time consumption/overall running time: 85.0624s / 43867.9736 s
env0_first_0:                 episode reward: -78.4000,                 loss: 0.1620
env0_second_0:                 episode reward: 78.4000,                 loss: 0.1194
env1_first_0:                 episode reward: -69.5500,                 loss: nan
env1_second_0:                 episode reward: 69.5500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 263.85,                last time consumption/overall running time: 67.9486s / 43935.9221 s
env0_first_0:                 episode reward: -75.3000,                 loss: 0.1635
env0_second_0:                 episode reward: 75.3000,                 loss: 0.1213
env1_first_0:                 episode reward: -74.8500,                 loss: nan
env1_second_0:                 episode reward: 74.8500,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 241.65,                last time consumption/overall running time: 62.6371s / 43998.5592 s
env0_first_0:                 episode reward: -93.4500,                 loss: 0.1860
env0_second_0:                 episode reward: 93.4500,                 loss: 0.1221
env1_first_0:                 episode reward: -73.6000,                 loss: nan
env1_second_0:                 episode reward: 73.6000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 262.05,                last time consumption/overall running time: 67.9077s / 44066.4670 s
env0_first_0:                 episode reward: -74.0000,                 loss: 0.1810
env0_second_0:                 episode reward: 74.0000,                 loss: 0.1231
env1_first_0:                 episode reward: -86.5000,                 loss: nan
env1_second_0:                 episode reward: 86.5000,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 233.9,                last time consumption/overall running time: 60.3245s / 44126.7915 s
env0_first_0:                 episode reward: -78.9500,                 loss: 0.2026
env0_second_0:                 episode reward: 78.9500,                 loss: 0.1321
env1_first_0:                 episode reward: -82.7000,                 loss: nan
env1_second_0:                 episode reward: 82.7000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 254.95,                last time consumption/overall running time: 66.7511s / 44193.5425 s
env0_first_0:                 episode reward: -81.2000,                 loss: 0.1886
env0_second_0:                 episode reward: 81.2000,                 loss: 0.1326
env1_first_0:                 episode reward: -88.3000,                 loss: nan
env1_second_0:                 episode reward: 88.3000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 242.0,                last time consumption/overall running time: 63.4381s / 44256.9806 s
env0_first_0:                 episode reward: -78.5500,                 loss: 0.1901
env0_second_0:                 episode reward: 78.5500,                 loss: 0.1269
env1_first_0:                 episode reward: -82.5000,                 loss: nan
env1_second_0:                 episode reward: 82.5000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 278.0,                last time consumption/overall running time: 74.2089s / 44331.1895 s
env0_first_0:                 episode reward: -70.5500,                 loss: 0.1820
env0_second_0:                 episode reward: 70.5500,                 loss: 0.1314
env1_first_0:                 episode reward: -94.3000,                 loss: nan
env1_second_0:                 episode reward: 94.3000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 287.4,                last time consumption/overall running time: 76.5220s / 44407.7115 s
env0_first_0:                 episode reward: -87.1000,                 loss: 0.1968
env0_second_0:                 episode reward: 87.1000,                 loss: 0.1377
env1_first_0:                 episode reward: -68.8500,                 loss: nan
env1_second_0:                 episode reward: 68.8500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 253.95,                last time consumption/overall running time: 65.8709s / 44473.5824 s
env0_first_0:                 episode reward: -83.4000,                 loss: 0.1909
env0_second_0:                 episode reward: 83.4000,                 loss: 0.1316
env1_first_0:                 episode reward: -75.2500,                 loss: nan
env1_second_0:                 episode reward: 75.2500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 299.7,                last time consumption/overall running time: 76.2401s / 44549.8225 s
env0_first_0:                 episode reward: -82.1500,                 loss: 0.1857
env0_second_0:                 episode reward: 82.1500,                 loss: 0.1256
env1_first_0:                 episode reward: -82.5000,                 loss: nan
env1_second_0:                 episode reward: 82.5000,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 274.85,                last time consumption/overall running time: 71.8801s / 44621.7026 s
env0_first_0:                 episode reward: -87.8500,                 loss: 0.1782
env0_second_0:                 episode reward: 87.8500,                 loss: 0.1241
env1_first_0:                 episode reward: -85.2500,                 loss: nan
env1_second_0:                 episode reward: 85.2500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 249.7,                last time consumption/overall running time: 63.5047s / 44685.2073 s
env0_first_0:                 episode reward: -82.5500,                 loss: 0.1672
env0_second_0:                 episode reward: 82.5500,                 loss: 0.1291
env1_first_0:                 episode reward: -87.6000,                 loss: nan
env1_second_0:                 episode reward: 87.6000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 257.55,                last time consumption/overall running time: 67.7796s / 44752.9869 s
env0_first_0:                 episode reward: -68.5000,                 loss: 0.1836
env0_second_0:                 episode reward: 68.5000,                 loss: 0.1262
env1_first_0:                 episode reward: -83.5500,                 loss: nan
env1_second_0:                 episode reward: 83.5500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 272.05,                last time consumption/overall running time: 71.2005s / 44824.1874 s
env0_first_0:                 episode reward: -81.6500,                 loss: 0.1834
env0_second_0:                 episode reward: 81.6500,                 loss: 0.1266
env1_first_0:                 episode reward: -79.9500,                 loss: nan
env1_second_0:                 episode reward: 79.9500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 258.15,                last time consumption/overall running time: 66.3842s / 44890.5716 s
env0_first_0:                 episode reward: -85.3500,                 loss: 0.1860
env0_second_0:                 episode reward: 85.3500,                 loss: 0.1235
env1_first_0:                 episode reward: -82.4500,                 loss: nan
env1_second_0:                 episode reward: 82.4500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 260.85,                last time consumption/overall running time: 68.8922s / 44959.4638 s
env0_first_0:                 episode reward: -77.9000,                 loss: 0.1802
env0_second_0:                 episode reward: 77.9000,                 loss: 0.1311
env1_first_0:                 episode reward: -82.5000,                 loss: nan
env1_second_0:                 episode reward: 82.5000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 248.2,                last time consumption/overall running time: 63.9159s / 45023.3797 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.2112
env0_second_0:                 episode reward: 86.4000,                 loss: 0.1313
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 270.25,                last time consumption/overall running time: 70.8768s / 45094.2566 s
env0_first_0:                 episode reward: -89.9000,                 loss: 0.1915
env0_second_0:                 episode reward: 89.9000,                 loss: 0.1312
env1_first_0:                 episode reward: -83.6500,                 loss: nan
env1_second_0:                 episode reward: 83.6500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 248.8,                last time consumption/overall running time: 65.5492s / 45159.8057 s
env0_first_0:                 episode reward: -86.1500,                 loss: 0.2237
env0_second_0:                 episode reward: 86.1500,                 loss: 0.1399
env1_first_0:                 episode reward: -91.8500,                 loss: nan
env1_second_0:                 episode reward: 91.8500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 235.8,                last time consumption/overall running time: 63.2116s / 45223.0173 s
env0_first_0:                 episode reward: -95.2000,                 loss: 0.2243
env0_second_0:                 episode reward: 95.2000,                 loss: 0.1484
env1_first_0:                 episode reward: -82.6000,                 loss: nan
env1_second_0:                 episode reward: 82.6000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 245.45,                last time consumption/overall running time: 62.5824s / 45285.5998 s
env0_first_0:                 episode reward: -89.8000,                 loss: 0.2268
env0_second_0:                 episode reward: 89.8000,                 loss: 0.1475
env1_first_0:                 episode reward: -88.7000,                 loss: nan
env1_second_0:                 episode reward: 88.7000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 253.2,                last time consumption/overall running time: 66.1146s / 45351.7143 s
env0_first_0:                 episode reward: -89.6500,                 loss: 0.2343
env0_second_0:                 episode reward: 89.6500,                 loss: 0.1458
env1_first_0:                 episode reward: -76.8000,                 loss: nan
env1_second_0:                 episode reward: 76.8000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 236.45,                last time consumption/overall running time: 62.6216s / 45414.3360 s
env0_first_0:                 episode reward: -85.7500,                 loss: 0.2276
env0_second_0:                 episode reward: 85.7500,                 loss: 0.1520
env1_first_0:                 episode reward: -89.0500,                 loss: nan
env1_second_0:                 episode reward: 89.0500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 251.7,                last time consumption/overall running time: 66.2452s / 45480.5812 s
env0_first_0:                 episode reward: -85.9000,                 loss: 0.2370
env0_second_0:                 episode reward: 85.9000,                 loss: 0.1596
env1_first_0:                 episode reward: -83.8000,                 loss: nan
env1_second_0:                 episode reward: 83.8000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 254.5,                last time consumption/overall running time: 67.9861s / 45548.5673 s
env0_first_0:                 episode reward: -91.5500,                 loss: 0.2507
env0_second_0:                 episode reward: 91.5500,                 loss: 0.1548
env1_first_0:                 episode reward: -90.2000,                 loss: nan
env1_second_0:                 episode reward: 90.2000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 247.85,                last time consumption/overall running time: 63.9474s / 45612.5147 s
env0_first_0:                 episode reward: -80.6500,                 loss: 0.2609
env0_second_0:                 episode reward: 80.6500,                 loss: 0.1539
env1_first_0:                 episode reward: -80.2500,                 loss: nan
env1_second_0:                 episode reward: 80.2500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 251.5,                last time consumption/overall running time: 64.7037s / 45677.2184 s
env0_first_0:                 episode reward: -85.3500,                 loss: 0.2861
env0_second_0:                 episode reward: 85.3500,                 loss: 0.1601
env1_first_0:                 episode reward: -81.2000,                 loss: nan
env1_second_0:                 episode reward: 81.2000,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 237.1,                last time consumption/overall running time: 61.5114s / 45738.7298 s
env0_first_0:                 episode reward: -85.0000,                 loss: 0.2613
env0_second_0:                 episode reward: 85.0000,                 loss: 0.1675
env1_first_0:                 episode reward: -81.5500,                 loss: nan
env1_second_0:                 episode reward: 81.5500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 236.0,                last time consumption/overall running time: 61.5431s / 45800.2729 s
env0_first_0:                 episode reward: -85.4500,                 loss: 0.2687
env0_second_0:                 episode reward: 85.4500,                 loss: 0.1560
env1_first_0:                 episode reward: -87.2500,                 loss: nan
env1_second_0:                 episode reward: 87.2500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 247.7,                last time consumption/overall running time: 61.8553s / 45862.1282 s
env0_first_0:                 episode reward: -82.8000,                 loss: 0.2645
env0_second_0:                 episode reward: 82.8000,                 loss: 0.1521
env1_first_0:                 episode reward: -86.5500,                 loss: nan
env1_second_0:                 episode reward: 86.5500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 263.05,                last time consumption/overall running time: 66.3717s / 45928.4999 s
env0_first_0:                 episode reward: -86.6500,                 loss: 0.2743
env0_second_0:                 episode reward: 86.6500,                 loss: 0.1657
env1_first_0:                 episode reward: -80.9000,                 loss: nan
env1_second_0:                 episode reward: 80.9000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 293.55,                last time consumption/overall running time: 75.8040s / 46004.3039 s
env0_first_0:                 episode reward: -80.2500,                 loss: 0.2593
env0_second_0:                 episode reward: 80.2500,                 loss: 0.1631
env1_first_0:                 episode reward: -72.4000,                 loss: nan
env1_second_0:                 episode reward: 72.4000,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 250.3,                last time consumption/overall running time: 65.2590s / 46069.5629 s
env0_first_0:                 episode reward: -94.1500,                 loss: 0.2722
env0_second_0:                 episode reward: 94.1500,                 loss: 0.1639
env1_first_0:                 episode reward: -63.9000,                 loss: nan
env1_second_0:                 episode reward: 63.9000,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 248.5,                last time consumption/overall running time: 65.4321s / 46134.9950 s
env0_first_0:                 episode reward: -83.2000,                 loss: 0.2626
env0_second_0:                 episode reward: 83.2000,                 loss: 0.1654
env1_first_0:                 episode reward: -82.9500,                 loss: nan
env1_second_0:                 episode reward: 82.9500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 249.05,                last time consumption/overall running time: 64.8024s / 46199.7974 s
env0_first_0:                 episode reward: -83.6000,                 loss: 0.2824
env0_second_0:                 episode reward: 83.6000,                 loss: 0.1516
env1_first_0:                 episode reward: -90.4500,                 loss: nan
env1_second_0:                 episode reward: 90.4500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 233.25,                last time consumption/overall running time: 60.4717s / 46260.2691 s
env0_first_0:                 episode reward: -91.7000,                 loss: 0.2773
env0_second_0:                 episode reward: 91.7000,                 loss: 0.1486
env1_first_0:                 episode reward: -85.0500,                 loss: nan
env1_second_0:                 episode reward: 85.0500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 229.65,                last time consumption/overall running time: 61.0277s / 46321.2968 s
env0_first_0:                 episode reward: -82.6500,                 loss: 0.2726
env0_second_0:                 episode reward: 82.6500,                 loss: 0.1574
env1_first_0:                 episode reward: -92.7000,                 loss: nan
env1_second_0:                 episode reward: 92.7000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 237.85,                last time consumption/overall running time: 61.9959s / 46383.2927 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.2759
env0_second_0:                 episode reward: 86.4500,                 loss: 0.1567
env1_first_0:                 episode reward: -87.3000,                 loss: nan
env1_second_0:                 episode reward: 87.3000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 239.45,                last time consumption/overall running time: 63.1204s / 46446.4131 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.2599
env0_second_0:                 episode reward: 89.3000,                 loss: 0.1605
env1_first_0:                 episode reward: -82.8000,                 loss: nan
env1_second_0:                 episode reward: 82.8000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 258.75,                last time consumption/overall running time: 68.0542s / 46514.4673 s
env0_first_0:                 episode reward: -88.3000,                 loss: 0.2702
env0_second_0:                 episode reward: 88.3000,                 loss: 0.1674
env1_first_0:                 episode reward: -86.0500,                 loss: nan
env1_second_0:                 episode reward: 86.0500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 242.9,                last time consumption/overall running time: 62.5288s / 46576.9962 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.2848
env0_second_0:                 episode reward: 88.5500,                 loss: 0.1786
env1_first_0:                 episode reward: -87.7000,                 loss: nan
env1_second_0:                 episode reward: 87.7000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 238.8,                last time consumption/overall running time: 61.4740s / 46638.4701 s
env0_first_0:                 episode reward: -93.1000,                 loss: 0.2746
env0_second_0:                 episode reward: 93.1000,                 loss: 0.1930
env1_first_0:                 episode reward: -79.2500,                 loss: nan
env1_second_0:                 episode reward: 79.2500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 240.4,                last time consumption/overall running time: 62.6864s / 46701.1566 s
env0_first_0:                 episode reward: -89.9500,                 loss: 0.2713
env0_second_0:                 episode reward: 89.9500,                 loss: 0.1980
env1_first_0:                 episode reward: -90.4000,                 loss: nan
env1_second_0:                 episode reward: 90.4000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 238.8,                last time consumption/overall running time: 60.4056s / 46761.5622 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.2581
env0_second_0:                 episode reward: 89.3000,                 loss: 0.1928
env1_first_0:                 episode reward: -92.0500,                 loss: nan
env1_second_0:                 episode reward: 92.0500,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 239.15,                last time consumption/overall running time: 61.6390s / 46823.2011 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.2570
env0_second_0:                 episode reward: 91.6500,                 loss: 0.1859
env1_first_0:                 episode reward: -87.1500,                 loss: nan
env1_second_0:                 episode reward: 87.1500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 233.8,                last time consumption/overall running time: 59.8308s / 46883.0320 s
env0_first_0:                 episode reward: -90.0000,                 loss: 0.2688
env0_second_0:                 episode reward: 90.0000,                 loss: 0.1667
env1_first_0:                 episode reward: -95.3000,                 loss: nan
env1_second_0:                 episode reward: 95.3000,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 238.35,                last time consumption/overall running time: 62.8983s / 46945.9303 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.2779
env0_second_0:                 episode reward: 85.8500,                 loss: 0.1710
env1_first_0:                 episode reward: -94.7000,                 loss: nan
env1_second_0:                 episode reward: 94.7000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 228.45,                last time consumption/overall running time: 59.2661s / 47005.1965 s
env0_first_0:                 episode reward: -92.3500,                 loss: 0.2716
env0_second_0:                 episode reward: 92.3500,                 loss: 0.1787
env1_first_0:                 episode reward: -93.7500,                 loss: nan
env1_second_0:                 episode reward: 93.7500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 241.6,                last time consumption/overall running time: 62.3211s / 47067.5176 s
env0_first_0:                 episode reward: -90.2500,                 loss: 0.2658
env0_second_0:                 episode reward: 90.2500,                 loss: 0.1984
env1_first_0:                 episode reward: -88.9500,                 loss: nan
env1_second_0:                 episode reward: 88.9500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 231.5,                last time consumption/overall running time: 60.8760s / 47128.3936 s
env0_first_0:                 episode reward: -97.6000,                 loss: 0.2671
env0_second_0:                 episode reward: 97.6000,                 loss: 0.1770
env1_first_0:                 episode reward: -86.6000,                 loss: nan
env1_second_0:                 episode reward: 86.6000,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 225.3,                last time consumption/overall running time: 58.3019s / 47186.6954 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.2667
env0_second_0:                 episode reward: 93.0500,                 loss: 0.1667
env1_first_0:                 episode reward: -91.0500,                 loss: nan
env1_second_0:                 episode reward: 91.0500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 225.9,                last time consumption/overall running time: 58.6217s / 47245.3172 s
env0_first_0:                 episode reward: -91.5500,                 loss: 0.2653
env0_second_0:                 episode reward: 91.5500,                 loss: 0.1661
env1_first_0:                 episode reward: -92.9000,                 loss: nan
env1_second_0:                 episode reward: 92.9000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 238.75,                last time consumption/overall running time: 60.3549s / 47305.6721 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.2431
env0_second_0:                 episode reward: 86.4500,                 loss: 0.1571
env1_first_0:                 episode reward: -94.7000,                 loss: nan
env1_second_0:                 episode reward: 94.7000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 225.95,                last time consumption/overall running time: 57.2254s / 47362.8975 s
env0_first_0:                 episode reward: -92.8000,                 loss: 0.2713
env0_second_0:                 episode reward: 92.8000,                 loss: 0.1780
env1_first_0:                 episode reward: -97.1500,                 loss: nan
env1_second_0:                 episode reward: 97.1500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 224.55,                last time consumption/overall running time: 56.6205s / 47419.5180 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.2413
env0_second_0:                 episode reward: 93.5500,                 loss: 0.1618
env1_first_0:                 episode reward: -94.8000,                 loss: nan
env1_second_0:                 episode reward: 94.8000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 227.85,                last time consumption/overall running time: 57.4924s / 47477.0104 s
env0_first_0:                 episode reward: -86.5500,                 loss: 0.2529
env0_second_0:                 episode reward: 86.5500,                 loss: 0.1446
env1_first_0:                 episode reward: -95.4000,                 loss: nan
env1_second_0:                 episode reward: 95.4000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 230.85,                last time consumption/overall running time: 60.5758s / 47537.5862 s
env0_first_0:                 episode reward: -87.6500,                 loss: 0.2760
env0_second_0:                 episode reward: 87.6500,                 loss: 0.1513
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 226.0,                last time consumption/overall running time: 56.8435s / 47594.4297 s
env0_first_0:                 episode reward: -95.1500,                 loss: 0.2583
env0_second_0:                 episode reward: 95.1500,                 loss: 0.1355
env1_first_0:                 episode reward: -84.9000,                 loss: nan
env1_second_0:                 episode reward: 84.9000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 237.35,                last time consumption/overall running time: 59.4945s / 47653.9243 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.2588
env0_second_0:                 episode reward: 93.3500,                 loss: 0.1434
env1_first_0:                 episode reward: -94.5500,                 loss: nan
env1_second_0:                 episode reward: 94.5500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 229.1,                last time consumption/overall running time: 59.0110s / 47712.9353 s
env0_first_0:                 episode reward: -91.6000,                 loss: 0.2645
env0_second_0:                 episode reward: 91.6000,                 loss: 0.1371
env1_first_0:                 episode reward: -88.8500,                 loss: nan
env1_second_0:                 episode reward: 88.8500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 228.8,                last time consumption/overall running time: 58.5001s / 47771.4353 s
env0_first_0:                 episode reward: -92.5000,                 loss: 0.2417
env0_second_0:                 episode reward: 92.5000,                 loss: 0.1476
env1_first_0:                 episode reward: -90.4500,                 loss: nan
env1_second_0:                 episode reward: 90.4500,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 223.15,                last time consumption/overall running time: 57.9668s / 47829.4021 s
env0_first_0:                 episode reward: -94.2000,                 loss: 0.2293
env0_second_0:                 episode reward: 94.2000,                 loss: 0.1485
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 230.0,                last time consumption/overall running time: 59.5063s / 47888.9084 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.2311
env0_second_0:                 episode reward: 92.4500,                 loss: 0.1502
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 218.6,                last time consumption/overall running time: 57.1689s / 47946.0773 s
env0_first_0:                 episode reward: -93.5000,                 loss: 0.2149
env0_second_0:                 episode reward: 93.5000,                 loss: 0.1528
env1_first_0:                 episode reward: -96.1000,                 loss: nan
env1_second_0:                 episode reward: 96.1000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 221.5,                last time consumption/overall running time: 55.9509s / 48002.0282 s
env0_first_0:                 episode reward: -85.1000,                 loss: 0.2206
env0_second_0:                 episode reward: 85.1000,                 loss: 0.1336
env1_first_0:                 episode reward: -95.1500,                 loss: nan
env1_second_0:                 episode reward: 95.1500,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 223.75,                last time consumption/overall running time: 58.3690s / 48060.3972 s
env0_first_0:                 episode reward: -91.0000,                 loss: 0.2250
env0_second_0:                 episode reward: 91.0000,                 loss: 0.1288
env1_first_0:                 episode reward: -92.8500,                 loss: nan
env1_second_0:                 episode reward: 92.8500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 231.15,                last time consumption/overall running time: 60.7168s / 48121.1140 s
env0_first_0:                 episode reward: -93.6000,                 loss: 0.2393
env0_second_0:                 episode reward: 93.6000,                 loss: 0.1415
env1_first_0:                 episode reward: -89.0500,                 loss: nan
env1_second_0:                 episode reward: 89.0500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 231.25,                last time consumption/overall running time: 58.8842s / 48179.9982 s
env0_first_0:                 episode reward: -95.4500,                 loss: 0.2279
env0_second_0:                 episode reward: 95.4500,                 loss: 0.1347
env1_first_0:                 episode reward: -91.8500,                 loss: nan
env1_second_0:                 episode reward: 91.8500,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 232.1,                last time consumption/overall running time: 59.9376s / 48239.9359 s
env0_first_0:                 episode reward: -96.9000,                 loss: 0.2450
env0_second_0:                 episode reward: 96.9000,                 loss: 0.1379
env1_first_0:                 episode reward: -83.9500,                 loss: nan
env1_second_0:                 episode reward: 83.9500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 220.95,                last time consumption/overall running time: 56.9183s / 48296.8542 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.2335
env0_second_0:                 episode reward: 88.6000,                 loss: 0.1427
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 231.4,                last time consumption/overall running time: 59.5580s / 48356.4122 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.2320
env0_second_0:                 episode reward: 91.6500,                 loss: 0.1307
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 245.2,                last time consumption/overall running time: 63.5387s / 48419.9509 s
env0_first_0:                 episode reward: -84.6500,                 loss: 0.2471
env0_second_0:                 episode reward: 84.6500,                 loss: 0.1390
env1_first_0:                 episode reward: -92.9000,                 loss: nan
env1_second_0:                 episode reward: 92.9000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 230.85,                last time consumption/overall running time: 61.1187s / 48481.0695 s
env0_first_0:                 episode reward: -90.6500,                 loss: 0.2484
env0_second_0:                 episode reward: 90.6500,                 loss: 0.1544
env1_first_0:                 episode reward: -89.5500,                 loss: nan
env1_second_0:                 episode reward: 89.5500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 245.9,                last time consumption/overall running time: 63.1574s / 48544.2269 s
env0_first_0:                 episode reward: -90.0000,                 loss: 0.2679
env0_second_0:                 episode reward: 90.0000,                 loss: 0.1429
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 225.9,                last time consumption/overall running time: 56.3457s / 48600.5726 s
env0_first_0:                 episode reward: -96.6000,                 loss: 0.2687
env0_second_0:                 episode reward: 96.6000,                 loss: 0.1545
env1_first_0:                 episode reward: -91.4000,                 loss: nan
env1_second_0:                 episode reward: 91.4000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 231.45,                last time consumption/overall running time: 60.0348s / 48660.6074 s
env0_first_0:                 episode reward: -92.7500,                 loss: 0.2884
env0_second_0:                 episode reward: 92.7500,                 loss: 0.1488
env1_first_0:                 episode reward: -85.1000,                 loss: nan
env1_second_0:                 episode reward: 85.1000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 232.5,                last time consumption/overall running time: 59.9511s / 48720.5585 s
env0_first_0:                 episode reward: -83.5500,                 loss: 0.2752
env0_second_0:                 episode reward: 83.5500,                 loss: 0.1441
env1_first_0:                 episode reward: -97.3000,                 loss: nan
env1_second_0:                 episode reward: 97.3000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 231.05,                last time consumption/overall running time: 57.2746s / 48777.8331 s
env0_first_0:                 episode reward: -89.9500,                 loss: 0.2843
env0_second_0:                 episode reward: 89.9500,                 loss: 0.1573
env1_first_0:                 episode reward: -96.7000,                 loss: nan
env1_second_0:                 episode reward: 96.7000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 217.15,                last time consumption/overall running time: 57.5280s / 48835.3611 s
env0_first_0:                 episode reward: -86.9000,                 loss: 0.2841
env0_second_0:                 episode reward: 86.9000,                 loss: 0.1476
env1_first_0:                 episode reward: -95.0500,                 loss: nan
env1_second_0:                 episode reward: 95.0500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 223.9,                last time consumption/overall running time: 58.0140s / 48893.3751 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.2767
env0_second_0:                 episode reward: 92.4000,                 loss: 0.1686
env1_first_0:                 episode reward: -93.5500,                 loss: nan
env1_second_0:                 episode reward: 93.5500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 230.15,                last time consumption/overall running time: 60.2937s / 48953.6688 s
env0_first_0:                 episode reward: -96.2500,                 loss: 0.2553
env0_second_0:                 episode reward: 96.2500,                 loss: 0.1516
env1_first_0:                 episode reward: -85.4500,                 loss: nan
env1_second_0:                 episode reward: 85.4500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 237.5,                last time consumption/overall running time: 61.5216s / 49015.1904 s
env0_first_0:                 episode reward: -86.7000,                 loss: 0.2601
env0_second_0:                 episode reward: 86.7000,                 loss: 0.1574
env1_first_0:                 episode reward: -93.8000,                 loss: nan
env1_second_0:                 episode reward: 93.8000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 222.75,                last time consumption/overall running time: 59.1792s / 49074.3696 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.2597
env0_second_0:                 episode reward: 90.3000,                 loss: 0.1501
env1_first_0:                 episode reward: -90.5000,                 loss: nan
env1_second_0:                 episode reward: 90.5000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 230.15,                last time consumption/overall running time: 60.2336s / 49134.6031 s
env0_first_0:                 episode reward: -88.0000,                 loss: 0.2553
env0_second_0:                 episode reward: 88.0000,                 loss: 0.1490
env1_first_0:                 episode reward: -94.4500,                 loss: nan
env1_second_0:                 episode reward: 94.4500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 231.2,                last time consumption/overall running time: 60.1512s / 49194.7544 s
env0_first_0:                 episode reward: -91.2500,                 loss: 0.2468
env0_second_0:                 episode reward: 91.2500,                 loss: 0.1517
env1_first_0:                 episode reward: -93.5500,                 loss: nan
env1_second_0:                 episode reward: 93.5500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 223.05,                last time consumption/overall running time: 58.6420s / 49253.3963 s
env0_first_0:                 episode reward: -91.5500,                 loss: 0.2362
env0_second_0:                 episode reward: 91.5500,                 loss: 0.1426
env1_first_0:                 episode reward: -95.7500,                 loss: nan
env1_second_0:                 episode reward: 95.7500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 225.1,                last time consumption/overall running time: 59.1834s / 49312.5797 s
env0_first_0:                 episode reward: -95.4000,                 loss: 0.2510
env0_second_0:                 episode reward: 95.4000,                 loss: 0.1389
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 228.85,                last time consumption/overall running time: 60.2312s / 49372.8109 s
env0_first_0:                 episode reward: -93.6000,                 loss: 0.2338
env0_second_0:                 episode reward: 93.6000,                 loss: 0.1400
env1_first_0:                 episode reward: -95.1000,                 loss: nan
env1_second_0:                 episode reward: 95.1000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 233.65,                last time consumption/overall running time: 59.7869s / 49432.5978 s
env0_first_0:                 episode reward: -89.3500,                 loss: 0.2404
env0_second_0:                 episode reward: 89.3500,                 loss: 0.1320
env1_first_0:                 episode reward: -90.5500,                 loss: nan
env1_second_0:                 episode reward: 90.5500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 222.45,                last time consumption/overall running time: 58.3977s / 49490.9955 s
env0_first_0:                 episode reward: -95.9000,                 loss: 0.2483
env0_second_0:                 episode reward: 95.9000,                 loss: 0.1295
env1_first_0:                 episode reward: -94.5500,                 loss: nan
env1_second_0:                 episode reward: 94.5500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 231.9,                last time consumption/overall running time: 60.0805s / 49551.0761 s
env0_first_0:                 episode reward: -88.7000,                 loss: 0.2537
env0_second_0:                 episode reward: 88.7000,                 loss: 0.1434
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 232.1,                last time consumption/overall running time: 60.7630s / 49611.8391 s
env0_first_0:                 episode reward: -92.8000,                 loss: 0.2509
env0_second_0:                 episode reward: 92.8000,                 loss: 0.1471
env1_first_0:                 episode reward: -92.9500,                 loss: nan
env1_second_0:                 episode reward: 92.9500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 223.85,                last time consumption/overall running time: 57.2033s / 49669.0423 s
env0_first_0:                 episode reward: -90.8000,                 loss: 0.2437
env0_second_0:                 episode reward: 90.8000,                 loss: 0.1393
env1_first_0:                 episode reward: -97.5500,                 loss: nan
env1_second_0:                 episode reward: 97.5500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 224.5,                last time consumption/overall running time: 58.6897s / 49727.7321 s
env0_first_0:                 episode reward: -96.2000,                 loss: 0.2214
env0_second_0:                 episode reward: 96.2000,                 loss: 0.1440
env1_first_0:                 episode reward: -95.4500,                 loss: nan
env1_second_0:                 episode reward: 95.4500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 219.5,                last time consumption/overall running time: 57.6385s / 49785.3706 s
env0_first_0:                 episode reward: -85.6500,                 loss: 0.2247
env0_second_0:                 episode reward: 85.6500,                 loss: 0.1384
env1_first_0:                 episode reward: -96.0000,                 loss: nan
env1_second_0:                 episode reward: 96.0000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 222.3,                last time consumption/overall running time: 59.0780s / 49844.4486 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.2227
env0_second_0:                 episode reward: 88.7500,                 loss: 0.1273
env1_first_0:                 episode reward: -93.6000,                 loss: nan
env1_second_0:                 episode reward: 93.6000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 219.65,                last time consumption/overall running time: 57.8381s / 49902.2867 s
env0_first_0:                 episode reward: -93.8500,                 loss: 0.2239
env0_second_0:                 episode reward: 93.8500,                 loss: 0.1299
env1_first_0:                 episode reward: -94.4500,                 loss: nan
env1_second_0:                 episode reward: 94.4500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 221.5,                last time consumption/overall running time: 57.0449s / 49959.3315 s
env0_first_0:                 episode reward: -97.8000,                 loss: 0.2291
env0_second_0:                 episode reward: 97.8000,                 loss: 0.1330
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 221.1,                last time consumption/overall running time: 58.4688s / 50017.8003 s
env0_first_0:                 episode reward: -90.2000,                 loss: 0.2136
env0_second_0:                 episode reward: 90.2000,                 loss: 0.1368
env1_first_0:                 episode reward: -92.1500,                 loss: nan
env1_second_0:                 episode reward: 92.1500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 220.4,                last time consumption/overall running time: 58.5967s / 50076.3970 s
env0_first_0:                 episode reward: -90.5500,                 loss: 0.2362
env0_second_0:                 episode reward: 90.5500,                 loss: 0.1391
env1_first_0:                 episode reward: -95.2500,                 loss: nan
env1_second_0:                 episode reward: 95.2500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 225.8,                last time consumption/overall running time: 59.8621s / 50136.2591 s
env0_first_0:                 episode reward: -96.7000,                 loss: 0.2151
env0_second_0:                 episode reward: 96.7000,                 loss: 0.1415
env1_first_0:                 episode reward: -94.6500,                 loss: nan
env1_second_0:                 episode reward: 94.6500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 223.55,                last time consumption/overall running time: 59.6036s / 50195.8628 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.2146
env0_second_0:                 episode reward: 90.3000,                 loss: 0.1305
env1_first_0:                 episode reward: -96.5000,                 loss: nan
env1_second_0:                 episode reward: 96.5000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 233.45,                last time consumption/overall running time: 61.9456s / 50257.8083 s
env0_first_0:                 episode reward: -86.3500,                 loss: 0.2074
env0_second_0:                 episode reward: 86.3500,                 loss: 0.1319
env1_first_0:                 episode reward: -84.7000,                 loss: nan
env1_second_0:                 episode reward: 84.7000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 223.9,                last time consumption/overall running time: 58.6446s / 50316.4529 s
env0_first_0:                 episode reward: -91.5000,                 loss: 0.2223
env0_second_0:                 episode reward: 91.5000,                 loss: 0.1328
env1_first_0:                 episode reward: -90.6000,                 loss: nan
env1_second_0:                 episode reward: 90.6000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 225.35,                last time consumption/overall running time: 58.4478s / 50374.9007 s
env0_first_0:                 episode reward: -94.0500,                 loss: 0.2065
env0_second_0:                 episode reward: 94.0500,                 loss: 0.1210
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 229.5,                last time consumption/overall running time: 59.8495s / 50434.7502 s
env0_first_0:                 episode reward: -94.1000,                 loss: 0.2068
env0_second_0:                 episode reward: 94.1000,                 loss: 0.1242
env1_first_0:                 episode reward: -87.6000,                 loss: nan
env1_second_0:                 episode reward: 87.6000,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 216.95,                last time consumption/overall running time: 57.6542s / 50492.4043 s
env0_first_0:                 episode reward: -91.8000,                 loss: 0.2272
env0_second_0:                 episode reward: 91.8000,                 loss: 0.1310
env1_first_0:                 episode reward: -95.9500,                 loss: nan
env1_second_0:                 episode reward: 95.9500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 220.5,                last time consumption/overall running time: 56.5796s / 50548.9839 s
env0_first_0:                 episode reward: -95.4000,                 loss: 0.2222
env0_second_0:                 episode reward: 95.4000,                 loss: 0.1283
env1_first_0:                 episode reward: -88.8000,                 loss: nan
env1_second_0:                 episode reward: 88.8000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 219.5,                last time consumption/overall running time: 56.4404s / 50605.4244 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.2410
env0_second_0:                 episode reward: 92.9500,                 loss: 0.1222
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 219.4,                last time consumption/overall running time: 57.2261s / 50662.6504 s
env0_first_0:                 episode reward: -93.9500,                 loss: 0.2273
env0_second_0:                 episode reward: 93.9500,                 loss: 0.1323
env1_first_0:                 episode reward: -92.2000,                 loss: nan
env1_second_0:                 episode reward: 92.2000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 227.2,                last time consumption/overall running time: 58.9980s / 50721.6484 s
env0_first_0:                 episode reward: -88.9500,                 loss: 0.2014
env0_second_0:                 episode reward: 88.9500,                 loss: 0.1230
env1_first_0:                 episode reward: -96.4500,                 loss: nan
env1_second_0:                 episode reward: 96.4500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 219.8,                last time consumption/overall running time: 56.4926s / 50778.1410 s
env0_first_0:                 episode reward: -93.8000,                 loss: 0.2160
env0_second_0:                 episode reward: 93.8000,                 loss: 0.1281
env1_first_0:                 episode reward: -95.9000,                 loss: nan
env1_second_0:                 episode reward: 95.9000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 222.9,                last time consumption/overall running time: 58.1951s / 50836.3361 s
env0_first_0:                 episode reward: -90.8000,                 loss: 0.2204
env0_second_0:                 episode reward: 90.8000,                 loss: 0.1266
env1_first_0:                 episode reward: -94.0500,                 loss: nan
env1_second_0:                 episode reward: 94.0500,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 229.75,                last time consumption/overall running time: 58.8351s / 50895.1711 s
env0_first_0:                 episode reward: -88.5000,                 loss: 0.2255
env0_second_0:                 episode reward: 88.5000,                 loss: 0.1180
env1_first_0:                 episode reward: -93.6000,                 loss: nan
env1_second_0:                 episode reward: 93.6000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 228.45,                last time consumption/overall running time: 59.2185s / 50954.3897 s
env0_first_0:                 episode reward: -89.2500,                 loss: 0.2203
env0_second_0:                 episode reward: 89.2500,                 loss: 0.1253
env1_first_0:                 episode reward: -83.3000,                 loss: nan
env1_second_0:                 episode reward: 83.3000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 223.05,                last time consumption/overall running time: 58.4965s / 51012.8862 s
env0_first_0:                 episode reward: -87.9000,                 loss: 0.2193
env0_second_0:                 episode reward: 87.9000,                 loss: 0.1187
env1_first_0:                 episode reward: -91.7000,                 loss: nan
env1_second_0:                 episode reward: 91.7000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 224.4,                last time consumption/overall running time: 57.9540s / 51070.8402 s
env0_first_0:                 episode reward: -90.9500,                 loss: 0.1945
env0_second_0:                 episode reward: 90.9500,                 loss: 0.1153
env1_first_0:                 episode reward: -88.9500,                 loss: nan
env1_second_0:                 episode reward: 88.9500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 223.65,                last time consumption/overall running time: 56.9084s / 51127.7487 s
env0_first_0:                 episode reward: -92.0500,                 loss: 0.1803
env0_second_0:                 episode reward: 92.0500,                 loss: 0.1067
env1_first_0:                 episode reward: -94.0500,                 loss: nan
env1_second_0:                 episode reward: 94.0500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 228.45,                last time consumption/overall running time: 59.2051s / 51186.9538 s
env0_first_0:                 episode reward: -86.3500,                 loss: 0.1988
env0_second_0:                 episode reward: 86.3500,                 loss: 0.1045
env1_first_0:                 episode reward: -98.6500,                 loss: nan
env1_second_0:                 episode reward: 98.6500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 230.55,                last time consumption/overall running time: 59.2667s / 51246.2205 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.1915
env0_second_0:                 episode reward: 88.5500,                 loss: 0.1074
env1_first_0:                 episode reward: -92.8500,                 loss: nan
env1_second_0:                 episode reward: 92.8500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 225.95,                last time consumption/overall running time: 59.1137s / 51305.3342 s
env0_first_0:                 episode reward: -87.0000,                 loss: 0.1967
env0_second_0:                 episode reward: 87.0000,                 loss: 0.1022
env1_first_0:                 episode reward: -87.0500,                 loss: nan
env1_second_0:                 episode reward: 87.0500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 229.15,                last time consumption/overall running time: 59.6741s / 51365.0083 s
env0_first_0:                 episode reward: -90.6000,                 loss: 0.2060
env0_second_0:                 episode reward: 90.6000,                 loss: 0.1064
env1_first_0:                 episode reward: -91.4500,                 loss: nan
env1_second_0:                 episode reward: 91.4500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 219.6,                last time consumption/overall running time: 57.0807s / 51422.0890 s
env0_first_0:                 episode reward: -88.2000,                 loss: 0.1924
env0_second_0:                 episode reward: 88.2000,                 loss: 0.1036
env1_first_0:                 episode reward: -82.5500,                 loss: nan
env1_second_0:                 episode reward: 82.5500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 229.65,                last time consumption/overall running time: 59.7783s / 51481.8673 s
env0_first_0:                 episode reward: -96.4500,                 loss: 0.2095
env0_second_0:                 episode reward: 96.4500,                 loss: 0.1121
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 221.75,                last time consumption/overall running time: 58.1243s / 51539.9916 s
env0_first_0:                 episode reward: -93.2500,                 loss: 0.2027
env0_second_0:                 episode reward: 93.2500,                 loss: 0.1115
env1_first_0:                 episode reward: -95.0000,                 loss: nan
env1_second_0:                 episode reward: 95.0000,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 221.15,                last time consumption/overall running time: 56.8962s / 51596.8878 s
env0_first_0:                 episode reward: -91.9500,                 loss: 0.2029
env0_second_0:                 episode reward: 91.9500,                 loss: 0.1146
env1_first_0:                 episode reward: -90.3500,                 loss: nan
env1_second_0:                 episode reward: 90.3500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 219.5,                last time consumption/overall running time: 57.4656s / 51654.3534 s
env0_first_0:                 episode reward: -92.8000,                 loss: 0.2021
env0_second_0:                 episode reward: 92.8000,                 loss: 0.1185
env1_first_0:                 episode reward: -96.0000,                 loss: nan
env1_second_0:                 episode reward: 96.0000,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 229.7,                last time consumption/overall running time: 58.8092s / 51713.1626 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.2157
env0_second_0:                 episode reward: 92.9500,                 loss: 0.1192
env1_first_0:                 episode reward: -83.1000,                 loss: nan
env1_second_0:                 episode reward: 83.1000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 222.75,                last time consumption/overall running time: 57.6518s / 51770.8144 s
env0_first_0:                 episode reward: -90.5500,                 loss: 0.2146
env0_second_0:                 episode reward: 90.5500,                 loss: 0.1133
env1_first_0:                 episode reward: -95.6500,                 loss: nan
env1_second_0:                 episode reward: 95.6500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 228.2,                last time consumption/overall running time: 59.5090s / 51830.3234 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.2158
env0_second_0:                 episode reward: 92.4000,                 loss: 0.1072
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 220.6,                last time consumption/overall running time: 57.0217s / 51887.3451 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.2245
env0_second_0:                 episode reward: 88.5500,                 loss: 0.0986
env1_first_0:                 episode reward: -96.2500,                 loss: nan
env1_second_0:                 episode reward: 96.2500,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 218.55,                last time consumption/overall running time: 56.8126s / 51944.1577 s
env0_first_0:                 episode reward: -96.6000,                 loss: 0.2095
env0_second_0:                 episode reward: 96.6000,                 loss: 0.1064
env1_first_0:                 episode reward: -90.4500,                 loss: nan
env1_second_0:                 episode reward: 90.4500,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 216.2,                last time consumption/overall running time: 58.2277s / 52002.3854 s
env0_first_0:                 episode reward: -97.4500,                 loss: 0.2090
env0_second_0:                 episode reward: 97.4500,                 loss: 0.1058
env1_first_0:                 episode reward: -90.5500,                 loss: nan
env1_second_0:                 episode reward: 90.5500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 219.0,                last time consumption/overall running time: 57.5088s / 52059.8942 s
env0_first_0:                 episode reward: -94.5000,                 loss: 0.2212
env0_second_0:                 episode reward: 94.5000,                 loss: 0.1032
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 220.3,                last time consumption/overall running time: 56.9663s / 52116.8605 s
env0_first_0:                 episode reward: -93.4500,                 loss: 0.2132
env0_second_0:                 episode reward: 93.4500,                 loss: 0.1040
env1_first_0:                 episode reward: -94.4500,                 loss: nan
env1_second_0:                 episode reward: 94.4500,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 219.4,                last time consumption/overall running time: 54.8038s / 52171.6643 s
env0_first_0:                 episode reward: -87.7500,                 loss: 0.2364
env0_second_0:                 episode reward: 87.7500,                 loss: 0.1088
env1_first_0:                 episode reward: -90.4000,                 loss: nan
env1_second_0:                 episode reward: 90.4000,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 227.15,                last time consumption/overall running time: 57.8752s / 52229.5394 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.2444
env0_second_0:                 episode reward: 93.0500,                 loss: 0.1146
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 222.3,                last time consumption/overall running time: 57.6393s / 52287.1787 s
env0_first_0:                 episode reward: -91.8000,                 loss: 0.2400
env0_second_0:                 episode reward: 91.8000,                 loss: 0.1098
env1_first_0:                 episode reward: -86.7500,                 loss: nan
env1_second_0:                 episode reward: 86.7500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 227.85,                last time consumption/overall running time: 59.8632s / 52347.0419 s
env0_first_0:                 episode reward: -92.7500,                 loss: 0.2483
env0_second_0:                 episode reward: 92.7500,                 loss: 0.1131
env1_first_0:                 episode reward: -91.8000,                 loss: nan
env1_second_0:                 episode reward: 91.8000,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 218.8,                last time consumption/overall running time: 57.2785s / 52404.3205 s
env0_first_0:                 episode reward: -90.9500,                 loss: 0.2202
env0_second_0:                 episode reward: 90.9500,                 loss: 0.1118
env1_first_0:                 episode reward: -96.6000,                 loss: nan
env1_second_0:                 episode reward: 96.6000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 222.45,                last time consumption/overall running time: 57.5452s / 52461.8656 s
env0_first_0:                 episode reward: -88.1000,                 loss: 0.2197
env0_second_0:                 episode reward: 88.1000,                 loss: 0.1126
env1_first_0:                 episode reward: -93.8500,                 loss: nan
env1_second_0:                 episode reward: 93.8500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 228.35,                last time consumption/overall running time: 60.6630s / 52522.5287 s
env0_first_0:                 episode reward: -94.1500,                 loss: 0.2219
env0_second_0:                 episode reward: 94.1500,                 loss: 0.1155
env1_first_0:                 episode reward: -92.0000,                 loss: nan
env1_second_0:                 episode reward: 92.0000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 224.9,                last time consumption/overall running time: 59.3701s / 52581.8988 s
env0_first_0:                 episode reward: -86.5000,                 loss: 0.2102
env0_second_0:                 episode reward: 86.5000,                 loss: 0.1207
env1_first_0:                 episode reward: -93.0000,                 loss: nan
env1_second_0:                 episode reward: 93.0000,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 228.75,                last time consumption/overall running time: 59.5370s / 52641.4358 s
env0_first_0:                 episode reward: -83.2000,                 loss: 0.2307
env0_second_0:                 episode reward: 83.2000,                 loss: 0.1279
env1_first_0:                 episode reward: -95.8000,                 loss: nan
env1_second_0:                 episode reward: 95.8000,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 222.15,                last time consumption/overall running time: 58.1162s / 52699.5520 s
env0_first_0:                 episode reward: -89.2500,                 loss: 0.2478
env0_second_0:                 episode reward: 89.2500,                 loss: 0.1253
env1_first_0:                 episode reward: -86.6000,                 loss: nan
env1_second_0:                 episode reward: 86.6000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 221.6,                last time consumption/overall running time: 57.4968s / 52757.0488 s
env0_first_0:                 episode reward: -91.6000,                 loss: 0.2514
env0_second_0:                 episode reward: 91.6000,                 loss: 0.1195
env1_first_0:                 episode reward: -92.0000,                 loss: nan
env1_second_0:                 episode reward: 92.0000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 225.85,                last time consumption/overall running time: 57.9705s / 52815.0193 s
env0_first_0:                 episode reward: -92.1500,                 loss: 0.2592
env0_second_0:                 episode reward: 92.1500,                 loss: 0.1178
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 220.05,                last time consumption/overall running time: 56.7342s / 52871.7536 s
env0_first_0:                 episode reward: -93.8500,                 loss: 0.2466
env0_second_0:                 episode reward: 93.8500,                 loss: 0.1101
env1_first_0:                 episode reward: -94.7500,                 loss: nan
env1_second_0:                 episode reward: 94.7500,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 218.65,                last time consumption/overall running time: 56.6161s / 52928.3696 s
env0_first_0:                 episode reward: -89.8000,                 loss: 0.2748
env0_second_0:                 episode reward: 89.8000,                 loss: 0.1175
env1_first_0:                 episode reward: -83.2500,                 loss: nan
env1_second_0:                 episode reward: 83.2500,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 234.25,                last time consumption/overall running time: 60.3718s / 52988.7415 s
env0_first_0:                 episode reward: -95.1000,                 loss: 0.2550
env0_second_0:                 episode reward: 95.1000,                 loss: 0.1068
env1_first_0:                 episode reward: -93.0500,                 loss: nan
env1_second_0:                 episode reward: 93.0500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 221.15,                last time consumption/overall running time: 58.4464s / 53047.1879 s
env0_first_0:                 episode reward: -83.3500,                 loss: 0.2519
env0_second_0:                 episode reward: 83.3500,                 loss: 0.1087
env1_first_0:                 episode reward: -95.6500,                 loss: nan
env1_second_0:                 episode reward: 95.6500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 219.3,                last time consumption/overall running time: 56.6466s / 53103.8345 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.2424
env0_second_0:                 episode reward: 92.6500,                 loss: 0.1115
env1_first_0:                 episode reward: -91.8000,                 loss: nan
env1_second_0:                 episode reward: 91.8000,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 225.5,                last time consumption/overall running time: 58.0935s / 53161.9280 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.2543
env0_second_0:                 episode reward: 89.6000,                 loss: 0.1146
env1_first_0:                 episode reward: -92.5000,                 loss: nan
env1_second_0:                 episode reward: 92.5000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 220.3,                last time consumption/overall running time: 55.8085s / 53217.7364 s
env0_first_0:                 episode reward: -91.8500,                 loss: 0.2537
env0_second_0:                 episode reward: 91.8500,                 loss: 0.1140
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 223.95,                last time consumption/overall running time: 57.4690s / 53275.2054 s
env0_first_0:                 episode reward: -88.5000,                 loss: 0.2563
env0_second_0:                 episode reward: 88.5000,                 loss: 0.1135
env1_first_0:                 episode reward: -89.6000,                 loss: nan
env1_second_0:                 episode reward: 89.6000,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 221.95,                last time consumption/overall running time: 57.7970s / 53333.0024 s
env0_first_0:                 episode reward: -89.2500,                 loss: 0.2567
env0_second_0:                 episode reward: 89.2500,                 loss: 0.1158
env1_first_0:                 episode reward: -95.3500,                 loss: nan
env1_second_0:                 episode reward: 95.3500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 216.45,                last time consumption/overall running time: 57.5059s / 53390.5083 s
env0_first_0:                 episode reward: -95.2000,                 loss: 0.2350
env0_second_0:                 episode reward: 95.2000,                 loss: 0.1107
env1_first_0:                 episode reward: -91.0500,                 loss: nan
env1_second_0:                 episode reward: 91.0500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 231.05,                last time consumption/overall running time: 59.6516s / 53450.1599 s
env0_first_0:                 episode reward: -97.3000,                 loss: 0.2530
env0_second_0:                 episode reward: 97.3000,                 loss: 0.1034
env1_first_0:                 episode reward: -88.8500,                 loss: nan
env1_second_0:                 episode reward: 88.8500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 219.05,                last time consumption/overall running time: 56.5875s / 53506.7474 s
env0_first_0:                 episode reward: -91.4500,                 loss: 0.2393
env0_second_0:                 episode reward: 91.4500,                 loss: 0.1112
env1_first_0:                 episode reward: -91.5500,                 loss: nan
env1_second_0:                 episode reward: 91.5500,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 227.7,                last time consumption/overall running time: 58.7775s / 53565.5249 s
env0_first_0:                 episode reward: -88.8500,                 loss: 0.2312
env0_second_0:                 episode reward: 88.8500,                 loss: 0.1133
env1_first_0:                 episode reward: -84.2000,                 loss: nan
env1_second_0:                 episode reward: 84.2000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 223.25,                last time consumption/overall running time: 58.1795s / 53623.7044 s
env0_first_0:                 episode reward: -95.9500,                 loss: 0.2625
env0_second_0:                 episode reward: 95.9500,                 loss: 0.1238
env1_first_0:                 episode reward: -83.2000,                 loss: nan
env1_second_0:                 episode reward: 83.2000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 218.3,                last time consumption/overall running time: 56.3084s / 53680.0128 s
env0_first_0:                 episode reward: -92.5000,                 loss: 0.2590
env0_second_0:                 episode reward: 92.5000,                 loss: 0.1149
env1_first_0:                 episode reward: -93.2000,                 loss: nan
env1_second_0:                 episode reward: 93.2000,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 228.65,                last time consumption/overall running time: 60.0012s / 53740.0140 s
env0_first_0:                 episode reward: -84.5500,                 loss: 0.2550
env0_second_0:                 episode reward: 84.5500,                 loss: 0.1021
env1_first_0:                 episode reward: -94.4500,                 loss: nan
env1_second_0:                 episode reward: 94.4500,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 217.35,                last time consumption/overall running time: 55.7109s / 53795.7249 s
env0_first_0:                 episode reward: -94.1000,                 loss: 0.2552
env0_second_0:                 episode reward: 94.1000,                 loss: 0.0990
env1_first_0:                 episode reward: -91.3500,                 loss: nan
env1_second_0:                 episode reward: 91.3500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 225.45,                last time consumption/overall running time: 59.6191s / 53855.3440 s
env0_first_0:                 episode reward: -94.4500,                 loss: 0.2681
env0_second_0:                 episode reward: 94.4500,                 loss: 0.0978
env1_first_0:                 episode reward: -91.8500,                 loss: nan
env1_second_0:                 episode reward: 91.8500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 235.65,                last time consumption/overall running time: 62.0197s / 53917.3637 s
env0_first_0:                 episode reward: -87.8000,                 loss: 0.2680
env0_second_0:                 episode reward: 87.8000,                 loss: 0.1030
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 217.55,                last time consumption/overall running time: 56.6022s / 53973.9659 s
env0_first_0:                 episode reward: -97.1000,                 loss: 0.2465
env0_second_0:                 episode reward: 97.1000,                 loss: 0.0943
env1_first_0:                 episode reward: -89.8500,                 loss: nan
env1_second_0:                 episode reward: 89.8500,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 225.55,                last time consumption/overall running time: 59.5061s / 54033.4720 s
env0_first_0:                 episode reward: -92.5500,                 loss: 0.2589
env0_second_0:                 episode reward: 92.5500,                 loss: 0.1055
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 225.3,                last time consumption/overall running time: 59.9865s / 54093.4586 s
env0_first_0:                 episode reward: -92.2000,                 loss: 0.2403
env0_second_0:                 episode reward: 92.2000,                 loss: 0.1074
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 222.0,                last time consumption/overall running time: 57.3921s / 54150.8506 s
env0_first_0:                 episode reward: -82.7500,                 loss: 0.2340
env0_second_0:                 episode reward: 82.7500,                 loss: 0.1026
env1_first_0:                 episode reward: -95.9000,                 loss: nan
env1_second_0:                 episode reward: 95.9000,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 227.65,                last time consumption/overall running time: 57.4444s / 54208.2950 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.2674
env0_second_0:                 episode reward: 87.7000,                 loss: 0.1097
env1_first_0:                 episode reward: -89.8500,                 loss: nan
env1_second_0:                 episode reward: 89.8500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 233.55,                last time consumption/overall running time: 60.1657s / 54268.4607 s
env0_first_0:                 episode reward: -81.2500,                 loss: 0.2779
env0_second_0:                 episode reward: 81.2500,                 loss: 0.1052
env1_first_0:                 episode reward: -92.1000,                 loss: nan
env1_second_0:                 episode reward: 92.1000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 229.15,                last time consumption/overall running time: 61.3760s / 54329.8367 s
env0_first_0:                 episode reward: -90.8000,                 loss: 0.2961
env0_second_0:                 episode reward: 90.8000,                 loss: 0.1070
env1_first_0:                 episode reward: -93.2500,                 loss: nan
env1_second_0:                 episode reward: 93.2500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 228.15,                last time consumption/overall running time: 60.1090s / 54389.9457 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.2739
env0_second_0:                 episode reward: 92.4000,                 loss: 0.1117
env1_first_0:                 episode reward: -85.6000,                 loss: nan
env1_second_0:                 episode reward: 85.6000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 227.0,                last time consumption/overall running time: 58.3216s / 54448.2672 s
env0_first_0:                 episode reward: -90.4500,                 loss: 0.2710
env0_second_0:                 episode reward: 90.4500,                 loss: 0.1221
env1_first_0:                 episode reward: -89.3000,                 loss: nan
env1_second_0:                 episode reward: 89.3000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 228.05,                last time consumption/overall running time: 59.9979s / 54508.2651 s
env0_first_0:                 episode reward: -94.0000,                 loss: 0.2813
env0_second_0:                 episode reward: 94.0000,                 loss: 0.1284
env1_first_0:                 episode reward: -84.9000,                 loss: nan
env1_second_0:                 episode reward: 84.9000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 224.0,                last time consumption/overall running time: 58.0196s / 54566.2848 s
env0_first_0:                 episode reward: -88.1000,                 loss: 0.2997
env0_second_0:                 episode reward: 88.1000,                 loss: 0.1220
env1_first_0:                 episode reward: -95.1500,                 loss: nan
env1_second_0:                 episode reward: 95.1500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 222.35,                last time consumption/overall running time: 58.4993s / 54624.7840 s
env0_first_0:                 episode reward: -87.1500,                 loss: 0.3280
env0_second_0:                 episode reward: 87.1500,                 loss: 0.1227
env1_first_0:                 episode reward: -89.2500,                 loss: nan
env1_second_0:                 episode reward: 89.2500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 233.75,                last time consumption/overall running time: 60.6658s / 54685.4498 s
env0_first_0:                 episode reward: -94.2000,                 loss: 0.3033
env0_second_0:                 episode reward: 94.2000,                 loss: 0.1303
env1_first_0:                 episode reward: -89.1000,                 loss: nan
env1_second_0:                 episode reward: 89.1000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 221.9,                last time consumption/overall running time: 58.6741s / 54744.1240 s
env0_first_0:                 episode reward: -92.3500,                 loss: 0.3020
env0_second_0:                 episode reward: 92.3500,                 loss: 0.1206
env1_first_0:                 episode reward: -85.8500,                 loss: nan
env1_second_0:                 episode reward: 85.8500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 220.65,                last time consumption/overall running time: 56.0108s / 54800.1348 s
env0_first_0:                 episode reward: -94.2000,                 loss: 0.2898
env0_second_0:                 episode reward: 94.2000,                 loss: 0.1109
env1_first_0:                 episode reward: -93.3000,                 loss: nan
env1_second_0:                 episode reward: 93.3000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 227.15,                last time consumption/overall running time: 59.0447s / 54859.1795 s
env0_first_0:                 episode reward: -87.8500,                 loss: 0.2713
env0_second_0:                 episode reward: 87.8500,                 loss: 0.1165
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 232.85,                last time consumption/overall running time: 60.0966s / 54919.2760 s
env0_first_0:                 episode reward: -97.3000,                 loss: 0.2583
env0_second_0:                 episode reward: 97.3000,                 loss: 0.1111
env1_first_0:                 episode reward: -84.1000,                 loss: nan
env1_second_0:                 episode reward: 84.1000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 229.55,                last time consumption/overall running time: 58.7714s / 54978.0475 s
env0_first_0:                 episode reward: -95.3500,                 loss: 0.2586
env0_second_0:                 episode reward: 95.3500,                 loss: 0.1094
env1_first_0:                 episode reward: -89.9000,                 loss: nan
env1_second_0:                 episode reward: 89.9000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 231.0,                last time consumption/overall running time: 60.4581s / 55038.5056 s
env0_first_0:                 episode reward: -95.5500,                 loss: 0.2383
env0_second_0:                 episode reward: 95.5500,                 loss: 0.1110
env1_first_0:                 episode reward: -80.4500,                 loss: nan
env1_second_0:                 episode reward: 80.4500,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 225.9,                last time consumption/overall running time: 60.4914s / 55098.9970 s
env0_first_0:                 episode reward: -89.0500,                 loss: 0.2586
env0_second_0:                 episode reward: 89.0500,                 loss: 0.1162
env1_first_0:                 episode reward: -95.2000,                 loss: nan
env1_second_0:                 episode reward: 95.2000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 226.75,                last time consumption/overall running time: 57.9283s / 55156.9252 s
env0_first_0:                 episode reward: -90.2500,                 loss: 0.2691
env0_second_0:                 episode reward: 90.2500,                 loss: 0.1102
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 220.45,                last time consumption/overall running time: 58.3540s / 55215.2792 s
env0_first_0:                 episode reward: -81.0500,                 loss: 0.2572
env0_second_0:                 episode reward: 81.0500,                 loss: 0.1086
env1_first_0:                 episode reward: -89.5500,                 loss: nan
env1_second_0:                 episode reward: 89.5500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 228.4,                last time consumption/overall running time: 60.4111s / 55275.6903 s
env0_first_0:                 episode reward: -84.6000,                 loss: 0.2714
env0_second_0:                 episode reward: 84.6000,                 loss: 0.1125
env1_first_0:                 episode reward: -92.4500,                 loss: nan
env1_second_0:                 episode reward: 92.4500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 259.7,                last time consumption/overall running time: 66.6319s / 55342.3222 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.2806
env0_second_0:                 episode reward: 88.5500,                 loss: 0.1206
env1_first_0:                 episode reward: -86.8500,                 loss: nan
env1_second_0:                 episode reward: 86.8500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 224.85,                last time consumption/overall running time: 58.4699s / 55400.7921 s
env0_first_0:                 episode reward: -89.6500,                 loss: 0.2691
env0_second_0:                 episode reward: 89.6500,                 loss: 0.1286
env1_first_0:                 episode reward: -87.8000,                 loss: nan
env1_second_0:                 episode reward: 87.8000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 235.15,                last time consumption/overall running time: 60.9867s / 55461.7787 s
env0_first_0:                 episode reward: -86.0500,                 loss: 0.2815
env0_second_0:                 episode reward: 86.0500,                 loss: 0.1225
env1_first_0:                 episode reward: -90.5000,                 loss: nan
env1_second_0:                 episode reward: 90.5000,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 220.7,                last time consumption/overall running time: 59.7708s / 55521.5495 s
env0_first_0:                 episode reward: -92.3000,                 loss: 0.2708
env0_second_0:                 episode reward: 92.3000,                 loss: 0.1288
env1_first_0:                 episode reward: -88.1500,                 loss: nan
env1_second_0:                 episode reward: 88.1500,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 224.05,                last time consumption/overall running time: 57.5867s / 55579.1361 s
env0_first_0:                 episode reward: -87.9500,                 loss: 0.2676
env0_second_0:                 episode reward: 87.9500,                 loss: 0.1204
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 228.35,                last time consumption/overall running time: 58.2352s / 55637.3713 s
env0_first_0:                 episode reward: -92.3000,                 loss: 0.2702
env0_second_0:                 episode reward: 92.3000,                 loss: 0.1218
env1_first_0:                 episode reward: -93.6000,                 loss: nan
env1_second_0:                 episode reward: 93.6000,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 221.15,                last time consumption/overall running time: 56.5973s / 55693.9686 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.2404
env0_second_0:                 episode reward: 88.6000,                 loss: 0.1187
env1_first_0:                 episode reward: -87.0500,                 loss: nan
env1_second_0:                 episode reward: 87.0500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 217.0,                last time consumption/overall running time: 56.9887s / 55750.9573 s
env0_first_0:                 episode reward: -86.9500,                 loss: 0.2633
env0_second_0:                 episode reward: 86.9500,                 loss: 0.1221
env1_first_0:                 episode reward: -90.1500,                 loss: nan
env1_second_0:                 episode reward: 90.1500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 220.05,                last time consumption/overall running time: 58.1454s / 55809.1028 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.2706
env0_second_0:                 episode reward: 93.0500,                 loss: 0.1117
env1_first_0:                 episode reward: -90.6500,                 loss: nan
env1_second_0:                 episode reward: 90.6500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 220.8,                last time consumption/overall running time: 57.1675s / 55866.2703 s
env0_first_0:                 episode reward: -86.5500,                 loss: 0.2806
env0_second_0:                 episode reward: 86.5500,                 loss: 0.1197
env1_first_0:                 episode reward: -87.0000,                 loss: nan
env1_second_0:                 episode reward: 87.0000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 221.2,                last time consumption/overall running time: 58.2917s / 55924.5620 s
env0_first_0:                 episode reward: -90.6000,                 loss: 0.2856
env0_second_0:                 episode reward: 90.6000,                 loss: 0.1284
env1_first_0:                 episode reward: -87.4000,                 loss: nan
env1_second_0:                 episode reward: 87.4000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 233.15,                last time consumption/overall running time: 60.4708s / 55985.0329 s
env0_first_0:                 episode reward: -93.2500,                 loss: 0.2713
env0_second_0:                 episode reward: 93.2500,                 loss: 0.1203
env1_first_0:                 episode reward: -95.4000,                 loss: nan
env1_second_0:                 episode reward: 95.4000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 220.5,                last time consumption/overall running time: 56.8462s / 56041.8791 s
env0_first_0:                 episode reward: -88.6000,                 loss: 0.2351
env0_second_0:                 episode reward: 88.6000,                 loss: 0.1205
env1_first_0:                 episode reward: -93.1500,                 loss: nan
env1_second_0:                 episode reward: 93.1500,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 228.15,                last time consumption/overall running time: 58.7719s / 56100.6510 s
env0_first_0:                 episode reward: -86.9000,                 loss: 0.2538
env0_second_0:                 episode reward: 86.9000,                 loss: 0.1248
env1_first_0:                 episode reward: -71.7000,                 loss: nan
env1_second_0:                 episode reward: 71.7000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 222.35,                last time consumption/overall running time: 58.4435s / 56159.0945 s
env0_first_0:                 episode reward: -91.1500,                 loss: 0.2639
env0_second_0:                 episode reward: 91.1500,                 loss: 0.1331
env1_first_0:                 episode reward: -87.7500,                 loss: nan
env1_second_0:                 episode reward: 87.7500,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 216.4,                last time consumption/overall running time: 58.8321s / 56217.9267 s
env0_first_0:                 episode reward: -92.9000,                 loss: 0.2452
env0_second_0:                 episode reward: 92.9000,                 loss: 0.1253
env1_first_0:                 episode reward: -88.7500,                 loss: nan
env1_second_0:                 episode reward: 88.7500,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 217.2,                last time consumption/overall running time: 57.3868s / 56275.3134 s
env0_first_0:                 episode reward: -84.0000,                 loss: 0.2503
env0_second_0:                 episode reward: 84.0000,                 loss: 0.1342
env1_first_0:                 episode reward: -95.5000,                 loss: nan
env1_second_0:                 episode reward: 95.5000,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 224.9,                last time consumption/overall running time: 57.2686s / 56332.5820 s
env0_first_0:                 episode reward: -84.1500,                 loss: 0.2389
env0_second_0:                 episode reward: 84.1500,                 loss: 0.1182
env1_first_0:                 episode reward: -88.1000,                 loss: nan
env1_second_0:                 episode reward: 88.1000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 227.75,                last time consumption/overall running time: 59.4385s / 56392.0205 s
env0_first_0:                 episode reward: -83.5000,                 loss: 0.2215
env0_second_0:                 episode reward: 83.5000,                 loss: 0.1342
env1_first_0:                 episode reward: -88.7500,                 loss: nan
env1_second_0:                 episode reward: 88.7500,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 219.35,                last time consumption/overall running time: 56.6252s / 56448.6457 s
env0_first_0:                 episode reward: -93.0500,                 loss: 0.2240
env0_second_0:                 episode reward: 93.0500,                 loss: 0.1333
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 231.2,                last time consumption/overall running time: 60.7007s / 56509.3464 s
env0_first_0:                 episode reward: -89.6000,                 loss: 0.2265
env0_second_0:                 episode reward: 89.6000,                 loss: 0.1279
env1_first_0:                 episode reward: -91.5000,                 loss: nan
env1_second_0:                 episode reward: 91.5000,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 235.3,                last time consumption/overall running time: 60.8031s / 56570.1495 s
env0_first_0:                 episode reward: -93.4000,                 loss: 0.2215
env0_second_0:                 episode reward: 93.4000,                 loss: 0.1263
env1_first_0:                 episode reward: -88.5500,                 loss: nan
env1_second_0:                 episode reward: 88.5500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 229.4,                last time consumption/overall running time: 59.5705s / 56629.7200 s
env0_first_0:                 episode reward: -92.4000,                 loss: 0.2483
env0_second_0:                 episode reward: 92.4000,                 loss: 0.1348
env1_first_0:                 episode reward: -86.7500,                 loss: nan
env1_second_0:                 episode reward: 86.7500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 233.7,                last time consumption/overall running time: 61.5039s / 56691.2240 s
env0_first_0:                 episode reward: -93.7000,                 loss: 0.2441
env0_second_0:                 episode reward: 93.7000,                 loss: 0.1308
env1_first_0:                 episode reward: -86.6500,                 loss: nan
env1_second_0:                 episode reward: 86.6500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 225.85,                last time consumption/overall running time: 58.4255s / 56749.6495 s
env0_first_0:                 episode reward: -93.0000,                 loss: 0.2510
env0_second_0:                 episode reward: 93.0000,                 loss: 0.1251
env1_first_0:                 episode reward: -92.3500,                 loss: nan
env1_second_0:                 episode reward: 92.3500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 223.65,                last time consumption/overall running time: 56.6384s / 56806.2879 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.2599
env0_second_0:                 episode reward: 88.7500,                 loss: 0.1244
env1_first_0:                 episode reward: -97.9500,                 loss: nan
env1_second_0:                 episode reward: 97.9500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 222.35,                last time consumption/overall running time: 56.4638s / 56862.7517 s
env0_first_0:                 episode reward: -94.0000,                 loss: 0.2594
env0_second_0:                 episode reward: 94.0000,                 loss: 0.1154
env1_first_0:                 episode reward: -92.2000,                 loss: nan
env1_second_0:                 episode reward: 92.2000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 233.3,                last time consumption/overall running time: 60.0577s / 56922.8094 s
env0_first_0:                 episode reward: -97.3000,                 loss: 0.2607
env0_second_0:                 episode reward: 97.3000,                 loss: 0.1194
env1_first_0:                 episode reward: -87.3500,                 loss: nan
env1_second_0:                 episode reward: 87.3500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 226.0,                last time consumption/overall running time: 58.9606s / 56981.7700 s
env0_first_0:                 episode reward: -92.9500,                 loss: 0.2667
env0_second_0:                 episode reward: 92.9500,                 loss: 0.1195
env1_first_0:                 episode reward: -86.8000,                 loss: nan
env1_second_0:                 episode reward: 86.8000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 223.15,                last time consumption/overall running time: 57.0190s / 57038.7890 s
env0_first_0:                 episode reward: -91.5000,                 loss: 0.2777
env0_second_0:                 episode reward: 91.5000,                 loss: 0.1207
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 222.5,                last time consumption/overall running time: 57.4894s / 57096.2784 s
env0_first_0:                 episode reward: -89.9500,                 loss: 0.2820
env0_second_0:                 episode reward: 89.9500,                 loss: 0.1149
env1_first_0:                 episode reward: -90.8000,                 loss: nan
env1_second_0:                 episode reward: 90.8000,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 226.0,                last time consumption/overall running time: 57.8321s / 57154.1105 s
env0_first_0:                 episode reward: -93.5000,                 loss: 0.2791
env0_second_0:                 episode reward: 93.5000,                 loss: 0.1213
env1_first_0:                 episode reward: -90.3500,                 loss: nan
env1_second_0:                 episode reward: 90.3500,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 228.7,                last time consumption/overall running time: 58.4474s / 57212.5579 s
env0_first_0:                 episode reward: -81.1500,                 loss: 0.2758
env0_second_0:                 episode reward: 81.1500,                 loss: 0.1147
env1_first_0:                 episode reward: -96.9000,                 loss: nan
env1_second_0:                 episode reward: 96.9000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 225.7,                last time consumption/overall running time: 57.5870s / 57270.1450 s
env0_first_0:                 episode reward: -89.8500,                 loss: 0.2933
env0_second_0:                 episode reward: 89.8500,                 loss: 0.1183
env1_first_0:                 episode reward: -93.5000,                 loss: nan
env1_second_0:                 episode reward: 93.5000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 225.4,                last time consumption/overall running time: 56.5707s / 57326.7157 s
env0_first_0:                 episode reward: -87.4000,                 loss: 0.3154
env0_second_0:                 episode reward: 87.4000,                 loss: 0.1180
env1_first_0:                 episode reward: -93.2500,                 loss: nan
env1_second_0:                 episode reward: 93.2500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 229.15,                last time consumption/overall running time: 59.6711s / 57386.3868 s
env0_first_0:                 episode reward: -84.4500,                 loss: 0.3067
env0_second_0:                 episode reward: 84.4500,                 loss: 0.1112
env1_first_0:                 episode reward: -95.1500,                 loss: nan
env1_second_0:                 episode reward: 95.1500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 216.55,                last time consumption/overall running time: 55.8225s / 57442.2093 s
env0_first_0:                 episode reward: -86.9000,                 loss: 0.2779
env0_second_0:                 episode reward: 86.9000,                 loss: 0.1158
env1_first_0:                 episode reward: -91.4500,                 loss: nan
env1_second_0:                 episode reward: 91.4500,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 227.5,                last time consumption/overall running time: 58.9369s / 57501.1462 s
env0_first_0:                 episode reward: -86.9500,                 loss: 0.2823
env0_second_0:                 episode reward: 86.9500,                 loss: 0.1087
env1_first_0:                 episode reward: -90.0500,                 loss: nan
env1_second_0:                 episode reward: 90.0500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 231.05,                last time consumption/overall running time: 60.4941s / 57561.6402 s
env0_first_0:                 episode reward: -93.6000,                 loss: 0.3038
env0_second_0:                 episode reward: 93.6000,                 loss: 0.1150
env1_first_0:                 episode reward: -89.2500,                 loss: nan
env1_second_0:                 episode reward: 89.2500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 225.05,                last time consumption/overall running time: 57.7747s / 57619.4149 s
env0_first_0:                 episode reward: -95.6500,                 loss: 0.2988
env0_second_0:                 episode reward: 95.6500,                 loss: 0.1030
env1_first_0:                 episode reward: -86.0000,                 loss: nan
env1_second_0:                 episode reward: 86.0000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 225.1,                last time consumption/overall running time: 59.1524s / 57678.5673 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.2905
env0_second_0:                 episode reward: 86.4500,                 loss: 0.1116
env1_first_0:                 episode reward: -90.6000,                 loss: nan
env1_second_0:                 episode reward: 90.6000,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 226.8,                last time consumption/overall running time: 60.6170s / 57739.1843 s
env0_first_0:                 episode reward: -95.7500,                 loss: 0.3124
env0_second_0:                 episode reward: 95.7500,                 loss: 0.1145
env1_first_0:                 episode reward: -85.2000,                 loss: nan
env1_second_0:                 episode reward: 85.2000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 223.35,                last time consumption/overall running time: 57.9897s / 57797.1740 s
env0_first_0:                 episode reward: -89.9500,                 loss: 0.2894
env0_second_0:                 episode reward: 89.9500,                 loss: 0.1185
env1_first_0:                 episode reward: -93.9000,                 loss: nan
env1_second_0:                 episode reward: 93.9000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 232.45,                last time consumption/overall running time: 57.8530s / 57855.0270 s
env0_first_0:                 episode reward: -91.5000,                 loss: 0.2975
env0_second_0:                 episode reward: 91.5000,                 loss: 0.1042
env1_first_0:                 episode reward: -88.0500,                 loss: nan
env1_second_0:                 episode reward: 88.0500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 217.8,                last time consumption/overall running time: 56.4482s / 57911.4752 s
env0_first_0:                 episode reward: -91.2500,                 loss: 0.2996
env0_second_0:                 episode reward: 91.2500,                 loss: 0.1007
env1_first_0:                 episode reward: -97.4500,                 loss: nan
env1_second_0:                 episode reward: 97.4500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 223.65,                last time consumption/overall running time: 57.0124s / 57968.4876 s
env0_first_0:                 episode reward: -89.3500,                 loss: 0.2785
env0_second_0:                 episode reward: 89.3500,                 loss: 0.1092
env1_first_0:                 episode reward: -94.6500,                 loss: nan
env1_second_0:                 episode reward: 94.6500,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 219.75,                last time consumption/overall running time: 56.4020s / 58024.8896 s
env0_first_0:                 episode reward: -90.9000,                 loss: 0.3010
env0_second_0:                 episode reward: 90.9000,                 loss: 0.1016
env1_first_0:                 episode reward: -93.3000,                 loss: nan
env1_second_0:                 episode reward: 93.3000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 233.9,                last time consumption/overall running time: 60.4554s / 58085.3450 s
env0_first_0:                 episode reward: -92.7000,                 loss: 0.2656
env0_second_0:                 episode reward: 92.7000,                 loss: 0.0966
env1_first_0:                 episode reward: -89.3000,                 loss: nan
env1_second_0:                 episode reward: 89.3000,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 227.35,                last time consumption/overall running time: 58.1830s / 58143.5281 s
env0_first_0:                 episode reward: -86.7000,                 loss: 0.2634
env0_second_0:                 episode reward: 86.7000,                 loss: 0.1125
env1_first_0:                 episode reward: -87.6500,                 loss: nan
env1_second_0:                 episode reward: 87.6500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 230.95,                last time consumption/overall running time: 60.0855s / 58203.6136 s
env0_first_0:                 episode reward: -75.1000,                 loss: 0.2558
env0_second_0:                 episode reward: 75.1000,                 loss: 0.1183
env1_first_0:                 episode reward: -95.3500,                 loss: nan
env1_second_0:                 episode reward: 95.3500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 229.6,                last time consumption/overall running time: 59.9338s / 58263.5474 s
env0_first_0:                 episode reward: -90.9000,                 loss: 0.2727
env0_second_0:                 episode reward: 90.9000,                 loss: 0.1149
env1_first_0:                 episode reward: -87.9000,                 loss: nan
env1_second_0:                 episode reward: 87.9000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 224.1,                last time consumption/overall running time: 57.8565s / 58321.4039 s
env0_first_0:                 episode reward: -85.7000,                 loss: 0.2685
env0_second_0:                 episode reward: 85.7000,                 loss: 0.1125
env1_first_0:                 episode reward: -82.8000,                 loss: nan
env1_second_0:                 episode reward: 82.8000,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 223.9,                last time consumption/overall running time: 59.1495s / 58380.5534 s
env0_first_0:                 episode reward: -89.1000,                 loss: 0.2652
env0_second_0:                 episode reward: 89.1000,                 loss: 0.1002
env1_first_0:                 episode reward: -90.0000,                 loss: nan
env1_second_0:                 episode reward: 90.0000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 222.7,                last time consumption/overall running time: 58.1484s / 58438.7018 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.2739
env0_second_0:                 episode reward: 88.7500,                 loss: 0.1096
env1_first_0:                 episode reward: -94.2000,                 loss: nan
env1_second_0:                 episode reward: 94.2000,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 234.3,                last time consumption/overall running time: 60.2598s / 58498.9615 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.3023
env0_second_0:                 episode reward: 93.3500,                 loss: 0.1044
env1_first_0:                 episode reward: -90.6000,                 loss: nan
env1_second_0:                 episode reward: 90.6000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 220.05,                last time consumption/overall running time: 58.4825s / 58557.4440 s
env0_first_0:                 episode reward: -93.2500,                 loss: 0.2833
env0_second_0:                 episode reward: 93.2500,                 loss: 0.1146
env1_first_0:                 episode reward: -77.4000,                 loss: nan
env1_second_0:                 episode reward: 77.4000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 220.45,                last time consumption/overall running time: 56.3642s / 58613.8082 s
env0_first_0:                 episode reward: -87.1500,                 loss: 0.2821
env0_second_0:                 episode reward: 87.1500,                 loss: 0.1008
env1_first_0:                 episode reward: -85.6000,                 loss: nan
env1_second_0:                 episode reward: 85.6000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 223.0,                last time consumption/overall running time: 57.3007s / 58671.1089 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.2785
env0_second_0:                 episode reward: 89.3000,                 loss: 0.1119
env1_first_0:                 episode reward: -96.2000,                 loss: nan
env1_second_0:                 episode reward: 96.2000,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 217.0,                last time consumption/overall running time: 57.8589s / 58728.9677 s
env0_first_0:                 episode reward: -89.6500,                 loss: 0.2915
env0_second_0:                 episode reward: 89.6500,                 loss: 0.1037
env1_first_0:                 episode reward: -92.2500,                 loss: nan
env1_second_0:                 episode reward: 92.2500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 220.4,                last time consumption/overall running time: 59.2982s / 58788.2659 s
env0_first_0:                 episode reward: -92.6500,                 loss: 0.2380
env0_second_0:                 episode reward: 92.6500,                 loss: 0.1010
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 224.75,                last time consumption/overall running time: 57.7751s / 58846.0410 s
env0_first_0:                 episode reward: -93.7000,                 loss: 0.2667
env0_second_0:                 episode reward: 93.7000,                 loss: 0.0974
env1_first_0:                 episode reward: -88.9000,                 loss: nan
env1_second_0:                 episode reward: 88.9000,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 231.85,                last time consumption/overall running time: 59.1645s / 58905.2056 s
env0_first_0:                 episode reward: -88.4000,                 loss: 0.2492
env0_second_0:                 episode reward: 88.4000,                 loss: 0.0930
env1_first_0:                 episode reward: -91.1500,                 loss: nan
env1_second_0:                 episode reward: 91.1500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 220.35,                last time consumption/overall running time: 57.4220s / 58962.6276 s
env0_first_0:                 episode reward: -90.1500,                 loss: 0.2270
env0_second_0:                 episode reward: 90.1500,                 loss: 0.0963
env1_first_0:                 episode reward: -79.4500,                 loss: nan
env1_second_0:                 episode reward: 79.4500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 225.9,                last time consumption/overall running time: 58.0660s / 59020.6935 s
env0_first_0:                 episode reward: -88.8500,                 loss: 0.2287
env0_second_0:                 episode reward: 88.8500,                 loss: 0.1069
env1_first_0:                 episode reward: -90.4000,                 loss: nan
env1_second_0:                 episode reward: 90.4000,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 228.15,                last time consumption/overall running time: 60.1296s / 59080.8232 s
env0_first_0:                 episode reward: -89.0000,                 loss: 0.2483
env0_second_0:                 episode reward: 89.0000,                 loss: 0.1089
env1_first_0:                 episode reward: -94.5500,                 loss: nan
env1_second_0:                 episode reward: 94.5500,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 232.5,                last time consumption/overall running time: 59.6958s / 59140.5190 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.2438
env0_second_0:                 episode reward: 92.4500,                 loss: 0.1086
env1_first_0:                 episode reward: -85.9000,                 loss: nan
env1_second_0:                 episode reward: 85.9000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 240.95,                last time consumption/overall running time: 62.2848s / 59202.8038 s
env0_first_0:                 episode reward: -80.9000,                 loss: 0.2363
env0_second_0:                 episode reward: 80.9000,                 loss: 0.1030
env1_first_0:                 episode reward: -91.2500,                 loss: nan
env1_second_0:                 episode reward: 91.2500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 249.35,                last time consumption/overall running time: 64.4620s / 59267.2658 s
env0_first_0:                 episode reward: -80.9500,                 loss: 0.2542
env0_second_0:                 episode reward: 80.9500,                 loss: 0.1204
env1_first_0:                 episode reward: -82.7500,                 loss: nan
env1_second_0:                 episode reward: 82.7500,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 253.05,                last time consumption/overall running time: 67.9670s / 59335.2327 s
env0_first_0:                 episode reward: -87.0000,                 loss: 0.2718
env0_second_0:                 episode reward: 87.0000,                 loss: 0.1218
env1_first_0:                 episode reward: -84.2500,                 loss: nan
env1_second_0:                 episode reward: 84.2500,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 236.1,                last time consumption/overall running time: 61.8033s / 59397.0361 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.2945
env0_second_0:                 episode reward: 85.8500,                 loss: 0.1192
env1_first_0:                 episode reward: -86.3000,                 loss: nan
env1_second_0:                 episode reward: 86.3000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 229.25,                last time consumption/overall running time: 60.3592s / 59457.3953 s
env0_first_0:                 episode reward: -92.8500,                 loss: 0.2945
env0_second_0:                 episode reward: 92.8500,                 loss: 0.1177
env1_first_0:                 episode reward: -83.1000,                 loss: nan
env1_second_0:                 episode reward: 83.1000,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 228.6,                last time consumption/overall running time: 58.7287s / 59516.1240 s
env0_first_0:                 episode reward: -92.5000,                 loss: 0.3351
env0_second_0:                 episode reward: 92.5000,                 loss: 0.1300
env1_first_0:                 episode reward: -85.7000,                 loss: nan
env1_second_0:                 episode reward: 85.7000,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 241.9,                last time consumption/overall running time: 63.5273s / 59579.6513 s
env0_first_0:                 episode reward: -84.7000,                 loss: 0.3360
env0_second_0:                 episode reward: 84.7000,                 loss: 0.1239
env1_first_0:                 episode reward: -86.0500,                 loss: nan
env1_second_0:                 episode reward: 86.0500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 240.25,                last time consumption/overall running time: 62.3690s / 59642.0203 s
env0_first_0:                 episode reward: -88.2500,                 loss: 0.3641
env0_second_0:                 episode reward: 88.2500,                 loss: 0.1283
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 224.65,                last time consumption/overall running time: 58.0801s / 59700.1004 s
env0_first_0:                 episode reward: -89.3000,                 loss: 0.3461
env0_second_0:                 episode reward: 89.3000,                 loss: 0.1191
env1_first_0:                 episode reward: -93.5500,                 loss: nan
env1_second_0:                 episode reward: 93.5500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 220.95,                last time consumption/overall running time: 57.0421s / 59757.1425 s
env0_first_0:                 episode reward: -86.6000,                 loss: 0.3344
env0_second_0:                 episode reward: 86.6000,                 loss: 0.1272
env1_first_0:                 episode reward: -97.5000,                 loss: nan
env1_second_0:                 episode reward: 97.5000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 224.95,                last time consumption/overall running time: 57.4390s / 59814.5815 s
env0_first_0:                 episode reward: -93.5500,                 loss: 0.2976
env0_second_0:                 episode reward: 93.5500,                 loss: 0.1268
env1_first_0:                 episode reward: -87.9500,                 loss: nan
env1_second_0:                 episode reward: 87.9500,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 217.6,                last time consumption/overall running time: 53.8171s / 59868.3986 s
env0_first_0:                 episode reward: -92.8500,                 loss: 0.2991
env0_second_0:                 episode reward: 92.8500,                 loss: 0.1361
env1_first_0:                 episode reward: -92.3500,                 loss: nan
env1_second_0:                 episode reward: 92.3500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 231.1,                last time consumption/overall running time: 60.5723s / 59928.9709 s
env0_first_0:                 episode reward: -85.2000,                 loss: 0.2803
env0_second_0:                 episode reward: 85.2000,                 loss: 0.1277
env1_first_0:                 episode reward: -85.7500,                 loss: nan
env1_second_0:                 episode reward: 85.7500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 222.9,                last time consumption/overall running time: 57.4496s / 59986.4205 s
env0_first_0:                 episode reward: -81.6500,                 loss: 0.2844
env0_second_0:                 episode reward: 81.6500,                 loss: 0.1342
env1_first_0:                 episode reward: -88.2500,                 loss: nan
env1_second_0:                 episode reward: 88.2500,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 220.25,                last time consumption/overall running time: 56.4272s / 60042.8477 s
env0_first_0:                 episode reward: -85.8000,                 loss: 0.2612
env0_second_0:                 episode reward: 85.8000,                 loss: 0.1280
env1_first_0:                 episode reward: -95.4500,                 loss: nan
env1_second_0:                 episode reward: 95.4500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 223.25,                last time consumption/overall running time: 58.8125s / 60101.6601 s
env0_first_0:                 episode reward: -85.9000,                 loss: 0.2666
env0_second_0:                 episode reward: 85.9000,                 loss: 0.1207
env1_first_0:                 episode reward: -88.2000,                 loss: nan
env1_second_0:                 episode reward: 88.2000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 228.95,                last time consumption/overall running time: 60.3073s / 60161.9675 s
env0_first_0:                 episode reward: -88.1500,                 loss: 0.2656
env0_second_0:                 episode reward: 88.1500,                 loss: 0.1133
env1_first_0:                 episode reward: -91.1000,                 loss: nan
env1_second_0:                 episode reward: 91.1000,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 242.95,                last time consumption/overall running time: 64.1157s / 60226.0831 s
env0_first_0:                 episode reward: -84.6000,                 loss: 0.2679
env0_second_0:                 episode reward: 84.6000,                 loss: 0.1131
env1_first_0:                 episode reward: -86.6500,                 loss: nan
env1_second_0:                 episode reward: 86.6500,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 223.55,                last time consumption/overall running time: 57.7157s / 60283.7989 s
env0_first_0:                 episode reward: -94.9500,                 loss: 0.2518
env0_second_0:                 episode reward: 94.9500,                 loss: 0.1128
env1_first_0:                 episode reward: -88.9000,                 loss: nan
env1_second_0:                 episode reward: 88.9000,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 241.35,                last time consumption/overall running time: 61.5750s / 60345.3739 s
env0_first_0:                 episode reward: -85.5000,                 loss: 0.2606
env0_second_0:                 episode reward: 85.5000,                 loss: 0.1178
env1_first_0:                 episode reward: -88.1500,                 loss: nan
env1_second_0:                 episode reward: 88.1500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 226.75,                last time consumption/overall running time: 58.7563s / 60404.1301 s
env0_first_0:                 episode reward: -88.4500,                 loss: 0.3059
env0_second_0:                 episode reward: 88.4500,                 loss: 0.1304
env1_first_0:                 episode reward: -89.3000,                 loss: nan
env1_second_0:                 episode reward: 89.3000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 234.3,                last time consumption/overall running time: 60.0667s / 60464.1969 s
env0_first_0:                 episode reward: -83.6500,                 loss: 0.2832
env0_second_0:                 episode reward: 83.6500,                 loss: 0.1319
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 224.45,                last time consumption/overall running time: 58.3208s / 60522.5177 s
env0_first_0:                 episode reward: -86.8500,                 loss: 0.3114
env0_second_0:                 episode reward: 86.8500,                 loss: 0.1239
env1_first_0:                 episode reward: -89.9000,                 loss: nan
env1_second_0:                 episode reward: 89.9000,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 227.6,                last time consumption/overall running time: 58.5512s / 60581.0689 s
env0_first_0:                 episode reward: -88.7500,                 loss: 0.2710
env0_second_0:                 episode reward: 88.7500,                 loss: 0.1235
env1_first_0:                 episode reward: -84.9500,                 loss: nan
env1_second_0:                 episode reward: 84.9500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 224.4,                last time consumption/overall running time: 56.9513s / 60638.0202 s
env0_first_0:                 episode reward: -89.7000,                 loss: 0.2829
env0_second_0:                 episode reward: 89.7000,                 loss: 0.1297
env1_first_0:                 episode reward: -89.5000,                 loss: nan
env1_second_0:                 episode reward: 89.5000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 229.75,                last time consumption/overall running time: 59.7258s / 60697.7461 s
env0_first_0:                 episode reward: -91.9500,                 loss: 0.2754
env0_second_0:                 episode reward: 91.9500,                 loss: 0.1340
env1_first_0:                 episode reward: -86.4500,                 loss: nan
env1_second_0:                 episode reward: 86.4500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 225.45,                last time consumption/overall running time: 59.4315s / 60757.1775 s
env0_first_0:                 episode reward: -92.0500,                 loss: 0.2887
env0_second_0:                 episode reward: 92.0500,                 loss: 0.1143
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 239.0,                last time consumption/overall running time: 62.7499s / 60819.9274 s
env0_first_0:                 episode reward: -81.6500,                 loss: 0.2817
env0_second_0:                 episode reward: 81.6500,                 loss: 0.1321
env1_first_0:                 episode reward: -91.0000,                 loss: nan
env1_second_0:                 episode reward: 91.0000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 223.05,                last time consumption/overall running time: 57.3415s / 60877.2689 s
env0_first_0:                 episode reward: -84.0500,                 loss: 0.2887
env0_second_0:                 episode reward: 84.0500,                 loss: 0.1216
env1_first_0:                 episode reward: -92.3000,                 loss: nan
env1_second_0:                 episode reward: 92.3000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 224.2,                last time consumption/overall running time: 57.8843s / 60935.1531 s
env0_first_0:                 episode reward: -93.3500,                 loss: 0.2912
env0_second_0:                 episode reward: 93.3500,                 loss: 0.1318
env1_first_0:                 episode reward: -92.0000,                 loss: nan
env1_second_0:                 episode reward: 92.0000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 236.85,                last time consumption/overall running time: 61.6476s / 60996.8007 s
env0_first_0:                 episode reward: -91.2000,                 loss: 0.2799
env0_second_0:                 episode reward: 91.2000,                 loss: 0.1312
env1_first_0:                 episode reward: -83.2500,                 loss: nan
env1_second_0:                 episode reward: 83.2500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 218.2,                last time consumption/overall running time: 56.7037s / 61053.5044 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.3000
env0_second_0:                 episode reward: 90.3000,                 loss: 0.1120
env1_first_0:                 episode reward: -93.0000,                 loss: nan
env1_second_0:                 episode reward: 93.0000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 233.4,                last time consumption/overall running time: 60.1274s / 61113.6318 s
env0_first_0:                 episode reward: -90.5000,                 loss: 0.2914
env0_second_0:                 episode reward: 90.5000,                 loss: 0.1190
env1_first_0:                 episode reward: -87.0500,                 loss: nan
env1_second_0:                 episode reward: 87.0500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 221.55,                last time consumption/overall running time: 57.3590s / 61170.9908 s
env0_first_0:                 episode reward: -96.0500,                 loss: 0.2885
env0_second_0:                 episode reward: 96.0500,                 loss: 0.1469
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 232.5,                last time consumption/overall running time: 61.3206s / 61232.3114 s
env0_first_0:                 episode reward: -84.5500,                 loss: 0.2812
env0_second_0:                 episode reward: 84.5500,                 loss: 0.1267
env1_first_0:                 episode reward: -89.2500,                 loss: nan
env1_second_0:                 episode reward: 89.2500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 217.55,                last time consumption/overall running time: 58.5620s / 61290.8734 s
env0_first_0:                 episode reward: -93.4000,                 loss: 0.2873
env0_second_0:                 episode reward: 93.4000,                 loss: 0.1374
env1_first_0:                 episode reward: -87.3500,                 loss: nan
env1_second_0:                 episode reward: 87.3500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 229.95,                last time consumption/overall running time: 60.7219s / 61351.5953 s
env0_first_0:                 episode reward: -90.3000,                 loss: 0.2500
env0_second_0:                 episode reward: 90.3000,                 loss: 0.1244
env1_first_0:                 episode reward: -87.8500,                 loss: nan
env1_second_0:                 episode reward: 87.8500,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 226.1,                last time consumption/overall running time: 57.0256s / 61408.6210 s
env0_first_0:                 episode reward: -93.5000,                 loss: 0.2499
env0_second_0:                 episode reward: 93.5000,                 loss: 0.1129
env1_first_0:                 episode reward: -90.2500,                 loss: nan
env1_second_0:                 episode reward: 90.2500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 226.55,                last time consumption/overall running time: 60.0728s / 61468.6938 s
env0_first_0:                 episode reward: -91.2500,                 loss: 0.2325
env0_second_0:                 episode reward: 91.2500,                 loss: 0.1276
env1_first_0:                 episode reward: -89.6000,                 loss: nan
env1_second_0:                 episode reward: 89.6000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 223.15,                last time consumption/overall running time: 59.2759s / 61527.9698 s
env0_first_0:                 episode reward: -93.2000,                 loss: 0.2207
env0_second_0:                 episode reward: 93.2000,                 loss: 0.1148
env1_first_0:                 episode reward: -88.6500,                 loss: nan
env1_second_0:                 episode reward: 88.6500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 225.85,                last time consumption/overall running time: 59.2815s / 61587.2512 s
env0_first_0:                 episode reward: -82.5000,                 loss: 0.2254
env0_second_0:                 episode reward: 82.5000,                 loss: 0.1064
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 243.25,                last time consumption/overall running time: 63.1356s / 61650.3869 s
env0_first_0:                 episode reward: -84.7000,                 loss: 0.2495
env0_second_0:                 episode reward: 84.7000,                 loss: 0.1187
env1_first_0:                 episode reward: -82.1000,                 loss: nan
env1_second_0:                 episode reward: 82.1000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 246.5,                last time consumption/overall running time: 64.2473s / 61714.6342 s
env0_first_0:                 episode reward: -79.2000,                 loss: 0.2334
env0_second_0:                 episode reward: 79.2000,                 loss: 0.1280
env1_first_0:                 episode reward: -91.7500,                 loss: nan
env1_second_0:                 episode reward: 91.7500,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 225.95,                last time consumption/overall running time: 58.3777s / 61773.0119 s
env0_first_0:                 episode reward: -92.0000,                 loss: 0.2353
env0_second_0:                 episode reward: 92.0000,                 loss: 0.1146
env1_first_0:                 episode reward: -80.5000,                 loss: nan
env1_second_0:                 episode reward: 80.5000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 248.55,                last time consumption/overall running time: 65.2076s / 61838.2195 s
env0_first_0:                 episode reward: -68.6000,                 loss: 0.2787
env0_second_0:                 episode reward: 68.6000,                 loss: 0.1253
env1_first_0:                 episode reward: -88.9000,                 loss: nan
env1_second_0:                 episode reward: 88.9000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 219.4,                last time consumption/overall running time: 56.1856s / 61894.4051 s
env0_first_0:                 episode reward: -86.1000,                 loss: 0.2845
env0_second_0:                 episode reward: 86.1000,                 loss: 0.1243
env1_first_0:                 episode reward: -95.7000,                 loss: nan
env1_second_0:                 episode reward: 95.7000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 236.7,                last time consumption/overall running time: 61.0042s / 61955.4093 s
env0_first_0:                 episode reward: -82.4500,                 loss: 0.2888
env0_second_0:                 episode reward: 82.4500,                 loss: 0.1131
env1_first_0:                 episode reward: -87.4500,                 loss: nan
env1_second_0:                 episode reward: 87.4500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 235.05,                last time consumption/overall running time: 59.6260s / 62015.0353 s
env0_first_0:                 episode reward: -89.5500,                 loss: 0.2813
env0_second_0:                 episode reward: 89.5500,                 loss: 0.1131
env1_first_0:                 episode reward: -93.9500,                 loss: nan
env1_second_0:                 episode reward: 93.9500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 221.65,                last time consumption/overall running time: 58.0095s / 62073.0449 s
env0_first_0:                 episode reward: -84.1500,                 loss: 0.2685
env0_second_0:                 episode reward: 84.1500,                 loss: 0.1152
env1_first_0:                 episode reward: -92.6500,                 loss: nan
env1_second_0:                 episode reward: 92.6500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 234.1,                last time consumption/overall running time: 60.7624s / 62133.8073 s
env0_first_0:                 episode reward: -86.4500,                 loss: 0.2616
env0_second_0:                 episode reward: 86.4500,                 loss: 0.1133
env1_first_0:                 episode reward: -85.9500,                 loss: nan
env1_second_0:                 episode reward: 85.9500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 230.85,                last time consumption/overall running time: 60.0413s / 62193.8485 s
env0_first_0:                 episode reward: -91.3500,                 loss: 0.2634
env0_second_0:                 episode reward: 91.3500,                 loss: 0.1187
env1_first_0:                 episode reward: -92.7500,                 loss: nan
env1_second_0:                 episode reward: 92.7500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 226.05,                last time consumption/overall running time: 58.7250s / 62252.5735 s
env0_first_0:                 episode reward: -83.5500,                 loss: 0.2538
env0_second_0:                 episode reward: 83.5500,                 loss: 0.1132
env1_first_0:                 episode reward: -93.3500,                 loss: nan
env1_second_0:                 episode reward: 93.3500,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 229.55,                last time consumption/overall running time: 60.0485s / 62312.6220 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.2493
env0_second_0:                 episode reward: 88.8000,                 loss: 0.1154
env1_first_0:                 episode reward: -92.6000,                 loss: nan
env1_second_0:                 episode reward: 92.6000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 247.45,                last time consumption/overall running time: 63.9364s / 62376.5584 s
env0_first_0:                 episode reward: -90.5500,                 loss: 0.2338
env0_second_0:                 episode reward: 90.5500,                 loss: 0.1101
env1_first_0:                 episode reward: -83.3500,                 loss: nan
env1_second_0:                 episode reward: 83.3500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 227.75,                last time consumption/overall running time: 59.5999s / 62436.1583 s
env0_first_0:                 episode reward: -89.4000,                 loss: 0.2380
env0_second_0:                 episode reward: 89.4000,                 loss: 0.1105
env1_first_0:                 episode reward: -90.6000,                 loss: nan
env1_second_0:                 episode reward: 90.6000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 225.9,                last time consumption/overall running time: 58.6759s / 62494.8342 s
env0_first_0:                 episode reward: -85.8000,                 loss: 0.2301
env0_second_0:                 episode reward: 85.8000,                 loss: 0.1107
env1_first_0:                 episode reward: -94.6500,                 loss: nan
env1_second_0:                 episode reward: 94.6500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 226.25,                last time consumption/overall running time: 59.3576s / 62554.1918 s
env0_first_0:                 episode reward: -85.4500,                 loss: 0.2334
env0_second_0:                 episode reward: 85.4500,                 loss: 0.1183
env1_first_0:                 episode reward: -93.0000,                 loss: nan
env1_second_0:                 episode reward: 93.0000,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 229.3,                last time consumption/overall running time: 59.9201s / 62614.1119 s
env0_first_0:                 episode reward: -86.7500,                 loss: 0.2241
env0_second_0:                 episode reward: 86.7500,                 loss: 0.1198
env1_first_0:                 episode reward: -93.5500,                 loss: nan
env1_second_0:                 episode reward: 93.5500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 237.0,                last time consumption/overall running time: 61.3034s / 62675.4153 s
env0_first_0:                 episode reward: -92.0000,                 loss: 0.2290
env0_second_0:                 episode reward: 92.0000,                 loss: 0.1211
env1_first_0:                 episode reward: -79.3500,                 loss: nan
env1_second_0:                 episode reward: 79.3500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 246.7,                last time consumption/overall running time: 63.5893s / 62739.0046 s
env0_first_0:                 episode reward: -84.2000,                 loss: 0.2279
env0_second_0:                 episode reward: 84.2000,                 loss: 0.1239
env1_first_0:                 episode reward: -78.9500,                 loss: nan
env1_second_0:                 episode reward: 78.9500,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 241.05,                last time consumption/overall running time: 62.7361s / 62801.7406 s
env0_first_0:                 episode reward: -82.3500,                 loss: 0.2493
env0_second_0:                 episode reward: 82.3500,                 loss: 0.1291
env1_first_0:                 episode reward: -85.4000,                 loss: nan
env1_second_0:                 episode reward: 85.4000,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 231.15,                last time consumption/overall running time: 58.1393s / 62859.8799 s
env0_first_0:                 episode reward: -78.9000,                 loss: 0.2581
env0_second_0:                 episode reward: 78.9000,                 loss: 0.1267
env1_first_0:                 episode reward: -87.6000,                 loss: nan
env1_second_0:                 episode reward: 87.6000,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 231.2,                last time consumption/overall running time: 59.4027s / 62919.2826 s
env0_first_0:                 episode reward: -74.7000,                 loss: 0.2469
env0_second_0:                 episode reward: 74.7000,                 loss: 0.1302
env1_first_0:                 episode reward: -88.4000,                 loss: nan
env1_second_0:                 episode reward: 88.4000,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 229.45,                last time consumption/overall running time: 59.7794s / 62979.0620 s
env0_first_0:                 episode reward: -94.3500,                 loss: 0.2476
env0_second_0:                 episode reward: 94.3500,                 loss: 0.1280
env1_first_0:                 episode reward: -84.4000,                 loss: nan
env1_second_0:                 episode reward: 84.4000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 238.0,                last time consumption/overall running time: 62.3888s / 63041.4508 s
env0_first_0:                 episode reward: -94.9500,                 loss: 0.2394
env0_second_0:                 episode reward: 94.9500,                 loss: 0.1265
env1_first_0:                 episode reward: -76.1500,                 loss: nan
env1_second_0:                 episode reward: 76.1500,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 246.65,                last time consumption/overall running time: 65.1534s / 63106.6042 s
env0_first_0:                 episode reward: -90.5500,                 loss: 0.2501
env0_second_0:                 episode reward: 90.5500,                 loss: 0.1279
env1_first_0:                 episode reward: -81.2000,                 loss: nan
env1_second_0:                 episode reward: 81.2000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 280.75,                last time consumption/overall running time: 72.0016s / 63178.6059 s
env0_first_0:                 episode reward: -84.1500,                 loss: 0.2722
env0_second_0:                 episode reward: 84.1500,                 loss: 0.1319
env1_first_0:                 episode reward: -86.1500,                 loss: nan
env1_second_0:                 episode reward: 86.1500,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 233.9,                last time consumption/overall running time: 61.0951s / 63239.7010 s
env0_first_0:                 episode reward: -84.1000,                 loss: 0.2687
env0_second_0:                 episode reward: 84.1000,                 loss: 0.1379
env1_first_0:                 episode reward: -80.9000,                 loss: nan
env1_second_0:                 episode reward: 80.9000,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 223.2,                last time consumption/overall running time: 59.7336s / 63299.4346 s
env0_first_0:                 episode reward: -78.7000,                 loss: 0.2755
env0_second_0:                 episode reward: 78.7000,                 loss: 0.1329
env1_first_0:                 episode reward: -86.9000,                 loss: nan
env1_second_0:                 episode reward: 86.9000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 235.3,                last time consumption/overall running time: 61.7924s / 63361.2269 s
env0_first_0:                 episode reward: -84.2500,                 loss: 0.2809
env0_second_0:                 episode reward: 84.2500,                 loss: 0.1369
env1_first_0:                 episode reward: -90.9500,                 loss: nan
env1_second_0:                 episode reward: 90.9500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 230.25,                last time consumption/overall running time: 60.4420s / 63421.6689 s
env0_first_0:                 episode reward: -90.4500,                 loss: 0.3035
env0_second_0:                 episode reward: 90.4500,                 loss: 0.1349
env1_first_0:                 episode reward: -83.2500,                 loss: nan
env1_second_0:                 episode reward: 83.2500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 231.95,                last time consumption/overall running time: 59.9033s / 63481.5722 s
env0_first_0:                 episode reward: -83.8000,                 loss: 0.2930
env0_second_0:                 episode reward: 83.8000,                 loss: 0.1338
env1_first_0:                 episode reward: -80.7000,                 loss: nan
env1_second_0:                 episode reward: 80.7000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 224.1,                last time consumption/overall running time: 56.2035s / 63537.7757 s
env0_first_0:                 episode reward: -85.8500,                 loss: 0.3132
env0_second_0:                 episode reward: 85.8500,                 loss: 0.1263
env1_first_0:                 episode reward: -86.6500,                 loss: nan
env1_second_0:                 episode reward: 86.6500,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 225.85,                last time consumption/overall running time: 58.3805s / 63596.1562 s
env0_first_0:                 episode reward: -73.9500,                 loss: 0.2818
env0_second_0:                 episode reward: 73.9500,                 loss: 0.1306
env1_first_0:                 episode reward: -90.9000,                 loss: nan
env1_second_0:                 episode reward: 90.9000,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 241.95,                last time consumption/overall running time: 62.5111s / 63658.6673 s
env0_first_0:                 episode reward: -89.1500,                 loss: 0.2566
env0_second_0:                 episode reward: 89.1500,                 loss: 0.1225
env1_first_0:                 episode reward: -69.8500,                 loss: nan
env1_second_0:                 episode reward: 69.8500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 235.35,                last time consumption/overall running time: 60.2884s / 63718.9557 s
env0_first_0:                 episode reward: -74.2500,                 loss: 0.2473
env0_second_0:                 episode reward: 74.2500,                 loss: 0.1227
env1_first_0:                 episode reward: -79.1500,                 loss: nan
env1_second_0:                 episode reward: 79.1500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 232.45,                last time consumption/overall running time: 59.5541s / 63778.5098 s
env0_first_0:                 episode reward: -76.6500,                 loss: 0.3054
env0_second_0:                 episode reward: 76.6500,                 loss: 0.1249
env1_first_0:                 episode reward: -79.8500,                 loss: nan
env1_second_0:                 episode reward: 79.8500,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 253.2,                last time consumption/overall running time: 63.8335s / 63842.3433 s
env0_first_0:                 episode reward: -75.9000,                 loss: 0.2768
env0_second_0:                 episode reward: 75.9000,                 loss: 0.1267
env1_first_0:                 episode reward: -88.8500,                 loss: nan
env1_second_0:                 episode reward: 88.8500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 249.75,                last time consumption/overall running time: 62.9299s / 63905.2732 s
env0_first_0:                 episode reward: -80.4000,                 loss: 0.2894
env0_second_0:                 episode reward: 80.4000,                 loss: 0.1319
env1_first_0:                 episode reward: -70.2500,                 loss: nan
env1_second_0:                 episode reward: 70.2500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 230.85,                last time consumption/overall running time: 56.8219s / 63962.0951 s
env0_first_0:                 episode reward: -75.3500,                 loss: 0.3082
env0_second_0:                 episode reward: 75.3500,                 loss: 0.1257
env1_first_0:                 episode reward: -92.7000,                 loss: nan
env1_second_0:                 episode reward: 92.7000,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 236.3,                last time consumption/overall running time: 61.6201s / 64023.7152 s
env0_first_0:                 episode reward: -83.5500,                 loss: 0.3082
env0_second_0:                 episode reward: 83.5500,                 loss: 0.1293
env1_first_0:                 episode reward: -70.0000,                 loss: nan
env1_second_0:                 episode reward: 70.0000,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 229.85,                last time consumption/overall running time: 59.5606s / 64083.2758 s
env0_first_0:                 episode reward: -91.9000,                 loss: 0.3160
env0_second_0:                 episode reward: 91.9000,                 loss: 0.1160
env1_first_0:                 episode reward: -84.5000,                 loss: nan
env1_second_0:                 episode reward: 84.5000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 242.35,                last time consumption/overall running time: 62.9037s / 64146.1795 s
env0_first_0:                 episode reward: -79.7000,                 loss: 0.2897
env0_second_0:                 episode reward: 79.7000,                 loss: 0.1163
env1_first_0:                 episode reward: -83.3500,                 loss: nan
env1_second_0:                 episode reward: 83.3500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 236.1,                last time consumption/overall running time: 61.5800s / 64207.7596 s
env0_first_0:                 episode reward: -90.3500,                 loss: 0.2834
env0_second_0:                 episode reward: 90.3500,                 loss: 0.1112
env1_first_0:                 episode reward: -75.8000,                 loss: nan
env1_second_0:                 episode reward: 75.8000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 241.4,                last time consumption/overall running time: 61.9215s / 64269.6811 s
env0_first_0:                 episode reward: -88.1000,                 loss: 0.3150
env0_second_0:                 episode reward: 88.1000,                 loss: 0.1219
env1_first_0:                 episode reward: -80.5500,                 loss: nan
env1_second_0:                 episode reward: 80.5500,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 228.9,                last time consumption/overall running time: 56.8070s / 64326.4880 s
env0_first_0:                 episode reward: -80.7000,                 loss: 0.3066
env0_second_0:                 episode reward: 80.7000,                 loss: 0.1176
env1_first_0:                 episode reward: -72.6000,                 loss: nan
env1_second_0:                 episode reward: 72.6000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 224.95,                last time consumption/overall running time: 55.9354s / 64382.4234 s
env0_first_0:                 episode reward: -80.5000,                 loss: 0.3293
env0_second_0:                 episode reward: 80.5000,                 loss: 0.1188
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 227.75,                last time consumption/overall running time: 56.1584s / 64438.5818 s
env0_first_0:                 episode reward: -83.6000,                 loss: 0.3511
env0_second_0:                 episode reward: 83.6000,                 loss: 0.1104
env1_first_0:                 episode reward: -77.0500,                 loss: nan
env1_second_0:                 episode reward: 77.0500,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 223.65,                last time consumption/overall running time: 55.3151s / 64493.8969 s
env0_first_0:                 episode reward: -90.6000,                 loss: 0.3304
env0_second_0:                 episode reward: 90.6000,                 loss: 0.1274
env1_first_0:                 episode reward: -64.7500,                 loss: nan
env1_second_0:                 episode reward: 64.7500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 227.95,                last time consumption/overall running time: 56.8972s / 64550.7941 s
env0_first_0:                 episode reward: -73.3500,                 loss: 0.3023
env0_second_0:                 episode reward: 73.3500,                 loss: 0.1142
env1_first_0:                 episode reward: -69.3500,                 loss: nan
env1_second_0:                 episode reward: 69.3500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 247.55,                last time consumption/overall running time: 65.2296s / 64616.0237 s
env0_first_0:                 episode reward: -91.2000,                 loss: 0.2973
env0_second_0:                 episode reward: 91.2000,                 loss: 0.1194
env1_first_0:                 episode reward: -71.4500,                 loss: nan
env1_second_0:                 episode reward: 71.4500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 227.05,                last time consumption/overall running time: 60.8562s / 64676.8799 s
env0_first_0:                 episode reward: -89.8500,                 loss: 0.2919
env0_second_0:                 episode reward: 89.8500,                 loss: 0.1091
env1_first_0:                 episode reward: -80.3000,                 loss: nan
env1_second_0:                 episode reward: 80.3000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 220.7,                last time consumption/overall running time: 57.5008s / 64734.3807 s
env0_first_0:                 episode reward: -91.4000,                 loss: 0.2667
env0_second_0:                 episode reward: 91.4000,                 loss: 0.1090
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 289.55,                last time consumption/overall running time: 74.6229s / 64809.0036 s
env0_first_0:                 episode reward: -82.1500,                 loss: 0.2582
env0_second_0:                 episode reward: 82.1500,                 loss: 0.1030
env1_first_0:                 episode reward: -77.9000,                 loss: nan
env1_second_0:                 episode reward: 77.9000,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 232.1,                last time consumption/overall running time: 61.0807s / 64870.0843 s
env0_first_0:                 episode reward: -75.3000,                 loss: 0.2631
env0_second_0:                 episode reward: 75.3000,                 loss: 0.1052
env1_first_0:                 episode reward: -88.0500,                 loss: nan
env1_second_0:                 episode reward: 88.0500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 240.5,                last time consumption/overall running time: 62.4910s / 64932.5753 s
env0_first_0:                 episode reward: -84.2500,                 loss: 0.2825
env0_second_0:                 episode reward: 84.2500,                 loss: 0.1050
env1_first_0:                 episode reward: -83.2500,                 loss: nan
env1_second_0:                 episode reward: 83.2500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 222.25,                last time consumption/overall running time: 57.0481s / 64989.6234 s
env0_first_0:                 episode reward: -95.4500,                 loss: 0.2865
env0_second_0:                 episode reward: 95.4500,                 loss: 0.1029
env1_first_0:                 episode reward: -83.4500,                 loss: nan
env1_second_0:                 episode reward: 83.4500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 240.3,                last time consumption/overall running time: 62.7531s / 65052.3765 s
env0_first_0:                 episode reward: -83.5000,                 loss: 0.2677
env0_second_0:                 episode reward: 83.5000,                 loss: 0.1014
env1_first_0:                 episode reward: -85.6000,                 loss: nan
env1_second_0:                 episode reward: 85.6000,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 234.05,                last time consumption/overall running time: 60.4669s / 65112.8435 s
env0_first_0:                 episode reward: -93.9500,                 loss: 0.2535
env0_second_0:                 episode reward: 93.9500,                 loss: 0.0920
env1_first_0:                 episode reward: -81.2500,                 loss: nan
env1_second_0:                 episode reward: 81.2500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 219.45,                last time consumption/overall running time: 56.8552s / 65169.6987 s
env0_first_0:                 episode reward: -88.8000,                 loss: 0.2594
env0_second_0:                 episode reward: 88.8000,                 loss: 0.1027
env1_first_0:                 episode reward: -81.7500,                 loss: nan
env1_second_0:                 episode reward: 81.7500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 219.6,                last time consumption/overall running time: 57.1012s / 65226.7999 s
env0_first_0:                 episode reward: -86.3500,                 loss: 0.2312
env0_second_0:                 episode reward: 86.3500,                 loss: 0.1064
env1_first_0:                 episode reward: -80.6500,                 loss: nan
env1_second_0:                 episode reward: 80.6500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 231.45,                last time consumption/overall running time: 58.9080s / 65285.7079 s
env0_first_0:                 episode reward: -88.8500,                 loss: 0.2261
env0_second_0:                 episode reward: 88.8500,                 loss: 0.1060
env1_first_0:                 episode reward: -83.9000,                 loss: nan
env1_second_0:                 episode reward: 83.9000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 236.6,                last time consumption/overall running time: 60.9042s / 65346.6121 s
env0_first_0:                 episode reward: -89.2000,                 loss: 0.2341
env0_second_0:                 episode reward: 89.2000,                 loss: 0.0965
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 229.25,                last time consumption/overall running time: 58.9879s / 65405.6000 s
env0_first_0:                 episode reward: -90.0500,                 loss: 0.2307
env0_second_0:                 episode reward: 90.0500,                 loss: 0.0976
env1_first_0:                 episode reward: -81.9500,                 loss: nan
env1_second_0:                 episode reward: 81.9500,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 236.1,                last time consumption/overall running time: 62.2044s / 65467.8044 s
env0_first_0:                 episode reward: -91.4500,                 loss: 0.2535
env0_second_0:                 episode reward: 91.4500,                 loss: 0.0991
env1_first_0:                 episode reward: -83.7000,                 loss: nan
env1_second_0:                 episode reward: 83.7000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 222.05,                last time consumption/overall running time: 58.9964s / 65526.8009 s
env0_first_0:                 episode reward: -75.9500,                 loss: 0.2579
env0_second_0:                 episode reward: 75.9500,                 loss: 0.1000
env1_first_0:                 episode reward: -89.4500,                 loss: nan
env1_second_0:                 episode reward: 89.4500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 225.8,                last time consumption/overall running time: 57.4302s / 65584.2310 s
env0_first_0:                 episode reward: -89.2500,                 loss: 0.2618
env0_second_0:                 episode reward: 89.2500,                 loss: 0.1084
env1_first_0:                 episode reward: -86.2500,                 loss: nan
env1_second_0:                 episode reward: 86.2500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 237.5,                last time consumption/overall running time: 60.9501s / 65645.1811 s
env0_first_0:                 episode reward: -87.1000,                 loss: 0.2769
env0_second_0:                 episode reward: 87.1000,                 loss: 0.1131
env1_first_0:                 episode reward: -89.1500,                 loss: nan
env1_second_0:                 episode reward: 89.1500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 232.65,                last time consumption/overall running time: 61.0654s / 65706.2465 s
env0_first_0:                 episode reward: -91.9500,                 loss: 0.2753
env0_second_0:                 episode reward: 91.9500,                 loss: 0.1233
env1_first_0:                 episode reward: -80.9000,                 loss: nan
env1_second_0:                 episode reward: 80.9000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 223.7,                last time consumption/overall running time: 58.3326s / 65764.5791 s
env0_first_0:                 episode reward: -93.9500,                 loss: 0.2818
env0_second_0:                 episode reward: 93.9500,                 loss: 0.1094
env1_first_0:                 episode reward: -77.8000,                 loss: nan
env1_second_0:                 episode reward: 77.8000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 237.55,                last time consumption/overall running time: 62.4244s / 65827.0035 s
env0_first_0:                 episode reward: -88.7000,                 loss: 0.2539
env0_second_0:                 episode reward: 88.7000,                 loss: 0.1205
env1_first_0:                 episode reward: -79.4500,                 loss: nan
env1_second_0:                 episode reward: 79.4500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 235.95,                last time consumption/overall running time: 61.6517s / 65888.6553 s
env0_first_0:                 episode reward: -90.2500,                 loss: 0.2679
env0_second_0:                 episode reward: 90.2500,                 loss: 0.1156
env1_first_0:                 episode reward: -72.1000,                 loss: nan
env1_second_0:                 episode reward: 72.1000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 233.2,                last time consumption/overall running time: 60.4531s / 65949.1083 s
env0_first_0:                 episode reward: -91.5500,                 loss: 0.2922
env0_second_0:                 episode reward: 91.5500,                 loss: 0.1110
env1_first_0:                 episode reward: -86.1000,                 loss: nan
env1_second_0:                 episode reward: 86.1000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 233.2,                last time consumption/overall running time: 59.3092s / 66008.4176 s
env0_first_0:                 episode reward: -91.1500,                 loss: 0.2638
env0_second_0:                 episode reward: 91.1500,                 loss: 0.1101
env1_first_0:                 episode reward: -84.6500,                 loss: nan
env1_second_0:                 episode reward: 84.6500,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 218.9,                last time consumption/overall running time: 56.7824s / 66065.2000 s
env0_first_0:                 episode reward: -73.9000,                 loss: 0.2615
env0_second_0:                 episode reward: 73.9000,                 loss: 0.1205
env1_first_0:                 episode reward: -91.9000,                 loss: nan
env1_second_0:                 episode reward: 91.9000,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 239.35,                last time consumption/overall running time: 62.4315s / 66127.6315 s
env0_first_0:                 episode reward: -83.5500,                 loss: 0.2602
env0_second_0:                 episode reward: 83.5500,                 loss: 0.1142
env1_first_0:                 episode reward: -90.3000,                 loss: nan
env1_second_0:                 episode reward: 90.3000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 225.85,                last time consumption/overall running time: 58.0070s / 66185.6386 s
env0_first_0:                 episode reward: -91.6500,                 loss: 0.2341
env0_second_0:                 episode reward: 91.6500,                 loss: 0.1223
env1_first_0:                 episode reward: -91.3500,                 loss: nan
env1_second_0:                 episode reward: 91.3500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 224.2,                last time consumption/overall running time: 56.8319s / 66242.4704 s
env0_first_0:                 episode reward: -83.9500,                 loss: 0.2229
env0_second_0:                 episode reward: 83.9500,                 loss: 0.1167
env1_first_0:                 episode reward: -89.4000,                 loss: nan
env1_second_0:                 episode reward: 89.4000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 227.2,                last time consumption/overall running time: 57.8564s / 66300.3269 s
env0_first_0:                 episode reward: -85.9000,                 loss: 0.2389
env0_second_0:                 episode reward: 85.9000,                 loss: 0.1068
env1_first_0:                 episode reward: -87.9000,                 loss: nan
env1_second_0:                 episode reward: 87.9000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 243.85,                last time consumption/overall running time: 62.3526s / 66362.6795 s
env0_first_0:                 episode reward: -87.3500,                 loss: 0.2503
env0_second_0:                 episode reward: 87.3500,                 loss: 0.0997
env1_first_0:                 episode reward: -84.3000,                 loss: nan
env1_second_0:                 episode reward: 84.3000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 222.7,                last time consumption/overall running time: 57.7502s / 66420.4297 s
env0_first_0:                 episode reward: -86.5000,                 loss: 0.2724
env0_second_0:                 episode reward: 86.5000,                 loss: 0.1076
env1_first_0:                 episode reward: -92.1000,                 loss: nan
env1_second_0:                 episode reward: 92.1000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 225.7,                last time consumption/overall running time: 57.4272s / 66477.8569 s
env0_first_0:                 episode reward: -85.4500,                 loss: 0.2628
env0_second_0:                 episode reward: 85.4500,                 loss: 0.1115
env1_first_0:                 episode reward: -93.1500,                 loss: nan
env1_second_0:                 episode reward: 93.1500,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 248.05,                last time consumption/overall running time: 63.8906s / 66541.7475 s
env0_first_0:                 episode reward: -97.2000,                 loss: 0.2532
env0_second_0:                 episode reward: 97.2000,                 loss: 0.1071
env1_first_0:                 episode reward: -80.2500,                 loss: nan
env1_second_0:                 episode reward: 80.2500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 236.25,                last time consumption/overall running time: 61.4274s / 66603.1749 s
env0_first_0:                 episode reward: -92.4500,                 loss: 0.2456
env0_second_0:                 episode reward: 92.4500,                 loss: 0.1048
env1_first_0:                 episode reward: -90.4000,                 loss: nan
env1_second_0:                 episode reward: 90.4000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 228.45,                last time consumption/overall running time: 58.5897s / 66661.7646 s
env0_first_0:                 episode reward: -87.6500,                 loss: 0.2435
env0_second_0:                 episode reward: 87.6500,                 loss: 0.1125
env1_first_0:                 episode reward: -80.4000,                 loss: nan
env1_second_0:                 episode reward: 80.4000,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 223.5,                last time consumption/overall running time: 57.4619s / 66719.2265 s
env0_first_0:                 episode reward: -87.5000,                 loss: 0.2407
env0_second_0:                 episode reward: 87.5000,                 loss: 0.1109
env1_first_0:                 episode reward: -82.8500,                 loss: nan
env1_second_0:                 episode reward: 82.8500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 227.55,                last time consumption/overall running time: 57.5021s / 66776.7286 s
env0_first_0:                 episode reward: -87.3000,                 loss: 0.2367
env0_second_0:                 episode reward: 87.3000,                 loss: 0.1085Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load boxing_v1 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rl/agents/nash_dqn_exploiter.py:197: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  action = torch.LongTensor(action).to(self.device)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env1_first_0:                 episode reward: -91.7000,                 loss: nan
env1_second_0:                 episode reward: 91.7000,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 232.1,                last time consumption/overall running time: 60.1828s / 66836.9114 s
env0_first_0:                 episode reward: -86.7500,                 loss: 0.2437
env0_second_0:                 episode reward: 86.7500,                 loss: 0.1159
env1_first_0:                 episode reward: -89.3500,                 loss: nan
env1_second_0:                 episode reward: 89.3500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 228.45,                last time consumption/overall running time: 60.7641s / 66897.6755 s
env0_first_0:                 episode reward: -92.8500,                 loss: 0.2328
env0_second_0:                 episode reward: 92.8500,                 loss: 0.1161
env1_first_0:                 episode reward: -81.5500,                 loss: nan
env1_second_0:                 episode reward: 81.5500,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 244.45,                last time consumption/overall running time: 65.4070s / 66963.0825 s
env0_first_0:                 episode reward: -78.0500,                 loss: 0.2372
env0_second_0:                 episode reward: 78.0500,                 loss: 0.1282
env1_first_0:                 episode reward: -92.5500,                 loss: nan
env1_second_0:                 episode reward: 92.5500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 223.55,                last time consumption/overall running time: 57.1574s / 67020.2399 s
env0_first_0:                 episode reward: -88.4000,                 loss: 0.2390
env0_second_0:                 episode reward: 88.4000,                 loss: 0.1188
env1_first_0:                 episode reward: -93.6500,                 loss: nan
env1_second_0:                 episode reward: 93.6500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 229.7,                last time consumption/overall running time: 58.1581s / 67078.3979 s
env0_first_0:                 episode reward: -87.7000,                 loss: 0.2276
env0_second_0:                 episode reward: 87.7000,                 loss: 0.1147
env1_first_0:                 episode reward: -94.0500,                 loss: nan
env1_second_0:                 episode reward: 94.0500,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 219.55,                last time consumption/overall running time: 58.2585s / 67136.6565 s
env0_first_0:                 episode reward: -86.4000,                 loss: 0.2252
env0_second_0:                 episode reward: 86.4000,                 loss: 0.1134
env1_first_0:                 episode reward: -90.6000,                 loss: nan
env1_second_0:                 episode reward: 90.6000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 230.5,                last time consumption/overall running time: 59.0324s / 67195.6889 s
env0_first_0:                 episode reward: -82.9000,                 loss: 0.2320
env0_second_0:                 episode reward: 82.9000,                 loss: 0.1130
env1_first_0:                 episode reward: -93.6000,                 loss: nan
env1_second_0:                 episode reward: 93.6000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 233.45,                last time consumption/overall running time: 60.0417s / 67255.7305 s
env0_first_0:                 episode reward: -86.2500,                 loss: 0.2308
env0_second_0:                 episode reward: 86.2500,                 loss: 0.1198
env1_first_0:                 episode reward: -90.2000,                 loss: nan
env1_second_0:                 episode reward: 90.2000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 219.5,                last time consumption/overall running time: 56.4177s / 67312.1482 s
env0_first_0:                 episode reward: -89.9000,                 loss: 0.2250
env0_second_0:                 episode reward: 89.9000,                 loss: 0.1109
env1_first_0:                 episode reward: -94.0000,                 loss: nan
env1_second_0:                 episode reward: 94.0000,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 223.7,                last time consumption/overall running time: 57.8702s / 67370.0185 s
env0_first_0:                 episode reward: -84.8000,                 loss: 0.2340
env0_second_0:                 episode reward: 84.8000,                 loss: 0.1104
env1_first_0:                 episode reward: -84.2500,                 loss: nan
env1_second_0:                 episode reward: 84.2500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 232.9,                last time consumption/overall running time: 59.4526s / 67429.4710 s
env0_first_0:                 episode reward: -88.5500,                 loss: 0.2147
env0_second_0:                 episode reward: 88.5500,                 loss: 0.1111
env1_first_0:                 episode reward: -74.4500,                 loss: nan
env1_second_0:                 episode reward: 74.4500,                 loss: nan
