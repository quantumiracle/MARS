pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
pong_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=6, bias=True)
    )
  )
)
No agent are not learnable.
Arguments:  {'env_name': 'pong_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 5, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220206_1455/pettingzoo_pong_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220206_1455/pettingzoo_pong_v2_nash_dqn.
Episode: 1/10000 (0.0100%),                 avg. length: 1348.0,                last time consumption/overall running time: 54.4544s / 54.4544 s
env0_first_0:                 episode reward: 4.0000,                 loss: 0.0809
env0_second_0:                 episode reward: -4.0000,                 loss: 0.0785
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 1310.25,                last time consumption/overall running time: 1226.7816s / 1281.2360 s
env0_first_0:                 episode reward: 2.8000,                 loss: 0.0896
env0_second_0:                 episode reward: -2.8000,                 loss: 0.0901
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 1520.2,                last time consumption/overall running time: 1589.2458s / 2870.4818 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.1330
env0_second_0:                 episode reward: -1.7500,                 loss: 0.1278
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1590.95,                last time consumption/overall running time: 1863.7809s / 4734.2627 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.1160
env0_second_0:                 episode reward: 4.9500,                 loss: 0.1208
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1629.25,                last time consumption/overall running time: 2102.9288s / 6837.1915 s
env0_first_0:                 episode reward: -12.2500,                 loss: 0.1026
env0_second_0:                 episode reward: 12.2500,                 loss: 0.1002
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 1800.5,                last time consumption/overall running time: 2346.7473s / 9183.9388 s
env0_first_0:                 episode reward: -11.8500,                 loss: 0.0899
env0_second_0:                 episode reward: 11.8500,                 loss: 0.0868
env1_first_0:                 episode reward: -8.9000,                 loss: nan
env1_second_0:                 episode reward: 8.9000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 2108.85,                last time consumption/overall running time: 2738.8047s / 11922.7435 s
env0_first_0:                 episode reward: -8.7500,                 loss: 0.0699
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0656
env1_first_0:                 episode reward: -9.1000,                 loss: nan
env1_second_0:                 episode reward: 9.1000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 2293.6,                last time consumption/overall running time: 2967.2843s / 14890.0278 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0610
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0586
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 2226.05,                last time consumption/overall running time: 2899.7999s / 17789.8277 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0566
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0571
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2346.35,                last time consumption/overall running time: 3082.0147s / 20871.8424 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0581
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0581
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 2352.7,                last time consumption/overall running time: 3082.4659s / 23954.3083 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0560
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0567
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 2483.3,                last time consumption/overall running time: 3246.6166s / 27200.9249 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0580
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0560
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 2349.7,                last time consumption/overall running time: 3075.8205s / 30276.7454 s
env0_first_0:                 episode reward: -7.8500,                 loss: 0.0554
env0_second_0:                 episode reward: 7.8500,                 loss: 0.0553
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 2362.4,                last time consumption/overall running time: 3090.6013s / 33367.3467 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0556
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0544
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 2349.55,                last time consumption/overall running time: 3062.3093s / 36429.6561 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0543
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0532
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 2388.0,                last time consumption/overall running time: 3123.6697s / 39553.3258 s
env0_first_0:                 episode reward: -7.6500,                 loss: 0.0524
env0_second_0:                 episode reward: 7.6500,                 loss: 0.0522
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 2453.9,                last time consumption/overall running time: 3213.5603s / 42766.8861 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0536
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0517
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 2421.2,                last time consumption/overall running time: 3154.3862s / 45921.2723 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0531
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0511
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 2468.3,                last time consumption/overall running time: 3225.4344s / 49146.7068 s
env0_first_0:                 episode reward: -6.4000,                 loss: 0.0536
env0_second_0:                 episode reward: 6.4000,                 loss: 0.0536
env1_first_0:                 episode reward: -5.5000,                 loss: nan
env1_second_0:                 episode reward: 5.5000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 2367.2,                last time consumption/overall running time: 3116.6113s / 52263.3181 s
env0_first_0:                 episode reward: -4.7000,                 loss: 0.0545
env0_second_0:                 episode reward: 4.7000,                 loss: 0.0546
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 2463.15,                last time consumption/overall running time: 3211.8557s / 55475.1738 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0536
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0535
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 2208.3,                last time consumption/overall running time: 2886.2356s / 58361.4094 s
env0_first_0:                 episode reward: -7.5500,                 loss: 0.0539
env0_second_0:                 episode reward: 7.5500,                 loss: 0.0525
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2504.9,                last time consumption/overall running time: 3282.1832s / 61643.5927 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0520
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0514
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 2452.05,                last time consumption/overall running time: 3194.7820s / 64838.3747 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0514
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0515
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 2502.35,                last time consumption/overall running time: 3279.6885s / 68118.0632 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0524
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0526
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 2535.75,                last time consumption/overall running time: 3323.8324s / 71441.8956 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.0520
env0_second_0:                 episode reward: 4.1000,                 loss: 0.0516
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2354.55,                last time consumption/overall running time: 3064.0317s / 74505.9273 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0522
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0508
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 2533.65,                last time consumption/overall running time: 3285.8748s / 77791.8021 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.0519
env0_second_0:                 episode reward: 2.8000,                 loss: 0.0513
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 2406.4,                last time consumption/overall running time: 3148.3114s / 80940.1135 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0510
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0501
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2443.4,                last time consumption/overall running time: 3181.9811s / 84122.0946 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0497
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0496
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2425.5,                last time consumption/overall running time: 3143.5554s / 87265.6501 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0493
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0488
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2553.6,                last time consumption/overall running time: 3332.2158s / 90597.8659 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0497
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0496
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2437.75,                last time consumption/overall running time: 3167.1635s / 93765.0294 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0488
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0502
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2552.1,                last time consumption/overall running time: 3316.2687s / 97081.2981 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0485
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0483
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 2329.05,                last time consumption/overall running time: 3038.0563s / 100119.3544 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0479
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0474
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2400.4,                last time consumption/overall running time: 3108.7825s / 103228.1369 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0487
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0490
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 2604.45,                last time consumption/overall running time: 3382.9201s / 106611.0570 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0489
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0489
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2494.55,                last time consumption/overall running time: 3243.6936s / 109854.7506 s
env0_first_0:                 episode reward: -7.3500,                 loss: 0.0493
env0_second_0:                 episode reward: 7.3500,                 loss: 0.0502
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2405.5,                last time consumption/overall running time: 3130.6565s / 112985.4072 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0503
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0514
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2399.75,                last time consumption/overall running time: 3118.4859s / 116103.8931 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0511
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0504
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2366.0,                last time consumption/overall running time: 3090.0300s / 119193.9231 s
env0_first_0:                 episode reward: -2.1500,                 loss: 0.0499
env0_second_0:                 episode reward: 2.1500,                 loss: 0.0494
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2466.2,                last time consumption/overall running time: 3232.2803s / 122426.2034 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0504
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0498
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 2452.8,                last time consumption/overall running time: 3184.3869s / 125610.5903 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0508
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0490
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2471.0,                last time consumption/overall running time: 3211.2385s / 128821.8288 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0497
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0488
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2387.95,                last time consumption/overall running time: 3092.1578s / 131913.9866 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0494
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0495
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2309.1,                last time consumption/overall running time: 2983.3573s / 134897.3438 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0488
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0485
env1_first_0:                 episode reward: -9.1500,                 loss: nan
env1_second_0:                 episode reward: 9.1500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 2477.95,                last time consumption/overall running time: 3189.5039s / 138086.8477 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0492
env0_second_0:                 episode reward: 4.2000,                 loss: 0.0493
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2399.5,                last time consumption/overall running time: 3102.4958s / 141189.3435 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.0507
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0495
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 2447.2,                last time consumption/overall running time: 3155.8250s / 144345.1686 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0511
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0513
env1_first_0:                 episode reward: -5.2500,                 loss: nan
env1_second_0:                 episode reward: 5.2500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 2392.5,                last time consumption/overall running time: 3086.6151s / 147431.7836 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0507
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0510
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 2311.8,                last time consumption/overall running time: 2997.8541s / 150429.6377 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0499
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0502
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2076.9,                last time consumption/overall running time: 2629.4785s / 153059.1162 s
env0_first_0:                 episode reward: -8.2000,                 loss: 0.0496
env0_second_0:                 episode reward: 8.2000,                 loss: 0.0499
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2180.5,                last time consumption/overall running time: 2716.6781s / 155775.7943 s
env0_first_0:                 episode reward: -8.5500,                 loss: 0.0506
env0_second_0:                 episode reward: 8.5500,                 loss: 0.0505
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2271.15,                last time consumption/overall running time: 2845.1849s / 158620.9792 s
env0_first_0:                 episode reward: -9.1000,                 loss: 0.0492
env0_second_0:                 episode reward: 9.1000,                 loss: 0.0495
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2185.8,                last time consumption/overall running time: 2742.0747s / 161363.0539 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0499
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0501
env1_first_0:                 episode reward: -8.4000,                 loss: nan
env1_second_0:                 episode reward: 8.4000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2431.65,                last time consumption/overall running time: 3018.1227s / 164381.1766 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0520
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0519
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2359.75,                last time consumption/overall running time: 2945.5325s / 167326.7091 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0524
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0518
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2277.4,                last time consumption/overall running time: 2845.5188s / 170172.2279 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0519
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0515
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2398.75,                last time consumption/overall running time: 2979.6500s / 173151.8779 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0517
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0518
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2313.9,                last time consumption/overall running time: 2875.9037s / 176027.7816 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0512
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0511
env1_first_0:                 episode reward: -8.6000,                 loss: nan
env1_second_0:                 episode reward: 8.6000,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2342.85,                last time consumption/overall running time: 2947.7701s / 178975.5516 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0508
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0510
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2426.1,                last time consumption/overall running time: 3021.5972s / 181997.1488 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0505
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0511
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2358.6,                last time consumption/overall running time: 2892.8298s / 184889.9786 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0515
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0519
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2350.7,                last time consumption/overall running time: 2894.5112s / 187784.4898 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0526
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0521
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 2316.5,                last time consumption/overall running time: 2829.0348s / 190613.5246 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0525
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0523
env1_first_0:                 episode reward: -4.4500,                 loss: nan
env1_second_0:                 episode reward: 4.4500,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2296.35,                last time consumption/overall running time: 2806.5172s / 193420.0418 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0532
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0532
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 2254.65,                last time consumption/overall running time: 2771.7147s / 196191.7565 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0519
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0529
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 2326.5,                last time consumption/overall running time: 2855.8098s / 199047.5663 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0523
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0514
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 2310.0,                last time consumption/overall running time: 2852.3862s / 201899.9525 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0527
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0518
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2207.45,                last time consumption/overall running time: 2732.1082s / 204632.0606 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0527
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0505
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2471.45,                last time consumption/overall running time: 3014.2388s / 207646.2994 s
env0_first_0:                 episode reward: -5.1000,                 loss: 0.0526
env0_second_0:                 episode reward: 5.1000,                 loss: 0.0527
env1_first_0:                 episode reward: -4.9500,                 loss: nan
env1_second_0:                 episode reward: 4.9500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2219.3,                last time consumption/overall running time: 2712.1862s / 210358.4856 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0532
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0531
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2211.35,                last time consumption/overall running time: 2726.6663s / 213085.1520 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0535
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0540
env1_first_0:                 episode reward: -6.1500,                 loss: nan
env1_second_0:                 episode reward: 6.1500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2326.55,                last time consumption/overall running time: 2840.3927s / 215925.5446 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0534
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0543
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2333.75,                last time consumption/overall running time: 2852.2311s / 218777.7757 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0521
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0518
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2322.3,                last time consumption/overall running time: 2855.8010s / 221633.5767 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0519
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0517
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2394.15,                last time consumption/overall running time: 2956.4018s / 224589.9785 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0534
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0526
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2300.25,                last time consumption/overall running time: 2824.3791s / 227414.3576 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0535
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0527
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 2279.75,                last time consumption/overall running time: 2797.1332s / 230211.4909 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0531
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0526
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 2387.1,                last time consumption/overall running time: 2920.6565s / 233132.1473 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0526
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0528
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 2280.5,                last time consumption/overall running time: 2777.7500s / 235909.8973 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0537
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0523
env1_first_0:                 episode reward: -7.6500,                 loss: nan
env1_second_0:                 episode reward: 7.6500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 2105.9,                last time consumption/overall running time: 2549.5585s / 238459.4558 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0530
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0526
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 2135.75,                last time consumption/overall running time: 2585.3811s / 241044.8370 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0532
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0536
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2180.4,                last time consumption/overall running time: 2633.5901s / 243678.4271 s
env0_first_0:                 episode reward: -7.0000,                 loss: 0.0534
env0_second_0:                 episode reward: 7.0000,                 loss: 0.0541
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 2219.3,                last time consumption/overall running time: 2631.2737s / 246309.7008 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0534
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0538
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 2238.25,                last time consumption/overall running time: 2664.4340s / 248974.1348 s
env0_first_0:                 episode reward: -6.9000,                 loss: 0.0539
env0_second_0:                 episode reward: 6.9000,                 loss: 0.0532
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2176.65,                last time consumption/overall running time: 2586.4354s / 251560.5702 s
env0_first_0:                 episode reward: -6.0000,                 loss: 0.0548
env0_second_0:                 episode reward: 6.0000,                 loss: 0.0536
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 2194.35,                last time consumption/overall running time: 2599.9737s / 254160.5439 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.0547
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0535
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 2198.2,                last time consumption/overall running time: 2622.6215s / 256783.1654 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0546
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0535
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 2274.8,                last time consumption/overall running time: 2702.2326s / 259485.3980 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0536
env0_second_0:                 episode reward: 6.5500,                 loss: 0.0532
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 2103.8,                last time consumption/overall running time: 2503.6386s / 261989.0366 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0542
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0537
env1_first_0:                 episode reward: -5.5500,                 loss: nan
env1_second_0:                 episode reward: 5.5500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2249.9,                last time consumption/overall running time: 2675.5568s / 264664.5934 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0550
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0545
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 2157.15,                last time consumption/overall running time: 2574.1630s / 267238.7564 s
env0_first_0:                 episode reward: -6.6500,                 loss: 0.0553
env0_second_0:                 episode reward: 6.6500,                 loss: 0.0553
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 2230.2,                last time consumption/overall running time: 2663.9098s / 269902.6661 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0560
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0554
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 2279.25,                last time consumption/overall running time: 2709.5454s / 272612.2115 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0553
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0548
env1_first_0:                 episode reward: -6.1000,                 loss: nan
env1_second_0:                 episode reward: 6.1000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 2160.35,                last time consumption/overall running time: 2570.3044s / 275182.5159 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0548
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0544
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 2293.7,                last time consumption/overall running time: 2727.1177s / 277909.6336 s
env0_first_0:                 episode reward: -5.3000,                 loss: 0.0549
env0_second_0:                 episode reward: 5.3000,                 loss: 0.0543
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2155.4,                last time consumption/overall running time: 2524.2402s / 280433.8738 s
env0_first_0:                 episode reward: -7.9500,                 loss: 0.0538
env0_second_0:                 episode reward: 7.9500,                 loss: 0.0530
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 2220.15,                last time consumption/overall running time: 2602.8489s / 283036.7226 s
env0_first_0:                 episode reward: -6.3500,                 loss: 0.0537
env0_second_0:                 episode reward: 6.3500,                 loss: 0.0536
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 2107.2,                last time consumption/overall running time: 2472.5959s / 285509.3185 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0535
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0540
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 2181.55,                last time consumption/overall running time: 2565.0417s / 288074.3603 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0539
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0539
env1_first_0:                 episode reward: -8.3000,                 loss: nan
env1_second_0:                 episode reward: 8.3000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 2349.35,                last time consumption/overall running time: 2738.9591s / 290813.3194 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0539
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0538
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 2177.5,                last time consumption/overall running time: 2556.0294s / 293369.3488 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0527
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0530
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 2134.65,                last time consumption/overall running time: 2505.4889s / 295874.8377 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0531
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0540
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 2182.65,                last time consumption/overall running time: 2606.9866s / 298481.8243 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0539
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0540
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 2191.2,                last time consumption/overall running time: 2589.9196s / 301071.7438 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0549
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0555
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 2105.55,                last time consumption/overall running time: 2474.6769s / 303546.4208 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0551
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0546
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 2173.3,                last time consumption/overall running time: 2602.4046s / 306148.8253 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0540
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0545
env1_first_0:                 episode reward: -8.1000,                 loss: nan
env1_second_0:                 episode reward: 8.1000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 2127.3,                last time consumption/overall running time: 2488.5176s / 308637.3429 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0541
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0543
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 2011.25,                last time consumption/overall running time: 2352.3616s / 310989.7045 s
env0_first_0:                 episode reward: -7.5000,                 loss: 0.0530
env0_second_0:                 episode reward: 7.5000,                 loss: 0.0527
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 2245.75,                last time consumption/overall running time: 2629.9271s / 313619.6316 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0535
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0531
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 2192.65,                last time consumption/overall running time: 2600.8403s / 316220.4719 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0539
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0540
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 2170.65,                last time consumption/overall running time: 2566.6760s / 318787.1479 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0537
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0531
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 2150.4,                last time consumption/overall running time: 2516.7028s / 321303.8507 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0549
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0536
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 2289.4,                last time consumption/overall running time: 2701.4870s / 324005.3376 s
env0_first_0:                 episode reward: -4.9000,                 loss: 0.0540
env0_second_0:                 episode reward: 4.9000,                 loss: 0.0542
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 2162.35,                last time consumption/overall running time: 2521.6162s / 326526.9538 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0537
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0526
env1_first_0:                 episode reward: -6.7500,                 loss: nan
env1_second_0:                 episode reward: 6.7500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 2271.85,                last time consumption/overall running time: 2637.2347s / 329164.1886 s
env0_first_0:                 episode reward: -5.8500,                 loss: 0.0550
env0_second_0:                 episode reward: 5.8500,                 loss: 0.0534
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 2152.9,                last time consumption/overall running time: 2516.2350s / 331680.4236 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0559
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0549
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 2212.25,                last time consumption/overall running time: 2609.7482s / 334290.1717 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0573
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0574
env1_first_0:                 episode reward: -7.3000,                 loss: nan
env1_second_0:                 episode reward: 7.3000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 2130.4,                last time consumption/overall running time: 2506.9353s / 336797.1071 s
env0_first_0:                 episode reward: -9.6500,                 loss: 0.0565
env0_second_0:                 episode reward: 9.6500,                 loss: 0.0559
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 2216.6,                last time consumption/overall running time: 2609.3705s / 339406.4776 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0542
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0537
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 2208.55,                last time consumption/overall running time: 2632.6994s / 342039.1770 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0547
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0540
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 2224.65,                last time consumption/overall running time: 2655.0824s / 344694.2594 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0557
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0556
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 2105.05,                last time consumption/overall running time: 2448.0000s / 347142.2594 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0541
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0559
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 2134.75,                last time consumption/overall running time: 2503.1125s / 349645.3719 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0550
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0551
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 2145.2,                last time consumption/overall running time: 2556.8570s / 352202.2289 s
env0_first_0:                 episode reward: -5.4500,                 loss: 0.0553
env0_second_0:                 episode reward: 5.4500,                 loss: 0.0550
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 2143.75,                last time consumption/overall running time: 2573.7576s / 354775.9865 s
env0_first_0:                 episode reward: -6.1500,                 loss: 0.0556
env0_second_0:                 episode reward: 6.1500,                 loss: 0.0549
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 2214.45,                last time consumption/overall running time: 2644.0867s / 357420.0731 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0555
env0_second_0:                 episode reward: 4.4500,                 loss: 0.0555
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2128.6,                last time consumption/overall running time: 2518.0091s / 359938.0822 s
env0_first_0:                 episode reward: -7.1000,                 loss: 0.0561
env0_second_0:                 episode reward: 7.1000,                 loss: 0.0557
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 2103.1,                last time consumption/overall running time: 2448.0304s / 362386.1126 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0556
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0549
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 2146.45,                last time consumption/overall running time: 2477.3666s / 364863.4792 s
env0_first_0:                 episode reward: -7.8000,                 loss: 0.0548
env0_second_0:                 episode reward: 7.8000,                 loss: 0.0542
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 2060.9,                last time consumption/overall running time: 2423.3890s / 367286.8682 s
env0_first_0:                 episode reward: -7.0500,                 loss: 0.0552
env0_second_0:                 episode reward: 7.0500,                 loss: 0.0545
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 2208.75,                last time consumption/overall running time: 2585.0499s / 369871.9181 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0549
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0557
env1_first_0:                 episode reward: -6.6500,                 loss: nan
env1_second_0:                 episode reward: 6.6500,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 2231.9,                last time consumption/overall running time: 2624.8899s / 372496.8080 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.0547
env0_second_0:                 episode reward: 4.6000,                 loss: 0.0534
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 2148.55,                last time consumption/overall running time: 2508.4959s / 375005.3039 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.0528
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0533
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 2157.9,                last time consumption/overall running time: 2556.9978s / 377562.3017 s
env0_first_0:                 episode reward: -4.3000,                 loss: 0.0550
env0_second_0:                 episode reward: 4.3000,                 loss: 0.0555
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 2077.3,                last time consumption/overall running time: 2436.7818s / 379999.0835 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0571
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0565
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 1989.3,                last time consumption/overall running time: 2324.8880s / 382323.9715 s
env0_first_0:                 episode reward: -10.1500,                 loss: 0.0579
env0_second_0:                 episode reward: 10.1500,                 loss: 0.0566
env1_first_0:                 episode reward: -6.5000,                 loss: nan
env1_second_0:                 episode reward: 6.5000,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 2122.95,                last time consumption/overall running time: 2497.5007s / 384821.4722 s
env0_first_0:                 episode reward: -5.2500,                 loss: 0.0565
env0_second_0:                 episode reward: 5.2500,                 loss: 0.0572
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 2201.1,                last time consumption/overall running time: 2553.6121s / 387375.0844 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0545
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0552
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 2102.8,                last time consumption/overall running time: 2472.2236s / 389847.3080 s
env0_first_0:                 episode reward: -8.1500,                 loss: 0.0544
env0_second_0:                 episode reward: 8.1500,                 loss: 0.0548
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2104.6,                last time consumption/overall running time: 2481.9355s / 392329.2435 s
env0_first_0:                 episode reward: -4.5500,                 loss: 0.0545
env0_second_0:                 episode reward: 4.5500,                 loss: 0.0545
env1_first_0:                 episode reward: -7.4000,                 loss: nan
env1_second_0:                 episode reward: 7.4000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 2048.4,                last time consumption/overall running time: 2419.6540s / 394748.8976 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0556
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0553
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 2151.3,                last time consumption/overall running time: 2520.4828s / 397269.3804 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0566
env0_second_0:                 episode reward: 4.0000,                 loss: 0.0558
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 2096.95,                last time consumption/overall running time: 2439.0539s / 399708.4343 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0569
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0563
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 2163.5,                last time consumption/overall running time: 2559.7308s / 402268.1651 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0556
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0558
env1_first_0:                 episode reward: -6.8500,                 loss: nan
env1_second_0:                 episode reward: 6.8500,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 2125.65,                last time consumption/overall running time: 2498.1718s / 404766.3369 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0559
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0548
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 2147.85,                last time consumption/overall running time: 2524.9777s / 407291.3146 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0559
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0550
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 2155.3,                last time consumption/overall running time: 2491.4512s / 409782.7658 s
env0_first_0:                 episode reward: -6.2500,                 loss: 0.0566
env0_second_0:                 episode reward: 6.2500,                 loss: 0.0558
env1_first_0:                 episode reward: -6.5500,                 loss: nan
env1_second_0:                 episode reward: 6.5500,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 2156.5,                last time consumption/overall running time: 2523.4129s / 412306.1787 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0565
env0_second_0:                 episode reward: 5.6500,                 loss: 0.0561
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 2135.85,                last time consumption/overall running time: 2519.4786s / 414825.6573 s
env0_first_0:                 episode reward: -4.6500,                 loss: 0.0560
env0_second_0:                 episode reward: 4.6500,                 loss: 0.0551
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2066.85,                last time consumption/overall running time: 2399.7438s / 417225.4011 s
env0_first_0:                 episode reward: -8.7500,                 loss: 0.0552
env0_second_0:                 episode reward: 8.7500,                 loss: 0.0555
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 2146.7,                last time consumption/overall running time: 2491.5162s / 419716.9173 s
env0_first_0:                 episode reward: -7.1500,                 loss: 0.0564
env0_second_0:                 episode reward: 7.1500,                 loss: 0.0551
env1_first_0:                 episode reward: -7.0500,                 loss: nan
env1_second_0:                 episode reward: 7.0500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 2138.4,                last time consumption/overall running time: 2547.2247s / 422264.1419 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0562
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0558
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 2200.6,                last time consumption/overall running time: 2574.2458s / 424838.3877 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0548
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0550
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 2050.8,                last time consumption/overall running time: 2375.2838s / 427213.6715 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0548
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0552
env1_first_0:                 episode reward: -5.6500,                 loss: nan
env1_second_0:                 episode reward: 5.6500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 2042.55,                last time consumption/overall running time: 2397.6931s / 429611.3646 s
env0_first_0:                 episode reward: -7.4500,                 loss: 0.0552
env0_second_0:                 episode reward: 7.4500,                 loss: 0.0553
env1_first_0:                 episode reward: -9.0500,                 loss: nan
env1_second_0:                 episode reward: 9.0500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 2074.4,                last time consumption/overall running time: 2368.1478s / 431979.5124 s
env0_first_0:                 episode reward: -8.5500,                 loss: 0.0555
env0_second_0:                 episode reward: 8.5500,                 loss: 0.0543
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 2131.2,                last time consumption/overall running time: 2424.1113s / 434403.6237 s
env0_first_0:                 episode reward: -5.9000,                 loss: 0.0541
env0_second_0:                 episode reward: 5.9000,                 loss: 0.0534
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 2045.0,                last time consumption/overall running time: 2346.9175s / 436750.5412 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0544
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0536
env1_first_0:                 episode reward: -8.6500,                 loss: nan
env1_second_0:                 episode reward: 8.6500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2169.25,                last time consumption/overall running time: 2495.1660s / 439245.7072 s
env0_first_0:                 episode reward: -5.7500,                 loss: 0.0543
env0_second_0:                 episode reward: 5.7500,                 loss: 0.0539
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 2192.95,                last time consumption/overall running time: 2512.8654s / 441758.5725 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0526
env0_second_0:                 episode reward: 3.9500,                 loss: 0.0529
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 2137.8,                last time consumption/overall running time: 2422.5747s / 444181.1472 s
env0_first_0:                 episode reward: -5.2000,                 loss: 0.0538
env0_second_0:                 episode reward: 5.2000,                 loss: 0.0539
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 2138.95,                last time consumption/overall running time: 2449.6160s / 446630.7632 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0546
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0546
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 2115.6,                last time consumption/overall running time: 2424.2172s / 449054.9804 s
env0_first_0:                 episode reward: -8.4500,                 loss: 0.0552
env0_second_0:                 episode reward: 8.4500,                 loss: 0.0560
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 2132.55,                last time consumption/overall running time: 2425.6772s / 451480.6576 s
env0_first_0:                 episode reward: -6.3000,                 loss: 0.0552
env0_second_0:                 episode reward: 6.3000,                 loss: 0.0553
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 2194.3,                last time consumption/overall running time: 2538.8080s / 454019.4657 s
env0_first_0:                 episode reward: -5.0000,                 loss: 0.0555
env0_second_0:                 episode reward: 5.0000,                 loss: 0.0558
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 2114.85,                last time consumption/overall running time: 2428.9674s / 456448.4330 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0551
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0558
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 2183.85,                last time consumption/overall running time: 2499.2005s / 458947.6335 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0536
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0552
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 2188.75,                last time consumption/overall running time: 2551.5792s / 461499.2127 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.0539
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0542
env1_first_0:                 episode reward: -5.3000,                 loss: nan
env1_second_0:                 episode reward: 5.3000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 2066.15,                last time consumption/overall running time: 2377.0753s / 463876.2880 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0527
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0533
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 2148.6,                last time consumption/overall running time: 2464.7029s / 466340.9909 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0533
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0539
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 2216.6,                last time consumption/overall running time: 2535.8577s / 468876.8486 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0549
env0_second_0:                 episode reward: 2.1000,                 loss: 0.0551
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 2083.75,                last time consumption/overall running time: 2474.1757s / 471351.0242 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0540
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0545
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 2102.55,                last time consumption/overall running time: 2426.8935s / 473777.9177 s
env0_first_0:                 episode reward: -5.5000,                 loss: 0.0544
env0_second_0:                 episode reward: 5.5000,                 loss: 0.0550
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 2130.45,                last time consumption/overall running time: 2482.7707s / 476260.6884 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0546
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0545
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 2171.2,                last time consumption/overall running time: 2506.6711s / 478767.3595 s
env0_first_0:                 episode reward: -5.0500,                 loss: 0.0543
env0_second_0:                 episode reward: 5.0500,                 loss: 0.0548
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 2141.25,                last time consumption/overall running time: 2463.9674s / 481231.3269 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0546
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0553
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 2247.7,                last time consumption/overall running time: 2572.2935s / 483803.6204 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0555
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0552
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 2119.3,                last time consumption/overall running time: 2453.8486s / 486257.4690 s
env0_first_0:                 episode reward: -8.8000,                 loss: 0.0548
env0_second_0:                 episode reward: 8.8000,                 loss: 0.0547
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 2250.95,                last time consumption/overall running time: 2583.3889s / 488840.8579 s
env0_first_0:                 episode reward: -4.3500,                 loss: 0.0533
env0_second_0:                 episode reward: 4.3500,                 loss: 0.0524
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 2159.75,                last time consumption/overall running time: 2497.8942s / 491338.7520 s
env0_first_0:                 episode reward: -5.1500,                 loss: 0.0532
env0_second_0:                 episode reward: 5.1500,                 loss: 0.0524
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 2029.6,                last time consumption/overall running time: 2326.4361s / 493665.1882 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0560
env0_second_0:                 episode reward: 4.8000,                 loss: 0.0551
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 2112.3,                last time consumption/overall running time: 2421.0027s / 496086.1909 s
env0_first_0:                 episode reward: -7.2000,                 loss: 0.0558
env0_second_0:                 episode reward: 7.2000,                 loss: 0.0551
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 2099.8,                last time consumption/overall running time: 2392.9453s / 498479.1361 s
env0_first_0:                 episode reward: -7.7000,                 loss: 0.0549
env0_second_0:                 episode reward: 7.7000,                 loss: 0.0553
env1_first_0:                 episode reward: -5.7500,                 loss: nan
env1_second_0:                 episode reward: 5.7500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 1965.35,                last time consumption/overall running time: 2269.6291s / 500748.7652 s
env0_first_0:                 episode reward: -8.0000,                 loss: 0.0552
env0_second_0:                 episode reward: 8.0000,                 loss: 0.0550
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 2089.2,                last time consumption/overall running time: 2432.6673s / 503181.4326 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0551
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0546
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 2217.9,                last time consumption/overall running time: 2521.5557s / 505702.9882 s
env0_first_0:                 episode reward: -6.1000,                 loss: 0.0548
env0_second_0:                 episode reward: 6.1000,                 loss: 0.0551
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2127.85,                last time consumption/overall running time: 2442.2227s / 508145.2110 s
env0_first_0:                 episode reward: -6.6000,                 loss: 0.0552
env0_second_0:                 episode reward: 6.6000,                 loss: 0.0551
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2122.35,                last time consumption/overall running time: 2412.2775s / 510557.4884 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.0549
env0_second_0:                 episode reward: 3.2000,                 loss: 0.0549
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2128.65,                last time consumption/overall running time: 2421.6312s / 512979.1197 s
env0_first_0:                 episode reward: -4.9500,                 loss: 0.0548
env0_second_0:                 episode reward: 4.9500,                 loss: 0.0540
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 2181.1,                last time consumption/overall running time: 2491.0893s / 515470.2089 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0551
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0551
env1_first_0:                 episode reward: -5.8500,                 loss: nan
env1_second_0:                 episode reward: 5.8500,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2064.7,                last time consumption/overall running time: 2358.8404s / 517829.0493 s
env0_first_0:                 episode reward: -5.9500,                 loss: 0.0552
env0_second_0:                 episode reward: 5.9500,                 loss: 0.0554
env1_first_0:                 episode reward: -6.9500,                 loss: nan
env1_second_0:                 episode reward: 6.9500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 2142.25,                last time consumption/overall running time: 2437.0659s / 520266.1152 s
env0_first_0:                 episode reward: -7.3000,                 loss: 0.0560
env0_second_0:                 episode reward: 7.3000,                 loss: 0.0570
env1_first_0:                 episode reward: -5.9000,                 loss: nan
env1_second_0:                 episode reward: 5.9000,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 2102.45,                last time consumption/overall running time: 2387.4563s / 522653.5715 s
env0_first_0:                 episode reward: -6.8000,                 loss: 0.0566
env0_second_0:                 episode reward: 6.8000,                 loss: 0.0566
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 2002.6,                last time consumption/overall running time: 2325.5283s / 524979.0998 s
env0_first_0:                 episode reward: -8.2500,                 loss: 0.0562
env0_second_0:                 episode reward: 8.2500,                 loss: 0.0561
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 2025.9,                last time consumption/overall running time: 2319.1656s / 527298.2654 s
env0_first_0:                 episode reward: -7.2500,                 loss: 0.0552
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0553
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 2109.8,                last time consumption/overall running time: 2392.8643s / 529691.1297 s
env0_first_0:                 episode reward: -10.3500,                 loss: 0.0541
env0_second_0:                 episode reward: 10.3500,                 loss: 0.0536
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 2099.65,                last time consumption/overall running time: 2390.1081s / 532081.2379 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0532
env0_second_0:                 episode reward: 8.4000,                 loss: 0.0522
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 2070.3,                last time consumption/overall running time: 2359.1869s / 534440.4248 s
env0_first_0:                 episode reward: -5.4000,                 loss: 0.0518
env0_second_0:                 episode reward: 5.4000,                 loss: 0.0521
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 2108.25,                last time consumption/overall running time: 2407.3829s / 536847.8077 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0530
env0_second_0:                 episode reward: 6.4500,                 loss: 0.0537
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 2206.45,                last time consumption/overall running time: 2525.8841s / 539373.6918 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.0545
env0_second_0:                 episode reward: 3.8000,                 loss: 0.0548
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 1979.15,                last time consumption/overall running time: 2261.1643s / 541634.8561 s
env0_first_0:                 episode reward: -8.8500,                 loss: 0.0537
env0_second_0:                 episode reward: 8.8500,                 loss: 0.0545
env1_first_0:                 episode reward: -7.8500,                 loss: nan
env1_second_0:                 episode reward: 7.8500,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 2056.9,                last time consumption/overall running time: 2379.8792s / 544014.7352 s
env0_first_0:                 episode reward: -5.5500,                 loss: 0.0539
env0_second_0:                 episode reward: 5.5500,                 loss: 0.0548
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 2162.15,                last time consumption/overall running time: 2476.6913s / 546491.4265 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0542
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0556
env1_first_0:                 episode reward: -6.3500,                 loss: nan
env1_second_0:                 episode reward: 6.3500,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 2037.5,                last time consumption/overall running time: 2330.8252s / 548822.2517 s
env0_first_0:                 episode reward: -4.5000,                 loss: 0.0537
env0_second_0:                 episode reward: 4.5000,                 loss: 0.0538
env1_first_0:                 episode reward: -7.9500,                 loss: nan
env1_second_0:                 episode reward: 7.9500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 2169.2,                last time consumption/overall running time: 2485.9220s / 551308.1737 s
env0_first_0:                 episode reward: -6.2000,                 loss: 0.0541
env0_second_0:                 episode reward: 6.2000,                 loss: 0.0537
env1_first_0:                 episode reward: -7.6000,                 loss: nan
env1_second_0:                 episode reward: 7.6000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 2041.2,                last time consumption/overall running time: 2367.5178s / 553675.6915 s
env0_first_0:                 episode reward: -7.4000,                 loss: 0.0547
env0_second_0:                 episode reward: 7.4000,                 loss: 0.0544
env1_first_0:                 episode reward: -5.0500,                 loss: nan
env1_second_0:                 episode reward: 5.0500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 2205.6,                last time consumption/overall running time: 2523.8369s / 556199.5283 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0535
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0554
env1_first_0:                 episode reward: -6.3000,                 loss: nan
env1_second_0:                 episode reward: 6.3000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 1991.15,                last time consumption/overall running time: 2319.5344s / 558519.0627 s
env0_first_0:                 episode reward: -8.9000,                 loss: 0.0542
env0_second_0:                 episode reward: 8.9000,                 loss: 0.0545
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 2161.95,                last time consumption/overall running time: 2485.2234s / 561004.2861 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0552
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0554
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 2071.5,                last time consumption/overall running time: 2368.3561s / 563372.6421 s
env0_first_0:                 episode reward: -6.0500,                 loss: 0.0553
env0_second_0:                 episode reward: 6.0500,                 loss: 0.0552
env1_first_0:                 episode reward: -7.4500,                 loss: nan
env1_second_0:                 episode reward: 7.4500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 2034.45,                last time consumption/overall running time: 2347.4719s / 565720.1140 s
env0_first_0:                 episode reward: -6.8500,                 loss: 0.0536
env0_second_0:                 episode reward: 6.8500,                 loss: 0.0536
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 2125.95,                last time consumption/overall running time: 2450.1710s / 568170.2850 s
env0_first_0:                 episode reward: -6.7500,                 loss: 0.0542
env0_second_0:                 episode reward: 6.7500,                 loss: 0.0541
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2104.4,                last time consumption/overall running time: 2440.4753s / 570610.7603 s
env0_first_0:                 episode reward: -5.8000,                 loss: 0.0539
env0_second_0:                 episode reward: 5.8000,                 loss: 0.0539
env1_first_0:                 episode reward: -7.5000,                 loss: nan
env1_second_0:                 episode reward: 7.5000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 2116.3,                last time consumption/overall running time: 2415.7391s / 573026.4994 s
env0_first_0:                 episode reward: -5.7000,                 loss: 0.0537
env0_second_0:                 episode reward: 5.7000,                 loss: 0.0537
env1_first_0:                 episode reward: -6.4500,                 loss: nan
env1_second_0:                 episode reward: 6.4500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 2063.0,                last time consumption/overall running time: 2384.2303s / 575410.7297 s
env0_first_0:                 episode reward: -6.5500,                 loss: 0.0542