{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Space Response Oracle (PSRO)\n",
    "This tutorial demonstrates Policy Space Response Oracle (PSRO) on a zero-sum Markov game, which is more general than extensive-from game (more general than normal-form game). \n",
    "\n",
    "For the payoff matrix, row player is maximizer, coloumn player is minimizer.\n",
    "\n",
    "Reference: A unified game-theoretic approach to multiagent reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 12.0, (1,), float32) Discrete(3)\n",
      "[[0], [0]]\n",
      "[[5], [5]] [0.5610583525729109, -0.5610583525729109] [False, False]\n",
      "[[6], [6]] [-0.7261994566288021, 0.7261994566288021] [False, False]\n",
      "[[9], [9]] [-0.8460871060267345, 0.8460871060267345] [True, True]\n",
      "Average initial state value of oracle Nash equilibrium for the first player:  -0.29607107415447115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.\n",
      "  warn(\"Converting G to a CSC matrix; may take a while.\")\n",
      "/home/quantumiracle/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.\n",
      "  warn(\"Converting A to a CSC matrix; may take a while.\")\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from mars.env.mdp import ArbitraryMDP, MDPWrapper\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "num_actions_per_player = 3\n",
    "num_trans = 3\n",
    "\n",
    "env = MDPWrapper(ArbitraryMDP(num_states=num_states, num_actions_per_player=num_actions_per_player, num_trans=num_trans))\n",
    "trans_matrices = env.env.trans_prob_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "reward_matrices = env.env.reward_matrices # shape: [dim_transition, dim_state, dim_action (p1*p2), dim_state]\n",
    "\n",
    "oracle_nash_v, oracle_nash_q, oracle_nash_strategies = env.NEsolver(verbose=False)\n",
    "oracle_v_star = oracle_nash_v[0]\n",
    "\n",
    "oracle_v_star = np.mean(oracle_v_star, axis=0)\n",
    "print(env.observation_space, env.action_space)\n",
    "# env.render()\n",
    "obs = env.reset()\n",
    "print(obs)\n",
    "done = False\n",
    "while not np.any(done):\n",
    "    obs, r, done, _ = env.step([1,0])\n",
    "    print(obs, r, done)\n",
    "print('Average initial state value of oracle Nash equilibrium for the first player: ', oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 2, 3)\n",
      "(3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(oracle_nash_strategies).shape)\n",
    "oracle_max_policy = np.array(oracle_nash_strategies)[:, :, 0, :]\n",
    "print(oracle_max_policy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy shape: \n",
      "(3, 3)\n",
      "(3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3)\n",
      "Q table shape: \n",
      "(3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3)\n",
      "(3, 3, 3, 3, 3, 3, 3, 3, 3)\n",
      "itr: 0, exploitability: 0.7608750685387422\n",
      "itr: 10, exploitability: 0.6376484629002362\n",
      "itr: 20, exploitability: 0.5712701680558167\n",
      "itr: 30, exploitability: 0.4885990951171036\n",
      "itr: 40, exploitability: 0.49823624270646377\n",
      "itr: 50, exploitability: 0.4414286175737034\n",
      "itr: 60, exploitability: 0.4497553799052441\n",
      "itr: 70, exploitability: 0.41266073665931896\n",
      "itr: 80, exploitability: 0.3776896500833102\n",
      "itr: 90, exploitability: 0.4090806504519\n",
      "itr: 100, exploitability: 0.36277023361898625\n",
      "itr: 110, exploitability: 0.3655436213761152\n",
      "itr: 120, exploitability: 0.36700103218322516\n",
      "itr: 130, exploitability: 0.3525810454654257\n",
      "itr: 140, exploitability: 0.3496366521265412\n",
      "itr: 150, exploitability: 0.3439277478444022\n",
      "itr: 160, exploitability: 0.3440173415810263\n",
      "itr: 170, exploitability: 0.3304426331108204\n",
      "itr: 180, exploitability: 0.33348757454717654\n",
      "itr: 190, exploitability: 0.329003741604775\n",
      "itr: 200, exploitability: 0.3327442730661414\n",
      "itr: 210, exploitability: 0.3332860946173302\n",
      "itr: 220, exploitability: 0.3290368576714508\n",
      "itr: 230, exploitability: 0.3324808551469078\n",
      "itr: 240, exploitability: 0.3256248224453515\n",
      "itr: 250, exploitability: 0.3231889958457552\n",
      "itr: 260, exploitability: 0.3234820125271402\n",
      "itr: 270, exploitability: 0.3198934491382734\n",
      "itr: 280, exploitability: 0.32049772034766694\n",
      "itr: 290, exploitability: 0.31677249601188456\n",
      "itr: 300, exploitability: 0.3197274592573817\n",
      "itr: 310, exploitability: 0.31796958312870655\n",
      "itr: 320, exploitability: 0.3186855295796431\n",
      "itr: 330, exploitability: 0.3180516545698335\n",
      "itr: 340, exploitability: 0.31531092598676186\n",
      "itr: 350, exploitability: 0.31685418594666326\n",
      "itr: 360, exploitability: 0.31700027771565126\n",
      "itr: 370, exploitability: 0.31356382155657386\n",
      "itr: 380, exploitability: 0.31048051698254947\n",
      "itr: 390, exploitability: 0.31453096344327236\n",
      "itr: 400, exploitability: 0.30960575097941695\n",
      "itr: 410, exploitability: 0.3092597505807445\n",
      "itr: 420, exploitability: 0.31219457521405686\n",
      "itr: 430, exploitability: 0.3105749201635285\n",
      "itr: 440, exploitability: 0.3095701844094966\n",
      "itr: 450, exploitability: 0.3076744809571157\n",
      "itr: 460, exploitability: 0.3067945896575814\n",
      "itr: 470, exploitability: 0.3083578301798755\n",
      "itr: 480, exploitability: 0.3068285723907146\n",
      "itr: 490, exploitability: 0.3060053633082102\n",
      "itr: 500, exploitability: 0.30600731229501643\n",
      "itr: 510, exploitability: 0.30611409461044997\n",
      "itr: 520, exploitability: 0.3056261127278804\n",
      "itr: 530, exploitability: 0.30408416372229363\n",
      "itr: 540, exploitability: 0.3036232021355826\n",
      "itr: 550, exploitability: 0.3025409262963752\n",
      "itr: 560, exploitability: 0.3016040185130723\n",
      "itr: 570, exploitability: 0.30268546003647995\n",
      "itr: 580, exploitability: 0.3031336476594318\n",
      "itr: 590, exploitability: 0.30219992483480523\n",
      "itr: 600, exploitability: 0.3026748518451584\n",
      "itr: 610, exploitability: 0.30181960746873254\n",
      "itr: 620, exploitability: 0.30203239758732275\n",
      "itr: 630, exploitability: 0.3019331931190291\n",
      "itr: 640, exploitability: 0.30150548155871126\n",
      "itr: 650, exploitability: 0.3011837994310442\n",
      "itr: 660, exploitability: 0.3007114954969435\n",
      "itr: 670, exploitability: 0.3009325699756675\n",
      "itr: 680, exploitability: 0.30064732926492793\n",
      "itr: 690, exploitability: 0.30085655500599057\n",
      "itr: 700, exploitability: 0.3005741057549253\n",
      "itr: 710, exploitability: 0.3004488261910078\n",
      "itr: 720, exploitability: 0.2999692001878324\n",
      "itr: 730, exploitability: 0.30001545866456303\n",
      "itr: 740, exploitability: 0.30046807822486693\n",
      "itr: 750, exploitability: 0.3008627249397276\n",
      "itr: 760, exploitability: 0.3004916926169198\n",
      "itr: 770, exploitability: 0.3004036702487215\n",
      "itr: 780, exploitability: 0.3004229141312827\n",
      "itr: 790, exploitability: 0.301083544786464\n",
      "itr: 800, exploitability: 0.3001207761190369\n",
      "itr: 810, exploitability: 0.29995963911392837\n",
      "itr: 820, exploitability: 0.3004328046407807\n",
      "itr: 830, exploitability: 0.2997322230141246\n",
      "itr: 840, exploitability: 0.2999180390222544\n",
      "itr: 850, exploitability: 0.3001772695512817\n",
      "itr: 860, exploitability: 0.29937597885928063\n",
      "itr: 870, exploitability: 0.2991795324802475\n",
      "itr: 880, exploitability: 0.2992657026700761\n",
      "itr: 890, exploitability: 0.29930426193428866\n",
      "itr: 900, exploitability: 0.2991185976956798\n",
      "itr: 910, exploitability: 0.2986927101500246\n",
      "itr: 920, exploitability: 0.29892488506464\n",
      "itr: 930, exploitability: 0.29843084719869856\n",
      "itr: 940, exploitability: 0.29834638360464344\n",
      "itr: 950, exploitability: 0.29847211218556274\n",
      "itr: 960, exploitability: 0.298545478025602\n",
      "itr: 970, exploitability: 0.2983240993529239\n",
      "itr: 980, exploitability: 0.29861952669884934\n",
      "itr: 990, exploitability: 0.29794916088923606\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from mars.equilibrium_solver import NashEquilibriumECOSSolver, NashEquilibriumCVXPYSolver\n",
    "import itertools\n",
    "\n",
    "\n",
    "def sample_from_categorical(dist):\n",
    "    \"\"\"\n",
    "    sample once from a categorical distribution, return the entry index.\n",
    "    dist: should be a list or array of probabilities for a categorical distribution\n",
    "    \"\"\"\n",
    "    sample_id = np.argmax(np.random.multinomial(1, dist))\n",
    "    sample_prob = dist[sample_id]\n",
    "    return sample_id, sample_prob\n",
    "\n",
    "def unified_state(s):\n",
    "    unified_s = s[0]%num_states\n",
    "    return unified_s\n",
    "\n",
    "def create_expand_policy(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    policies = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            policy =  (1./num_actions_per_player) * np.zeros((*intial_dim, num_actions_per_player))\n",
    "        else:\n",
    "            policy =  (1./num_actions_per_player) * np.ones((*intial_dim, num_actions_per_player))\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        policies.append(policy)\n",
    "\n",
    "    return policies\n",
    "\n",
    "def create_expand_value():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim],\n",
    "        ...\n",
    "    ]    \n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states]\n",
    "    for i in range(num_trans):\n",
    "        value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_actions_per_player, num_actions_per_player, num_states])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "\n",
    "def create_expand_Q(zero_ini=False):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    [   [state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        [state_dim, action_dim, action_dim, state_dim, action_dim, action_dim, state_dim, action_dim, action_dim],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    intial_dim = [num_states, num_actions_per_player, num_actions_per_player]\n",
    "    for i in range(num_trans):\n",
    "        if zero_ini:\n",
    "            value =  (1./num_actions_per_player) * np.zeros(intial_dim)\n",
    "        else:\n",
    "            value =  (1./num_actions_per_player) * np.ones(intial_dim)\n",
    "        # incremental shape\n",
    "        intial_dim.extend([num_states, num_actions_per_player, num_actions_per_player])\n",
    "        values.append(value)\n",
    "\n",
    "    return values\n",
    "\n",
    "def get_posterior_policy(policy_set, meta_prior, side):\n",
    "    posterior_policy = create_expand_policy(zero_ini=True)  # zero initiate to sum later\n",
    "    num_policies = len(policy_set)\n",
    "\n",
    "    def get_all_traj_for_transition(num_transition, single_side=False):  # TESTED\n",
    "        \"\"\"\n",
    "        step = 1:\n",
    "        [\n",
    "            [s0,a0,b0], [s0,a0,b1],...\n",
    "        ]\n",
    "        step = 2:\n",
    "        [\n",
    "            [s0,a0,b0,s1,a1,b1], [s1,a0,b0,s1,a1,b1],\n",
    "        ]\n",
    "        ...\n",
    "        \"\"\"\n",
    "        if single_side: # lack of one action\n",
    "            ranges = (num_transition-1)*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)] + [range(num_states), range(num_actions_per_player)]\n",
    "        else:\n",
    "            ranges = num_transition*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)]\n",
    "        all_possible_trajs = list(itertools.product(*ranges))\n",
    "\n",
    "        return all_possible_trajs\n",
    "\n",
    "    \n",
    "    def split_traj(traj, side):  # TESTED\n",
    "        \"\"\"\n",
    "        s0, a0, b0, s1, a1, b1, ... sn-1, an-1, bn-1, sn ->\n",
    "        side = max:\n",
    "        [], [s0,a0], [s0,a0,b0,s1,a1], [s0,a0,b0,s1,a1,b1,s2,a2], ...\n",
    "        side = min:\n",
    "        [s0,b0], [s0,a0,b0,s1,b1], [s0,a0,b0,s1,a1,b1,s2,b2], ...\n",
    "        \"\"\"\n",
    "        split_trajs = []\n",
    "        i=2\n",
    "        while i<=len(traj):  # max length same as original traj but lack of one action (due to the side choice)\n",
    "            if side == 'max':\n",
    "                split_trajs.append(traj[:i])\n",
    "            else:\n",
    "                split_trajs.append(np.concatenate([traj[:i-1], [traj[i]]]))\n",
    "            i = i+3 # 3 due to (s,a,b)\n",
    "\n",
    "        return split_trajs\n",
    "\n",
    "\n",
    "    def get_likelihood(policy, side):\n",
    "        likelihoods = create_expand_Q() # (s,a,b) rather than (s,a) or (s,b)\n",
    "        for i in range(num_trans):\n",
    "            # number of transitions is i+1\n",
    "            # each transition is (s,a,b)\n",
    "            # each trajectory of transitions is [(s1,a1,b1), (s2,a2,b2), ...] of length num_transition\n",
    "            trajs = get_all_traj_for_transition(num_transition=i+1)   \n",
    "            for traj in trajs:\n",
    "                # split trajectories to be a powerset, \n",
    "                # and return only one-side action for the last transition,\n",
    "                # b.c. in likelihood there is only one-side probability\n",
    "                trajs_list = split_traj(traj, side) \n",
    "                likelihood = 1.\n",
    "                for j, t in enumerate(trajs_list):  # this product corresponding to likelihood for h step (not h-1)\n",
    "                    likelihood = likelihood*policy[j][tuple(t)]  # scalar\n",
    "                likelihoods[i][tuple(traj)] = likelihood  # needs to know this likelihood stored for transition i should be used for posterior transition i+1\n",
    "        return likelihoods\n",
    "\n",
    "    likelihoods = []\n",
    "    for pi_i in policy_set:\n",
    "        likelihoods.append(get_likelihood(pi_i, side))\n",
    "\n",
    "    def get_denoms(likelihoods, rho):\n",
    "        denoms = create_expand_Q(zero_ini=True)\n",
    "        for j, rho_j in enumerate(rho):  # over policy in policy set\n",
    "            for i in range(len(likelihoods[j])):  # over transition\n",
    "                trajs = get_all_traj_for_transition(i+1) \n",
    "                for traj in trajs:\n",
    "                    denoms[i][tuple(traj)] += rho_j*likelihoods[j][i][tuple(traj)]\n",
    "\n",
    "        return denoms\n",
    "\n",
    "    denoms = get_denoms(likelihoods, meta_prior)\n",
    "\n",
    "    for pi_i, rho_i, likelihood_i, in zip(policy_set, meta_prior, likelihoods):  # loop over policy set\n",
    "\n",
    "        for i, (p,) in enumerate(zip(pi_i)): # loop over transition\n",
    "            taus = get_all_traj_for_transition(i+1, single_side=True)\n",
    "            for tau in taus:\n",
    "                if i==0:\n",
    "                    posterior_policy[i][tuple(tau)] += p[tuple(tau)]*rho_i # posterior = prior for the first transition\n",
    "                else:\n",
    "                    tau_h_1 = tau[:-2]  # remove the last (s,a/b) for the likelihood and denominator since they take trajectory until (h-1)\n",
    "                    # import pdb; pdb.set_trace()\n",
    "                    posterior_policy[i][tuple(tau)] += p[tuple(tau)]*rho_i*likelihood_i[i-1][tuple(tau_h_1)]/denoms[i-1][tuple(tau_h_1)]\n",
    "\n",
    "    return posterior_policy\n",
    "\n",
    "def broadcast_shape(input, output_shape):\n",
    "    \"\"\" broadcast input to have the same shape as output_to_be \"\"\"\n",
    "    len_input_shape = len(np.array(input).shape)\n",
    "    incrs_shape = output_shape[:-len_input_shape]+len_input_shape*(1,)\n",
    "    output = np.tile(input, incrs_shape)\n",
    "    return output\n",
    "\n",
    "def get_best_response_policy(player):\n",
    "    given_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], player['side'])\n",
    "    # print(given_policy)\n",
    "    br_policy = create_expand_policy()  # best response policy\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))  # broadcast the shape of reward matrix to be expand Q shape plus additional state dimension\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if player['side'] == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', given_policy[i], br_q[i])\n",
    "            arg_id = np.argmin(mu_dot_q, axis=-1)  # min player takes minimum as best response against max player's policy\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], given_policy[i])\n",
    "            arg_id = np.argmax(q_dot_nu, axis=-1)   # vice versa    \n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)     \n",
    "        \n",
    "        br_policy[i] = (np.arange(num_actions_per_player) == arg_id[...,None]).astype(int)  # from extreme (min/max) idx to one-hot simplex\n",
    "        # print(br_policy[i].shape, br_q[i].shape)\n",
    "\n",
    "    return br_policy\n",
    "\n",
    "def best_response_value(trans_prob_matrices, reward_matrices, policy, num_actions, side='max'):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    br_v = create_expand_value()\n",
    "    br_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, br_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            br_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = br_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            br_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        if side == 'max':\n",
    "            mu_dot_q = np.einsum('...i, ...ij->...j', policy[i], br_q[i])\n",
    "            br_v[i] = np.min(mu_dot_q, axis=-1)\n",
    "        else:\n",
    "            q_dot_nu = np.einsum('...ij, ...j->...i', br_q[i], policy[i])\n",
    "            br_v[i] = np.max(q_dot_nu, axis=-1)         \n",
    "\n",
    "    avg_init_br_v = -np.mean(br_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_br_v\n",
    "\n",
    "def get_best_response_value(player):\n",
    "    # wrong! this is the weighted average (by meta strategy) of exploitability for each policy in policy set\n",
    "    # per_policy_exploits = []\n",
    "    # for p in policy_set:\n",
    "    #     per_policy_exploits.append(best_response_value(trans_matrices, reward_matrices, p, num_actions_per_player, player['side']))\n",
    "    # per_policy_exploits = np.array(per_policy_exploits)\n",
    "    # print(per_policy_exploits)\n",
    "    # exploitability = per_policy_exploits @ meta_strategy  # average over policy set weighted by meta strategy\n",
    "\n",
    "    # should still get the posterior policy as the mixture policy to exploit!\n",
    "    posterior_policy = get_posterior_policy(player['policy_set'], player['meta_strategy'], player['side'])\n",
    "    exploitability = best_response_value(trans_matrices, reward_matrices, posterior_policy, num_actions_per_player, player['side'])\n",
    "\n",
    "    return exploitability\n",
    "\n",
    "def policy_against_policy_value(trans_prob_matrices, reward_matrices, max_policy, min_policy, num_actions):\n",
    "    \"\"\"\n",
    "    Formulas for calculating best response values:\n",
    "    1. Nash strategies: (\\pi_a^*, \\pi_b^*) = \\min \\max Q(s,a,b), \n",
    "        where Q(s,a,b) = r(s,a,b) + \\gamma \\min \\max Q(s',a',b') (this is the definition of Nash Q-value);\n",
    "    2. Best response (of max player) value: Br V(s) = \\min_b \\pi(s,a) Br Q(s,a,b)  (Br Q is the oracle best response Q value)\n",
    "    \"\"\"\n",
    "    eval_v = create_expand_value()\n",
    "    eval_q = create_expand_Q()\n",
    "\n",
    "    for i in range(num_trans-1, -1, -1):  # inverse indexing\n",
    "        tm = trans_matrices[i]\n",
    "        rm = reward_matrices[i]\n",
    "\n",
    "        rm_ = np.array(rm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        tm_ = np.array(tm).reshape(num_states, num_actions_per_player, num_actions_per_player, num_states)\n",
    "        expand_rm = broadcast_shape(rm_, eval_q[i].shape+(num_states,))\n",
    "        if i == num_trans-1:\n",
    "            expand_tm = broadcast_shape(tm_, expand_rm.shape)\n",
    "            eval_q[i] =  np.sum(expand_rm*expand_tm, axis=-1)\n",
    "        else:\n",
    "            v = eval_v[i+1]\n",
    "            v_before_trans = expand_rm + v  # expand_rm and v are same shape\n",
    "            expand_tm = broadcast_shape(tm_, v_before_trans.shape)  # get the same shape as v_before_trans\n",
    "            eval_q[i] = np.sum(v_before_trans*expand_tm, axis=-1)  # only sum over last dim: state\n",
    "\n",
    "        mu_dot_q = np.einsum('...i, ...ij->...j', max_policy[i], eval_q[i])\n",
    "        mu_dot_q_dot_nu = np.einsum('...kj, ...kj->...k', mu_dot_q, min_policy[i])\n",
    "        eval_v[i] = mu_dot_q_dot_nu    \n",
    "\n",
    "    avg_init_eval_v = np.mean(eval_v[0])  # average best response value of initial states; minus for making it positive\n",
    "    return avg_init_eval_v\n",
    "\n",
    "def update_matrix(matrix, side, row):\n",
    "    \"\"\"\n",
    "    Adding a new policy to the league will add a row or column to the present evaluation matrix, after it has\n",
    "    been evaluated against all policies in the opponent's league. Whether adding a row or column depends on whether\n",
    "    it is the row player (idx=0) or column player (idx=1). \n",
    "    For example, if current evaluation matrix is:\n",
    "    2  1\n",
    "    -1 3\n",
    "    After adding a new policy for row player with evaluated average episode reward (a,b) against current two policies in \n",
    "    the league of the column player, it gives: \n",
    "    it gives:\n",
    "    2   1\n",
    "    -1  3\n",
    "    a   b    \n",
    "    \"\"\" \n",
    "    if side == 'max': # for row player\n",
    "        if matrix.shape[0] == 0: \n",
    "            matrix = np.array([row])\n",
    "        else:  # add row\n",
    "            matrix = np.vstack([matrix, row])\n",
    "    else: # for column player\n",
    "        if matrix.shape[0] == 0:  # the first update\n",
    "            matrix = np.array([row]) \n",
    "        else:  # add column\n",
    "            matrix=np.hstack([matrix, np.array([row]).T])\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sample_policy(policy_set, dist):\n",
    "    policy_id, _ = sample_from_categorical(dist)\n",
    "    policy = policy_set[policy_id]   \n",
    "    return policy \n",
    "\n",
    "# test_policy = create_expand_policy() \n",
    "# get_best_response(test_policy)\n",
    "\n",
    "def psro(env, save_path, solve_episodes = 1000):\n",
    "    evaluation_matrix = np.array([[0]])\n",
    "    ini_max_policy = create_expand_policy()\n",
    "    ini_min_policy = create_expand_policy()\n",
    "    max_player = {\n",
    "        'side': 'max',\n",
    "        'policy_set': [ini_max_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "    min_player = {\n",
    "        'side': 'min',\n",
    "        'policy_set': [ini_min_policy],\n",
    "        'meta_strategy': np.array([1.])\n",
    "    }\n",
    "\n",
    "    max_side_q_table = create_expand_Q()\n",
    "    min_side_q_table = create_expand_Q()\n",
    " \n",
    "    print('policy shape: ')\n",
    "    for p in ini_max_policy:\n",
    "        print(p.shape)\n",
    "    print('Q table shape: ')\n",
    "    for q in max_side_q_table:\n",
    "        print(q.shape)\n",
    "\n",
    "    exploitability_records = []\n",
    "\n",
    "    for i in range(solve_episodes):\n",
    "        \n",
    "        if i % 2 == 0:  # update min player side\n",
    "            new_policy = get_best_response_policy(max_player)  # get best response against the max player\n",
    "            min_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in max_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=p, min_policy=new_policy, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'min', eval_scores)\n",
    "\n",
    "        else: \n",
    "            new_policy = get_best_response_policy(min_player) # get best response against the min player\n",
    "            max_player['policy_set'].append(new_policy) # add new policy to policy set to form mixture\n",
    "\n",
    "            # update evaluation matrix\n",
    "            eval_scores = []\n",
    "            for p in min_player['policy_set']:\n",
    "                score = policy_against_policy_value(trans_matrices, reward_matrices, max_policy=new_policy, min_policy=p, num_actions=num_actions_per_player)\n",
    "                eval_scores.append(score)  # score always from the max player's view\n",
    "            evaluation_matrix = update_matrix(evaluation_matrix, 'max', eval_scores)\n",
    "\n",
    "        if len(max_player['policy_set']) * len(min_player['policy_set']) >= 2:  # enough policies to get Nash\n",
    "            (max_player['meta_strategy'], min_player['meta_strategy']), _ = NashEquilibriumCVXPYSolver(evaluation_matrix)\n",
    "        else: # uniform for initialization only\n",
    "            max_policies = len(max_player['policy_set'])\n",
    "            max_player['meta_strategy'] = 1./max_policies*np.ones(max_policies)\n",
    "            min_policies = len(min_player['policy_set'])\n",
    "            min_player['meta_strategy'] = 1./min_policies*np.ones(min_policies)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            exploitability = get_best_response_value(max_player)  # best response of the max player\n",
    "            print(f'itr: {i}, exploitability: {exploitability}', )\n",
    "            exploitability_records.append(exploitability)\n",
    "            np.save(save_path, exploitability_records)\n",
    "\n",
    "save_path = 'psro_exp.npy'\n",
    "psro(env, save_path, solve_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f362d22b438>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "exp_data = np.load(save_path)\n",
    "plt.title('PSRO')\n",
    "plt.xlabel('iter(x10)')\n",
    "plt.ylabel('exploitability')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot(exp_data+oracle_v_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4, 5, 6, 7]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "import copy\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "num_states = 3\n",
    "\n",
    "\n",
    "def get_all_traj_for_transition(transition_step):\n",
    "    \"\"\"\n",
    "    step = 1:\n",
    "    [\n",
    "        [s0,a0,b0], [s0,a0,b1],...\n",
    "    ]\n",
    "    step = 2:\n",
    "    [\n",
    "        [s0,a0,b0,s1,a1,b1], [s1,a0,b0,s1,a1,b1],\n",
    "    ]\n",
    "    ...\n",
    "    \"\"\"\n",
    "    ranges = transition_step*[range(num_states), range(num_actions_per_player), range(num_actions_per_player)]\n",
    "    all_possible_trajs = list(itertools.product(*ranges))\n",
    "\n",
    "    return all_possible_trajs\n",
    "\n",
    "def split_traj(traj, side):\n",
    "    \"\"\"\n",
    "    s0, a0, b0, s1, a1, b1, ... sn-1, an-1, bn-1, sn ->\n",
    "    side = max:\n",
    "    [], [s0,a0], [s0,a0,b0,s1,a1], [s0,a0,b0,s1,a1,b1,s2,a2], ...\n",
    "    side = min:\n",
    "    [s0,b0], [s0,a0,b0,s1,b1], [s0,a0,b0,s1,a1,b1,s2,b2], ...\n",
    "    \"\"\"\n",
    "    split_trajs = []\n",
    "    i=2\n",
    "    while i<=len(traj):  # shorter than original traj\n",
    "        if side == 'max':\n",
    "            split_trajs.append(traj[:i])\n",
    "        else:\n",
    "            split_trajs.append(np.concatenate([traj[:i-1], [traj[i]]]))\n",
    "        i = i+3 # 3 due to (s,a,b)\n",
    "\n",
    "    return split_trajs\n",
    "\n",
    "\n",
    "# get_all_traj_for_transition(2)\n",
    "split_traj(traj=[0,1,2,3,4,5,6,7,8], side='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28c6861e59928cb790236f7047915368f37afc12f670e78fd0101a6f825a02b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('x': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
