pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
ParallelDQN(
  (net): MLP(
    (body): Sequential(
      (0): Linear(in_features=128, out_features=128, bias=True)
      (1): ReLU()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): ReLU()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): ReLU()
      (6): Linear(in_features=128, out_features=128, bias=True)
      (7): ReLU()
      (8): Linear(in_features=128, out_features=18, bias=True)
    )
  )
)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'DQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000}, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'marl_method': 'fictitious_selfplay2', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False, 'selfplay_score_delta': 50, 'trainable_agent_idx': 0, 'opponent_idx': 1}}
Save models to : /home/zihan/research/MARS/data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2. 
 Save logs to: /home/zihan/research/MARS/data/log/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2.
Episode: 1/10000 (0.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 147.6635s / 147.6635 s
env0_first_0:                 episode reward: 219.0000,                 loss: 0.0102
env0_second_0:                 episode reward: -219.0000,                 loss: nan
env1_first_0:                 episode reward: 219.0000,                 loss: nan
env1_second_0:                 episode reward: -219.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 4440.0646s / 4587.7281 s
env0_first_0:                 episode reward: 217.1000,                 loss: 0.0099
env0_second_0:                 episode reward: -217.1000,                 loss: nan
env1_first_0:                 episode reward: 216.8500,                 loss: nan
env1_second_0:                 episode reward: -216.8500,                 loss: nan
Score delta: 434.6, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/21_0.
Episode: 41/10000 (0.4100%),                 avg. length: 2361.8,                last time consumption/overall running time: 1052.2202s / 5639.9483 s
env0_first_0:                 episode reward: -10.6000,                 loss: nan
env0_second_0:                 episode reward: 10.6000,                 loss: 0.0061
env1_first_0:                 episode reward: -10.2500,                 loss: nan
env1_second_0:                 episode reward: 10.2500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 1669.8,                last time consumption/overall running time: 750.7135s / 6390.6617 s
env0_first_0:                 episode reward: -20.6500,                 loss: nan
env0_second_0:                 episode reward: 20.6500,                 loss: 0.0054
env1_first_0:                 episode reward: -21.0500,                 loss: nan
env1_second_0:                 episode reward: 21.0500,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 1599.65,                last time consumption/overall running time: 720.3210s / 7110.9827 s
env0_first_0:                 episode reward: -23.6500,                 loss: 0.0106
env0_second_0:                 episode reward: 23.6500,                 loss: 0.0044
env1_first_0:                 episode reward: -24.1500,                 loss: nan
env1_second_0:                 episode reward: 24.1500,                 loss: nan
Score delta: 50.4, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/69_1.
Episode: 101/10000 (1.0100%),                 avg. length: 1648.95,                last time consumption/overall running time: 743.0741s / 7854.0568 s
env0_first_0:                 episode reward: -24.0000,                 loss: 0.0075
env0_second_0:                 episode reward: 24.0000,                 loss: nan
env1_first_0:                 episode reward: -24.5500,                 loss: nan
env1_second_0:                 episode reward: 24.5500,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 1893.4,                last time consumption/overall running time: 853.1307s / 8707.1875 s
env0_first_0:                 episode reward: -20.0000,                 loss: 0.0052
env0_second_0:                 episode reward: 20.0000,                 loss: nan
env1_first_0:                 episode reward: -20.1000,                 loss: nan
env1_second_0:                 episode reward: 20.1000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 3397.5,                last time consumption/overall running time: 1528.2708s / 10235.4583 s
env0_first_0:                 episode reward: -6.4500,                 loss: 0.0056
env0_second_0:                 episode reward: 6.4500,                 loss: nan
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 3289.2,                last time consumption/overall running time: 1479.0449s / 11714.5031 s
env0_first_0:                 episode reward: -12.2500,                 loss: 0.0056
env0_second_0:                 episode reward: 12.2500,                 loss: nan
env1_first_0:                 episode reward: -6.2000,                 loss: nan
env1_second_0:                 episode reward: 6.2000,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 2092.2,                last time consumption/overall running time: 941.1797s / 12655.6828 s
env0_first_0:                 episode reward: 19.0500,                 loss: 0.0044
env0_second_0:                 episode reward: -19.0500,                 loss: nan
env1_first_0:                 episode reward: 17.9000,                 loss: nan
env1_second_0:                 episode reward: -17.9000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 1842.45,                last time consumption/overall running time: 830.5495s / 13486.2323 s
env0_first_0:                 episode reward: 19.9000,                 loss: 0.0021
env0_second_0:                 episode reward: -19.9000,                 loss: nan
env1_first_0:                 episode reward: 20.9000,                 loss: nan
env1_second_0:                 episode reward: -20.9000,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 1725.9,                last time consumption/overall running time: 777.8665s / 14264.0988 s
env0_first_0:                 episode reward: 22.2500,                 loss: 0.0016
env0_second_0:                 episode reward: -22.2500,                 loss: nan
env1_first_0:                 episode reward: 21.2500,                 loss: nan
env1_second_0:                 episode reward: -21.2500,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 1828.35,                last time consumption/overall running time: 824.0980s / 15088.1968 s
env0_first_0:                 episode reward: 21.0500,                 loss: 0.0013
env0_second_0:                 episode reward: -21.0500,                 loss: nan
env1_first_0:                 episode reward: 20.9000,                 loss: nan
env1_second_0:                 episode reward: -20.9000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 1837.25,                last time consumption/overall running time: 826.9112s / 15915.1080 s
env0_first_0:                 episode reward: 19.8000,                 loss: 0.0016
env0_second_0:                 episode reward: -19.8000,                 loss: nan
env1_first_0:                 episode reward: 21.3000,                 loss: nan
env1_second_0:                 episode reward: -21.3000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 1752.65,                last time consumption/overall running time: 788.4520s / 16703.5599 s
env0_first_0:                 episode reward: 21.1500,                 loss: 0.0017
env0_second_0:                 episode reward: -21.1500,                 loss: nan
env1_first_0:                 episode reward: 21.4500,                 loss: nan
env1_second_0:                 episode reward: -21.4500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 1771.05,                last time consumption/overall running time: 796.6784s / 17500.2383 s
env0_first_0:                 episode reward: 21.7000,                 loss: 0.0014
env0_second_0:                 episode reward: -21.7000,                 loss: nan
env1_first_0:                 episode reward: 20.1500,                 loss: nan
env1_second_0:                 episode reward: -20.1500,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 1727.35,                last time consumption/overall running time: 777.9065s / 18278.1448 s
env0_first_0:                 episode reward: 19.0500,                 loss: 0.0015
env0_second_0:                 episode reward: -19.0500,                 loss: nan
env1_first_0:                 episode reward: 21.4000,                 loss: nan
env1_second_0:                 episode reward: -21.4000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 1748.1,                last time consumption/overall running time: 786.4418s / 19064.5867 s
env0_first_0:                 episode reward: 20.6500,                 loss: 0.0016
env0_second_0:                 episode reward: -20.6500,                 loss: nan
env1_first_0:                 episode reward: 20.9500,                 loss: nan
env1_second_0:                 episode reward: -20.9500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 1678.15,                last time consumption/overall running time: 755.1986s / 19819.7853 s
env0_first_0:                 episode reward: 21.3000,                 loss: 0.0015
env0_second_0:                 episode reward: -21.3000,                 loss: nan
env1_first_0:                 episode reward: 22.2000,                 loss: nan
env1_second_0:                 episode reward: -22.2000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 1629.1,                last time consumption/overall running time: 733.8841s / 20553.6694 s
env0_first_0:                 episode reward: 21.6500,                 loss: 0.0015
env0_second_0:                 episode reward: -21.6500,                 loss: nan
env1_first_0:                 episode reward: 21.7000,                 loss: nan
env1_second_0:                 episode reward: -21.7000,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 1569.0,                last time consumption/overall running time: 706.7225s / 21260.3919 s
env0_first_0:                 episode reward: 22.5500,                 loss: 0.0016
env0_second_0:                 episode reward: -22.5500,                 loss: nan
env1_first_0:                 episode reward: 21.9500,                 loss: nan
env1_second_0:                 episode reward: -21.9500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 1533.0,                last time consumption/overall running time: 689.9932s / 21950.3852 s
env0_first_0:                 episode reward: 24.0000,                 loss: 0.0015
env0_second_0:                 episode reward: -24.0000,                 loss: nan
env1_first_0:                 episode reward: 24.5000,                 loss: nan
env1_second_0:                 episode reward: -24.5000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 2010.35,                last time consumption/overall running time: 905.3728s / 22855.7579 s
env0_first_0:                 episode reward: 3.7000,                 loss: 0.0017
env0_second_0:                 episode reward: -3.7000,                 loss: 0.0045
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Score delta: 51.2, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/430_0.
Episode: 461/10000 (4.6100%),                 avg. length: 3422.95,                last time consumption/overall running time: 1542.8333s / 24398.5912 s
env0_first_0:                 episode reward: -15.0500,                 loss: nan
env0_second_0:                 episode reward: 15.0500,                 loss: 0.0048
env1_first_0:                 episode reward: -10.7500,                 loss: nan
env1_second_0:                 episode reward: 10.7500,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 1788.75,                last time consumption/overall running time: 805.2267s / 25203.8179 s
env0_first_0:                 episode reward: -25.6500,                 loss: 0.0104
env0_second_0:                 episode reward: 25.6500,                 loss: 0.0053
env1_first_0:                 episode reward: -25.1000,                 loss: nan
env1_second_0:                 episode reward: 25.1000,                 loss: nan
Score delta: 50.2, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/473_1.
Episode: 501/10000 (5.0100%),                 avg. length: 1733.4,                last time consumption/overall running time: 781.8222s / 25985.6401 s
env0_first_0:                 episode reward: -24.8000,                 loss: 0.0079
env0_second_0:                 episode reward: 24.8000,                 loss: nan
env1_first_0:                 episode reward: -22.8000,                 loss: nan
env1_second_0:                 episode reward: 22.8000,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 2593.45,                last time consumption/overall running time: 1178.7176s / 27164.3578 s
env0_first_0:                 episode reward: -26.0500,                 loss: 0.0064
env0_second_0:                 episode reward: 26.0500,                 loss: nan
env1_first_0:                 episode reward: -25.4000,                 loss: nan
env1_second_0:                 episode reward: 25.4000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 3868.0,                last time consumption/overall running time: 1755.5469s / 28919.9047 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.0063
env0_second_0:                 episode reward: 0.8500,                 loss: nan
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 4052.0,                last time consumption/overall running time: 1839.4229s / 30759.3275 s
env0_first_0:                 episode reward: -8.4000,                 loss: 0.0066
env0_second_0:                 episode reward: 8.4000,                 loss: nan
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 2175.85,                last time consumption/overall running time: 988.6863s / 31748.0138 s
env0_first_0:                 episode reward: 18.9000,                 loss: 0.0045
env0_second_0:                 episode reward: -18.9000,                 loss: nan
env1_first_0:                 episode reward: 19.5500,                 loss: nan
env1_second_0:                 episode reward: -19.5500,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 2535.1,                last time consumption/overall running time: 1152.7918s / 32900.8056 s
env0_first_0:                 episode reward: 19.6500,                 loss: 0.0037
env0_second_0:                 episode reward: -19.6500,                 loss: nan
env1_first_0:                 episode reward: 17.6000,                 loss: nan
env1_second_0:                 episode reward: -17.6000,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 2134.55,                last time consumption/overall running time: 969.9444s / 33870.7499 s
env0_first_0:                 episode reward: 20.1500,                 loss: 0.0035
env0_second_0:                 episode reward: -20.1500,                 loss: nan
env1_first_0:                 episode reward: 19.7500,                 loss: nan
env1_second_0:                 episode reward: -19.7500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 2224.05,                last time consumption/overall running time: 1007.2074s / 34877.9573 s
env0_first_0:                 episode reward: 18.4000,                 loss: 0.0032
env0_second_0:                 episode reward: -18.4000,                 loss: nan
env1_first_0:                 episode reward: 20.3500,                 loss: nan
env1_second_0:                 episode reward: -20.3500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 2083.55,                last time consumption/overall running time: 940.1621s / 35818.1194 s
env0_first_0:                 episode reward: 22.0000,                 loss: 0.0028
env0_second_0:                 episode reward: -22.0000,                 loss: nan
env1_first_0:                 episode reward: 19.7000,                 loss: nan
env1_second_0:                 episode reward: -19.7000,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 1968.2,                last time consumption/overall running time: 887.7558s / 36705.8752 s
env0_first_0:                 episode reward: 20.3500,                 loss: 0.0028
env0_second_0:                 episode reward: -20.3500,                 loss: nan
env1_first_0:                 episode reward: 20.3000,                 loss: nan
env1_second_0:                 episode reward: -20.3000,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 2315.85,                last time consumption/overall running time: 1114.9179s / 37820.7931 s
env0_first_0:                 episode reward: 21.0000,                 loss: 0.0031
env0_second_0:                 episode reward: -21.0000,                 loss: nan
env1_first_0:                 episode reward: 20.4000,                 loss: nan
env1_second_0:                 episode reward: -20.4000,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 1954.35,                last time consumption/overall running time: 1037.2675s / 38858.0605 s
env0_first_0:                 episode reward: 22.3500,                 loss: 0.0034
env0_second_0:                 episode reward: -22.3500,                 loss: nan
env1_first_0:                 episode reward: 21.4500,                 loss: nan
env1_second_0:                 episode reward: -21.4500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 2129.85,                last time consumption/overall running time: 1134.3255s / 39992.3860 s
env0_first_0:                 episode reward: 20.5500,                 loss: 0.0030
env0_second_0:                 episode reward: -20.5500,                 loss: nan
env1_first_0:                 episode reward: 20.2000,                 loss: nan
env1_second_0:                 episode reward: -20.2000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 2197.85,                last time consumption/overall running time: 1172.0241s / 41164.4101 s
env0_first_0:                 episode reward: 21.2500,                 loss: 0.0031
env0_second_0:                 episode reward: -21.2500,                 loss: nan
env1_first_0:                 episode reward: 20.4000,                 loss: nan
env1_second_0:                 episode reward: -20.4000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 2100.95,                last time consumption/overall running time: 1120.1601s / 42284.5703 s
env0_first_0:                 episode reward: 21.2000,                 loss: 0.0030
env0_second_0:                 episode reward: -21.2000,                 loss: nan
env1_first_0:                 episode reward: 20.2500,                 loss: nan
env1_second_0:                 episode reward: -20.2500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 2040.55,                last time consumption/overall running time: 1087.3789s / 43371.9492 s
env0_first_0:                 episode reward: 21.1500,                 loss: 0.0029
env0_second_0:                 episode reward: -21.1500,                 loss: nan
env1_first_0:                 episode reward: 22.2000,                 loss: nan
env1_second_0:                 episode reward: -22.2000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 2092.6,                last time consumption/overall running time: 1115.0591s / 44487.0082 s
env0_first_0:                 episode reward: 21.9500,                 loss: 0.0027
env0_second_0:                 episode reward: -21.9500,                 loss: nan
env1_first_0:                 episode reward: 22.2000,                 loss: nan
env1_second_0:                 episode reward: -22.2000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 1970.35,                last time consumption/overall running time: 1050.0737s / 45537.0819 s
env0_first_0:                 episode reward: 22.5000,                 loss: 0.0030
env0_second_0:                 episode reward: -22.5000,                 loss: nan
env1_first_0:                 episode reward: 20.6500,                 loss: nan
env1_second_0:                 episode reward: -20.6500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 2175.1,                last time consumption/overall running time: 1159.7272s / 46696.8091 s
env0_first_0:                 episode reward: 19.3000,                 loss: 0.0031
env0_second_0:                 episode reward: -19.3000,                 loss: nan
env1_first_0:                 episode reward: 19.9500,                 loss: nan
env1_second_0:                 episode reward: -19.9500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 2084.85,                last time consumption/overall running time: 1111.2251s / 47808.0342 s
env0_first_0:                 episode reward: 22.0500,                 loss: 0.0031
env0_second_0:                 episode reward: -22.0500,                 loss: nan
env1_first_0:                 episode reward: 19.3000,                 loss: nan
env1_second_0:                 episode reward: -19.3000,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 2842.2,                last time consumption/overall running time: 1522.2974s / 49330.3316 s
env0_first_0:                 episode reward: 1.8500,                 loss: 0.0029
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0056
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Score delta: 50.2, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/884_0.
Episode: 921/10000 (9.2100%),                 avg. length: 3444.9,                last time consumption/overall running time: 1841.0945s / 51171.4261 s
env0_first_0:                 episode reward: -0.9500,                 loss: nan
env0_second_0:                 episode reward: 0.9500,                 loss: 0.0057
env1_first_0:                 episode reward: 3.9000,                 loss: nan
env1_second_0:                 episode reward: -3.9000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 2561.5,                last time consumption/overall running time: 1366.2754s / 52537.7015 s
env0_first_0:                 episode reward: -17.6000,                 loss: nan
env0_second_0:                 episode reward: 17.6000,                 loss: 0.0054
env1_first_0:                 episode reward: -21.2000,                 loss: nan
env1_second_0:                 episode reward: 21.2000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 3508.45,                last time consumption/overall running time: 1869.3219s / 54407.0234 s
env0_first_0:                 episode reward: -11.5000,                 loss: 0.0043
env0_second_0:                 episode reward: 11.5000,                 loss: 0.0050
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Score delta: 60.8, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/947_1.
Episode: 981/10000 (9.8100%),                 avg. length: 2718.6,                last time consumption/overall running time: 1449.1719s / 55856.1952 s
env0_first_0:                 episode reward: 14.2000,                 loss: 0.0036
env0_second_0:                 episode reward: -14.2000,                 loss: nan
env1_first_0:                 episode reward: 14.2000,                 loss: nan
env1_second_0:                 episode reward: -14.2000,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 3558.4,                last time consumption/overall running time: 1896.0633s / 57752.2585 s
env0_first_0:                 episode reward: 16.3000,                 loss: 0.0034
env0_second_0:                 episode reward: -16.3000,                 loss: nan
env1_first_0:                 episode reward: 13.0000,                 loss: nan
env1_second_0:                 episode reward: -13.0000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 2970.6,                last time consumption/overall running time: 1583.2119s / 59335.4704 s
env0_first_0:                 episode reward: 17.4500,                 loss: 0.0032
env0_second_0:                 episode reward: -17.4500,                 loss: nan
env1_first_0:                 episode reward: 18.7500,                 loss: nan
env1_second_0:                 episode reward: -18.7500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 2989.45,                last time consumption/overall running time: 1591.9050s / 60927.3754 s
env0_first_0:                 episode reward: 15.5500,                 loss: 0.0033
env0_second_0:                 episode reward: -15.5500,                 loss: nan
env1_first_0:                 episode reward: 19.6000,                 loss: nan
env1_second_0:                 episode reward: -19.6000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 2642.65,                last time consumption/overall running time: 1409.4211s / 62336.7965 s
env0_first_0:                 episode reward: 17.6500,                 loss: 0.0032
env0_second_0:                 episode reward: -17.6500,                 loss: nan
env1_first_0:                 episode reward: 17.9000,                 loss: nan
env1_second_0:                 episode reward: -17.9000,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 2557.3,                last time consumption/overall running time: 1363.4049s / 63700.2014 s
env0_first_0:                 episode reward: 16.5000,                 loss: 0.0029
env0_second_0:                 episode reward: -16.5000,                 loss: nan
env1_first_0:                 episode reward: 20.2500,                 loss: nan
env1_second_0:                 episode reward: -20.2500,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 2424.5,                last time consumption/overall running time: 1292.9783s / 64993.1796 s
env0_first_0:                 episode reward: 19.5000,                 loss: 0.0029
env0_second_0:                 episode reward: -19.5000,                 loss: nan
env1_first_0:                 episode reward: 19.6000,                 loss: nan
env1_second_0:                 episode reward: -19.6000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 2312.5,                last time consumption/overall running time: 1232.9511s / 66226.1308 s
env0_first_0:                 episode reward: 17.9000,                 loss: 0.0028
env0_second_0:                 episode reward: -17.9000,                 loss: nan
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 2463.7,                last time consumption/overall running time: 1313.2125s / 67539.3433 s
env0_first_0:                 episode reward: 18.4000,                 loss: 0.0031
env0_second_0:                 episode reward: -18.4000,                 loss: nan
env1_first_0:                 episode reward: 20.0500,                 loss: nan
env1_second_0:                 episode reward: -20.0500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 2334.15,                last time consumption/overall running time: 1244.6976s / 68784.0408 s
env0_first_0:                 episode reward: 17.5500,                 loss: 0.0031
env0_second_0:                 episode reward: -17.5500,                 loss: nan
env1_first_0:                 episode reward: 19.6500,                 loss: nan
env1_second_0:                 episode reward: -19.6500,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 2547.85,                last time consumption/overall running time: 1358.8197s / 70142.8606 s
env0_first_0:                 episode reward: 19.5000,                 loss: 0.0029
env0_second_0:                 episode reward: -19.5000,                 loss: nan
env1_first_0:                 episode reward: 16.4500,                 loss: nan
env1_second_0:                 episode reward: -16.4500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 2441.0,                last time consumption/overall running time: 1300.2622s / 71443.1228 s
env0_first_0:                 episode reward: 17.8500,                 loss: 0.0030
env0_second_0:                 episode reward: -17.8500,                 loss: nan
env1_first_0:                 episode reward: 18.8500,                 loss: nan
env1_second_0:                 episode reward: -18.8500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 2443.4,                last time consumption/overall running time: 1302.1942s / 72745.3170 s
env0_first_0:                 episode reward: 20.6500,                 loss: 0.0028
env0_second_0:                 episode reward: -20.6500,                 loss: nan
env1_first_0:                 episode reward: 20.7000,                 loss: nan
env1_second_0:                 episode reward: -20.7000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 2582.85,                last time consumption/overall running time: 1375.8230s / 74121.1400 s
env0_first_0:                 episode reward: 17.8000,                 loss: 0.0030
env0_second_0:                 episode reward: -17.8000,                 loss: nan
env1_first_0:                 episode reward: 19.4500,                 loss: nan
env1_second_0:                 episode reward: -19.4500,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 2269.35,                last time consumption/overall running time: 1209.2607s / 75330.4008 s
env0_first_0:                 episode reward: 21.5500,                 loss: 0.0032
env0_second_0:                 episode reward: -21.5500,                 loss: nan
env1_first_0:                 episode reward: 21.8500,                 loss: nan
env1_second_0:                 episode reward: -21.8500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 2508.45,                last time consumption/overall running time: 1336.6591s / 76667.0599 s
env0_first_0:                 episode reward: 20.2000,                 loss: 0.0029
env0_second_0:                 episode reward: -20.2000,                 loss: nan
env1_first_0:                 episode reward: 22.4000,                 loss: nan
env1_second_0:                 episode reward: -22.4000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 2246.6,                last time consumption/overall running time: 1197.2009s / 77864.2608 s
env0_first_0:                 episode reward: 22.1000,                 loss: 0.0031
env0_second_0:                 episode reward: -22.1000,                 loss: nan
env1_first_0:                 episode reward: 21.4500,                 loss: nan
env1_second_0:                 episode reward: -21.4500,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 2246.5,                last time consumption/overall running time: 1196.5912s / 79060.8520 s
env0_first_0:                 episode reward: 23.4000,                 loss: 0.0031
env0_second_0:                 episode reward: -23.4000,                 loss: nan
env1_first_0:                 episode reward: 22.6000,                 loss: nan
env1_second_0:                 episode reward: -22.6000,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 2261.7,                last time consumption/overall running time: 1205.1356s / 80265.9877 s
env0_first_0:                 episode reward: 20.5500,                 loss: 0.0032
env0_second_0:                 episode reward: -20.5500,                 loss: nan
env1_first_0:                 episode reward: 22.1500,                 loss: nan
env1_second_0:                 episode reward: -22.1500,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 2378.9,                last time consumption/overall running time: 1266.9674s / 81532.9551 s
env0_first_0:                 episode reward: 20.5500,                 loss: 0.0032
env0_second_0:                 episode reward: -20.5500,                 loss: nan
env1_first_0:                 episode reward: 21.8500,                 loss: nan
env1_second_0:                 episode reward: -21.8500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 2406.85,                last time consumption/overall running time: 1282.5452s / 82815.5003 s
env0_first_0:                 episode reward: 21.1000,                 loss: 0.0034
env0_second_0:                 episode reward: -21.1000,                 loss: nan
env1_first_0:                 episode reward: 20.7000,                 loss: nan
env1_second_0:                 episode reward: -20.7000,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 2480.8,                last time consumption/overall running time: 1321.8651s / 84137.3654 s
env0_first_0:                 episode reward: 19.8500,                 loss: 0.0031
env0_second_0:                 episode reward: -19.8500,                 loss: nan
env1_first_0:                 episode reward: 18.3000,                 loss: nan
env1_second_0:                 episode reward: -18.3000,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 2409.55,                last time consumption/overall running time: 1283.3605s / 85420.7259 s
env0_first_0:                 episode reward: 22.1000,                 loss: 0.0029
env0_second_0:                 episode reward: -22.1000,                 loss: nan
env1_first_0:                 episode reward: 22.1500,                 loss: nan
env1_second_0:                 episode reward: -22.1500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 2454.65,                last time consumption/overall running time: 1307.6744s / 86728.4003 s
env0_first_0:                 episode reward: 21.7500,                 loss: 0.0031
env0_second_0:                 episode reward: -21.7500,                 loss: nan
env1_first_0:                 episode reward: 23.1500,                 loss: nan
env1_second_0:                 episode reward: -23.1500,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 2261.95,                last time consumption/overall running time: 1205.2562s / 87933.6565 s
env0_first_0:                 episode reward: 19.9500,                 loss: 0.0033
env0_second_0:                 episode reward: -19.9500,                 loss: nan
env1_first_0:                 episode reward: 22.0500,                 loss: nan
env1_second_0:                 episode reward: -22.0500,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 2290.8,                last time consumption/overall running time: 1220.9227s / 89154.5792 s
env0_first_0:                 episode reward: 19.2500,                 loss: 0.0028
env0_second_0:                 episode reward: -19.2500,                 loss: nan
env1_first_0:                 episode reward: 20.9500,                 loss: nan
env1_second_0:                 episode reward: -20.9500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 2299.8,                last time consumption/overall running time: 1225.8761s / 90380.4554 s
env0_first_0:                 episode reward: 22.5500,                 loss: 0.0028
env0_second_0:                 episode reward: -22.5500,                 loss: nan
env1_first_0:                 episode reward: 19.2500,                 loss: nan
env1_second_0:                 episode reward: -19.2500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 2309.4,                last time consumption/overall running time: 1230.4356s / 91610.8910 s
env0_first_0:                 episode reward: 20.7500,                 loss: 0.0028
env0_second_0:                 episode reward: -20.7500,                 loss: nan
env1_first_0:                 episode reward: 21.2000,                 loss: nan
env1_second_0:                 episode reward: -21.2000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 2315.95,                last time consumption/overall running time: 1235.1658s / 92846.0568 s
env0_first_0:                 episode reward: 20.6500,                 loss: 0.0030
env0_second_0:                 episode reward: -20.6500,                 loss: nan
env1_first_0:                 episode reward: 19.6500,                 loss: nan
env1_second_0:                 episode reward: -19.6500,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 2231.75,                last time consumption/overall running time: 1190.1844s / 94036.2413 s
env0_first_0:                 episode reward: 20.5000,                 loss: 0.0031
env0_second_0:                 episode reward: -20.5000,                 loss: nan
env1_first_0:                 episode reward: 22.6500,                 loss: nan
env1_second_0:                 episode reward: -22.6500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 2524.15,                last time consumption/overall running time: 1346.5037s / 95382.7450 s
env0_first_0:                 episode reward: 21.6000,                 loss: 0.0030
env0_second_0:                 episode reward: -21.6000,                 loss: nan
env1_first_0:                 episode reward: 17.7000,                 loss: nan
env1_second_0:                 episode reward: -17.7000,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 2396.55,                last time consumption/overall running time: 1277.7763s / 96660.5213 s
env0_first_0:                 episode reward: 21.5500,                 loss: 0.0032
env0_second_0:                 episode reward: -21.5500,                 loss: nan
env1_first_0:                 episode reward: 20.9500,                 loss: nan
env1_second_0:                 episode reward: -20.9500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 2164.05,                last time consumption/overall running time: 1152.3919s / 97812.9132 s
env0_first_0:                 episode reward: 22.0000,                 loss: 0.0031
env0_second_0:                 episode reward: -22.0000,                 loss: nan
env1_first_0:                 episode reward: 20.8500,                 loss: nan
env1_second_0:                 episode reward: -20.8500,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 2224.7,                last time consumption/overall running time: 1183.5834s / 98996.4965 s
env0_first_0:                 episode reward: 21.8000,                 loss: 0.0029
env0_second_0:                 episode reward: -21.8000,                 loss: nan
env1_first_0:                 episode reward: 21.1500,                 loss: nan
env1_second_0:                 episode reward: -21.1500,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 2299.25,                last time consumption/overall running time: 1222.9691s / 100219.4657 s
env0_first_0:                 episode reward: 21.2500,                 loss: 0.0029
env0_second_0:                 episode reward: -21.2500,                 loss: nan
env1_first_0:                 episode reward: 21.1500,                 loss: nan
env1_second_0:                 episode reward: -21.1500,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 2084.3,                last time consumption/overall running time: 1108.2161s / 101327.6818 s
env0_first_0:                 episode reward: 21.9500,                 loss: 0.0027
env0_second_0:                 episode reward: -21.9500,                 loss: nan
env1_first_0:                 episode reward: 24.2000,                 loss: nan
env1_second_0:                 episode reward: -24.2000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 2332.55,                last time consumption/overall running time: 1240.4539s / 102568.1357 s
env0_first_0:                 episode reward: 21.3500,                 loss: 0.0028
env0_second_0:                 episode reward: -21.3500,                 loss: nan
env1_first_0:                 episode reward: 19.9500,                 loss: nan
env1_second_0:                 episode reward: -19.9500,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 2113.4,                last time consumption/overall running time: 1124.2721s / 103692.4078 s
env0_first_0:                 episode reward: 20.2000,                 loss: 0.0027
env0_second_0:                 episode reward: -20.2000,                 loss: nan
env1_first_0:                 episode reward: 21.3500,                 loss: nan
env1_second_0:                 episode reward: -21.3500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 2106.1,                last time consumption/overall running time: 1120.3931s / 104812.8009 s
env0_first_0:                 episode reward: 21.4500,                 loss: 0.0026
env0_second_0:                 episode reward: -21.4500,                 loss: nan
env1_first_0:                 episode reward: 20.9000,                 loss: nan
env1_second_0:                 episode reward: -20.9000,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 2226.95,                last time consumption/overall running time: 1184.4932s / 105997.2941 s
env0_first_0:                 episode reward: 21.9000,                 loss: 0.0025
env0_second_0:                 episode reward: -21.9000,                 loss: nan
env1_first_0:                 episode reward: 21.2000,                 loss: nan
env1_second_0:                 episode reward: -21.2000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 2148.85,                last time consumption/overall running time: 1143.4293s / 107140.7234 s
env0_first_0:                 episode reward: 22.0500,                 loss: 0.0027
env0_second_0:                 episode reward: -22.0500,                 loss: nan
env1_first_0:                 episode reward: 19.5000,                 loss: nan
env1_second_0:                 episode reward: -19.5000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 2349.1,                last time consumption/overall running time: 1249.0627s / 108389.7861 s
env0_first_0:                 episode reward: 18.6000,                 loss: 0.0029
env0_second_0:                 episode reward: -18.6000,                 loss: nan
env1_first_0:                 episode reward: 21.3000,                 loss: nan
env1_second_0:                 episode reward: -21.3000,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 2104.15,                last time consumption/overall running time: 1119.6277s / 109509.4138 s
env0_first_0:                 episode reward: 21.5500,                 loss: 0.0028
env0_second_0:                 episode reward: -21.5500,                 loss: nan
env1_first_0:                 episode reward: 22.6500,                 loss: nan
env1_second_0:                 episode reward: -22.6500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 2142.2,                last time consumption/overall running time: 1140.5568s / 110649.9706 s
env0_first_0:                 episode reward: 23.1500,                 loss: 0.0029
env0_second_0:                 episode reward: -23.1500,                 loss: nan
env1_first_0:                 episode reward: 23.6500,                 loss: nan
env1_second_0:                 episode reward: -23.6500,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 2243.35,                last time consumption/overall running time: 1193.0890s / 111843.0596 s
env0_first_0:                 episode reward: 21.5000,                 loss: 0.0028
env0_second_0:                 episode reward: -21.5000,                 loss: nan
env1_first_0:                 episode reward: 22.3500,                 loss: nan
env1_second_0:                 episode reward: -22.3500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 2105.55,                last time consumption/overall running time: 1120.4868s / 112963.5463 s
env0_first_0:                 episode reward: 21.8000,                 loss: 0.0029
env0_second_0:                 episode reward: -21.8000,                 loss: nan
env1_first_0:                 episode reward: 22.1000,                 loss: nan
env1_second_0:                 episode reward: -22.1000,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 2035.0,                last time consumption/overall running time: 1082.4072s / 114045.9536 s
env0_first_0:                 episode reward: 22.5500,                 loss: 0.0028
env0_second_0:                 episode reward: -22.5500,                 loss: nan
env1_first_0:                 episode reward: 20.5500,                 loss: nan
env1_second_0:                 episode reward: -20.5500,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 2179.3,                last time consumption/overall running time: 1159.1683s / 115205.1218 s
env0_first_0:                 episode reward: 22.0500,                 loss: 0.0028
env0_second_0:                 episode reward: -22.0500,                 loss: nan
env1_first_0:                 episode reward: 22.1000,                 loss: nan
env1_second_0:                 episode reward: -22.1000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 2360.75,                last time consumption/overall running time: 1255.3653s / 116460.4872 s
env0_first_0:                 episode reward: 19.6000,                 loss: 0.0031
env0_second_0:                 episode reward: -19.6000,                 loss: nan
env1_first_0:                 episode reward: 20.9500,                 loss: nan
env1_second_0:                 episode reward: -20.9500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 2551.05,                last time consumption/overall running time: 1356.3813s / 117816.8684 s
env0_first_0:                 episode reward: 25.7000,                 loss: 0.0031
env0_second_0:                 episode reward: -25.7000,                 loss: 0.0072
env1_first_0:                 episode reward: 24.1000,                 loss: nan
env1_second_0:                 episode reward: -24.1000,                 loss: nan
Score delta: 51.6, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/1957_0.
Episode: 1981/10000 (19.8100%),                 avg. length: 3558.15,                last time consumption/overall running time: 1890.5348s / 119707.4033 s
env0_first_0:                 episode reward: 0.0000,                 loss: nan
env0_second_0:                 episode reward: 0.0000,                 loss: 0.0065
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 2901.75,                last time consumption/overall running time: 1542.7258s / 121250.1290 s
env0_first_0:                 episode reward: -4.4000,                 loss: nan
env0_second_0:                 episode reward: 4.4000,                 loss: 0.0057
env1_first_0:                 episode reward: -18.1000,                 loss: nan
env1_second_0:                 episode reward: 18.1000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 3799.4,                last time consumption/overall running time: 2019.6956s / 123269.8247 s
env0_first_0:                 episode reward: -7.2500,                 loss: nan
env0_second_0:                 episode reward: 7.2500,                 loss: 0.0063
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 3365.4,                last time consumption/overall running time: 1789.0632s / 125058.8879 s
env0_first_0:                 episode reward: -14.6500,                 loss: 0.0045
env0_second_0:                 episode reward: 14.6500,                 loss: 0.0057
env1_first_0:                 episode reward: -12.7000,                 loss: nan
env1_second_0:                 episode reward: 12.7000,                 loss: nan
Score delta: 62.8, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/2036_1.
Episode: 2061/10000 (20.6100%),                 avg. length: 4553.55,                last time consumption/overall running time: 2420.2775s / 127479.1654 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0045
env0_second_0:                 episode reward: -7.6500,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 6156.8,                last time consumption/overall running time: 3271.9849s / 130751.1503 s
env0_first_0:                 episode reward: 13.2000,                 loss: 0.0047
env0_second_0:                 episode reward: -13.2000,                 loss: nan
env1_first_0:                 episode reward: 20.0500,                 loss: nan
env1_second_0:                 episode reward: -20.0500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 6464.3,                last time consumption/overall running time: 3435.3391s / 134186.4894 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.0055
env0_second_0:                 episode reward: 4.1500,                 loss: nan
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 5475.95,                last time consumption/overall running time: 2911.1099s / 137097.5993 s
env0_first_0:                 episode reward: 12.0000,                 loss: 0.0049
env0_second_0:                 episode reward: -12.0000,                 loss: 0.0062
env1_first_0:                 episode reward: 6.3000,                 loss: nan
env1_second_0:                 episode reward: -6.3000,                 loss: nan
Score delta: 81.0, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/2112_0.
Episode: 2141/10000 (21.4100%),                 avg. length: 3775.0,                last time consumption/overall running time: 2006.3310s / 139103.9303 s
env0_first_0:                 episode reward: -21.5000,                 loss: 0.0055
env0_second_0:                 episode reward: 21.5000,                 loss: 0.0060
env1_first_0:                 episode reward: -24.0000,                 loss: nan
env1_second_0:                 episode reward: 24.0000,                 loss: nan
Score delta: 50.4, save the model to .//data/model/20220119_0524/pettingzoo_tennis_v2_fictitious_selfplay2/2135_1.
Episode: 2161/10000 (21.6100%),                 avg. length: 4867.25,                last time consumption/overall running time: 2587.1947s / 141691.1250 s
env0_first_0:                 episode reward: 5.7500,                 loss: 0.0046
env0_second_0:                 episode reward: -5.7500,                 loss: nan
env1_first_0:                 episode reward: 2.4500,                 loss: nan
env1_second_0:                 episode reward: -2.4500,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 5415.65,                last time consumption/overall running time: 2852.1498s / 144543.2748 s
env0_first_0:                 episode reward: 4.4500,                 loss: 0.0034
env0_second_0:                 episode reward: -4.4500,                 loss: nan
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 4206.6,                last time consumption/overall running time: 2095.6484s / 146638.9232 s
env0_first_0:                 episode reward: 11.6000,                 loss: 0.0032
env0_second_0:                 episode reward: -11.6000,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 5140.1,                last time consumption/overall running time: 2559.2311s / 149198.1543 s
env0_first_0:                 episode reward: 8.7000,                 loss: 0.0032
env0_second_0:                 episode reward: -8.7000,                 loss: nan
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 4179.5,                last time consumption/overall running time: 2059.6594s / 151257.8136 s
env0_first_0:                 episode reward: 12.5000,                 loss: 0.0029
env0_second_0:                 episode reward: -12.5000,                 loss: nan
env1_first_0:                 episode reward: 11.2000,                 loss: nan
env1_second_0:                 episode reward: -11.2000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 4452.75,                last time consumption/overall running time: 2075.3368s / 153333.1504 s
env0_first_0:                 episode reward: 13.2500,                 loss: 0.0027
env0_second_0:                 episode reward: -13.2500,                 loss: nan
env1_first_0:                 episode reward: 12.7500,                 loss: nan
env1_second_0:                 episode reward: -12.7500,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 3468.35,                last time consumption/overall running time: 1615.4318s / 154948.5822 s
env0_first_0:                 episode reward: 14.7000,                 loss: 0.0028
env0_second_0:                 episode reward: -14.7000,                 loss: nan
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 3300.6,                last time consumption/overall running time: 1538.0119s / 156486.5941 s
env0_first_0:                 episode reward: 13.7000,                 loss: 0.0028
env0_second_0:                 episode reward: -13.7000,                 loss: nan
env1_first_0:                 episode reward: 12.3500,                 loss: nan
env1_second_0:                 episode reward: -12.3500,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 3838.35,                last time consumption/overall running time: 1788.5748s / 158275.1689 s
env0_first_0:                 episode reward: 14.4500,                 loss: 0.0031
env0_second_0:                 episode reward: -14.4500,                 loss: nan
env1_first_0:                 episode reward: 13.4500,                 loss: nan
env1_second_0:                 episode reward: -13.4500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 3415.35,                last time consumption/overall running time: 1591.8706s / 159867.0395 s
env0_first_0:                 episode reward: 14.2000,                 loss: 0.0030
env0_second_0:                 episode reward: -14.2000,                 loss: nan
env1_first_0:                 episode reward: 15.2500,                 loss: nan
env1_second_0:                 episode reward: -15.2500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 3331.6,                last time consumption/overall running time: 1463.2267s / 161330.2662 s
env0_first_0:                 episode reward: 15.4500,                 loss: 0.0027
env0_second_0:                 episode reward: -15.4500,                 loss: nan
env1_first_0:                 episode reward: 14.1000,                 loss: nan
env1_second_0:                 episode reward: -14.1000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 3768.0,                last time consumption/overall running time: 1634.7064s / 162964.9726 s
env0_first_0:                 episode reward: 9.1000,                 loss: 0.0028
env0_second_0:                 episode reward: -9.1000,                 loss: nan
env1_first_0:                 episode reward: 10.4500,                 loss: nan
env1_second_0:                 episode reward: -10.4500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 3212.9,                last time consumption/overall running time: 1392.7548s / 164357.7274 s
env0_first_0:                 episode reward: 14.3000,                 loss: 0.0027
env0_second_0:                 episode reward: -14.3000,                 loss: nan
env1_first_0:                 episode reward: 14.9000,                 loss: nan
env1_second_0:                 episode reward: -14.9000,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 3290.05,                last time consumption/overall running time: 1425.6104s / 165783.3377 s
env0_first_0:                 episode reward: 16.1000,                 loss: 0.0025
env0_second_0:                 episode reward: -16.1000,                 loss: nan
env1_first_0:                 episode reward: 15.3000,                 loss: nan
env1_second_0:                 episode reward: -15.3000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 3320.5,                last time consumption/overall running time: 1440.0003s / 167223.3381 s
env0_first_0:                 episode reward: 5.4000,                 loss: 0.0025
env0_second_0:                 episode reward: -5.4000,                 loss: nan
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 3701.25,                last time consumption/overall running time: 1604.7179s / 168828.0559 s
env0_first_0:                 episode reward: 15.2000,                 loss: 0.0036
env0_second_0:                 episode reward: -15.2000,                 loss: nan
env1_first_0:                 episode reward: 13.2500,                 loss: nan
env1_second_0:                 episode reward: -13.2500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 3325.0,                last time consumption/overall running time: 1441.2060s / 170269.2619 s
env0_first_0:                 episode reward: 14.2500,                 loss: 0.0024
env0_second_0:                 episode reward: -14.2500,                 loss: nan
env1_first_0:                 episode reward: 16.6500,                 loss: nan
env1_second_0:                 episode reward: -16.6500,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 2816.2,                last time consumption/overall running time: 1219.8029s / 171489.0649 s
env0_first_0:                 episode reward: 16.7000,                 loss: 0.0024
env0_second_0:                 episode reward: -16.7000,                 loss: nan
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 3502.45,                last time consumption/overall running time: 1517.2000s / 173006.2648 s
env0_first_0:                 episode reward: 15.2000,                 loss: 0.0024
env0_second_0:                 episode reward: -15.2000,                 loss: nan
env1_first_0:                 episode reward: 17.1500,                 loss: nan
env1_second_0:                 episode reward: -17.1500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 2868.1,                last time consumption/overall running time: 1243.3918s / 174249.6566 s
env0_first_0:                 episode reward: 15.9000,                 loss: 0.0024
env0_second_0:                 episode reward: -15.9000,                 loss: nan
env1_first_0:                 episode reward: 14.2000,                 loss: nan
env1_second_0:                 episode reward: -14.2000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 2934.1,                last time consumption/overall running time: 1271.7974s / 175521.4540 s
env0_first_0:                 episode reward: 17.4500,                 loss: 0.0026
env0_second_0:                 episode reward: -17.4500,                 loss: nan
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 4010.1,                last time consumption/overall running time: 1737.0119s / 177258.4658 s
env0_first_0:                 episode reward: 15.8000,                 loss: 0.0029
env0_second_0:                 episode reward: -15.8000,                 loss: nan
env1_first_0:                 episode reward: 10.2500,                 loss: nan
env1_second_0:                 episode reward: -10.2500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 3320.3,                last time consumption/overall running time: 1437.3079s / 178695.7737 s
env0_first_0:                 episode reward: 12.4500,                 loss: 0.0030
env0_second_0:                 episode reward: -12.4500,                 loss: nan
env1_first_0:                 episode reward: 13.5000,                 loss: nan
env1_second_0:                 episode reward: -13.5000,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 3011.8,                last time consumption/overall running time: 1303.9119s / 179999.6856 s
env0_first_0:                 episode reward: 14.8000,                 loss: 0.0026
env0_second_0:                 episode reward: -14.8000,                 loss: nan
env1_first_0:                 episode reward: 16.1500,                 loss: nan
env1_second_0:                 episode reward: -16.1500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 3032.65,                last time consumption/overall running time: 1312.6320s / 181312.3177 s
env0_first_0:                 episode reward: 14.9000,                 loss: 0.0023
env0_second_0:                 episode reward: -14.9000,                 loss: nan
env1_first_0:                 episode reward: 17.7000,                 loss: nan
env1_second_0:                 episode reward: -17.7000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 3218.55,                last time consumption/overall running time: 1393.5026s / 182705.8203 s
env0_first_0:                 episode reward: 7.9500,                 loss: 0.0030
env0_second_0:                 episode reward: -7.9500,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 3149.25,                last time consumption/overall running time: 1363.1499s / 184068.9702 s
env0_first_0:                 episode reward: 10.8000,                 loss: 0.0032
env0_second_0:                 episode reward: -10.8000,                 loss: nan
env1_first_0:                 episode reward: 9.6500,                 loss: nan
env1_second_0:                 episode reward: -9.6500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 3462.4,                last time consumption/overall running time: 1499.0326s / 185568.0028 s
env0_first_0:                 episode reward: 13.9000,                 loss: 0.0029
env0_second_0:                 episode reward: -13.9000,                 loss: nan
env1_first_0:                 episode reward: 10.9500,                 loss: nan
env1_second_0:                 episode reward: -10.9500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 2814.7,                last time consumption/overall running time: 1217.8931s / 186785.8960 s
env0_first_0:                 episode reward: 17.0000,                 loss: 0.0027
env0_second_0:                 episode reward: -17.0000,                 loss: nan
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 2879.95,                last time consumption/overall running time: 1245.8072s / 188031.7032 s
env0_first_0:                 episode reward: 16.8500,                 loss: 0.0025
env0_second_0:                 episode reward: -16.8500,                 loss: nan
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 3127.1,                last time consumption/overall running time: 1352.7682s / 189384.4714 s
env0_first_0:                 episode reward: 10.8000,                 loss: 0.0023
env0_second_0:                 episode reward: -10.8000,                 loss: nan
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 4057.85,                last time consumption/overall running time: 1753.0493s / 191137.5207 s
env0_first_0:                 episode reward: 1.6500,                 loss: 0.0045
env0_second_0:                 episode reward: -1.6500,                 loss: nan
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 2876.75,                last time consumption/overall running time: 1242.3179s / 192379.8386 s
env0_first_0:                 episode reward: 14.6500,                 loss: 0.0024
env0_second_0:                 episode reward: -14.6500,                 loss: nan
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 2891.1,                last time consumption/overall running time: 1247.3580s / 193627.1966 s
env0_first_0:                 episode reward: 14.5500,                 loss: 0.0025
env0_second_0:                 episode reward: -14.5500,                 loss: nan
env1_first_0:                 episode reward: 16.1000,                 loss: nan
env1_second_0:                 episode reward: -16.1000,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 3291.3,                last time consumption/overall running time: 1419.1969s / 195046.3934 s
env0_first_0:                 episode reward: 13.2000,                 loss: 0.0026
env0_second_0:                 episode reward: -13.2000,                 loss: nan
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 3303.6,                last time consumption/overall running time: 1423.8115s / 196470.2049 s
env0_first_0:                 episode reward: 13.5500,                 loss: 0.0030
env0_second_0:                 episode reward: -13.5500,                 loss: nan
env1_first_0:                 episode reward: 11.1500,                 loss: nan
env1_second_0:                 episode reward: -11.1500,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 4057.45,                last time consumption/overall running time: 1747.9001s / 198218.1050 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0047
env0_second_0:                 episode reward: 3.0500,                 loss: nan
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 3208.6,                last time consumption/overall running time: 1380.9952s / 199599.1002 s
env0_first_0:                 episode reward: 12.9000,                 loss: 0.0030
env0_second_0:                 episode reward: -12.9000,                 loss: nan
env1_first_0:                 episode reward: 13.0000,                 loss: nan
env1_second_0:                 episode reward: -13.0000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 3184.45,                last time consumption/overall running time: 1368.0219s / 200967.1222 s
env0_first_0:                 episode reward: 15.5500,                 loss: 0.0025
env0_second_0:                 episode reward: -15.5500,                 loss: nan
env1_first_0:                 episode reward: 17.1000,                 loss: nan
env1_second_0:                 episode reward: -17.1000,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 3270.45,                last time consumption/overall running time: 1403.8633s / 202370.9854 s
env0_first_0:                 episode reward: 12.0000,                 loss: 0.0031
env0_second_0:                 episode reward: -12.0000,                 loss: nan
env1_first_0:                 episode reward: 10.6500,                 loss: nan
env1_second_0:                 episode reward: -10.6500,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 2597.25,                last time consumption/overall running time: 1115.3082s / 203486.2936 s
env0_first_0:                 episode reward: 15.7000,                 loss: 0.0028
env0_second_0:                 episode reward: -15.7000,                 loss: nan
env1_first_0:                 episode reward: 18.8000,                 loss: nan
env1_second_0:                 episode reward: -18.8000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 2842.45,                last time consumption/overall running time: 1221.3313s / 204707.6249 s
env0_first_0:                 episode reward: 14.1500,                 loss: 0.0025
env0_second_0:                 episode reward: -14.1500,                 loss: nan
env1_first_0:                 episode reward: 16.8500,                 loss: nan
env1_second_0:                 episode reward: -16.8500,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 2888.8,                last time consumption/overall running time: 1240.6904s / 205948.3153 s
env0_first_0:                 episode reward: 15.6500,                 loss: 0.0025
env0_second_0:                 episode reward: -15.6500,                 loss: nan
env1_first_0:                 episode reward: 13.8000,                 loss: nan
env1_second_0:                 episode reward: -13.8000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 2965.55,                last time consumption/overall running time: 1274.4416s / 207222.7569 s
env0_first_0:                 episode reward: 13.6000,                 loss: 0.0028
env0_second_0:                 episode reward: -13.6000,                 loss: nan
env1_first_0:                 episode reward: 14.1500,                 loss: nan
env1_second_0:                 episode reward: -14.1500,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 4728.35,                last time consumption/overall running time: 2029.7421s / 209252.4990 s
env0_first_0:                 episode reward: -30.3000,                 loss: 0.0059
env0_second_0:                 episode reward: 30.3000,                 loss: nan
env1_first_0:                 episode reward: -29.6500,                 loss: nan
env1_second_0:                 episode reward: 29.6500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 3305.4,                last time consumption/overall running time: 1418.8169s / 210671.3159 s
env0_first_0:                 episode reward: 15.7000,                 loss: 0.0037
env0_second_0:                 episode reward: -15.7000,                 loss: nan
env1_first_0:                 episode reward: 13.0000,                 loss: nan
env1_second_0:                 episode reward: -13.0000,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 3110.75,                last time consumption/overall running time: 1335.5424s / 212006.8584 s
env0_first_0:                 episode reward: 13.0500,                 loss: 0.0025
env0_second_0:                 episode reward: -13.0500,                 loss: nan
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 3206.65,                last time consumption/overall running time: 1377.5196s / 213384.3780 s
env0_first_0:                 episode reward: 16.7000,                 loss: 0.0027
env0_second_0:                 episode reward: -16.7000,                 loss: nan
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 3562.05,                last time consumption/overall running time: 1463.5964s / 214847.9744 s
env0_first_0:                 episode reward: 12.3500,                 loss: 0.0030
env0_second_0:                 episode reward: -12.3500,                 loss: nan
env1_first_0:                 episode reward: 12.6500,                 loss: nan
env1_second_0:                 episode reward: -12.6500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 3062.95,                last time consumption/overall running time: 1240.1272s / 216088.1016 s
env0_first_0:                 episode reward: 13.0000,                 loss: 0.0024
env0_second_0:                 episode reward: -13.0000,                 loss: nan
env1_first_0:                 episode reward: 12.5000,                 loss: nan
env1_second_0:                 episode reward: -12.5000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 3383.7,                last time consumption/overall running time: 1368.2393s / 217456.3410 s
env0_first_0:                 episode reward: 10.3500,                 loss: 0.0034
env0_second_0:                 episode reward: -10.3500,                 loss: nan
env1_first_0:                 episode reward: 5.7500,                 loss: nan
env1_second_0:                 episode reward: -5.7500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 3124.1,                last time consumption/overall running time: 1262.6271s / 218718.9681 s
env0_first_0:                 episode reward: 13.6500,                 loss: 0.0029
env0_second_0:                 episode reward: -13.6500,                 loss: nan
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 2965.6,                last time consumption/overall running time: 1197.4736s / 219916.4417 s
env0_first_0:                 episode reward: 10.1000,                 loss: 0.0024
env0_second_0:                 episode reward: -10.1000,                 loss: nan
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 3063.0,                last time consumption/overall running time: 1237.8654s / 221154.3071 s
env0_first_0:                 episode reward: 13.4500,                 loss: 0.0037
env0_second_0:                 episode reward: -13.4500,                 loss: nan
env1_first_0:                 episode reward: 12.2500,                 loss: nan
env1_second_0:                 episode reward: -12.2500,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 3111.7,                last time consumption/overall running time: 1257.0909s / 222411.3980 s
env0_first_0:                 episode reward: 17.0000,                 loss: 0.0024
env0_second_0:                 episode reward: -17.0000,                 loss: nan
env1_first_0:                 episode reward: 14.5000,                 loss: nan
env1_second_0:                 episode reward: -14.5000,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 2944.45,                last time consumption/overall running time: 1189.0740s / 223600.4720 s
env0_first_0:                 episode reward: 14.7500,                 loss: 0.0026
env0_second_0:                 episode reward: -14.7500,                 loss: nan
env1_first_0:                 episode reward: 11.6500,                 loss: nan
env1_second_0:                 episode reward: -11.6500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 2714.7,                last time consumption/overall running time: 1095.8267s / 224696.2987 s
env0_first_0:                 episode reward: 18.6000,                 loss: 0.0027
env0_second_0:                 episode reward: -18.6000,                 loss: nan
env1_first_0:                 episode reward: 16.2500,                 loss: nan
env1_second_0:                 episode reward: -16.2500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 2646.1,                last time consumption/overall running time: 1068.8109s / 225765.1096 s
env0_first_0:                 episode reward: 19.1500,                 loss: 0.0024
env0_second_0:                 episode reward: -19.1500,                 loss: nan
env1_first_0:                 episode reward: 18.2500,                 loss: nan
env1_second_0:                 episode reward: -18.2500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 2814.15,                last time consumption/overall running time: 1135.8268s / 226900.9365 s
env0_first_0:                 episode reward: 12.8000,                 loss: 0.0022
env0_second_0:                 episode reward: -12.8000,                 loss: nan
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 3270.4,                last time consumption/overall running time: 1260.8074s / 228161.7439 s
env0_first_0:                 episode reward: 12.0500,                 loss: 0.0032
env0_second_0:                 episode reward: -12.0500,                 loss: nan
env1_first_0:                 episode reward: 13.2000,                 loss: nan
env1_second_0:                 episode reward: -13.2000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 2834.95,                last time consumption/overall running time: 1029.4253s / 229191.1692 s
env0_first_0:                 episode reward: 14.8000,                 loss: 0.0026
env0_second_0:                 episode reward: -14.8000,                 loss: nan
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 2915.75,                last time consumption/overall running time: 1036.6569s / 230227.8260 s
env0_first_0:                 episode reward: 17.0500,                 loss: 0.0024
env0_second_0:                 episode reward: -17.0500,                 loss: nan
env1_first_0:                 episode reward: 16.6000,                 loss: nan
env1_second_0:                 episode reward: -16.6000,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 2930.45,                last time consumption/overall running time: 1040.8854s / 231268.7114 s
env0_first_0:                 episode reward: 15.4500,                 loss: 0.0023
env0_second_0:                 episode reward: -15.4500,                 loss: nan
env1_first_0:                 episode reward: 15.9500,                 loss: nan
env1_second_0:                 episode reward: -15.9500,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 2996.55,                last time consumption/overall running time: 1066.7050s / 232335.4164 s
env0_first_0:                 episode reward: 15.1500,                 loss: 0.0025
env0_second_0:                 episode reward: -15.1500,                 loss: nan
env1_first_0:                 episode reward: 16.7000,                 loss: nan
env1_second_0:                 episode reward: -16.7000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 2712.95,                last time consumption/overall running time: 964.7428s / 233300.1593 s
env0_first_0:                 episode reward: 17.6500,                 loss: 0.0022
env0_second_0:                 episode reward: -17.6500,                 loss: nan
env1_first_0:                 episode reward: 18.3500,                 loss: nan
env1_second_0:                 episode reward: -18.3500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 3347.15,                last time consumption/overall running time: 1189.4883s / 234489.6475 s
env0_first_0:                 episode reward: 12.7000,                 loss: 0.0026
env0_second_0:                 episode reward: -12.7000,                 loss: nan
env1_first_0:                 episode reward: 12.3500,                 loss: nan
env1_second_0:                 episode reward: -12.3500,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 3048.75,                last time consumption/overall running time: 1083.8521s / 235573.4996 s
env0_first_0:                 episode reward: 17.0500,                 loss: 0.0028
env0_second_0:                 episode reward: -17.0500,                 loss: nan
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 2988.75,                last time consumption/overall running time: 1061.5317s / 236635.0313 s
env0_first_0:                 episode reward: 11.1000,                 loss: 0.0027
env0_second_0:                 episode reward: -11.1000,                 loss: nan
env1_first_0:                 episode reward: 10.1500,                 loss: nan
env1_second_0:                 episode reward: -10.1500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 2889.35,                last time consumption/overall running time: 1027.4731s / 237662.5044 s
env0_first_0:                 episode reward: 14.1500,                 loss: 0.0035
env0_second_0:                 episode reward: -14.1500,                 loss: nan
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 3224.65,                last time consumption/overall running time: 1145.9661s / 238808.4706 s
env0_first_0:                 episode reward: 3.9500,                 loss: 0.0038
env0_second_0:                 episode reward: -3.9500,                 loss: nan
env1_first_0:                 episode reward: 5.5500,                 loss: nan
env1_second_0:                 episode reward: -5.5500,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 3069.9,                last time consumption/overall running time: 1091.6558s / 239900.1264 s
env0_first_0:                 episode reward: 8.8000,                 loss: 0.0029
env0_second_0:                 episode reward: -8.8000,                 loss: nan
env1_first_0:                 episode reward: 13.4500,                 loss: nan
env1_second_0:                 episode reward: -13.4500,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 2728.35,                last time consumption/overall running time: 970.7339s / 240870.8603 s
env0_first_0:                 episode reward: 15.4500,                 loss: 0.0031
env0_second_0:                 episode reward: -15.4500,                 loss: nan
env1_first_0:                 episode reward: 16.3500,                 loss: nan
env1_second_0:                 episode reward: -16.3500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 2993.1,                last time consumption/overall running time: 1064.8739s / 241935.7342 s
env0_first_0:                 episode reward: 16.0500,                 loss: 0.0024
env0_second_0:                 episode reward: -16.0500,                 loss: nan
env1_first_0:                 episode reward: 15.2500,                 loss: nan
env1_second_0:                 episode reward: -15.2500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 2859.5,                last time consumption/overall running time: 1016.6392s / 242952.3734 s
env0_first_0:                 episode reward: 17.3000,                 loss: 0.0023
env0_second_0:                 episode reward: -17.3000,                 loss: nan
env1_first_0:                 episode reward: 16.0000,                 loss: nan
env1_second_0:                 episode reward: -16.0000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 2747.85,                last time consumption/overall running time: 976.6177s / 243928.9911 s
env0_first_0:                 episode reward: 13.3500,                 loss: 0.0025
env0_second_0:                 episode reward: -13.3500,                 loss: nan
env1_first_0:                 episode reward: 14.7500,                 loss: nan
env1_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 3146.15,                last time consumption/overall running time: 1118.3512s / 245047.3423 s
env0_first_0:                 episode reward: 11.8000,                 loss: 0.0023
env0_second_0:                 episode reward: -11.8000,                 loss: nan
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 3197.2,                last time consumption/overall running time: 1137.1952s / 246184.5375 s
env0_first_0:                 episode reward: 14.3500,                 loss: 0.0030
env0_second_0:                 episode reward: -14.3500,                 loss: nan
env1_first_0:                 episode reward: 14.7500,                 loss: nan
env1_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 3649.25,                last time consumption/overall running time: 1297.9039s / 247482.4414 s
env0_first_0:                 episode reward: 2.5500,                 loss: 0.0034
env0_second_0:                 episode reward: -2.5500,                 loss: nan
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 2884.35,                last time consumption/overall running time: 1026.1385s / 248508.5799 s
env0_first_0:                 episode reward: 16.8500,                 loss: 0.0029
env0_second_0:                 episode reward: -16.8500,                 loss: nan
env1_first_0:                 episode reward: 17.2000,                 loss: nan
env1_second_0:                 episode reward: -17.2000,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 2982.1,                last time consumption/overall running time: 1060.6596s / 249569.2395 s
env0_first_0:                 episode reward: 15.5500,                 loss: 0.0024
env0_second_0:                 episode reward: -15.5500,                 loss: nan
env1_first_0:                 episode reward: 16.8500,                 loss: nan
env1_second_0:                 episode reward: -16.8500,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 2803.85,                last time consumption/overall running time: 997.9195s / 250567.1590 s
env0_first_0:                 episode reward: 17.6500,                 loss: 0.0022
env0_second_0:                 episode reward: -17.6500,                 loss: nan
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 2575.95,                last time consumption/overall running time: 916.4717s / 251483.6307 s
env0_first_0:                 episode reward: 18.2500,                 loss: 0.0022
env0_second_0:                 episode reward: -18.2500,                 loss: nan
env1_first_0:                 episode reward: 17.8000,                 loss: nan
env1_second_0:                 episode reward: -17.8000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 2779.7,                last time consumption/overall running time: 988.0913s / 252471.7220 s
env0_first_0:                 episode reward: 16.1000,                 loss: 0.0022
env0_second_0:                 episode reward: -16.1000,                 loss: nan
env1_first_0:                 episode reward: 17.9500,                 loss: nan
env1_second_0:                 episode reward: -17.9500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 2836.2,                last time consumption/overall running time: 1006.6965s / 253478.4185 s
env0_first_0:                 episode reward: 15.1000,                 loss: 0.0022
env0_second_0:                 episode reward: -15.1000,                 loss: nan
env1_first_0:                 episode reward: 18.0000,                 loss: nan
env1_second_0:                 episode reward: -18.0000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 2884.25,                last time consumption/overall running time: 1024.6529s / 254503.0714 s
env0_first_0:                 episode reward: 17.4000,                 loss: 0.0020
env0_second_0:                 episode reward: -17.4000,                 loss: nan
env1_first_0:                 episode reward: 17.7500,                 loss: nan
env1_second_0:                 episode reward: -17.7500,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 2836.5,                last time consumption/overall running time: 1006.5825s / 255509.6539 s
env0_first_0:                 episode reward: 14.2500,                 loss: 0.0022
env0_second_0:                 episode reward: -14.2500,                 loss: nan
env1_first_0:                 episode reward: 17.3500,                 loss: nan
env1_second_0:                 episode reward: -17.3500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 2501.25,                last time consumption/overall running time: 887.6971s / 256397.3510 s
env0_first_0:                 episode reward: 17.2000,                 loss: 0.0022
env0_second_0:                 episode reward: -17.2000,                 loss: nan
env1_first_0:                 episode reward: 17.7500,                 loss: nan
env1_second_0:                 episode reward: -17.7500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 2850.8,                last time consumption/overall running time: 1011.9081s / 257409.2592 s
env0_first_0:                 episode reward: 16.4500,                 loss: 0.0021
env0_second_0:                 episode reward: -16.4500,                 loss: nan
env1_first_0:                 episode reward: 17.2000,                 loss: nan
env1_second_0:                 episode reward: -17.2000,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 3104.85,                last time consumption/overall running time: 1101.4495s / 258510.7086 s
env0_first_0:                 episode reward: 17.0500,                 loss: 0.0021
env0_second_0:                 episode reward: -17.0500,                 loss: nan
env1_first_0:                 episode reward: 16.5000,                 loss: nan
env1_second_0:                 episode reward: -16.5000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 2596.2,                last time consumption/overall running time: 920.5735s / 259431.2822 s
env0_first_0:                 episode reward: 13.9000,                 loss: 0.0025
env0_second_0:                 episode reward: -13.9000,                 loss: nan
env1_first_0:                 episode reward: 16.5000,                 loss: nan
env1_second_0:                 episode reward: -16.5000,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 2621.9,                last time consumption/overall running time: 929.0910s / 260360.3732 s
env0_first_0:                 episode reward: 17.5000,                 loss: 0.0021
env0_second_0:                 episode reward: -17.5000,                 loss: nan
env1_first_0:                 episode reward: 17.6000,                 loss: nan
env1_second_0:                 episode reward: -17.6000,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 2835.75,                last time consumption/overall running time: 1005.1769s / 261365.5501 s
env0_first_0:                 episode reward: 17.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -17.7500,                 loss: nan
env1_first_0:                 episode reward: 16.1000,                 loss: nan
env1_second_0:                 episode reward: -16.1000,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 2935.35,                last time consumption/overall running time: 1039.4679s / 262405.0180 s
env0_first_0:                 episode reward: 17.5000,                 loss: 0.0023
env0_second_0:                 episode reward: -17.5000,                 loss: nan
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 2885.55,                last time consumption/overall running time: 1021.0474s / 263426.0654 s
env0_first_0:                 episode reward: 16.8500,                 loss: 0.0022
env0_second_0:                 episode reward: -16.8500,                 loss: nan
env1_first_0:                 episode reward: 18.2000,                 loss: nan
env1_second_0:                 episode reward: -18.2000,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 2797.15,                last time consumption/overall running time: 990.4668s / 264416.5322 s
env0_first_0:                 episode reward: 17.7000,                 loss: 0.0020
env0_second_0:                 episode reward: -17.7000,                 loss: nan
env1_first_0:                 episode reward: 17.6000,                 loss: nan
env1_second_0:                 episode reward: -17.6000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 2662.45,                last time consumption/overall running time: 944.2950s / 265360.8272 s
env0_first_0:                 episode reward: 14.3500,                 loss: 0.0023
env0_second_0:                 episode reward: -14.3500,                 loss: nan
env1_first_0:                 episode reward: 18.4500,                 loss: nan
env1_second_0:                 episode reward: -18.4500,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 2724.25,                last time consumption/overall running time: 965.7099s / 266326.5371 s
env0_first_0:                 episode reward: 19.1500,                 loss: 0.0022
env0_second_0:                 episode reward: -19.1500,                 loss: nan
env1_first_0:                 episode reward: 18.1000,                 loss: nan
env1_second_0:                 episode reward: -18.1000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 2930.8,                last time consumption/overall running time: 1037.4108s / 267363.9479 s
env0_first_0:                 episode reward: 11.4500,                 loss: 0.0023
env0_second_0:                 episode reward: -11.4500,                 loss: nan
env1_first_0:                 episode reward: 13.2000,                 loss: nan
env1_second_0:                 episode reward: -13.2000,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 3001.95,                last time consumption/overall running time: 1061.8124s / 268425.7603 s
env0_first_0:                 episode reward: 13.1000,                 loss: 0.0029
env0_second_0:                 episode reward: -13.1000,                 loss: nan
env1_first_0:                 episode reward: 16.7000,                 loss: nan
env1_second_0:                 episode reward: -16.7000,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 3368.5,                last time consumption/overall running time: 1192.7762s / 269618.5365 s
env0_first_0:                 episode reward: 6.7500,                 loss: 0.0038
env0_second_0:                 episode reward: -6.7500,                 loss: nan
env1_first_0:                 episode reward: 5.0500,                 loss: nan
env1_second_0:                 episode reward: -5.0500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 2628.0,                last time consumption/overall running time: 930.9090s / 270549.4454 s
env0_first_0:                 episode reward: 13.4500,                 loss: 0.0022
env0_second_0:                 episode reward: -13.4500,                 loss: nan
env1_first_0:                 episode reward: 16.8000,                 loss: nan
env1_second_0:                 episode reward: -16.8000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 2486.3,                last time consumption/overall running time: 880.4071s / 271429.8526 s
env0_first_0:                 episode reward: 18.2500,                 loss: 0.0020
env0_second_0:                 episode reward: -18.2500,                 loss: nan
env1_first_0:                 episode reward: 19.3000,                 loss: nan
env1_second_0:                 episode reward: -19.3000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 2581.35,                last time consumption/overall running time: 914.3010s / 272344.1536 s
env0_first_0:                 episode reward: 16.4000,                 loss: 0.0019
env0_second_0:                 episode reward: -16.4000,                 loss: nan
env1_first_0:                 episode reward: 18.3000,                 loss: nan
env1_second_0:                 episode reward: -18.3000,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 2819.5,                last time consumption/overall running time: 998.8366s / 273342.9902 s
env0_first_0:                 episode reward: 16.7000,                 loss: 0.0022
env0_second_0:                 episode reward: -16.7000,                 loss: nan
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 2863.8,                last time consumption/overall running time: 1014.9776s / 274357.9678 s
env0_first_0:                 episode reward: 15.1000,                 loss: 0.0025
env0_second_0:                 episode reward: -15.1000,                 loss: nan
env1_first_0:                 episode reward: 16.2500,                 loss: nan
env1_second_0:                 episode reward: -16.2500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 2704.6,                last time consumption/overall running time: 958.1625s / 275316.1302 s
env0_first_0:                 episode reward: 15.1000,                 loss: 0.0024
env0_second_0:                 episode reward: -15.1000,                 loss: nan
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 2597.05,                last time consumption/overall running time: 920.1470s / 276236.2772 s
env0_first_0:                 episode reward: 17.5000,                 loss: 0.0023
env0_second_0:                 episode reward: -17.5000,                 loss: nan
env1_first_0:                 episode reward: 17.4000,                 loss: nan
env1_second_0:                 episode reward: -17.4000,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 3689.8,                last time consumption/overall running time: 1306.4490s / 277542.7263 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0042
env0_second_0:                 episode reward: 4.0000,                 loss: nan
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 2830.65,                last time consumption/overall running time: 1002.2314s / 278544.9577 s
env0_first_0:                 episode reward: 17.5000,                 loss: 0.0024
env0_second_0:                 episode reward: -17.5000,                 loss: nan
env1_first_0:                 episode reward: 16.2000,                 loss: nan
env1_second_0:                 episode reward: -16.2000,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 2861.6,                last time consumption/overall running time: 1012.3345s / 279557.2922 s
env0_first_0:                 episode reward: 17.1000,                 loss: 0.0023
env0_second_0:                 episode reward: -17.1000,                 loss: nan
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 2738.05,                last time consumption/overall running time: 968.2047s / 280525.4969 s
env0_first_0:                 episode reward: 14.6500,                 loss: 0.0029
env0_second_0:                 episode reward: -14.6500,                 loss: nan
env1_first_0:                 episode reward: 14.7000,                 loss: nan
env1_second_0:                 episode reward: -14.7000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 2875.25,                last time consumption/overall running time: 1016.1372s / 281541.6340 s
env0_first_0:                 episode reward: 14.0500,                 loss: 0.0025
env0_second_0:                 episode reward: -14.0500,                 loss: nan
env1_first_0:                 episode reward: 15.2000,                 loss: nan
env1_second_0:                 episode reward: -15.2000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 3022.5,                last time consumption/overall running time: 1068.9149s / 282610.5490 s
env0_first_0:                 episode reward: 14.2500,                 loss: 0.0026
env0_second_0:                 episode reward: -14.2500,                 loss: nan
env1_first_0:                 episode reward: 14.2000,                 loss: nan
env1_second_0:                 episode reward: -14.2000,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 2798.4,                last time consumption/overall running time: 989.8853s / 283600.4343 s
env0_first_0:                 episode reward: 16.1500,                 loss: 0.0023
env0_second_0:                 episode reward: -16.1500,                 loss: nan
env1_first_0:                 episode reward: 17.7000,                 loss: nan
env1_second_0:                 episode reward: -17.7000,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 2809.6,                last time consumption/overall running time: 993.8460s / 284594.2803 s
env0_first_0:                 episode reward: 18.0000,                 loss: 0.0023
env0_second_0:                 episode reward: -18.0000,                 loss: nan
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 2939.9,                last time consumption/overall running time: 1039.7928s / 285634.0732 s
env0_first_0:                 episode reward: 16.2000,                 loss: 0.0022
env0_second_0:                 episode reward: -16.2000,                 loss: nan
env1_first_0:                 episode reward: 11.0500,                 loss: nan
env1_second_0:                 episode reward: -11.0500,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 3344.15,                last time consumption/overall running time: 1182.5824s / 286816.6556 s
env0_first_0:                 episode reward: -4.4500,                 loss: 0.0044
env0_second_0:                 episode reward: 4.4500,                 loss: nan
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 2796.85,                last time consumption/overall running time: 988.6311s / 287805.2867 s
env0_first_0:                 episode reward: 15.5500,                 loss: 0.0024
env0_second_0:                 episode reward: -15.5500,                 loss: nan
env1_first_0:                 episode reward: 16.8500,                 loss: nan
env1_second_0:                 episode reward: -16.8500,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 2801.15,                last time consumption/overall running time: 990.1732s / 288795.4599 s
env0_first_0:                 episode reward: 10.9500,                 loss: 0.0025
env0_second_0:                 episode reward: -10.9500,                 loss: nan
env1_first_0:                 episode reward: 12.7500,                 loss: nan
env1_second_0:                 episode reward: -12.7500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 2761.6,                last time consumption/overall running time: 976.5550s / 289772.0149 s
env0_first_0:                 episode reward: 16.6000,                 loss: 0.0027
env0_second_0:                 episode reward: -16.6000,                 loss: nan
env1_first_0:                 episode reward: 17.2500,                 loss: nan
env1_second_0:                 episode reward: -17.2500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 2842.35,                last time consumption/overall running time: 1003.3081s / 290775.3229 s
env0_first_0:                 episode reward: 15.1000,                 loss: 0.0022
env0_second_0:                 episode reward: -15.1000,                 loss: nan
env1_first_0:                 episode reward: 11.8500,                 loss: nan
env1_second_0:                 episode reward: -11.8500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 2715.15,                last time consumption/overall running time: 957.2517s / 291732.5747 s
env0_first_0:                 episode reward: 14.5500,                 loss: 0.0029
env0_second_0:                 episode reward: -14.5500,                 loss: nan
env1_first_0:                 episode reward: 16.8000,                 loss: nan
env1_second_0:                 episode reward: -16.8000,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 2840.15,                last time consumption/overall running time: 1001.3915s / 292733.9662 s
env0_first_0:                 episode reward: 14.5500,                 loss: 0.0022
env0_second_0:                 episode reward: -14.5500,                 loss: nan
env1_first_0:                 episode reward: 13.1000,                 loss: nan
env1_second_0:                 episode reward: -13.1000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 2607.2,                last time consumption/overall running time: 917.9998s / 293651.9660 s
env0_first_0:                 episode reward: 18.5000,                 loss: 0.0024
env0_second_0:                 episode reward: -18.5000,                 loss: nan
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 2453.3,                last time consumption/overall running time: 863.6095s / 294515.5754 s
env0_first_0:                 episode reward: 17.7000,                 loss: 0.0019
env0_second_0:                 episode reward: -17.7000,                 loss: nan
env1_first_0:                 episode reward: 19.0500,                 loss: nan
env1_second_0:                 episode reward: -19.0500,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 2704.0,                last time consumption/overall running time: 952.3718s / 295467.9472 s
env0_first_0:                 episode reward: 17.8500,                 loss: 0.0020
env0_second_0:                 episode reward: -17.8500,                 loss: nan
env1_first_0:                 episode reward: 18.4500,                 loss: nan
env1_second_0:                 episode reward: -18.4500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 3305.5,                last time consumption/overall running time: 1163.0926s / 296631.0398 s
env0_first_0:                 episode reward: 4.2000,                 loss: 0.0041
env0_second_0:                 episode reward: -4.2000,                 loss: nan
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 2584.45,                last time consumption/overall running time: 910.3150s / 297541.3548 s
env0_first_0:                 episode reward: 16.9000,                 loss: 0.0020
env0_second_0:                 episode reward: -16.9000,                 loss: nan
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 2666.5,                last time consumption/overall running time: 938.0503s / 298479.4050 s
env0_first_0:                 episode reward: 13.9500,                 loss: 0.0022
env0_second_0:                 episode reward: -13.9500,                 loss: nan
env1_first_0:                 episode reward: 16.1000,                 loss: nan
env1_second_0:                 episode reward: -16.1000,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 2578.6,                last time consumption/overall running time: 907.0148s / 299386.4198 s
env0_first_0:                 episode reward: 15.4500,                 loss: 0.0023
env0_second_0:                 episode reward: -15.4500,                 loss: nan
env1_first_0:                 episode reward: 20.0000,                 loss: nan
env1_second_0:                 episode reward: -20.0000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 2917.7,                last time consumption/overall running time: 1025.5656s / 300411.9853 s
env0_first_0:                 episode reward: 16.9500,                 loss: 0.0021
env0_second_0:                 episode reward: -16.9500,                 loss: nan
env1_first_0:                 episode reward: 16.0000,                 loss: nan
env1_second_0:                 episode reward: -16.0000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 2737.7,                last time consumption/overall running time: 963.1458s / 301375.1312 s
env0_first_0:                 episode reward: 16.3500,                 loss: 0.0021
env0_second_0:                 episode reward: -16.3500,                 loss: nan
env1_first_0:                 episode reward: 17.6500,                 loss: nan
env1_second_0:                 episode reward: -17.6500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 2653.35,                last time consumption/overall running time: 932.7106s / 302307.8418 s
env0_first_0:                 episode reward: 17.0500,                 loss: 0.0022
env0_second_0:                 episode reward: -17.0500,                 loss: nan
env1_first_0:                 episode reward: 15.5000,                 loss: nan
env1_second_0:                 episode reward: -15.5000,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 2751.15,                last time consumption/overall running time: 967.2905s / 303275.1323 s
env0_first_0:                 episode reward: 16.3000,                 loss: 0.0020
env0_second_0:                 episode reward: -16.3000,                 loss: nan
env1_first_0:                 episode reward: 18.2000,                 loss: nan
env1_second_0:                 episode reward: -18.2000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 2694.1,                last time consumption/overall running time: 946.4403s / 304221.5726 s
env0_first_0:                 episode reward: 16.9000,                 loss: 0.0020
env0_second_0:                 episode reward: -16.9000,                 loss: nan
env1_first_0:                 episode reward: 18.6500,                 loss: nan
env1_second_0:                 episode reward: -18.6500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 2509.65,                last time consumption/overall running time: 880.7305s / 305102.3031 s
env0_first_0:                 episode reward: 17.3000,                 loss: 0.0020
env0_second_0:                 episode reward: -17.3000,                 loss: nan
env1_first_0:                 episode reward: 18.9500,                 loss: nan
env1_second_0:                 episode reward: -18.9500,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 2618.0,                last time consumption/overall running time: 919.6419s / 306021.9450 s
env0_first_0:                 episode reward: 18.3000,                 loss: 0.0019
env0_second_0:                 episode reward: -18.3000,                 loss: nan
env1_first_0:                 episode reward: 18.0000,                 loss: nan
env1_second_0:                 episode reward: -18.0000,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 2583.15,                last time consumption/overall running time: 906.3970s / 306928.3420 s
env0_first_0:                 episode reward: 18.0000,                 loss: 0.0018
env0_second_0:                 episode reward: -18.0000,                 loss: nan
env1_first_0:                 episode reward: 19.8500,                 loss: nan
env1_second_0:                 episode reward: -19.8500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 2793.0,                last time consumption/overall running time: 980.3320s / 307908.6740 s
env0_first_0:                 episode reward: 15.0000,                 loss: 0.0019
env0_second_0:                 episode reward: -15.0000,                 loss: nan
env1_first_0:                 episode reward: 18.8000,                 loss: nan
env1_second_0:                 episode reward: -18.8000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 2569.9,                last time consumption/overall running time: 899.0632s / 308807.7372 s
env0_first_0:                 episode reward: 19.8000,                 loss: 0.0020
env0_second_0:                 episode reward: -19.8000,                 loss: nan
env1_first_0:                 episode reward: 19.0000,                 loss: nan
env1_second_0:                 episode reward: -19.0000,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 2903.3,                last time consumption/overall running time: 1016.1340s / 309823.8712 s
env0_first_0:                 episode reward: 16.4000,                 loss: 0.0020
env0_second_0:                 episode reward: -16.4000,                 loss: nan
env1_first_0:                 episode reward: 18.1000,                 loss: nan
env1_second_0:                 episode reward: -18.1000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 3126.95,                last time consumption/overall running time: 1093.5578s / 310917.4290 s
env0_first_0:                 episode reward: 9.5500,                 loss: 0.0022
env0_second_0:                 episode reward: -9.5500,                 loss: nan
env1_first_0:                 episode reward: 13.0500,                 loss: nan
env1_second_0:                 episode reward: -13.0500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 3080.7,                last time consumption/overall running time: 1077.2689s / 311994.6979 s
env0_first_0:                 episode reward: 15.2000,                 loss: 0.0026
env0_second_0:                 episode reward: -15.2000,                 loss: nan
env1_first_0:                 episode reward: 15.6500,                 loss: nan
env1_second_0:                 episode reward: -15.6500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 2706.75,                last time consumption/overall running time: 947.3324s / 312942.0303 s
env0_first_0:                 episode reward: 18.7500,                 loss: 0.0020
env0_second_0:                 episode reward: -18.7500,                 loss: nan
env1_first_0:                 episode reward: 18.2500,                 loss: nan
env1_second_0:                 episode reward: -18.2500,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 2744.5,                last time consumption/overall running time: 960.6324s / 313902.6627 s
env0_first_0:                 episode reward: 18.5500,                 loss: 0.0019
env0_second_0:                 episode reward: -18.5500,                 loss: nan
env1_first_0:                 episode reward: 17.4000,                 loss: nan
env1_second_0:                 episode reward: -17.4000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 2540.25,                last time consumption/overall running time: 888.8217s / 314791.4844 s
env0_first_0:                 episode reward: 19.5000,                 loss: 0.0020
env0_second_0:                 episode reward: -19.5000,                 loss: nan
env1_first_0:                 episode reward: 17.3500,                 loss: nan
env1_second_0:                 episode reward: -17.3500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 2611.05,                last time consumption/overall running time: 913.2840s / 315704.7684 s
env0_first_0:                 episode reward: 13.2000,                 loss: 0.0022
env0_second_0:                 episode reward: -13.2000,                 loss: nan
env1_first_0:                 episode reward: 16.0000,                 loss: nan
env1_second_0:                 episode reward: -16.0000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 2975.1,                last time consumption/overall running time: 1039.1223s / 316743.8907 s
env0_first_0:                 episode reward: 10.0000,                 loss: 0.0029
env0_second_0:                 episode reward: -10.0000,                 loss: nan
env1_first_0:                 episode reward: 10.3000,                 loss: nan
env1_second_0:                 episode reward: -10.3000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 2865.95,                last time consumption/overall running time: 1000.7913s / 317744.6821 s
env0_first_0:                 episode reward: 10.9500,                 loss: 0.0029
env0_second_0:                 episode reward: -10.9500,                 loss: nan
env1_first_0:                 episode reward: 17.3500,                 loss: nan
env1_second_0:                 episode reward: -17.3500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 3054.65,                last time consumption/overall running time: 1065.1520s / 318809.8340 s
env0_first_0:                 episode reward: 18.0000,                 loss: 0.0024
env0_second_0:                 episode reward: -18.0000,                 loss: nan
env1_first_0:                 episode reward: 14.5500,                 loss: nan
env1_second_0:                 episode reward: -14.5500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 2548.85,                last time consumption/overall running time: 888.8683s / 319698.7023 s
env0_first_0:                 episode reward: 15.0000,                 loss: 0.0021
env0_second_0:                 episode reward: -15.0000,                 loss: nan
env1_first_0:                 episode reward: 15.8500,                 loss: nan
env1_second_0:                 episode reward: -15.8500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 2722.4,                last time consumption/overall running time: 949.9591s / 320648.6614 s
env0_first_0:                 episode reward: 15.3000,                 loss: 0.0023
env0_second_0:                 episode reward: -15.3000,                 loss: nan
env1_first_0:                 episode reward: 17.2500,                 loss: nan
env1_second_0:                 episode reward: -17.2500,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 2932.3,                last time consumption/overall running time: 1024.3889s / 321673.0503 s
env0_first_0:                 episode reward: 17.9500,                 loss: 0.0023
env0_second_0:                 episode reward: -17.9500,                 loss: nan
env1_first_0:                 episode reward: 11.1500,                 loss: nan
env1_second_0:                 episode reward: -11.1500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 2595.1,                last time consumption/overall running time: 904.7469s / 322577.7972 s
env0_first_0:                 episode reward: 16.4500,                 loss: 0.0021
env0_second_0:                 episode reward: -16.4500,                 loss: nan
env1_first_0:                 episode reward: 19.1500,                 loss: nan
env1_second_0:                 episode reward: -19.1500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 2720.95,                last time consumption/overall running time: 947.3708s / 323525.1680 s
env0_first_0:                 episode reward: 18.7000,                 loss: 0.0019
env0_second_0:                 episode reward: -18.7000,                 loss: nan
env1_first_0:                 episode reward: 17.3000,                 loss: nan
env1_second_0:                 episode reward: -17.3000,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 2532.25,                last time consumption/overall running time: 881.3019s / 324406.4699 s
env0_first_0:                 episode reward: 16.7000,                 loss: 0.0019
env0_second_0:                 episode reward: -16.7000,                 loss: nan
env1_first_0:                 episode reward: 18.4500,                 loss: nan
env1_second_0:                 episode reward: -18.4500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 2633.1,                last time consumption/overall running time: 917.0182s / 325323.4881 s
env0_first_0:                 episode reward: 18.0500,                 loss: 0.0021
env0_second_0:                 episode reward: -18.0500,                 loss: nan
env1_first_0:                 episode reward: 17.8500,                 loss: nan
env1_second_0:                 episode reward: -17.8500,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 2953.8,                last time consumption/overall running time: 1028.5967s / 326352.0848 s
env0_first_0:                 episode reward: 10.6500,                 loss: 0.0024
env0_second_0:                 episode reward: -10.6500,                 loss: nan
env1_first_0:                 episode reward: 16.2000,                 loss: nan
env1_second_0:                 episode reward: -16.2000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 2695.4,                last time consumption/overall running time: 938.5060s / 327290.5908 s
env0_first_0:                 episode reward: 14.9000,                 loss: 0.0022
env0_second_0:                 episode reward: -14.9000,                 loss: nan
env1_first_0:                 episode reward: 16.8000,                 loss: nan
env1_second_0:                 episode reward: -16.8000,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 2604.5,                last time consumption/overall running time: 906.9898s / 328197.5807 s
env0_first_0:                 episode reward: 19.4000,                 loss: 0.0024
env0_second_0:                 episode reward: -19.4000,                 loss: nan
env1_first_0:                 episode reward: 17.4500,                 loss: nan
env1_second_0:                 episode reward: -17.4500,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 2723.45,                last time consumption/overall running time: 948.3235s / 329145.9042 s
env0_first_0:                 episode reward: 15.9000,                 loss: 0.0018
env0_second_0:                 episode reward: -15.9000,                 loss: nan
env1_first_0:                 episode reward: 17.7500,                 loss: nan
env1_second_0:                 episode reward: -17.7500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 3186.4,                last time consumption/overall running time: 1110.5154s / 330256.4196 s
env0_first_0:                 episode reward: 5.6000,                 loss: 0.0030
env0_second_0:                 episode reward: -5.6000,                 loss: nan
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 3075.4,                last time consumption/overall running time: 1071.2254s / 331327.6450 s
env0_first_0:                 episode reward: 19.2500,                 loss: 0.0031
env0_second_0:                 episode reward: -19.2500,                 loss: nan
env1_first_0:                 episode reward: 13.2500,                 loss: nan
env1_second_0:                 episode reward: -13.2500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 2921.25,                last time consumption/overall running time: 1017.3077s / 332344.9527 s
env0_first_0:                 episode reward: 17.8500,                 loss: 0.0023
env0_second_0:                 episode reward: -17.8500,                 loss: nan
env1_first_0:                 episode reward: 16.4000,                 loss: nan
env1_second_0:                 episode reward: -16.4000,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 3033.1,                last time consumption/overall running time: 1020.7539s / 333365.7066 s
env0_first_0:                 episode reward: 16.9000,                 loss: 0.0020
env0_second_0:                 episode reward: -16.9000,                 loss: nan
env1_first_0:                 episode reward: 16.6500,                 loss: nan
env1_second_0:                 episode reward: -16.6500,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 2676.4,                last time consumption/overall running time: 864.8623s / 334230.5689 s
env0_first_0:                 episode reward: 19.0500,                 loss: 0.0022
env0_second_0:                 episode reward: -19.0500,                 loss: nan
env1_first_0:                 episode reward: 17.2000,                 loss: nan
env1_second_0:                 episode reward: -17.2000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 2277.65,                last time consumption/overall running time: 735.7016s / 334966.2705 s
env0_first_0:                 episode reward: 16.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -16.7500,                 loss: nan
env1_first_0:                 episode reward: 19.3500,                 loss: nan
env1_second_0:                 episode reward: -19.3500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 2601.95,                last time consumption/overall running time: 840.5391s / 335806.8096 s
env0_first_0:                 episode reward: 19.0000,                 loss: 0.0019
env0_second_0:                 episode reward: -19.0000,                 loss: nan
env1_first_0:                 episode reward: 19.4500,                 loss: nan
env1_second_0:                 episode reward: -19.4500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 2980.85,                last time consumption/overall running time: 962.2288s / 336769.0384 s
env0_first_0:                 episode reward: 17.9000,                 loss: 0.0019
env0_second_0:                 episode reward: -17.9000,                 loss: nan
env1_first_0:                 episode reward: 14.6500,                 loss: nan
env1_second_0:                 episode reward: -14.6500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 2679.2,                last time consumption/overall running time: 865.6244s / 337634.6628 s
env0_first_0:                 episode reward: 15.6500,                 loss: 0.0023
env0_second_0:                 episode reward: -15.6500,                 loss: nan
env1_first_0:                 episode reward: 11.3500,                 loss: nan
env1_second_0:                 episode reward: -11.3500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 2763.95,                last time consumption/overall running time: 893.4950s / 338528.1578 s
env0_first_0:                 episode reward: 15.7500,                 loss: 0.0025
env0_second_0:                 episode reward: -15.7500,                 loss: nan
env1_first_0:                 episode reward: 18.4000,                 loss: nan
env1_second_0:                 episode reward: -18.4000,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 2944.7,                last time consumption/overall running time: 950.0615s / 339478.2193 s
env0_first_0:                 episode reward: 14.5500,                 loss: 0.0022
env0_second_0:                 episode reward: -14.5500,                 loss: nan
env1_first_0:                 episode reward: 17.5500,                 loss: nan
env1_second_0:                 episode reward: -17.5500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 2525.05,                last time consumption/overall running time: 758.7537s / 340236.9730 s
env0_first_0:                 episode reward: 17.6500,                 loss: 0.0020
env0_second_0:                 episode reward: -17.6500,                 loss: nan
env1_first_0:                 episode reward: 18.6000,                 loss: nan
env1_second_0:                 episode reward: -18.6000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 2923.75,                last time consumption/overall running time: 874.6712s / 341111.6442 s
env0_first_0:                 episode reward: 14.1000,                 loss: 0.0019
env0_second_0:                 episode reward: -14.1000,                 loss: nan
env1_first_0:                 episode reward: 18.9000,                 loss: nan
env1_second_0:                 episode reward: -18.9000,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 3121.45,                last time consumption/overall running time: 932.4335s / 342044.0777 s
env0_first_0:                 episode reward: 3.2500,                 loss: 0.0026
env0_second_0:                 episode reward: -3.2500,                 loss: nan
env1_first_0:                 episode reward: 6.1000,                 loss: nan
env1_second_0:                 episode reward: -6.1000,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 2937.2,                last time consumption/overall running time: 878.3141s / 342922.3918 s
env0_first_0:                 episode reward: 13.2000,                 loss: 0.0036
env0_second_0:                 episode reward: -13.2000,                 loss: nan
env1_first_0:                 episode reward: 14.3500,                 loss: nan
env1_second_0:                 episode reward: -14.3500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 2837.0,                last time consumption/overall running time: 847.2777s / 343769.6695 s
env0_first_0:                 episode reward: 13.7500,                 loss: 0.0024
env0_second_0:                 episode reward: -13.7500,                 loss: nan
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 3288.5,                last time consumption/overall running time: 980.9294s / 344750.5989 s
env0_first_0:                 episode reward: 6.3500,                 loss: 0.0028
env0_second_0:                 episode reward: -6.3500,                 loss: nan
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 3152.8,                last time consumption/overall running time: 941.1862s / 345691.7851 s
env0_first_0:                 episode reward: 17.6500,                 loss: 0.0031
env0_second_0:                 episode reward: -17.6500,                 loss: nan
env1_first_0:                 episode reward: 14.4500,                 loss: nan
env1_second_0:                 episode reward: -14.4500,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 3301.85,                last time consumption/overall running time: 986.5284s / 346678.3135 s
env0_first_0:                 episode reward: 12.0500,                 loss: 0.0028
env0_second_0:                 episode reward: -12.0500,                 loss: nan
env1_first_0:                 episode reward: 11.7000,                 loss: nan
env1_second_0:                 episode reward: -11.7000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 2847.55,                last time consumption/overall running time: 852.0452s / 347530.3587 s
env0_first_0:                 episode reward: 16.1000,                 loss: 0.0022
env0_second_0:                 episode reward: -16.1000,                 loss: nan
env1_first_0:                 episode reward: 18.0000,                 loss: nan
env1_second_0:                 episode reward: -18.0000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 2559.4,                last time consumption/overall running time: 765.7448s / 348296.1035 s
env0_first_0:                 episode reward: 13.3500,                 loss: 0.0022
env0_second_0:                 episode reward: -13.3500,                 loss: nan
env1_first_0:                 episode reward: 17.6000,                 loss: nan
env1_second_0:                 episode reward: -17.6000,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 2878.55,                last time consumption/overall running time: 860.3088s / 349156.4122 s
env0_first_0:                 episode reward: 15.8000,                 loss: 0.0021
env0_second_0:                 episode reward: -15.8000,                 loss: nan
env1_first_0:                 episode reward: 16.3000,                 loss: nan
env1_second_0:                 episode reward: -16.3000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 3063.05,                last time consumption/overall running time: 916.1924s / 350072.6046 s
env0_first_0:                 episode reward: 16.9500,                 loss: 0.0025
env0_second_0:                 episode reward: -16.9500,                 loss: nan
env1_first_0:                 episode reward: 11.6000,                 loss: nan
env1_second_0:                 episode reward: -11.6000,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 3042.95,                last time consumption/overall running time: 908.6275s / 350981.2321 s
env0_first_0:                 episode reward: 16.9000,                 loss: 0.0024
env0_second_0:                 episode reward: -16.9000,                 loss: nan
env1_first_0:                 episode reward: 13.3000,                 loss: nan
env1_second_0:                 episode reward: -13.3000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 2622.95,                last time consumption/overall running time: 782.5799s / 351763.8121 s
env0_first_0:                 episode reward: 17.2500,                 loss: 0.0022
env0_second_0:                 episode reward: -17.2500,                 loss: nan
env1_first_0:                 episode reward: 18.5000,                 loss: nan
env1_second_0:                 episode reward: -18.5000,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 3262.45,                last time consumption/overall running time: 973.2385s / 352737.0506 s
env0_first_0:                 episode reward: 16.0500,                 loss: 0.0022
env0_second_0:                 episode reward: -16.0500,                 loss: nan
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 3155.0,                last time consumption/overall running time: 941.9983s / 353679.0488 s
env0_first_0:                 episode reward: 15.3500,                 loss: 0.0022
env0_second_0:                 episode reward: -15.3500,                 loss: nan
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 2908.35,                last time consumption/overall running time: 866.3415s / 354545.3903 s
env0_first_0:                 episode reward: 15.6500,                 loss: 0.0020
env0_second_0:                 episode reward: -15.6500,                 loss: nan
env1_first_0:                 episode reward: 18.7500,                 loss: nan
env1_second_0:                 episode reward: -18.7500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 3435.15,                last time consumption/overall running time: 1022.8483s / 355568.2385 s
env0_first_0:                 episode reward: 10.5500,                 loss: 0.0030
env0_second_0:                 episode reward: -10.5500,                 loss: nan
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 3078.85,                last time consumption/overall running time: 917.1034s / 356485.3419 s
env0_first_0:                 episode reward: 15.0000,                 loss: 0.0027
env0_second_0:                 episode reward: -15.0000,                 loss: nan
env1_first_0:                 episode reward: 14.1500,                 loss: nan
env1_second_0:                 episode reward: -14.1500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 2876.1,                last time consumption/overall running time: 802.7924s / 357288.1343 s
env0_first_0:                 episode reward: 17.8500,                 loss: 0.0025
env0_second_0:                 episode reward: -17.8500,                 loss: nan
env1_first_0:                 episode reward: 10.2000,                 loss: nan
env1_second_0:                 episode reward: -10.2000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 2991.7,                last time consumption/overall running time: 818.8166s / 358106.9509 s
env0_first_0:                 episode reward: 11.7000,                 loss: 0.0027
env0_second_0:                 episode reward: -11.7000,                 loss: nan
env1_first_0:                 episode reward: 13.0500,                 loss: nan
env1_second_0:                 episode reward: -13.0500,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 3033.95,                last time consumption/overall running time: 831.1380s / 358938.0890 s
env0_first_0:                 episode reward: 12.2000,                 loss: 0.0028
env0_second_0:                 episode reward: -12.2000,                 loss: nan
env1_first_0:                 episode reward: 12.7500,                 loss: nan
env1_second_0:                 episode reward: -12.7500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 3045.55,                last time consumption/overall running time: 832.4269s / 359770.5158 s
env0_first_0:                 episode reward: 8.9500,                 loss: 0.0025
env0_second_0:                 episode reward: -8.9500,                 loss: nan
env1_first_0:                 episode reward: 13.4000,                 loss: nan
env1_second_0:                 episode reward: -13.4000,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 4121.4,                last time consumption/overall running time: 1130.0182s / 360900.5340 s
env0_first_0:                 episode reward: -5.6500,                 loss: 0.0044
env0_second_0:                 episode reward: 5.6500,                 loss: nan
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 3170.6,                last time consumption/overall running time: 867.8979s / 361768.4320 s
env0_first_0:                 episode reward: 15.4000,                 loss: 0.0036
env0_second_0:                 episode reward: -15.4000,                 loss: nan
env1_first_0:                 episode reward: 11.2000,                 loss: nan
env1_second_0:                 episode reward: -11.2000,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 2682.55,                last time consumption/overall running time: 733.9370s / 362502.3690 s
env0_first_0:                 episode reward: 13.4000,                 loss: 0.0023
env0_second_0:                 episode reward: -13.4000,                 loss: nan
env1_first_0:                 episode reward: 18.3000,                 loss: nan
env1_second_0:                 episode reward: -18.3000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 3807.45,                last time consumption/overall running time: 1042.9621s / 363545.3311 s
env0_first_0:                 episode reward: 7.3500,                 loss: 0.0036
env0_second_0:                 episode reward: -7.3500,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 3189.6,                last time consumption/overall running time: 873.5896s / 364418.9207 s
env0_first_0:                 episode reward: 12.6500,                 loss: 0.0025
env0_second_0:                 episode reward: -12.6500,                 loss: nan
env1_first_0:                 episode reward: 15.0500,                 loss: nan
env1_second_0:                 episode reward: -15.0500,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 3421.1,                last time consumption/overall running time: 937.4051s / 365356.3257 s
env0_first_0:                 episode reward: 13.9500,                 loss: 0.0031
env0_second_0:                 episode reward: -13.9500,                 loss: nan
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 3144.15,                last time consumption/overall running time: 859.9816s / 366216.3073 s
env0_first_0:                 episode reward: 14.4000,                 loss: 0.0022
env0_second_0:                 episode reward: -14.4000,                 loss: nan
env1_first_0:                 episode reward: 14.7500,                 loss: nan
env1_second_0:                 episode reward: -14.7500,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 3339.1,                last time consumption/overall running time: 913.8575s / 367130.1648 s
env0_first_0:                 episode reward: 18.3500,                 loss: 0.0025
env0_second_0:                 episode reward: -18.3500,                 loss: nan
env1_first_0:                 episode reward: 12.5000,                 loss: nan
env1_second_0:                 episode reward: -12.5000,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 2997.05,                last time consumption/overall running time: 818.1255s / 367948.2903 s
env0_first_0:                 episode reward: 15.9000,                 loss: 0.0021
env0_second_0:                 episode reward: -15.9000,                 loss: nan
env1_first_0:                 episode reward: 16.7000,                 loss: nan
env1_second_0:                 episode reward: -16.7000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 3241.95,                last time consumption/overall running time: 888.7557s / 368837.0460 s
env0_first_0:                 episode reward: 12.7000,                 loss: 0.0024
env0_second_0:                 episode reward: -12.7000,                 loss: nan
env1_first_0:                 episode reward: 17.2500,                 loss: nan
env1_second_0:                 episode reward: -17.2500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 2716.65,                last time consumption/overall running time: 746.7223s / 369583.7683 s
env0_first_0:                 episode reward: 18.4000,                 loss: 0.0023
env0_second_0:                 episode reward: -18.4000,                 loss: nan
env1_first_0:                 episode reward: 16.6000,                 loss: nan
env1_second_0:                 episode reward: -16.6000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 2770.2,                last time consumption/overall running time: 757.2536s / 370341.0219 s
env0_first_0:                 episode reward: 15.2000,                 loss: 0.0021
env0_second_0:                 episode reward: -15.2000,                 loss: nan
env1_first_0:                 episode reward: 18.3000,                 loss: nan
env1_second_0:                 episode reward: -18.3000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 3308.4,                last time consumption/overall running time: 901.1417s / 371242.1636 s
env0_first_0:                 episode reward: 10.9500,                 loss: 0.0026
env0_second_0:                 episode reward: -10.9500,                 loss: nan
env1_first_0:                 episode reward: 5.1000,                 loss: nan
env1_second_0:                 episode reward: -5.1000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 3387.35,                last time consumption/overall running time: 923.9259s / 372166.0895 s
env0_first_0:                 episode reward: 20.5000,                 loss: 0.0030
env0_second_0:                 episode reward: -20.5000,                 loss: nan
env1_first_0:                 episode reward: 11.6500,                 loss: nan
env1_second_0:                 episode reward: -11.6500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 2886.4,                last time consumption/overall running time: 788.9473s / 372955.0368 s
env0_first_0:                 episode reward: 12.9000,                 loss: 0.0024
env0_second_0:                 episode reward: -12.9000,                 loss: nan
env1_first_0:                 episode reward: 12.5500,                 loss: nan
env1_second_0:                 episode reward: -12.5500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 3048.35,                last time consumption/overall running time: 830.4855s / 373785.5223 s
env0_first_0:                 episode reward: 11.4000,                 loss: 0.0027
env0_second_0:                 episode reward: -11.4000,                 loss: nan
env1_first_0:                 episode reward: 12.8500,                 loss: nan
env1_second_0:                 episode reward: -12.8500,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 2948.45,                last time consumption/overall running time: 801.9164s / 374587.4386 s
env0_first_0:                 episode reward: 17.6500,                 loss: 0.0026
env0_second_0:                 episode reward: -17.6500,                 loss: nan
env1_first_0:                 episode reward: 18.0500,                 loss: nan
env1_second_0:                 episode reward: -18.0500,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 2873.8,                last time consumption/overall running time: 781.9354s / 375369.3740 s
env0_first_0:                 episode reward: 12.7000,                 loss: 0.0022
env0_second_0:                 episode reward: -12.7000,                 loss: nan
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 2975.55,                last time consumption/overall running time: 809.1492s / 376178.5232 s
env0_first_0:                 episode reward: 14.6000,                 loss: 0.0023
env0_second_0:                 episode reward: -14.6000,                 loss: nan
env1_first_0:                 episode reward: 18.5500,                 loss: nan
env1_second_0:                 episode reward: -18.5500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 3000.35,                last time consumption/overall running time: 814.8158s / 376993.3391 s
env0_first_0:                 episode reward: 14.4000,                 loss: 0.0028
env0_second_0:                 episode reward: -14.4000,                 loss: nan
env1_first_0:                 episode reward: 12.3000,                 loss: nan
env1_second_0:                 episode reward: -12.3000,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 3131.8,                last time consumption/overall running time: 850.9833s / 377844.3224 s
env0_first_0:                 episode reward: 19.1000,                 loss: 0.0024
env0_second_0:                 episode reward: -19.1000,                 loss: nan
env1_first_0:                 episode reward: 7.4000,                 loss: nan
env1_second_0:                 episode reward: -7.4000,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 3047.85,                last time consumption/overall running time: 826.3758s / 378670.6981 s
env0_first_0:                 episode reward: 14.6500,                 loss: 0.0025
env0_second_0:                 episode reward: -14.6500,                 loss: nan
env1_first_0:                 episode reward: 15.5000,                 loss: nan
env1_second_0:                 episode reward: -15.5000,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 2699.55,                last time consumption/overall running time: 734.5514s / 379405.2495 s
env0_first_0:                 episode reward: 17.2000,                 loss: 0.0022
env0_second_0:                 episode reward: -17.2000,                 loss: nan
env1_first_0:                 episode reward: 18.4000,                 loss: nan
env1_second_0:                 episode reward: -18.4000,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 2510.85,                last time consumption/overall running time: 682.6513s / 380087.9009 s
env0_first_0:                 episode reward: 12.7000,                 loss: 0.0020
env0_second_0:                 episode reward: -12.7000,                 loss: nan
env1_first_0:                 episode reward: 19.9500,                 loss: nan
env1_second_0:                 episode reward: -19.9500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 2558.5,                last time consumption/overall running time: 695.5570s / 380783.4578 s
env0_first_0:                 episode reward: 18.0500,                 loss: 0.0023
env0_second_0:                 episode reward: -18.0500,                 loss: nan
env1_first_0:                 episode reward: 14.5000,                 loss: nan
env1_second_0:                 episode reward: -14.5000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 2676.0,                last time consumption/overall running time: 727.6540s / 381511.1119 s
env0_first_0:                 episode reward: 16.6000,                 loss: 0.0020
env0_second_0:                 episode reward: -16.6000,                 loss: nan
env1_first_0:                 episode reward: 19.0000,                 loss: nan
env1_second_0:                 episode reward: -19.0000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 3209.45,                last time consumption/overall running time: 871.5514s / 382382.6633 s
env0_first_0:                 episode reward: 10.1500,                 loss: 0.0024
env0_second_0:                 episode reward: -10.1500,                 loss: nan
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 3317.3,                last time consumption/overall running time: 900.0174s / 383282.6807 s
env0_first_0:                 episode reward: 14.8000,                 loss: 0.0031
env0_second_0:                 episode reward: -14.8000,                 loss: nan
env1_first_0:                 episode reward: 14.4500,                 loss: nan
env1_second_0:                 episode reward: -14.4500,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 3002.8,                last time consumption/overall running time: 816.7227s / 384099.4034 s
env0_first_0:                 episode reward: 17.0000,                 loss: 0.0023
env0_second_0:                 episode reward: -17.0000,                 loss: nan
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 2817.7,                last time consumption/overall running time: 765.5243s / 384864.9277 s
env0_first_0:                 episode reward: 15.5000,                 loss: 0.0022
env0_second_0:                 episode reward: -15.5000,                 loss: nan
env1_first_0:                 episode reward: 16.9000,                 loss: nan
env1_second_0:                 episode reward: -16.9000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 2728.2,                last time consumption/overall running time: 871.3705s / 385736.2982 s
env0_first_0:                 episode reward: 14.7000,                 loss: 0.0025
env0_second_0:                 episode reward: -14.7000,                 loss: nan
env1_first_0:                 episode reward: 14.9500,                 loss: nan
env1_second_0:                 episode reward: -14.9500,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 3160.35,                last time consumption/overall running time: 1190.0202s / 386926.3184 s
env0_first_0:                 episode reward: 14.5000,                 loss: 0.0023
env0_second_0:                 episode reward: -14.5000,                 loss: nan
env1_first_0:                 episode reward: 13.8000,                 loss: nan
env1_second_0:                 episode reward: -13.8000,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 3002.95,                last time consumption/overall running time: 1132.3493s / 388058.6677 s
env0_first_0:                 episode reward: 15.8500,                 loss: 0.0022
env0_second_0:                 episode reward: -15.8500,                 loss: nan
env1_first_0:                 episode reward: 18.3500,                 loss: nan
env1_second_0:                 episode reward: -18.3500,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 2969.55,                last time consumption/overall running time: 1117.2897s / 389175.9575 s
env0_first_0:                 episode reward: 12.2000,                 loss: 0.0024
env0_second_0:                 episode reward: -12.2000,                 loss: nan
env1_first_0:                 episode reward: 16.8500,                 loss: nan
env1_second_0:                 episode reward: -16.8500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 2616.6,                last time consumption/overall running time: 984.5785s / 390160.5359 s
env0_first_0:                 episode reward: 19.6500,                 loss: 0.0021
env0_second_0:                 episode reward: -19.6500,                 loss: nan
env1_first_0:                 episode reward: 17.5000,                 loss: nan
env1_second_0:                 episode reward: -17.5000,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 2504.65,                last time consumption/overall running time: 943.3771s / 391103.9130 s
env0_first_0:                 episode reward: 16.8000,                 loss: 0.0018
env0_second_0:                 episode reward: -16.8000,                 loss: nan
env1_first_0:                 episode reward: 17.2500,                 loss: nan
env1_second_0:                 episode reward: -17.2500,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 2784.55,                last time consumption/overall running time: 1050.5950s / 392154.5080 s
env0_first_0:                 episode reward: 10.0000,                 loss: 0.0027
env0_second_0:                 episode reward: -10.0000,                 loss: nan
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 3303.05,                last time consumption/overall running time: 1242.4748s / 393396.9828 s
env0_first_0:                 episode reward: 7.5000,                 loss: 0.0031
env0_second_0:                 episode reward: -7.5000,                 loss: nan
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 2774.7,                last time consumption/overall running time: 1041.6630s / 394438.6458 s
env0_first_0:                 episode reward: 14.4500,                 loss: 0.0027
env0_second_0:                 episode reward: -14.4500,                 loss: nan
env1_first_0:                 episode reward: 15.4000,                 loss: nan
env1_second_0:                 episode reward: -15.4000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 3237.35,                last time consumption/overall running time: 1216.0973s / 395654.7431 s
env0_first_0:                 episode reward: 9.9000,                 loss: 0.0031
env0_second_0:                 episode reward: -9.9000,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 2939.65,                last time consumption/overall running time: 1101.3941s / 396756.1372 s
env0_first_0:                 episode reward: 17.1000,                 loss: 0.0026
env0_second_0:                 episode reward: -17.1000,                 loss: nan
env1_first_0:                 episode reward: 17.1500,                 loss: nan
env1_second_0:                 episode reward: -17.1500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 2669.1,                last time consumption/overall running time: 1002.5102s / 397758.6474 s
env0_first_0:                 episode reward: 17.1500,                 loss: 0.0023
env0_second_0:                 episode reward: -17.1500,                 loss: nan
env1_first_0:                 episode reward: 15.2500,                 loss: nan
env1_second_0:                 episode reward: -15.2500,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 2641.85,                last time consumption/overall running time: 994.6994s / 398753.3468 s
env0_first_0:                 episode reward: 15.0500,                 loss: 0.0020
env0_second_0:                 episode reward: -15.0500,                 loss: nan
env1_first_0:                 episode reward: 20.0000,                 loss: nan
env1_second_0:                 episode reward: -20.0000,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 2750.35,                last time consumption/overall running time: 1033.3230s / 399786.6698 s
env0_first_0:                 episode reward: 18.3000,                 loss: 0.0022
env0_second_0:                 episode reward: -18.3000,                 loss: nan
env1_first_0:                 episode reward: 13.3000,                 loss: nan
env1_second_0:                 episode reward: -13.3000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 2809.9,                last time consumption/overall running time: 1056.2459s / 400842.9157 s
env0_first_0:                 episode reward: 17.7000,                 loss: 0.0021
env0_second_0:                 episode reward: -17.7000,                 loss: nan
env1_first_0:                 episode reward: 19.2500,                 loss: nan
env1_second_0:                 episode reward: -19.2500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 2941.9,                last time consumption/overall running time: 1105.4645s / 401948.3802 s
env0_first_0:                 episode reward: 14.3500,                 loss: 0.0021
env0_second_0:                 episode reward: -14.3500,                 loss: nan
env1_first_0:                 episode reward: 13.0500,                 loss: nan
env1_second_0:                 episode reward: -13.0500,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 3049.05,                last time consumption/overall running time: 1145.4897s / 403093.8699 s
env0_first_0:                 episode reward: 16.3500,                 loss: 0.0025
env0_second_0:                 episode reward: -16.3500,                 loss: nan
env1_first_0:                 episode reward: 13.3500,                 loss: nan
env1_second_0:                 episode reward: -13.3500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 2592.95,                last time consumption/overall running time: 975.0980s / 404068.9679 s
env0_first_0:                 episode reward: 15.6500,                 loss: 0.0020
env0_second_0:                 episode reward: -15.6500,                 loss: nan
env1_first_0:                 episode reward: 17.4000,                 loss: nan
env1_second_0:                 episode reward: -17.4000,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 2712.15,                last time consumption/overall running time: 1018.6377s / 405087.6056 s
env0_first_0:                 episode reward: 15.8500,                 loss: 0.0023
env0_second_0:                 episode reward: -15.8500,                 loss: nan
env1_first_0:                 episode reward: 16.5000,                 loss: nan
env1_second_0:                 episode reward: -16.5000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 2812.65,                last time consumption/overall running time: 1054.5309s / 406142.1365 s
env0_first_0:                 episode reward: 17.5000,                 loss: 0.0019
env0_second_0:                 episode reward: -17.5000,                 loss: nan
env1_first_0:                 episode reward: 17.4000,                 loss: nan
env1_second_0:                 episode reward: -17.4000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 3072.8,                last time consumption/overall running time: 1151.6777s / 407293.8141 s
env0_first_0:                 episode reward: 10.5000,                 loss: 0.0026
env0_second_0:                 episode reward: -10.5000,                 loss: nan
env1_first_0:                 episode reward: 15.8500,                 loss: nan
env1_second_0:                 episode reward: -15.8500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 3089.75,                last time consumption/overall running time: 1161.7542s / 408455.5683 s
env0_first_0:                 episode reward: 16.9000,                 loss: 0.0021
env0_second_0:                 episode reward: -16.9000,                 loss: nan
env1_first_0:                 episode reward: 16.2000,                 loss: nan
env1_second_0:                 episode reward: -16.2000,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 2546.7,                last time consumption/overall running time: 956.0064s / 409411.5747 s
env0_first_0:                 episode reward: 16.4000,                 loss: 0.0025
env0_second_0:                 episode reward: -16.4000,                 loss: nan
env1_first_0:                 episode reward: 13.9000,                 loss: nan
env1_second_0:                 episode reward: -13.9000,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 2917.6,                last time consumption/overall running time: 1092.4964s / 410504.0712 s
env0_first_0:                 episode reward: 15.6000,                 loss: 0.0020
env0_second_0:                 episode reward: -15.6000,                 loss: nan
env1_first_0:                 episode reward: 18.6500,                 loss: nan
env1_second_0:                 episode reward: -18.6500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 2623.15,                last time consumption/overall running time: 983.6722s / 411487.7434 s
env0_first_0:                 episode reward: 19.4000,                 loss: 0.0020
env0_second_0:                 episode reward: -19.4000,                 loss: nan
env1_first_0:                 episode reward: 17.3500,                 loss: nan
env1_second_0:                 episode reward: -17.3500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 3232.0,                last time consumption/overall running time: 1212.0424s / 412699.7858 s
env0_first_0:                 episode reward: 18.4500,                 loss: 0.0018
env0_second_0:                 episode reward: -18.4500,                 loss: nan
env1_first_0:                 episode reward: 16.3000,                 loss: nan
env1_second_0:                 episode reward: -16.3000,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 3130.05,                last time consumption/overall running time: 1174.2484s / 413874.0343 s
env0_first_0:                 episode reward: 16.3000,                 loss: 0.0020
env0_second_0:                 episode reward: -16.3000,                 loss: nan
env1_first_0:                 episode reward: 18.1500,                 loss: nan
env1_second_0:                 episode reward: -18.1500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 2759.55,                last time consumption/overall running time: 1036.5624s / 414910.5967 s
env0_first_0:                 episode reward: 17.2000,                 loss: 0.0018
env0_second_0:                 episode reward: -17.2000,                 loss: nan
env1_first_0:                 episode reward: 18.3000,                 loss: nan
env1_second_0:                 episode reward: -18.3000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 3064.45,                last time consumption/overall running time: 1148.8328s / 416059.4295 s
env0_first_0:                 episode reward: 17.4000,                 loss: 0.0020
env0_second_0:                 episode reward: -17.4000,                 loss: nan
env1_first_0:                 episode reward: 17.8000,                 loss: nan
env1_second_0:                 episode reward: -17.8000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 3017.95,                last time consumption/overall running time: 1132.3432s / 417191.7727 s
env0_first_0:                 episode reward: 16.2500,                 loss: 0.0021
env0_second_0:                 episode reward: -16.2500,                 loss: nan
env1_first_0:                 episode reward: 18.0500,                 loss: nan
env1_second_0:                 episode reward: -18.0500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 3082.15,                last time consumption/overall running time: 1155.7446s / 418347.5173 s
env0_first_0:                 episode reward: 18.0500,                 loss: 0.0019
env0_second_0:                 episode reward: -18.0500,                 loss: nan
env1_first_0:                 episode reward: 16.2000,                 loss: nan
env1_second_0:                 episode reward: -16.2000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 3137.0,                last time consumption/overall running time: 1176.0602s / 419523.5775 s
env0_first_0:                 episode reward: 9.4500,                 loss: 0.0025
env0_second_0:                 episode reward: -9.4500,                 loss: nan
env1_first_0:                 episode reward: 17.9000,                 loss: nan
env1_second_0:                 episode reward: -17.9000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 2891.35,                last time consumption/overall running time: 1084.5905s / 420608.1681 s
env0_first_0:                 episode reward: 18.2500,                 loss: 0.0019
env0_second_0:                 episode reward: -18.2500,                 loss: nan
env1_first_0:                 episode reward: 16.5000,                 loss: nan
env1_second_0:                 episode reward: -16.5000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 3040.25,                last time consumption/overall running time: 1139.6143s / 421747.7824 s
env0_first_0:                 episode reward: 13.7500,                 loss: 0.0021
env0_second_0:                 episode reward: -13.7500,                 loss: nan
env1_first_0:                 episode reward: 16.4500,                 loss: nan
env1_second_0:                 episode reward: -16.4500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 3042.75,                last time consumption/overall running time: 1137.8829s / 422885.6653 s
env0_first_0:                 episode reward: 14.6500,                 loss: 0.0021
env0_second_0:                 episode reward: -14.6500,                 loss: nan
env1_first_0:                 episode reward: 16.4500,                 loss: nan
env1_second_0:                 episode reward: -16.4500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 6070.55,                last time consumption/overall running time: 2269.3612s / 425155.0265 s
env0_first_0:                 episode reward: -53.7000,                 loss: 0.0068
env0_second_0:                 episode reward: 53.7000,                 loss: nan
env1_first_0:                 episode reward: -55.4500,                 loss: nan
env1_second_0:                 episode reward: 55.4500,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 4135.15,                last time consumption/overall running time: 1547.0995s / 426702.1259 s
env0_first_0:                 episode reward: 10.8000,                 loss: 0.0050
env0_second_0:                 episode reward: -10.8000,                 loss: nan
env1_first_0:                 episode reward: 3.5500,                 loss: nan
env1_second_0:                 episode reward: -3.5500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 3571.6,                last time consumption/overall running time: 1336.4852s / 428038.6112 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.0032
env0_second_0:                 episode reward: 0.2500,                 loss: nan
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 4627.45,                last time consumption/overall running time: 1728.5314s / 429767.1426 s
env0_first_0:                 episode reward: -10.5500,                 loss: 0.0052
env0_second_0:                 episode reward: 10.5500,                 loss: nan
env1_first_0:                 episode reward: -16.0500,                 loss: nan
env1_second_0:                 episode reward: 16.0500,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 3786.25,                last time consumption/overall running time: 1416.9823s / 431184.1249 s
env0_first_0:                 episode reward: 11.2000,                 loss: 0.0037
env0_second_0:                 episode reward: -11.2000,                 loss: nan
env1_first_0:                 episode reward: 11.7000,                 loss: nan
env1_second_0:                 episode reward: -11.7000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 3180.55,                last time consumption/overall running time: 1189.6115s / 432373.7364 s
env0_first_0:                 episode reward: 13.9500,                 loss: 0.0028
env0_second_0:                 episode reward: -13.9500,                 loss: nan
env1_first_0:                 episode reward: 11.9000,                 loss: nan
env1_second_0:                 episode reward: -11.9000,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 3117.7,                last time consumption/overall running time: 1165.6269s / 433539.3633 s
env0_first_0:                 episode reward: 14.6500,                 loss: 0.0025
env0_second_0:                 episode reward: -14.6500,                 loss: nan
env1_first_0:                 episode reward: 12.8000,                 loss: nan
env1_second_0:                 episode reward: -12.8000,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 3257.5,                last time consumption/overall running time: 1220.7959s / 434760.1592 s
env0_first_0:                 episode reward: 12.5000,                 loss: 0.0027
env0_second_0:                 episode reward: -12.5000,                 loss: nan
env1_first_0:                 episode reward: 14.5000,                 loss: nan
env1_second_0:                 episode reward: -14.5000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 3316.15,                last time consumption/overall running time: 1240.3556s / 436000.5149 s
env0_first_0:                 episode reward: 15.2000,                 loss: 0.0024
env0_second_0:                 episode reward: -15.2000,                 loss: nan
env1_first_0:                 episode reward: 15.5500,                 loss: nan
env1_second_0:                 episode reward: -15.5500,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 3185.75,                last time consumption/overall running time: 1192.8219s / 437193.3367 s
env0_first_0:                 episode reward: 15.4000,                 loss: 0.0027
env0_second_0:                 episode reward: -15.4000,                 loss: nan
env1_first_0:                 episode reward: 13.1000,                 loss: nan
env1_second_0:                 episode reward: -13.1000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 2949.95,                last time consumption/overall running time: 1102.8653s / 438296.2020 s
env0_first_0:                 episode reward: 16.6500,                 loss: 0.0021
env0_second_0:                 episode reward: -16.6500,                 loss: nan
env1_first_0:                 episode reward: 16.8500,                 loss: nan
env1_second_0:                 episode reward: -16.8500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 3237.4,                last time consumption/overall running time: 1210.8833s / 439507.0853 s
env0_first_0:                 episode reward: 17.2000,                 loss: 0.0022
env0_second_0:                 episode reward: -17.2000,                 loss: nan
env1_first_0:                 episode reward: 14.0000,                 loss: nan
env1_second_0:                 episode reward: -14.0000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 3175.65,                last time consumption/overall running time: 1188.6920s / 440695.7773 s
env0_first_0:                 episode reward: 16.1000,                 loss: 0.0023
env0_second_0:                 episode reward: -16.1000,                 loss: nan
env1_first_0:                 episode reward: 16.9500,                 loss: nan
env1_second_0:                 episode reward: -16.9500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 3239.1,                last time consumption/overall running time: 1209.7422s / 441905.5194 s
env0_first_0:                 episode reward: 7.0000,                 loss: 0.0030
env0_second_0:                 episode reward: -7.0000,                 loss: nan
env1_first_0:                 episode reward: 8.8500,                 loss: nan
env1_second_0:                 episode reward: -8.8500,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 3161.85,                last time consumption/overall running time: 1181.0138s / 443086.5332 s
env0_first_0:                 episode reward: 13.6000,                 loss: 0.0029
env0_second_0:                 episode reward: -13.6000,                 loss: nan
env1_first_0:                 episode reward: 16.2000,                 loss: nan
env1_second_0:                 episode reward: -16.2000,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 2908.4,                last time consumption/overall running time: 1087.4496s / 444173.9828 s
env0_first_0:                 episode reward: 16.0000,                 loss: 0.0025
env0_second_0:                 episode reward: -16.0000,                 loss: nan
env1_first_0:                 episode reward: 17.8000,                 loss: nan
env1_second_0:                 episode reward: -17.8000,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 3023.45,                last time consumption/overall running time: 1127.9791s / 445301.9619 s
env0_first_0:                 episode reward: 8.3000,                 loss: 0.0029
env0_second_0:                 episode reward: -8.3000,                 loss: nan
env1_first_0:                 episode reward: 14.1500,                 loss: nan
env1_second_0:                 episode reward: -14.1500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 3130.35,                last time consumption/overall running time: 1168.0478s / 446470.0097 s
env0_first_0:                 episode reward: 16.6500,                 loss: 0.0023
env0_second_0:                 episode reward: -16.6500,                 loss: nan
env1_first_0:                 episode reward: 16.2000,                 loss: nan
env1_second_0:                 episode reward: -16.2000,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 3414.05,                last time consumption/overall running time: 1271.8816s / 447741.8913 s
env0_first_0:                 episode reward: 8.1000,                 loss: 0.0029
env0_second_0:                 episode reward: -8.1000,                 loss: nan
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 2928.9,                last time consumption/overall running time: 1088.3572s / 448830.2484 s
env0_first_0:                 episode reward: 15.3000,                 loss: 0.0029
env0_second_0:                 episode reward: -15.3000,                 loss: nan
env1_first_0:                 episode reward: 18.8500,                 loss: nan
env1_second_0:                 episode reward: -18.8500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 2723.25,                last time consumption/overall running time: 1006.6550s / 449836.9035 s
env0_first_0:                 episode reward: 17.7500,                 loss: 0.0023
env0_second_0:                 episode reward: -17.7500,                 loss: nan
env1_first_0:                 episode reward: 17.6500,                 loss: nan
env1_second_0:                 episode reward: -17.6500,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 3926.95,                last time consumption/overall running time: 1451.5804s / 451288.4838 s
env0_first_0:                 episode reward: 6.5500,                 loss: 0.0028
env0_second_0:                 episode reward: -6.5500,                 loss: nan
env1_first_0:                 episode reward: 6.3500,                 loss: nan
env1_second_0:                 episode reward: -6.3500,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 3047.0,                last time consumption/overall running time: 1127.1633s / 452415.6472 s
env0_first_0:                 episode reward: 14.7000,                 loss: 0.0031
env0_second_0:                 episode reward: -14.7000,                 loss: nan
env1_first_0:                 episode reward: 16.7500,                 loss: nan
env1_second_0:                 episode reward: -16.7500,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 3138.2,                last time consumption/overall running time: 1159.6714s / 453575.3185 s
env0_first_0:                 episode reward: 9.2000,                 loss: 0.0023
env0_second_0:                 episode reward: -9.2000,                 loss: nan
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 3706.1,                last time consumption/overall running time: 1369.1664s / 454944.4849 s
env0_first_0:                 episode reward: 8.0500,                 loss: 0.0030
env0_second_0:                 episode reward: -8.0500,                 loss: nan
env1_first_0:                 episode reward: 13.9500,                 loss: nan
env1_second_0:                 episode reward: -13.9500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 3393.3,                last time consumption/overall running time: 1250.4683s / 456194.9532 s
env0_first_0:                 episode reward: 6.1500,                 loss: 0.0032
env0_second_0:                 episode reward: -6.1500,                 loss: nan
env1_first_0:                 episode reward: 9.4000,                 loss: nan
env1_second_0:                 episode reward: -9.4000,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 2875.15,                last time consumption/overall running time: 1062.0264s / 457256.9797 s
env0_first_0:                 episode reward: 17.3500,                 loss: 0.0026
env0_second_0:                 episode reward: -17.3500,                 loss: nan
env1_first_0:                 episode reward: 14.4500,                 loss: nan
env1_second_0:                 episode reward: -14.4500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 3932.5,                last time consumption/overall running time: 1449.3691s / 458706.3487 s
env0_first_0:                 episode reward: 9.9500,                 loss: 0.0028
env0_second_0:                 episode reward: -9.9500,                 loss: nan
env1_first_0:                 episode reward: 8.8000,                 loss: nan
env1_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 2995.7,                last time consumption/overall running time: 1106.5583s / 459812.9071 s
env0_first_0:                 episode reward: 15.8500,                 loss: 0.0022
env0_second_0:                 episode reward: -15.8500,                 loss: nan
env1_first_0:                 episode reward: 14.2500,                 loss: nan
env1_second_0:                 episode reward: -14.2500,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 3521.55,                last time consumption/overall running time: 1301.7531s / 461114.6601 s
env0_first_0:                 episode reward: 12.1000,                 loss: 0.0023
env0_second_0:                 episode reward: -12.1000,                 loss: nan
env1_first_0:                 episode reward: 15.6500,                 loss: nan
env1_second_0:                 episode reward: -15.6500,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 3462.95,                last time consumption/overall running time: 1280.7580s / 462395.4182 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0030
env0_second_0:                 episode reward: -7.0500,                 loss: nan
env1_first_0:                 episode reward: 5.6000,                 loss: nan
env1_second_0:                 episode reward: -5.6000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 3033.8,                last time consumption/overall running time: 1120.3104s / 463515.7285 s
env0_first_0:                 episode reward: 17.2000,                 loss: 0.0029
env0_second_0:                 episode reward: -17.2000,                 loss: nan
env1_first_0:                 episode reward: 13.1500,                 loss: nan
env1_second_0:                 episode reward: -13.1500,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 3383.75,                last time consumption/overall running time: 1247.7659s / 464763.4944 s
env0_first_0:                 episode reward: 7.3000,                 loss: 0.0031
env0_second_0:                 episode reward: -7.3000,                 loss: nan
env1_first_0:                 episode reward: 9.7500,                 loss: nan
env1_second_0:                 episode reward: -9.7500,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 3206.45,                last time consumption/overall running time: 1185.0548s / 465948.5492 s
env0_first_0:                 episode reward: 11.7500,                 loss: 0.0027
env0_second_0:                 episode reward: -11.7500,                 loss: nan
env1_first_0:                 episode reward: 12.8500,                 loss: nan
env1_second_0:                 episode reward: -12.8500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 2778.7,                last time consumption/overall running time: 1127.6833s / 467076.2326 s
env0_first_0:                 episode reward: 19.5500,                 loss: 0.0024
env0_second_0:                 episode reward: -19.5500,                 loss: nan
env1_first_0:                 episode reward: 15.0500,                 loss: nan
env1_second_0:                 episode reward: -15.0500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 4237.7,                last time consumption/overall running time: 1897.1060s / 468973.3385 s
env0_first_0:                 episode reward: 1.7500,                 loss: 0.0032
env0_second_0:                 episode reward: -1.7500,                 loss: nan
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 3315.4,                last time consumption/overall running time: 1481.6086s / 470454.9471 s
env0_first_0:                 episode reward: 11.8000,                 loss: 0.0035
env0_second_0:                 episode reward: -11.8000,                 loss: nan
env1_first_0:                 episode reward: 10.7000,                 loss: nan
env1_second_0:                 episode reward: -10.7000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 3385.3,                last time consumption/overall running time: 1512.4488s / 471967.3959 s
env0_first_0:                 episode reward: 10.3500,                 loss: 0.0030
env0_second_0:                 episode reward: -10.3500,                 loss: nan
env1_first_0:                 episode reward: 18.2500,                 loss: nan
env1_second_0:                 episode reward: -18.2500,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 3194.55,                last time consumption/overall running time: 1427.4145s / 473394.8104 s
env0_first_0:                 episode reward: 15.1000,                 loss: 0.0024
env0_second_0:                 episode reward: -15.1000,                 loss: nan
env1_first_0:                 episode reward: 13.6000,                 loss: nan
env1_second_0:                 episode reward: -13.6000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 3480.1,                last time consumption/overall running time: 1554.9459s / 474949.7563 s
env0_first_0:                 episode reward: 12.4000,                 loss: 0.0029
env0_second_0:                 episode reward: -12.4000,                 loss: nan
env1_first_0:                 episode reward: 14.3000,                 loss: nan
env1_second_0:                 episode reward: -14.3000,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 3367.75,                last time consumption/overall running time: 1505.4105s / 476455.1668 s
env0_first_0:                 episode reward: 12.3000,                 loss: 0.0024
env0_second_0:                 episode reward: -12.3000,                 loss: nan
env1_first_0:                 episode reward: 11.4500,                 loss: nan
env1_second_0:                 episode reward: -11.4500,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 3096.1,                last time consumption/overall running time: 1382.1546s / 477837.3214 s
env0_first_0:                 episode reward: 11.9500,                 loss: 0.0031
env0_second_0:                 episode reward: -11.9500,                 loss: nan
env1_first_0:                 episode reward: 14.3500,                 loss: nan
env1_second_0:                 episode reward: -14.3500,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 3510.8,                last time consumption/overall running time: 1568.1035s / 479405.4249 s
env0_first_0:                 episode reward: 11.1000,                 loss: 0.0023
env0_second_0:                 episode reward: -11.1000,                 loss: nan
env1_first_0:                 episode reward: 8.8000,                 loss: nan
env1_second_0:                 episode reward: -8.8000,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 3346.85,                last time consumption/overall running time: 1496.0977s / 480901.5226 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.0043
env0_second_0:                 episode reward: 2.7500,                 loss: nan
env1_first_0:                 episode reward: 9.7000,                 loss: nan
env1_second_0:                 episode reward: -9.7000,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 3076.95,                last time consumption/overall running time: 1374.8157s / 482276.3383 s
env0_first_0:                 episode reward: 14.7000,                 loss: 0.0025
env0_second_0:                 episode reward: -14.7000,                 loss: nan
env1_first_0:                 episode reward: 16.0500,                 loss: nan
env1_second_0:                 episode reward: -16.0500,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 3431.9,                last time consumption/overall running time: 1533.7258s / 483810.0641 s
env0_first_0:                 episode reward: 6.6000,                 loss: 0.0032
env0_second_0:                 episode reward: -6.6000,                 loss: nan
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 3254.65,                last time consumption/overall running time: 1453.3416s / 485263.4057 s
env0_first_0:                 episode reward: 15.3500,                 loss: 0.0026
env0_second_0:                 episode reward: -15.3500,                 loss: nan
env1_first_0:                 episode reward: 17.4000,                 loss: nan
env1_second_0:                 episode reward: -17.4000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 2642.0,                last time consumption/overall running time: 1181.7729s / 486445.1786 s
env0_first_0:                 episode reward: 15.7500,                 loss: 0.0019
env0_second_0:                 episode reward: -15.7500,                 loss: nan
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 2834.95,                last time consumption/overall running time: 1266.9142s / 487712.0929 s
env0_first_0:                 episode reward: 18.0000,                 loss: 0.0022
env0_second_0:                 episode reward: -18.0000,                 loss: nan
env1_first_0:                 episode reward: 16.3500,                 loss: nan
env1_second_0:                 episode reward: -16.3500,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 3314.5,                last time consumption/overall running time: 1481.1018s / 489193.1947 s
env0_first_0:                 episode reward: 10.1500,                 loss: 0.0025
env0_second_0:                 episode reward: -10.1500,                 loss: nan
env1_first_0:                 episode reward: 8.1500,                 loss: nan
env1_second_0:                 episode reward: -8.1500,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 3408.15,                last time consumption/overall running time: 1524.9055s / 490718.1002 s
env0_first_0:                 episode reward: 16.4500,                 loss: 0.0025
env0_second_0:                 episode reward: -16.4500,                 loss: nan
env1_first_0:                 episode reward: 16.2000,                 loss: nan
env1_second_0:                 episode reward: -16.2000,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 3122.65,                last time consumption/overall running time: 1396.6462s / 492114.7464 s
env0_first_0:                 episode reward: 17.2500,                 loss: 0.0022
env0_second_0:                 episode reward: -17.2500,                 loss: nan
env1_first_0:                 episode reward: 17.0500,                 loss: nan
env1_second_0:                 episode reward: -17.0500,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 2659.9,                last time consumption/overall running time: 1191.2208s / 493305.9672 s
env0_first_0:                 episode reward: 14.6500,                 loss: 0.0023
env0_second_0:                 episode reward: -14.6500,                 loss: nan
env1_first_0:                 episode reward: 16.5500,                 loss: nan
env1_second_0:                 episode reward: -16.5500,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 3213.1,                last time consumption/overall running time: 1439.6341s / 494745.6012 s
env0_first_0:                 episode reward: 16.7000,                 loss: 0.0024
env0_second_0:                 episode reward: -16.7000,                 loss: nan
env1_first_0:                 episode reward: 10.7000,                 loss: nan
env1_second_0:                 episode reward: -10.7000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 2927.05,                last time consumption/overall running time: 1309.2002s / 496054.8015 s
env0_first_0:                 episode reward: 16.9000,                 loss: 0.0023
env0_second_0:                 episode reward: -16.9000,                 loss: nan
env1_first_0:                 episode reward: 15.8500,                 loss: nan
env1_second_0:                 episode reward: -15.8500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 3005.8,                last time consumption/overall running time: 1342.6733s / 497397.4748 s
env0_first_0:                 episode reward: 13.4500,                 loss: 0.0023
env0_second_0:                 episode reward: -13.4500,                 loss: nan
env1_first_0:                 episode reward: 17.6500,                 loss: nan
env1_second_0:                 episode reward: -17.6500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 2927.15,                last time consumption/overall running time: 1307.9819s / 498705.4567 s
env0_first_0:                 episode reward: 15.9000,                 loss: 0.0023
env0_second_0:                 episode reward: -15.9000,                 loss: nan
env1_first_0:                 episode reward: 15.9000,                 loss: nan
env1_second_0:                 episode reward: -15.9000,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 3452.15,                last time consumption/overall running time: 1543.6135s / 500249.0702 s
env0_first_0:                 episode reward: 4.5500,                 loss: 0.0028
env0_second_0:                 episode reward: -4.5500,                 loss: nan
env1_first_0:                 episode reward: 6.2000,                 loss: nan
env1_second_0:                 episode reward: -6.2000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 3219.2,                last time consumption/overall running time: 1439.7823s / 501688.8525 s
env0_first_0:                 episode reward: 13.8000,                 loss: 0.0034
env0_second_0:                 episode reward: -13.8000,                 loss: nan
env1_first_0:                 episode reward: 11.5000,                 loss: nan
env1_second_0:                 episode reward: -11.5000,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 2939.4,                last time consumption/overall running time: 1314.5483s / 503003.4008 s
env0_first_0:                 episode reward: 14.1000,                 loss: 0.0024
env0_second_0:                 episode reward: -14.1000,                 loss: nan
env1_first_0:                 episode reward: 17.4000,                 loss: nan
env1_second_0:                 episode reward: -17.4000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 3524.1,                last time consumption/overall running time: 1574.0800s / 504577.4808 s
env0_first_0:                 episode reward: 4.9000,                 loss: 0.0028
env0_second_0:                 episode reward: -4.9000,                 loss: nan
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 4269.85,                last time consumption/overall running time: 1909.7183s / 506487.1991 s
env0_first_0:                 episode reward: -8.6000,                 loss: 0.0053
env0_second_0:                 episode reward: 8.6000,                 loss: nan
env1_first_0:                 episode reward: -13.0500,                 loss: nan
env1_second_0:                 episode reward: 13.0500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 3700.45,                last time consumption/overall running time: 1654.5762s / 508141.7752 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.0033
env0_second_0:                 episode reward: 0.7500,                 loss: nan
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 3204.25,                last time consumption/overall running time: 1666.9434s / 509808.7186 s
env0_first_0:                 episode reward: 12.9500,                 loss: 0.0037
env0_second_0:                 episode reward: -12.9500,                 loss: nan
env1_first_0:                 episode reward: 11.7500,                 loss: nan
env1_second_0:                 episode reward: -11.7500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 3685.65,                last time consumption/overall running time: 2168.1286s / 511976.8472 s
env0_first_0:                 episode reward: 6.3000,                 loss: 0.0030
env0_second_0:                 episode reward: -6.3000,                 loss: nan
env1_first_0:                 episode reward: 11.1000,                 loss: nan
env1_second_0:                 episode reward: -11.1000,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 3633.25,                last time consumption/overall running time: 2132.5708s / 514109.4180 s
env0_first_0:                 episode reward: 5.8500,                 loss: 0.0036
env0_second_0:                 episode reward: -5.8500,                 loss: nan
env1_first_0:                 episode reward: 10.4500,                 loss: nan
env1_second_0:                 episode reward: -10.4500,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 3697.75,                last time consumption/overall running time: 2168.1624s / 516277.5804 s
env0_first_0:                 episode reward: 11.1000,                 loss: 0.0030
env0_second_0:                 episode reward: -11.1000,                 loss: nan
env1_first_0:                 episode reward: 9.5000,                 loss: nan
env1_second_0:                 episode reward: -9.5000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 3773.35,                last time consumption/overall running time: 2209.6676s / 518487.2480 s
env0_first_0:                 episode reward: -4.8000,                 loss: 0.0039
env0_second_0:                 episode reward: 4.8000,                 loss: nan
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 3952.9,                last time consumption/overall running time: 2315.7872s / 520803.0352 s
env0_first_0:                 episode reward: 3.5000,                 loss: 0.0042
env0_second_0:                 episode reward: -3.5000,                 loss: nan
env1_first_0:                 episode reward: 6.6500,                 loss: nan
env1_second_0:                 episode reward: -6.6500,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 3557.35,                last time consumption/overall running time: 2085.6695s / 522888.7047 s
env0_first_0:                 episode reward: 8.7000,                 loss: 0.0030
env0_second_0:                 episode reward: -8.7000,                 loss: nan
env1_first_0:                 episode reward: 11.9000,                 loss: nan
env1_second_0:                 episode reward: -11.9000,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 3690.3,                last time consumption/overall running time: 2168.8809s / 525057.5855 s
env0_first_0:                 episode reward: -2.0500,                 loss: 0.0039
env0_second_0:                 episode reward: 2.0500,                 loss: nan
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 3274.6,                last time consumption/overall running time: 1925.1067s / 526982.6923 s
env0_first_0:                 episode reward: 11.6000,                 loss: 0.0028
env0_second_0:                 episode reward: -11.6000,                 loss: nan
env1_first_0:                 episode reward: 13.9000,                 loss: nan
env1_second_0:                 episode reward: -13.9000,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 3254.3,                last time consumption/overall running time: 1908.3346s / 528891.0268 s
env0_first_0:                 episode reward: 14.2500,                 loss: 0.0023
env0_second_0:                 episode reward: -14.2500,                 loss: nan
env1_first_0:                 episode reward: 12.4000,                 loss: nan
env1_second_0:                 episode reward: -12.4000,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 3675.15,                last time consumption/overall running time: 2156.5431s / 531047.5700 s
env0_first_0:                 episode reward: 14.9000,                 loss: 0.0024
env0_second_0:                 episode reward: -14.9000,                 loss: nan
env1_first_0:                 episode reward: 13.7500,                 loss: nan
env1_second_0:                 episode reward: -13.7500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 3949.8,                last time consumption/overall running time: 2315.8824s / 533363.4524 s
env0_first_0:                 episode reward: -10.2500,                 loss: 0.0042
env0_second_0:                 episode reward: 10.2500,                 loss: nan
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 3364.65,                last time consumption/overall running time: 1971.3733s / 535334.8256 s
env0_first_0:                 episode reward: 12.2000,                 loss: 0.0027
env0_second_0:                 episode reward: -12.2000,                 loss: nan
env1_first_0:                 episode reward: 13.7000,                 loss: nan
env1_second_0:                 episode reward: -13.7000,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 3176.5,                last time consumption/overall running time: 1861.2887s / 537196.1143 s
env0_first_0:                 episode reward: 14.4500,                 loss: 0.0026
env0_second_0:                 episode reward: -14.4500,                 loss: nan
env1_first_0:                 episode reward: 12.0500,                 loss: nan
env1_second_0:                 episode reward: -12.0500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 3593.3,                last time consumption/overall running time: 2102.2815s / 539298.3958 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.0027
env0_second_0:                 episode reward: -1.0000,                 loss: nan
env1_first_0:                 episode reward: 6.7000,                 loss: nan
env1_second_0:                 episode reward: -6.7000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 3223.95,                last time consumption/overall running time: 1885.5730s / 541183.9688 s
env0_first_0:                 episode reward: 9.6000,                 loss: 0.0039
env0_second_0:                 episode reward: -9.6000,                 loss: nan
env1_first_0:                 episode reward: 14.4500,                 loss: nan
env1_second_0:                 episode reward: -14.4500,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 3297.4,                last time consumption/overall running time: 1926.9540s / 543110.9228 s
env0_first_0:                 episode reward: 11.6500,                 loss: 0.0026
env0_second_0:                 episode reward: -11.6500,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 3452.95,                last time consumption/overall running time: 2018.3953s / 545129.3181 s
env0_first_0:                 episode reward: 3.0000,                 loss: 0.0033
env0_second_0:                 episode reward: -3.0000,                 loss: nan
env1_first_0:                 episode reward: 7.6500,                 loss: nan
env1_second_0:                 episode reward: -7.6500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 3195.4,                last time consumption/overall running time: 1868.0623s / 546997.3803 s
env0_first_0:                 episode reward: 11.3000,                 loss: 0.0033
env0_second_0:                 episode reward: -11.3000,                 loss: nan
env1_first_0:                 episode reward: 15.6000,                 loss: nan
env1_second_0:                 episode reward: -15.6000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 3184.85,                last time consumption/overall running time: 1860.9029s / 548858.2833 s
env0_first_0:                 episode reward: 13.9500,                 loss: 0.0027
env0_second_0:                 episode reward: -13.9500,                 loss: nan
env1_first_0:                 episode reward: 14.0500,                 loss: nan
env1_second_0:                 episode reward: -14.0500,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 3202.25,                last time consumption/overall running time: 1875.4840s / 550733.7673 s
env0_first_0:                 episode reward: 15.6500,                 loss: 0.0023
env0_second_0:                 episode reward: -15.6500,                 loss: nan
env1_first_0:                 episode reward: 14.2000,                 loss: nan
env1_second_0:                 episode reward: -14.2000,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 3425.6,                last time consumption/overall running time: 2001.0702s / 552734.8374 s
env0_first_0:                 episode reward: 5.7000,                 loss: 0.0031
env0_second_0:                 episode reward: -5.7000,                 loss: nan
env1_first_0:                 episode reward: 8.5000,                 loss: nan
env1_second_0:                 episode reward: -8.5000,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 3579.6,                last time consumption/overall running time: 2093.2529s / 554828.0903 s
env0_first_0:                 episode reward: 4.9500,                 loss: 0.0037
env0_second_0:                 episode reward: -4.9500,                 loss: nan
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 3232.25,                last time consumption/overall running time: 1888.8706s / 556716.9608 s
env0_first_0:                 episode reward: 16.6500,                 loss: 0.0028
env0_second_0:                 episode reward: -16.6500,                 loss: nan
env1_first_0:                 episode reward: 16.1500,                 loss: nan
env1_second_0:                 episode reward: -16.1500,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 3196.6,                last time consumption/overall running time: 1830.2207s / 558547.1815 s
env0_first_0:                 episode reward: 12.0500,                 loss: 0.0023
env0_second_0:                 episode reward: -12.0500,                 loss: nan
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 2996.5,                last time consumption/overall running time: 1651.8213s / 560199.0029 s
env0_first_0:                 episode reward: 17.3000,                 loss: 0.0023
env0_second_0:                 episode reward: -17.3000,                 loss: nan
env1_first_0:                 episode reward: 14.5500,                 loss: nan
env1_second_0:                 episode reward: -14.5500,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 3390.45,                last time consumption/overall running time: 1865.2902s / 562064.2931 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0032
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: 9.4500,                 loss: nan
env1_second_0:                 episode reward: -9.4500,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 2822.1,                last time consumption/overall running time: 1484.9736s / 563549.2667 s
env0_first_0:                 episode reward: 13.6000,                 loss: 0.0027
env0_second_0:                 episode reward: -13.6000,                 loss: nan
env1_first_0:                 episode reward: 17.2500,                 loss: nan
env1_second_0:                 episode reward: -17.2500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 4168.2,                last time consumption/overall running time: 2190.2832s / 565739.5500 s
env0_first_0:                 episode reward: -8.6000,                 loss: 0.0045
env0_second_0:                 episode reward: 8.6000,                 loss: nan
env1_first_0:                 episode reward: -8.5500,                 loss: nan
env1_second_0:                 episode reward: 8.5500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 4211.4,                last time consumption/overall running time: 2212.1373s / 567951.6873 s
env0_first_0:                 episode reward: -11.3000,                 loss: 0.0047
env0_second_0:                 episode reward: 11.3000,                 loss: nan
env1_first_0:                 episode reward: -12.3500,                 loss: nan
env1_second_0:                 episode reward: 12.3500,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 3443.65,                last time consumption/overall running time: 1810.2377s / 569761.9250 s
env0_first_0:                 episode reward: 12.9000,                 loss: 0.0035
env0_second_0:                 episode reward: -12.9000,                 loss: nan
env1_first_0:                 episode reward: 10.6500,                 loss: nan
env1_second_0:                 episode reward: -10.6500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 3318.95,                last time consumption/overall running time: 1745.8301s / 571507.7551 s
env0_first_0:                 episode reward: 7.7000,                 loss: 0.0031
env0_second_0:                 episode reward: -7.7000,                 loss: nan
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 3202.45,                last time consumption/overall running time: 1678.1642s / 573185.9194 s
env0_first_0:                 episode reward: 13.9500,                 loss: 0.0029
env0_second_0:                 episode reward: -13.9500,                 loss: nan
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 3924.95,                last time consumption/overall running time: 1936.8882s / 575122.8076 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.0034
env0_second_0:                 episode reward: 2.1000,                 loss: nan
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 3030.9,                last time consumption/overall running time: 1498.1996s / 576621.0072 s
env0_first_0:                 episode reward: 15.4500,                 loss: 0.0035
env0_second_0:                 episode reward: -15.4500,                 loss: nan
env1_first_0:                 episode reward: 14.3000,                 loss: nan
env1_second_0:                 episode reward: -14.3000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 3627.2,                last time consumption/overall running time: 1785.9832s / 578406.9904 s
env0_first_0:                 episode reward: 9.3000,                 loss: 0.0028
env0_second_0:                 episode reward: -9.3000,                 loss: nan
env1_first_0:                 episode reward: 10.6500,                 loss: nan
env1_second_0:                 episode reward: -10.6500,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 3414.15,                last time consumption/overall running time: 1686.6735s / 580093.6639 s
env0_first_0:                 episode reward: 14.1000,                 loss: 0.0026
env0_second_0:                 episode reward: -14.1000,                 loss: nan
env1_first_0:                 episode reward: 15.1500,                 loss: nan
env1_second_0:                 episode reward: -15.1500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 2967.35,                last time consumption/overall running time: 1464.5649s / 581558.2288 s
env0_first_0:                 episode reward: 16.9500,                 loss: 0.0024
env0_second_0:                 episode reward: -16.9500,                 loss: nan
env1_first_0:                 episode reward: 16.8500,                 loss: nan
env1_second_0:                 episode reward: -16.8500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 3274.4,                last time consumption/overall running time: 1618.8407s / 583177.0695 s
env0_first_0:                 episode reward: 15.3000,                 loss: 0.0022
env0_second_0:                 episode reward: -15.3000,                 loss: nan
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 3179.3,                last time consumption/overall running time: 1483.4775s / 584660.5470 s
env0_first_0:                 episode reward: 12.8000,                 loss: 0.0025
env0_second_0:                 episode reward: -12.8000,                 loss: nan
env1_first_0:                 episode reward: 11.3000,                 loss: nan
env1_second_0:                 episode reward: -11.3000,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 2818.75,                last time consumption/overall running time: 1230.8365s / 585891.3835 s
env0_first_0:                 episode reward: 13.3000,                 loss: 0.0024
env0_second_0:                 episode reward: -13.3000,                 loss: nan
env1_first_0:                 episode reward: 15.1000,                 loss: nan
env1_second_0:                 episode reward: -15.1000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 2984.7,                last time consumption/overall running time: 1302.1787s / 587193.5623 s
env0_first_0:                 episode reward: 16.3500,                 loss: 0.0025
env0_second_0:                 episode reward: -16.3500,                 loss: nan
env1_first_0:                 episode reward: 14.4000,                 loss: nan
env1_second_0:                 episode reward: -14.4000,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 3447.05,                last time consumption/overall running time: 1430.7023s / 588624.2646 s
env0_first_0:                 episode reward: 5.4500,                 loss: 0.0032
env0_second_0:                 episode reward: -5.4500,                 loss: nan
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 2988.4,                last time consumption/overall running time: 1235.8491s / 589860.1137 s
env0_first_0:                 episode reward: 12.0500,                 loss: 0.0033
env0_second_0:                 episode reward: -12.0500,                 loss: nan
env1_first_0:                 episode reward: 7.1000,                 loss: nan
env1_second_0:                 episode reward: -7.1000,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 3425.45,                last time consumption/overall running time: 1413.2322s / 591273.3459 s
env0_first_0:                 episode reward: 4.3500,                 loss: 0.0036
env0_second_0:                 episode reward: -4.3500,                 loss: nan
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 3339.4,                last time consumption/overall running time: 1373.3217s / 592646.6676 s
env0_first_0:                 episode reward: 10.2000,                 loss: 0.0035
env0_second_0:                 episode reward: -10.2000,                 loss: nan
env1_first_0:                 episode reward: 8.7500,                 loss: nan
env1_second_0:                 episode reward: -8.7500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 2996.3,                last time consumption/overall running time: 1233.6007s / 593880.2684 s
env0_first_0:                 episode reward: 14.7000,                 loss: 0.0029
env0_second_0:                 episode reward: -14.7000,                 loss: nan
env1_first_0:                 episode reward: 12.0500,                 loss: nan
env1_second_0:                 episode reward: -12.0500,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 3767.95,                last time consumption/overall running time: 1551.3831s / 595431.6515 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0033
env0_second_0:                 episode reward: 2.3500,                 loss: nan
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 3338.45,                last time consumption/overall running time: 1376.9400s / 596808.5915 s
env0_first_0:                 episode reward: 9.8000,                 loss: 0.0038
env0_second_0:                 episode reward: -9.8000,                 loss: nan
env1_first_0:                 episode reward: 7.4500,                 loss: nan
env1_second_0:                 episode reward: -7.4500,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 3049.05,                last time consumption/overall running time: 1200.1712s / 598008.7626 s
env0_first_0:                 episode reward: 15.1500,                 loss: 0.0029
env0_second_0:                 episode reward: -15.1500,                 loss: nan
env1_first_0:                 episode reward: 13.5000,                 loss: nan
env1_second_0:                 episode reward: -13.5000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 3960.3,                last time consumption/overall running time: 1536.3994s / 599545.1621 s
env0_first_0:                 episode reward: 4.8000,                 loss: 0.0036
env0_second_0:                 episode reward: -4.8000,                 loss: nan
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 3407.35,                last time consumption/overall running time: 1322.8239s / 600867.9860 s
env0_first_0:                 episode reward: 12.1000,                 loss: 0.0030
env0_second_0:                 episode reward: -12.1000,                 loss: nan
env1_first_0:                 episode reward: 13.8500,                 loss: nan
env1_second_0:                 episode reward: -13.8500,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 3003.5,                last time consumption/overall running time: 1168.4542s / 602036.4402 s
env0_first_0:                 episode reward: 15.0500,                 loss: 0.0024
env0_second_0:                 episode reward: -15.0500,                 loss: nan
env1_first_0:                 episode reward: 16.0000,                 loss: nan
env1_second_0:                 episode reward: -16.0000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 3033.4,                last time consumption/overall running time: 1179.7682s / 603216.2084 s
env0_first_0:                 episode reward: 7.0500,                 loss: 0.0024
env0_second_0:                 episode reward: -7.0500,                 loss: nan
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 3090.05,                last time consumption/overall running time: 1201.7379s / 604417.9463 s
env0_first_0:                 episode reward: 9.0000,                 loss: 0.0039
env0_second_0:                 episode reward: -9.0000,                 loss: nan
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nanLoad tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rollout.py:21: FutureWarning: arrays to stack must be passed as a "sequence" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.
  rollout_normal(env, model, save_id, args)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/lib/npyio.py:528: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arr = np.asanyarray(arr)

Episode: 9721/10000 (97.2100%),                 avg. length: 3272.75,                last time consumption/overall running time: 1271.2858s / 605689.2321 s
env0_first_0:                 episode reward: 16.2500,                 loss: 0.0034
env0_second_0:                 episode reward: -16.2500,                 loss: nan
env1_first_0:                 episode reward: 14.4500,                 loss: nan
env1_second_0:                 episode reward: -14.4500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 3194.2,                last time consumption/overall running time: 1238.8681s / 606928.1002 s
env0_first_0:                 episode reward: 12.3000,                 loss: 0.0026
env0_second_0:                 episode reward: -12.3000,                 loss: nan
env1_first_0:                 episode reward: 14.3000,                 loss: nan
env1_second_0:                 episode reward: -14.3000,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 3084.35,                last time consumption/overall running time: 1196.6311s / 608124.7313 s
env0_first_0:                 episode reward: 15.8000,                 loss: 0.0029
env0_second_0:                 episode reward: -15.8000,                 loss: nan
env1_first_0:                 episode reward: 11.4000,                 loss: nan
env1_second_0:                 episode reward: -11.4000,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 2881.9,                last time consumption/overall running time: 1118.5584s / 609243.2897 s
env0_first_0:                 episode reward: 14.3000,                 loss: 0.0023
env0_second_0:                 episode reward: -14.3000,                 loss: nan
env1_first_0:                 episode reward: 15.3500,                 loss: nan
env1_second_0:                 episode reward: -15.3500,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 2917.75,                last time consumption/overall running time: 1132.7420s / 610376.0317 s
env0_first_0:                 episode reward: 13.1500,                 loss: 0.0026
env0_second_0:                 episode reward: -13.1500,                 loss: nan
env1_first_0:                 episode reward: 16.3000,                 loss: nan
env1_second_0:                 episode reward: -16.3000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 3222.1,                last time consumption/overall running time: 1250.5881s / 611626.6198 s
env0_first_0:                 episode reward: 12.6500,                 loss: 0.0025
env0_second_0:                 episode reward: -12.6500,                 loss: nan
env1_first_0:                 episode reward: 15.7000,                 loss: nan
env1_second_0:                 episode reward: -15.7000,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 3167.25,                last time consumption/overall running time: 1232.0267s / 612858.6465 s
env0_first_0:                 episode reward: 14.3000,                 loss: 0.0029
env0_second_0:                 episode reward: -14.3000,                 loss: nan
env1_first_0:                 episode reward: 6.9500,                 loss: nan
env1_second_0:                 episode reward: -6.9500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 3485.7,                last time consumption/overall running time: 1353.7409s / 614212.3874 s
env0_first_0:                 episode reward: 13.4000,                 loss: 0.0028
env0_second_0:                 episode reward: -13.4000,                 loss: nan
env1_first_0:                 episode reward: 10.7500,                 loss: nan
env1_second_0:                 episode reward: -10.7500,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 3137.65,                last time consumption/overall running time: 1217.3299s / 615429.7173 s
env0_first_0:                 episode reward: 7.6500,                 loss: 0.0033
env0_second_0:                 episode reward: -7.6500,                 loss: nan
env1_first_0:                 episode reward: 11.0500,                 loss: nan
env1_second_0:                 episode reward: -11.0500,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 4152.95,                last time consumption/overall running time: 1610.3178s / 617040.0351 s
env0_first_0:                 episode reward: -13.5500,                 loss: 0.0043
env0_second_0:                 episode reward: 13.5500,                 loss: nan
env1_first_0:                 episode reward: -13.2500,                 loss: nan
env1_second_0:                 episode reward: 13.2500,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 3325.75,                last time consumption/overall running time: 1289.1835s / 618329.2186 s
env0_first_0:                 episode reward: 11.4000,                 loss: 0.0044
env0_second_0:                 episode reward: -11.4000,                 loss: nan
env1_first_0:                 episode reward: 13.3000,                 loss: nan
env1_second_0:                 episode reward: -13.3000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 3138.1,                last time consumption/overall running time: 1219.0793s / 619548.2979 s
env0_first_0:                 episode reward: 17.2500,                 loss: 0.0025
env0_second_0:                 episode reward: -17.2500,                 loss: nan
env1_first_0:                 episode reward: 12.4500,                 loss: nan
env1_second_0:                 episode reward: -12.4500,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 2856.1,                last time consumption/overall running time: 1109.7301s / 620658.0280 s
env0_first_0:                 episode reward: 11.7000,                 loss: 0.0024
env0_second_0:                 episode reward: -11.7000,                 loss: nan
env1_first_0:                 episode reward: 15.0500,                 loss: nan
env1_second_0:                 episode reward: -15.0500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 4021.65,                last time consumption/overall running time: 1556.6908s / 622214.7188 s
env0_first_0:                 episode reward: -11.3000,                 loss: 0.0053
env0_second_0:                 episode reward: 11.3000,                 loss: nan
env1_first_0:                 episode reward: -16.3000,                 loss: nan
env1_second_0:                 episode reward: 16.3000,                 loss: nan
