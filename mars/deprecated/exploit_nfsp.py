from utils.func import LoadYAML2Dict
from env.import_env import make_env
from rollout import rollout
from rl.algorithm import *

### Load configurations
yaml_file = 'confs/pettingzoo_boxingv1_nfsp'
# yaml_file = 'confs/lasertag_LaserTagsmallv0_nfsp'
args = LoadYAML2Dict(yaml_file, toAttr=True)
print(args)

## Change/specify some arguments if necessary
args.against_baseline = False
args.test = False
# args.render = True
args.exploit = True
# args.load_model_full_path = '../model/pettingzoo_boxing_v1_nfsp_NFSP_0/_5000-0'
args.load_model_full_path = '../model/pettingzoo_boxing_v1_nfsp_NFSP_20210827002234/4000_0'
# args.load_model_full_path = '../model/lasertag_LaserTag-small3-v0_nfsp_NFSP_0/_9900-0'
args.num_envs = 1
args.seed = 1
args.eta = 0.  # average policy: eta=0; best response policy: eta=1 

# if exploit the best response policy (Q-network), set epsilon to always be 1.
# args.algorithm_spec['eps_start'] = 1.
# args.algorithm_spec['eps_final'] = 1.


### Create env
env = make_env(args)
print(env)

### Specify models for each agent
trained_model = eval(args.algorithm)(env, args)

# args.net_architecture['hidden_dim_list'] = [64, 64]  # boxing_v1, same as nfsp agent

# args.algorithm_spec['eps_final'] = 0.1
exploiter = DQN(env, args)
trained_model.fix()

model = MultiAgent(env, [trained_model, exploiter], args)

### Rollout
rollout(env, model, args)