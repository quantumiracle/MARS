wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/MARS/wandb/run-20221007_045958-34soqlm7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robosumo_RoboSumo-Ant-vs-Ant-v0_nash_ppo_202210070459
wandb: ‚≠êÔ∏è View project at https://wandb.ai/quantumiracle/Pettingzoo_MARS
wandb: üöÄ View run at https://wandb.ai/quantumiracle/Pettingzoo_MARS/runs/34soqlm7
robosumo_RoboSumo-Ant-vs-Ant-v0
default:  {'env_name': 'RoboSumo-Ant-vs-Ant-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': False, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': False, 'wandb_entity': '', 'wandb_project': '', 'wandb_group': '', 'wandb_name': '', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
{'env_name': 'RoboSumo-Ant-vs-Ant-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': '', 'wandb_group': '', 'wandb_name': '', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
args:  {'env_name': 'RoboSumo-Ant-vs-Ant-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': 'Pettingzoo_MARS', 'wandb_group': '202210070459', 'wandb_name': 'robosumo_RoboSumo-Ant-vs-Ant-v0_nash_ppo_202210070459', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
RoboSumo-Ant-vs-Ant-v0 robosumo
record video: interval 1000, length 300
Load RoboSumo-Ant-vs-Ant-v0 environment in type robosumo.
Env observation space: Box(-inf, inf, (120,), float32) action space: Box(-1.0, 1.0, (8,), float32)
random seed: 991
<RecordVideo<mars.env.wrappers.mars_wrappers.RoboSumoWrapper object at 0x7f397840f1d0>>
gaussian_policy 8 Box(-1.0, 1.0, (8,), float32)
[MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=120, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=120, bias=True)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=16, bias=True)
    (2): Tanh()
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=16, bias=True)
    (2): Tanh()
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)] MLP(
  (body): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)
gaussian_policy 8 Box(-1.0, 1.0, (8,), float32)
[MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=120, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=128, bias=True)
    (3): ReLU()
    (4): Linear(in_features=128, out_features=120, bias=True)
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=16, bias=True)
    (2): Tanh()
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): Linear(in_features=128, out_features=16, bias=True)
    (2): Tanh()
  )
)] [MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
), MLP(
  (body): Sequential(
    (0): Linear(in_features=120, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)] MLP(
  (body): Sequential(
    (0): Linear(in_features=240, out_features=128, bias=True)
    (1): ReLU()
    (2): Linear(in_features=128, out_features=1, bias=True)
  )
)
gaussian_policy 8 Box(-1.0, 1.0, (8,), float32)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'RoboSumo-Ant-vs-Ant-v0', 'env_type': 'robosumo', 'num_envs': 1, 'ram': True, 'seed': 'random', 'record_video': True, 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True, 'max_grad_norm': 0.5, 'entropy_coeff': 0.01, 'vf_coeff': 0.5, 'policy_loss_coeff': 0.08}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'save_id': 202210070459, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'wandb_activate': True, 'wandb_entity': 'quantumiracle', 'wandb_project': 'Pettingzoo_MARS', 'wandb_group': '202210070459', 'wandb_name': 'robosumo_RoboSumo-Ant-vs-Ant-v0_nash_ppo_202210070459', 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'feature': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}, 'policy': {'hidden_dim_list': [128], 'hidden_activation': False, 'output_activation': 'Tanh'}, 'value': {'hidden_dim_list': [128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}, 'record_video_interval': 1000}
Save models to : /data/zihan/research/MARS/data/model/202210070459/robosumo_RoboSumo-Ant-vs-Ant-v0_nash_ppo. 
 Save logs to: /data/zihan/research/MARS/data/log/202210070459/robosumo_RoboSumo-Ant-vs-Ant-v0_nash_ppo.
Episode: 1/50000 (0.0020%),                 avg. length: 43.0,                last time consumption/overall running time: 0.30s / 0.30 s
first_0:                 episode reward: -2189.7962,                 loss: 728894.9961
second_0:                 episode reward: 1816.2171,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 52.45,                last time consumption/overall running time: 5.87s / 6.17 s
first_0:                 episode reward: -819.0719,                 loss: 747812.0180
second_0:                 episode reward: 388.2870,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 55.0,                last time consumption/overall running time: 5.85s / 12.02 s
first_0:                 episode reward: 275.6040,                 loss: 913693.0442
second_0:                 episode reward: -722.3436,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 68.85,                last time consumption/overall running time: 6.39s / 18.41 s
first_0:                 episode reward: 448.1464,                 loss: 844251.0407
second_0:                 episode reward: -975.6160,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 53.7,                last time consumption/overall running time: 5.62s / 24.03 s
first_0:                 episode reward: -361.9852,                 loss: 851453.1097
second_0:                 episode reward: -114.8654,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 65.65,                last time consumption/overall running time: 6.41s / 30.44 s
first_0:                 episode reward: -297.0901,                 loss: 865516.8517
second_0:                 episode reward: -300.9560,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 56.95,                last time consumption/overall running time: 5.92s / 36.36 s
first_0:                 episode reward: -1198.3872,                 loss: 931096.1981
second_0:                 episode reward: 647.9518,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 42.7,                last time consumption/overall running time: 5.04s / 41.41 s
first_0:                 episode reward: -215.2788,                 loss: 994273.1527
second_0:                 episode reward: -171.5789,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 76.25,                last time consumption/overall running time: 7.50s / 48.91 s
first_0:                 episode reward: -387.1599,                 loss: 627248.2687
second_0:                 episode reward: -326.8759,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 43.45,                last time consumption/overall running time: 5.55s / 54.46 s
first_0:                 episode reward: 91.0710,                 loss: 848241.5581
second_0:                 episode reward: -502.5570,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 43.7,                last time consumption/overall running time: 5.17s / 59.63 s
first_0:                 episode reward: 595.5435,                 loss: 960235.1438
second_0:                 episode reward: -1017.5694,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 46.45,                last time consumption/overall running time: 5.18s / 64.81 s
first_0:                 episode reward: 10.7880,                 loss: 1015844.3290
second_0:                 episode reward: -406.5145,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 73.5,                last time consumption/overall running time: 6.91s / 71.72 s
first_0:                 episode reward: -143.2893,                 loss: 780149.9933
second_0:                 episode reward: -551.1864,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 66.6,                last time consumption/overall running time: 6.35s / 78.07 s
first_0:                 episode reward: 0.1362,                 loss: 789757.0892
second_0:                 episode reward: -653.2810,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 88.5,                last time consumption/overall running time: 8.00s / 86.07 s
first_0:                 episode reward: 140.1172,                 loss: 687473.2552
second_0:                 episode reward: -978.4511,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 43.0,                last time consumption/overall running time: 4.98s / 91.05 s
first_0:                 episode reward: 6.2059,                 loss: 934983.4309
second_0:                 episode reward: -437.3274,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 77.55,                last time consumption/overall running time: 7.16s / 98.21 s
first_0:                 episode reward: -851.2097,                 loss: 626089.7593
second_0:                 episode reward: 68.7939,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 67.2,                last time consumption/overall running time: 6.54s / 104.76 s
first_0:                 episode reward: -530.1764,                 loss: 676471.4070
second_0:                 episode reward: -173.2907,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 81.4,                last time consumption/overall running time: 7.60s / 112.36 s
first_0:                 episode reward: -125.0899,                 loss: 655421.4871
second_0:                 episode reward: -732.5122,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 64.8,                last time consumption/overall running time: 6.37s / 118.73 s
first_0:                 episode reward: -661.6774,                 loss: 729769.0069
second_0:                 episode reward: -44.3254,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 57.95,                last time consumption/overall running time: 5.84s / 124.57 s
first_0:                 episode reward: -80.8073,                 loss: 759168.1347
second_0:                 episode reward: -517.2204,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 59.0,                last time consumption/overall running time: 6.14s / 130.70 s
first_0:                 episode reward: -585.6530,                 loss: 814891.1232
second_0:                 episode reward: -37.4609,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 85.65,                last time consumption/overall running time: 7.89s / 138.59 s
first_0:                 episode reward: 608.1265,                 loss: 655646.0981
second_0:                 episode reward: -1535.1512,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 60.8,                last time consumption/overall running time: 6.35s / 144.94 s
first_0:                 episode reward: -995.6756,                 loss: 811279.4905
second_0:                 episode reward: 381.9020,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 70.3,                last time consumption/overall running time: 6.95s / 151.90 s
first_0:                 episode reward: -597.1332,                 loss: 693023.4720
second_0:                 episode reward: -170.8672,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 64.75,                last time consumption/overall running time: 6.62s / 158.51 s
first_0:                 episode reward: -1363.6302,                 loss: 629542.9564
second_0:                 episode reward: 683.3572,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 73.6,                last time consumption/overall running time: 7.15s / 165.66 s
first_0:                 episode reward: -100.7319,                 loss: 713104.7867
second_0:                 episode reward: -691.5571,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 57.3,                last time consumption/overall running time: 6.27s / 171.94 s
first_0:                 episode reward: -528.8439,                 loss: 649283.8013
second_0:                 episode reward: -117.6929,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 53.65,                last time consumption/overall running time: 5.81s / 177.74 s
first_0:                 episode reward: -707.0940,                 loss: 836834.7569
second_0:                 episode reward: 84.0636,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 56.7,                last time consumption/overall running time: 5.98s / 183.73 s
first_0:                 episode reward: -718.7055,                 loss: 679553.0168
second_0:                 episode reward: 106.6487,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 58.45,                last time consumption/overall running time: 6.06s / 189.79 s
first_0:                 episode reward: 177.7897,                 loss: 868329.2655
second_0:                 episode reward: -814.7729,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 82.6,                last time consumption/overall running time: 8.03s / 197.81 s
first_0:                 episode reward: -883.6533,                 loss: 571048.4095
second_0:                 episode reward: -44.9105,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 66.1,                last time consumption/overall running time: 6.49s / 204.31 s
first_0:                 episode reward: -807.0199,                 loss: 817957.8530
second_0:                 episode reward: 46.4229,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 70.45,                last time consumption/overall running time: 6.98s / 211.29 s
first_0:                 episode reward: -249.5369,                 loss: 675471.2365
second_0:                 episode reward: -546.4387,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 53.5,                last time consumption/overall running time: 5.61s / 216.90 s
first_0:                 episode reward: -517.4577,                 loss: 880659.6658
second_0:                 episode reward: -94.0059,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 53.0,                last time consumption/overall running time: 5.60s / 222.50 s
first_0:                 episode reward: -685.4662,                 loss: 742843.7014
second_0:                 episode reward: 122.5033,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 78.7,                last time consumption/overall running time: 7.23s / 229.74 s
first_0:                 episode reward: -857.7417,                 loss: 661142.3502
second_0:                 episode reward: -47.9901,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 60.4,                last time consumption/overall running time: 5.97s / 235.71 s
first_0:                 episode reward: -235.7202,                 loss: 903225.2283
second_0:                 episode reward: -438.4309,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 57.1,                last time consumption/overall running time: 5.76s / 241.48 s
first_0:                 episode reward: -937.4325,                 loss: 663812.6344
second_0:                 episode reward: 302.6905,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 47.7,                last time consumption/overall running time: 5.08s / 246.56 s
first_0:                 episode reward: -881.5919,                 loss: 822347.5853
second_0:                 episode reward: 371.6517,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 73.2,                last time consumption/overall running time: 6.84s / 253.40 s
first_0:                 episode reward: -297.8466,                 loss: 730176.8659
second_0:                 episode reward: -514.6485,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 57.75,                last time consumption/overall running time: 5.96s / 259.36 s
first_0:                 episode reward: -14.5367,                 loss: 1029866.6132
second_0:                 episode reward: -659.3937,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 53.55,                last time consumption/overall running time: 5.70s / 265.06 s
first_0:                 episode reward: 300.7996,                 loss: 806131.6514
second_0:                 episode reward: -928.2445,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 65.0,                last time consumption/overall running time: 6.88s / 271.94 s
first_0:                 episode reward: -334.6367,                 loss: 745841.9821
second_0:                 episode reward: -483.3098,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 54.55,                last time consumption/overall running time: 6.05s / 277.99 s
first_0:                 episode reward: -601.7631,                 loss: 706771.9575
second_0:                 episode reward: -25.3887,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 75.55,                last time consumption/overall running time: 7.22s / 285.21 s
first_0:                 episode reward: -461.0170,                 loss: 823405.5444
second_0:                 episode reward: -497.0393,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 70.1,                last time consumption/overall running time: 6.62s / 291.82 s
first_0:                 episode reward: -510.5212,                 loss: 820235.1936
second_0:                 episode reward: -346.3806,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 46.05,                last time consumption/overall running time: 5.09s / 296.92 s