pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
SlimeVolley-v0 slimevolley
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'SlimeVolley-v0', 'env_type': 'slimevolley', 'num_envs': 2, 'ram': True, 'seed': 'random', 'against_baseline': False, 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 32, 'max_episodes': 50000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'Tanh', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [64, 64, 64], 'hidden_activation': 'Tanh', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220128_1512/slimevolley_SlimeVolley-v0_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220128_1512/slimevolley_SlimeVolley-v0_nash_ppo.
Episode: 1/50000 (0.0020%),                 avg. length: 582.0,                last time consumption/overall running time: 3.4399s / 3.4399 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.0403
env0_second_0:                 episode reward: 1.0000,                 loss: -0.0490
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 553.95,                last time consumption/overall running time: 54.8772s / 58.3171 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.0371
env0_second_0:                 episode reward: -0.5500,                 loss: 0.0780
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 625.85,                last time consumption/overall running time: 60.5328s / 118.8499 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.1408
env0_second_0:                 episode reward: 0.6500,                 loss: 0.1783
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 569.15,                last time consumption/overall running time: 55.0642s / 173.9140 s
env0_first_0:                 episode reward: 1.1500,                 loss: 0.1372
env0_second_0:                 episode reward: -1.1500,                 loss: 0.1614
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 585.15,                last time consumption/overall running time: 57.5136s / 231.4276 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.1468
env0_second_0:                 episode reward: 0.6000,                 loss: 0.1565
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 619.45,                last time consumption/overall running time: 59.9152s / 291.3428 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.1466
env0_second_0:                 episode reward: -0.7500,                 loss: 0.1533
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 558.8,                last time consumption/overall running time: 54.3630s / 345.7058 s
env0_first_0:                 episode reward: 1.2500,                 loss: 0.1812
env0_second_0:                 episode reward: -1.2500,                 loss: 0.1848
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 561.9,                last time consumption/overall running time: 55.2495s / 400.9554 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.1871
env0_second_0:                 episode reward: -0.5500,                 loss: 0.1933
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 582.6,                last time consumption/overall running time: 57.2586s / 458.2140 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1746
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1884
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 565.75,                last time consumption/overall running time: 55.2828s / 513.4968 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1643
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1954
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 576.45,                last time consumption/overall running time: 55.9533s / 569.4501 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1727
env0_second_0:                 episode reward: -0.1500,                 loss: 0.1877
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 618.35,                last time consumption/overall running time: 59.9690s / 629.4191 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.1655
env0_second_0:                 episode reward: -0.1500,                 loss: 0.1861
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 551.35,                last time consumption/overall running time: 55.1565s / 684.5757 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1837
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1885
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 599.55,                last time consumption/overall running time: 58.0959s / 742.6716 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.1896
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2107
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 613.0,                last time consumption/overall running time: 59.3300s / 802.0016 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.1968
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2115
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 539.95,                last time consumption/overall running time: 53.6446s / 855.6461 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2053
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2232
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 568.95,                last time consumption/overall running time: 55.8830s / 911.5291 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2193
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2314
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 596.7,                last time consumption/overall running time: 57.9379s / 969.4670 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2271
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2427
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 541.0,                last time consumption/overall running time: 53.8992s / 1023.3662 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2401
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2397
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 560.35,                last time consumption/overall running time: 54.6079s / 1077.9741 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2522
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2640
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 587.25,                last time consumption/overall running time: 57.0440s / 1135.0181 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.2966
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3011
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 616.55,                last time consumption/overall running time: 60.6331s / 1195.6513 s
env0_first_0:                 episode reward: 0.9500,                 loss: 0.2921
env0_second_0:                 episode reward: -0.9500,                 loss: 0.3041
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 564.5,                last time consumption/overall running time: 56.6459s / 1252.2971 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.3113
env0_second_0:                 episode reward: -0.2000,                 loss: 0.3094
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 550.95,                last time consumption/overall running time: 55.4939s / 1307.7911 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2904
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2881
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 581.9,                last time consumption/overall running time: 57.2046s / 1364.9957 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.3092
env0_second_0:                 episode reward: -0.5000,                 loss: 0.3177
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 546.25,                last time consumption/overall running time: 53.8526s / 1418.8483 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.2947
env0_second_0:                 episode reward: 0.7000,                 loss: 0.2955
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 560.3,                last time consumption/overall running time: 55.6201s / 1474.4684 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2846
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2975
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 556.5,                last time consumption/overall running time: 54.6691s / 1529.1375 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.3265
env0_second_0:                 episode reward: 0.6500,                 loss: 0.3337
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 571.05,                last time consumption/overall running time: 55.3745s / 1584.5120 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.3011
env0_second_0:                 episode reward: 0.7000,                 loss: 0.3114
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 557.95,                last time consumption/overall running time: 54.8894s / 1639.4013 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2995
env0_second_0:                 episode reward: -0.4500,                 loss: 0.3129
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 556.3,                last time consumption/overall running time: 54.3057s / 1693.7070 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.3386
env0_second_0:                 episode reward: 0.6000,                 loss: 0.3564
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 579.1,                last time consumption/overall running time: 56.0948s / 1749.8018 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.3616
env0_second_0:                 episode reward: -0.3000,                 loss: 0.3826
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 584.85,                last time consumption/overall running time: 57.6171s / 1807.4189 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.3382
env0_second_0:                 episode reward: 0.2500,                 loss: 0.3478
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 579.55,                last time consumption/overall running time: 56.4845s / 1863.9034 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.3294
env0_second_0:                 episode reward: 0.4000,                 loss: 0.3390
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 547.05,                last time consumption/overall running time: 53.8362s / 1917.7396 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.3136
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3289
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 512.7,                last time consumption/overall running time: 50.7508s / 1968.4904 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.3033
env0_second_0:                 episode reward: 1.1000,                 loss: 0.3232
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 554.35,                last time consumption/overall running time: 54.5088s / 2022.9992 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2586
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2723
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 571.75,                last time consumption/overall running time: 55.6476s / 2078.6468 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2536
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2751
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 557.15,                last time consumption/overall running time: 54.5273s / 2133.1741 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2716
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2794
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 596.6,                last time consumption/overall running time: 58.8149s / 2191.9890 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2546
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2667
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 598.6,                last time consumption/overall running time: 57.8450s / 2249.8340 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2706
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2765
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 537.05,                last time consumption/overall running time: 53.3972s / 2303.2312 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2818
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2818
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 594.6,                last time consumption/overall running time: 58.0197s / 2361.2509 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2827
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2888
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 570.45,                last time consumption/overall running time: 55.7378s / 2416.9887 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2439
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2507
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 605.75,                last time consumption/overall running time: 58.4735s / 2475.4623 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.2982
env0_second_0:                 episode reward: 0.8500,                 loss: 0.3069
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 546.65,                last time consumption/overall running time: 53.3491s / 2528.8113 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2584
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2605
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 574.7,                last time consumption/overall running time: 56.6650s / 2585.4764 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2432
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2571
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 594.0,                last time consumption/overall running time: 58.5329s / 2644.0092 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2621
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2707
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 590.8,                last time consumption/overall running time: 57.5379s / 2701.5471 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2565
env0_second_0:                 episode reward: -0.1500,                 loss: 0.2619
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 620.05,                last time consumption/overall running time: 59.2746s / 2760.8217 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2600
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2701
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 580.35,                last time consumption/overall running time: 56.2255s / 2817.0472 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2511
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2604
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 576.95,                last time consumption/overall running time: 56.7308s / 2873.7780 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2574
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2626
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 599.6,                last time consumption/overall running time: 58.4121s / 2932.1901 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2605
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2744
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 574.2,                last time consumption/overall running time: 57.0271s / 2989.2171 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2569
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2859
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 606.35,                last time consumption/overall running time: 59.2055s / 3048.4227 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2549
env0_second_0:                 episode reward: 0.0000,                 loss: 0.2776
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 580.4,                last time consumption/overall running time: 56.2070s / 3104.6297 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.2654
env0_second_0:                 episode reward: -0.6000,                 loss: 0.2832
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 578.65,                last time consumption/overall running time: 54.9527s / 3159.5824 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2579
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2622
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 554.35,                last time consumption/overall running time: 54.3399s / 3213.9223 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.2646
env0_second_0:                 episode reward: 0.8500,                 loss: 0.2780
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 614.5,                last time consumption/overall running time: 59.9652s / 3273.8874 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2815
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2975
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 570.2,                last time consumption/overall running time: 55.7938s / 3329.6812 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2586
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2907
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 1201/50000 (2.4020%),                 avg. length: 591.3,                last time consumption/overall running time: 57.2196s / 3386.9008 s
env0_first_0:                 episode reward: -1.2500,                 loss: 0.2462
env0_second_0:                 episode reward: 1.2500,                 loss: 0.2640
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 1221/50000 (2.4420%),                 avg. length: 575.9,                last time consumption/overall running time: 55.8656s / 3442.7664 s
env0_first_0:                 episode reward: -1.0000,                 loss: 0.2592
env0_second_0:                 episode reward: 1.0000,                 loss: 0.2713
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1241/50000 (2.4820%),                 avg. length: 564.85,                last time consumption/overall running time: 55.1383s / 3497.9047 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.2793
env0_second_0:                 episode reward: -0.6500,                 loss: 0.2938
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1261/50000 (2.5220%),                 avg. length: 539.05,                last time consumption/overall running time: 53.2819s / 3551.1866 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.3040
env0_second_0:                 episode reward: -0.1500,                 loss: 0.3113
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1281/50000 (2.5620%),                 avg. length: 573.25,                last time consumption/overall running time: 56.4288s / 3607.6154 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.2643
env0_second_0:                 episode reward: -0.9000,                 loss: 0.2851
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1301/50000 (2.6020%),                 avg. length: 588.3,                last time consumption/overall running time: 56.9162s / 3664.5316 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2717
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2803
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 1321/50000 (2.6420%),                 avg. length: 576.9,                last time consumption/overall running time: 56.1429s / 3720.6745 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2296
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2302
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1341/50000 (2.6820%),                 avg. length: 600.8,                last time consumption/overall running time: 57.5440s / 3778.2184 s
env0_first_0:                 episode reward: -0.4500,                 loss: 0.2469
env0_second_0:                 episode reward: 0.4500,                 loss: 0.2477
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 1361/50000 (2.7220%),                 avg. length: 555.55,                last time consumption/overall running time: 53.8404s / 3832.0588 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2718
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2863
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1381/50000 (2.7620%),                 avg. length: 564.65,                last time consumption/overall running time: 55.0081s / 3887.0669 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2647
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2701
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1401/50000 (2.8020%),                 avg. length: 610.35,                last time consumption/overall running time: 59.4561s / 3946.5231 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2572
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2696
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1421/50000 (2.8420%),                 avg. length: 602.5,                last time consumption/overall running time: 58.6200s / 4005.1431 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.2517
env0_second_0:                 episode reward: 0.8500,                 loss: 0.2675
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1441/50000 (2.8820%),                 avg. length: 536.3,                last time consumption/overall running time: 52.4023s / 4057.5454 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2461
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2608
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1461/50000 (2.9220%),                 avg. length: 566.4,                last time consumption/overall running time: 54.7635s / 4112.3089 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2809
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2935
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1481/50000 (2.9620%),                 avg. length: 551.75,                last time consumption/overall running time: 53.8109s / 4166.1198 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2634
env0_second_0:                 episode reward: -0.1500,                 loss: 0.2788
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 1501/50000 (3.0020%),                 avg. length: 575.4,                last time consumption/overall running time: 55.7045s / 4221.8243 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2444
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2609
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1521/50000 (3.0420%),                 avg. length: 588.75,                last time consumption/overall running time: 56.8014s / 4278.6258 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.2952
env0_second_0:                 episode reward: 0.3000,                 loss: 0.2993
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1541/50000 (3.0820%),                 avg. length: 613.55,                last time consumption/overall running time: 59.1507s / 4337.7765 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2732
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2795
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1561/50000 (3.1220%),                 avg. length: 557.15,                last time consumption/overall running time: 54.1271s / 4391.9037 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2886
env0_second_0:                 episode reward: 0.5000,                 loss: 0.2891
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 1581/50000 (3.1620%),                 avg. length: 598.9,                last time consumption/overall running time: 57.1097s / 4449.0134 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.3008
env0_second_0:                 episode reward: -0.1500,                 loss: 0.2952
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1601/50000 (3.2020%),                 avg. length: 604.05,                last time consumption/overall running time: 57.8907s / 4506.9041 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2981
env0_second_0:                 episode reward: -0.1500,                 loss: 0.3107
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 1621/50000 (3.2420%),                 avg. length: 551.65,                last time consumption/overall running time: 55.2177s / 4562.1218 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3210
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3190
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 1641/50000 (3.2820%),                 avg. length: 536.9,                last time consumption/overall running time: 53.8695s / 4615.9913 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.3310
env0_second_0:                 episode reward: 0.2500,                 loss: 0.3326
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1661/50000 (3.3220%),                 avg. length: 598.3,                last time consumption/overall running time: 58.6227s / 4674.6140 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.3483
env0_second_0:                 episode reward: 0.3500,                 loss: 0.3454
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1681/50000 (3.3620%),                 avg. length: 601.65,                last time consumption/overall running time: 59.0706s / 4733.6846 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.3078
env0_second_0:                 episode reward: 0.8000,                 loss: 0.3162
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1701/50000 (3.4020%),                 avg. length: 557.9,                last time consumption/overall running time: 54.7033s / 4788.3879 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.3207
env0_second_0:                 episode reward: -0.2500,                 loss: 0.3160
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1721/50000 (3.4420%),                 avg. length: 575.15,                last time consumption/overall running time: 56.0542s / 4844.4420 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2818
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2815
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 1741/50000 (3.4820%),                 avg. length: 530.4,                last time consumption/overall running time: 52.5731s / 4897.0151 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.3015
env0_second_0:                 episode reward: 1.4000,                 loss: 0.3048
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 1761/50000 (3.5220%),                 avg. length: 574.15,                last time consumption/overall running time: 56.2615s / 4953.2766 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2971
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2934
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 1781/50000 (3.5620%),                 avg. length: 552.65,                last time consumption/overall running time: 54.3146s / 5007.5913 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.2968
env0_second_0:                 episode reward: -0.5000,                 loss: 0.3002
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 1801/50000 (3.6020%),                 avg. length: 540.45,                last time consumption/overall running time: 52.8459s / 5060.4372 s
env0_first_0:                 episode reward: 1.0000,                 loss: 0.2959
env0_second_0:                 episode reward: -1.0000,                 loss: 0.2989
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 1821/50000 (3.6420%),                 avg. length: 499.15,                last time consumption/overall running time: 49.4670s / 5109.9042 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.3169
env0_second_0:                 episode reward: 0.3000,                 loss: 0.3156
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 1841/50000 (3.6820%),                 avg. length: 563.5,                last time consumption/overall running time: 54.3282s / 5164.2324 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.3084
env0_second_0:                 episode reward: 0.3000,                 loss: 0.3101
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1861/50000 (3.7220%),                 avg. length: 557.85,                last time consumption/overall running time: 54.6677s / 5218.9001 s
env0_first_0:                 episode reward: 0.6500,                 loss: 0.2990
env0_second_0:                 episode reward: -0.6500,                 loss: 0.2886
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 1881/50000 (3.7620%),                 avg. length: 592.9,                last time consumption/overall running time: 57.5875s / 5276.4876 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2838
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2787
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1901/50000 (3.8020%),                 avg. length: 591.7,                last time consumption/overall running time: 57.0508s / 5333.5384 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2561
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2492
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1921/50000 (3.8420%),                 avg. length: 582.95,                last time consumption/overall running time: 55.8049s / 5389.3433 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2228
env0_second_0:                 episode reward: 0.0000,                 loss: 0.2183
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1941/50000 (3.8820%),                 avg. length: 591.05,                last time consumption/overall running time: 57.6606s / 5447.0039 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2932
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2850
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1961/50000 (3.9220%),                 avg. length: 559.45,                last time consumption/overall running time: 54.2992s / 5501.3031 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2982
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2982
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1981/50000 (3.9620%),                 avg. length: 577.7,                last time consumption/overall running time: 56.5477s / 5557.8507 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2902
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2901
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2001/50000 (4.0020%),                 avg. length: 574.4,                last time consumption/overall running time: 56.4018s / 5614.2525 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.3164
env0_second_0:                 episode reward: -0.5000,                 loss: 0.3224
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 2021/50000 (4.0420%),                 avg. length: 580.35,                last time consumption/overall running time: 57.1251s / 5671.3776 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.3079
env0_second_0:                 episode reward: 1.0500,                 loss: 0.3087
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 2041/50000 (4.0820%),                 avg. length: 565.4,                last time consumption/overall running time: 54.5110s / 5725.8886 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.3140
env0_second_0:                 episode reward: -0.6000,                 loss: 0.3137
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2061/50000 (4.1220%),                 avg. length: 569.05,                last time consumption/overall running time: 55.0943s / 5780.9829 s
env0_first_0:                 episode reward: -1.7500,                 loss: 0.2543
env0_second_0:                 episode reward: 1.7500,                 loss: 0.2462
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2081/50000 (4.1620%),                 avg. length: 584.85,                last time consumption/overall running time: 57.2202s / 5838.2031 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2960
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2932
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 2101/50000 (4.2020%),                 avg. length: 586.85,                last time consumption/overall running time: 57.7315s / 5895.9346 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.2931
env0_second_0:                 episode reward: -0.3500,                 loss: 0.2957
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 2121/50000 (4.2420%),                 avg. length: 584.25,                last time consumption/overall running time: 57.0654s / 5952.9999 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2805
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2831
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2141/50000 (4.2820%),                 avg. length: 588.25,                last time consumption/overall running time: 56.6110s / 6009.6110 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.2837
env0_second_0:                 episode reward: 0.8500,                 loss: 0.2801
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 2161/50000 (4.3220%),                 avg. length: 582.95,                last time consumption/overall running time: 56.7701s / 6066.3811 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2945
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2927
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 2181/50000 (4.3620%),                 avg. length: 549.6,                last time consumption/overall running time: 53.6563s / 6120.0374 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.3120
env0_second_0:                 episode reward: -0.2000,                 loss: 0.3118
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2201/50000 (4.4020%),                 avg. length: 578.85,                last time consumption/overall running time: 56.6032s / 6176.6406 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2964
env0_second_0:                 episode reward: -0.4500,                 loss: 0.3059
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2221/50000 (4.4420%),                 avg. length: 586.75,                last time consumption/overall running time: 57.6951s / 6234.3357 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.3011
env0_second_0:                 episode reward: 0.8000,                 loss: 0.2913
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2241/50000 (4.4820%),                 avg. length: 573.6,                last time consumption/overall running time: 57.2699s / 6291.6056 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3008
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2987
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2261/50000 (4.5220%),                 avg. length: 561.45,                last time consumption/overall running time: 55.9399s / 6347.5456 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2871
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2872
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2281/50000 (4.5620%),                 avg. length: 569.55,                last time consumption/overall running time: 55.6654s / 6403.2109 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2860
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2963
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 2301/50000 (4.6020%),                 avg. length: 572.55,                last time consumption/overall running time: 55.4356s / 6458.6465 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2751
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2768
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 2321/50000 (4.6420%),                 avg. length: 549.8,                last time consumption/overall running time: 54.9480s / 6513.5945 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2964
env0_second_0:                 episode reward: 0.5000,                 loss: 0.2903
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2341/50000 (4.6820%),                 avg. length: 547.85,                last time consumption/overall running time: 54.4463s / 6568.0409 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2898
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2895
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 2361/50000 (4.7220%),                 avg. length: 559.95,                last time consumption/overall running time: 54.8299s / 6622.8707 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3020
env0_second_0:                 episode reward: -0.1000,                 loss: 0.2936
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 2381/50000 (4.7620%),                 avg. length: 597.55,                last time consumption/overall running time: 57.8131s / 6680.6838 s
env0_first_0:                 episode reward: 0.1500,                 loss: 0.2934
env0_second_0:                 episode reward: -0.1500,                 loss: 0.3009
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 2401/50000 (4.8020%),                 avg. length: 552.3,                last time consumption/overall running time: 54.3121s / 6734.9959 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2988
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2968
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 2421/50000 (4.8420%),                 avg. length: 541.95,                last time consumption/overall running time: 53.6076s / 6788.6035 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2976
env0_second_0:                 episode reward: 0.5000,                 loss: 0.2967
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2441/50000 (4.8820%),                 avg. length: 568.6,                last time consumption/overall running time: 55.5361s / 6844.1396 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3120
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3098
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2461/50000 (4.9220%),                 avg. length: 569.25,                last time consumption/overall running time: 55.6305s / 6899.7701 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.3136
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3103
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 2481/50000 (4.9620%),                 avg. length: 583.6,                last time consumption/overall running time: 56.9135s / 6956.6835 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2795
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2850
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 2501/50000 (5.0020%),                 avg. length: 542.35,                last time consumption/overall running time: 53.2145s / 7009.8980 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.3164
env0_second_0:                 episode reward: 0.8500,                 loss: 0.3160
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2521/50000 (5.0420%),                 avg. length: 588.45,                last time consumption/overall running time: 56.6468s / 7066.5448 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.3095
env0_second_0:                 episode reward: -0.3000,                 loss: 0.3101
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 2541/50000 (5.0820%),                 avg. length: 542.5,                last time consumption/overall running time: 53.4956s / 7120.0405 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.3011
env0_second_0:                 episode reward: -0.2000,                 loss: 0.3003
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 2561/50000 (5.1220%),                 avg. length: 577.45,                last time consumption/overall running time: 56.9616s / 7177.0021 s
env0_first_0:                 episode reward: 0.7500,                 loss: 0.2898
env0_second_0:                 episode reward: -0.7500,                 loss: 0.2935
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 2581/50000 (5.1620%),                 avg. length: 562.75,                last time consumption/overall running time: 55.3251s / 7232.3271 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2875
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2929
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 2601/50000 (5.2020%),                 avg. length: 568.85,                last time consumption/overall running time: 54.9800s / 7287.3071 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.3181
env0_second_0:                 episode reward: 0.0000,                 loss: 0.3173
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2621/50000 (5.2420%),                 avg. length: 539.4,                last time consumption/overall running time: 52.9147s / 7340.2218 s
env0_first_0:                 episode reward: 0.2000,                 loss: 0.2945
env0_second_0:                 episode reward: -0.2000,                 loss: 0.2966
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2641/50000 (5.2820%),                 avg. length: 616.9,                last time consumption/overall running time: 59.5849s / 7399.8067 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2702
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2761
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2661/50000 (5.3220%),                 avg. length: 623.85,                last time consumption/overall running time: 59.7223s / 7459.5290 s
env0_first_0:                 episode reward: 0.6000,                 loss: 0.2274
env0_second_0:                 episode reward: -0.6000,                 loss: 0.2368
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 2681/50000 (5.3620%),                 avg. length: 621.25,                last time consumption/overall running time: 60.1964s / 7519.7255 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.2546
env0_second_0:                 episode reward: -0.5000,                 loss: 0.2640
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2701/50000 (5.4020%),                 avg. length: 594.45,                last time consumption/overall running time: 57.9147s / 7577.6402 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2486
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2634
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 2721/50000 (5.4420%),                 avg. length: 609.1,                last time consumption/overall running time: 59.7478s / 7637.3880 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2608
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2712
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 2741/50000 (5.4820%),                 avg. length: 570.55,                last time consumption/overall running time: 55.9405s / 7693.3284 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.2447
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2554
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 2761/50000 (5.5220%),                 avg. length: 541.25,                last time consumption/overall running time: 53.5373s / 7746.8658 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2653
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2655
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 2781/50000 (5.5620%),                 avg. length: 581.3,                last time consumption/overall running time: 57.1203s / 7803.9861 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3134
env0_second_0:                 episode reward: 0.1000,                 loss: 0.3198
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 2801/50000 (5.6020%),                 avg. length: 621.4,                last time consumption/overall running time: 60.1115s / 7864.0976 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2988
env0_second_0:                 episode reward: 0.0000,                 loss: 0.3035
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 2821/50000 (5.6420%),                 avg. length: 552.7,                last time consumption/overall running time: 54.5685s / 7918.6662 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.2818
env0_second_0:                 episode reward: 0.8000,                 loss: 0.2929
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2841/50000 (5.6820%),                 avg. length: 543.5,                last time consumption/overall running time: 54.3386s / 7973.0048 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.2802
env0_second_0:                 episode reward: 0.6000,                 loss: 0.2929
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 2861/50000 (5.7220%),                 avg. length: 578.95,                last time consumption/overall running time: 58.0834s / 8031.0882 s
env0_first_0:                 episode reward: -0.6500,                 loss: 0.2837
env0_second_0:                 episode reward: 0.6500,                 loss: 0.2952
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 2881/50000 (5.7620%),                 avg. length: 561.25,                last time consumption/overall running time: 55.9064s / 8086.9946 s
env0_first_0:                 episode reward: 0.9000,                 loss: 0.2964
env0_second_0:                 episode reward: -0.9000,                 loss: 0.2931
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 2901/50000 (5.8020%),                 avg. length: 578.95,                last time consumption/overall running time: 56.5851s / 8143.5798 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.3008
env0_second_0:                 episode reward: -0.5000,                 loss: 0.3055
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 2921/50000 (5.8420%),                 avg. length: 548.25,                last time consumption/overall running time: 54.4544s / 8198.0341 s
env0_first_0:                 episode reward: 0.1000,                 loss: 0.2969
env0_second_0:                 episode reward: -0.1000,                 loss: 0.3064
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2941/50000 (5.8820%),                 avg. length: 545.0,                last time consumption/overall running time: 54.0041s / 8252.0383 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2810
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2892
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 2961/50000 (5.9220%),                 avg. length: 588.1,                last time consumption/overall running time: 57.2619s / 8309.3001 s
env0_first_0:                 episode reward: 0.3500,                 loss: 0.3070
env0_second_0:                 episode reward: -0.3500,                 loss: 0.3092
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 2981/50000 (5.9620%),                 avg. length: 607.3,                last time consumption/overall running time: 58.3068s / 8367.6070 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2794
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2901
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3001/50000 (6.0020%),                 avg. length: 574.05,                last time consumption/overall running time: 56.0096s / 8423.6166 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.2974
env0_second_0:                 episode reward: 1.1000,                 loss: 0.3033
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3021/50000 (6.0420%),                 avg. length: 545.95,                last time consumption/overall running time: 52.9692s / 8476.5858 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.3034
env0_second_0:                 episode reward: -0.4000,                 loss: 0.3088
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 3041/50000 (6.0820%),                 avg. length: 544.95,                last time consumption/overall running time: 52.7372s / 8529.3230 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.3229
env0_second_0:                 episode reward: -0.4000,                 loss: 0.3307
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3061/50000 (6.1220%),                 avg. length: 596.15,                last time consumption/overall running time: 57.9977s / 8587.3207 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3231
env0_second_0:                 episode reward: 0.1000,                 loss: 0.3301
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 3081/50000 (6.1620%),                 avg. length: 563.2,                last time consumption/overall running time: 55.2953s / 8642.6160 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.2609
env0_second_0:                 episode reward: 1.1000,                 loss: 0.2692
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 3101/50000 (6.2020%),                 avg. length: 611.4,                last time consumption/overall running time: 59.3813s / 8701.9973 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.3004
env0_second_0:                 episode reward: 0.8000,                 loss: 0.3073
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3121/50000 (6.2420%),                 avg. length: 555.45,                last time consumption/overall running time: 54.4011s / 8756.3984 s
env0_first_0:                 episode reward: 0.4000,                 loss: 0.2771
env0_second_0:                 episode reward: -0.4000,                 loss: 0.2881
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3141/50000 (6.2820%),                 avg. length: 529.3,                last time consumption/overall running time: 51.8912s / 8808.2896 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.2565
env0_second_0:                 episode reward: 0.7500,                 loss: 0.2663
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3161/50000 (6.3220%),                 avg. length: 587.85,                last time consumption/overall running time: 56.3071s / 8864.5967 s
env0_first_0:                 episode reward: -0.1500,                 loss: 0.2732
env0_second_0:                 episode reward: 0.1500,                 loss: 0.2815
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3181/50000 (6.3620%),                 avg. length: 581.65,                last time consumption/overall running time: 56.0298s / 8920.6265 s
env0_first_0:                 episode reward: -0.5500,                 loss: 0.2380
env0_second_0:                 episode reward: 0.5500,                 loss: 0.2526
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3201/50000 (6.4020%),                 avg. length: 581.6,                last time consumption/overall running time: 56.1614s / 8976.7880 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.2672
env0_second_0:                 episode reward: 0.1000,                 loss: 0.2818
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3221/50000 (6.4420%),                 avg. length: 583.1,                last time consumption/overall running time: 57.0693s / 9033.8573 s
env0_first_0:                 episode reward: -0.2000,                 loss: 0.2415
env0_second_0:                 episode reward: 0.2000,                 loss: 0.2509
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3241/50000 (6.4820%),                 avg. length: 621.2,                last time consumption/overall running time: 60.0845s / 9093.9418 s
env0_first_0:                 episode reward: -0.0500,                 loss: 0.2229
env0_second_0:                 episode reward: 0.0500,                 loss: 0.2232
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 3261/50000 (6.5220%),                 avg. length: 565.9,                last time consumption/overall running time: 55.4576s / 9149.3995 s
env0_first_0:                 episode reward: 0.5500,                 loss: 0.2680
env0_second_0:                 episode reward: -0.5500,                 loss: 0.2710
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3281/50000 (6.5620%),                 avg. length: 619.7,                last time consumption/overall running time: 60.1631s / 9209.5626 s
env0_first_0:                 episode reward: 0.3000,                 loss: 0.2720
env0_second_0:                 episode reward: -0.3000,                 loss: 0.2779
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 3301/50000 (6.6020%),                 avg. length: 621.9,                last time consumption/overall running time: 61.5239s / 9271.0865 s
env0_first_0:                 episode reward: 0.2500,                 loss: 0.2577
env0_second_0:                 episode reward: -0.2500,                 loss: 0.2669
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 3321/50000 (6.6420%),                 avg. length: 606.25,                last time consumption/overall running time: 59.6112s / 9330.6977 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.2705
env0_second_0:                 episode reward: 0.2500,                 loss: 0.2803
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3341/50000 (6.6820%),                 avg. length: 638.0,                last time consumption/overall running time: 61.5848s / 9392.2824 s
env0_first_0:                 episode reward: -0.8000,                 loss: 0.2111
env0_second_0:                 episode reward: 0.8000,                 loss: 0.2119
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3361/50000 (6.7220%),                 avg. length: 570.25,                last time consumption/overall running time: 56.3838s / 9448.6662 s
env0_first_0:                 episode reward: 0.5000,                 loss: 0.2531
env0_second_0:                 episode reward: -0.5000,                 loss: 0.2586
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3381/50000 (6.7620%),                 avg. length: 647.5,                last time consumption/overall running time: 63.1622s / 9511.8284 s
env0_first_0:                 episode reward: 0.4500,                 loss: 0.2170
env0_second_0:                 episode reward: -0.4500,                 loss: 0.2215
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3401/50000 (6.8020%),                 avg. length: 621.85,                last time consumption/overall running time: 60.4479s / 9572.2764 s
env0_first_0:                 episode reward: 0.8000,                 loss: 0.2515
env0_second_0:                 episode reward: -0.8000,                 loss: 0.2568
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 3421/50000 (6.8420%),                 avg. length: 584.4,                last time consumption/overall running time: 56.5885s / 9628.8649 s
env0_first_0:                 episode reward: -1.0500,                 loss: 0.2680
env0_second_0:                 episode reward: 1.0500,                 loss: 0.2723
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3441/50000 (6.8820%),                 avg. length: 657.5,                last time consumption/overall running time: 63.5561s / 9692.4210 s
env0_first_0:                 episode reward: 0.0000,                 loss: 0.2188
env0_second_0:                 episode reward: 0.0000,                 loss: 0.2211
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3461/50000 (6.9220%),                 avg. length: 630.25,                last time consumption/overall running time: 60.9292s / 9753.3502 s
env0_first_0:                 episode reward: 0.0500,                 loss: 0.2195
env0_second_0:                 episode reward: -0.0500,                 loss: 0.2306
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3481/50000 (6.9620%),                 avg. length: 634.8,                last time consumption/overall running time: 60.7117s / 9814.0619 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1860
env0_second_0:                 episode reward: 0.3000,                 loss: 0.1948
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 3501/50000 (7.0020%),                 avg. length: 622.0,                last time consumption/overall running time: 60.1822s / 9874.2442 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1824
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1826
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3521/50000 (7.0420%),                 avg. length: 643.0,                last time consumption/overall running time: 62.4663s / 9936.7105 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.1942
env0_second_0:                 episode reward: 0.6000,                 loss: 0.2028
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 3541/50000 (7.0820%),                 avg. length: 636.15,                last time consumption/overall running time: 61.6349s / 9998.3454 s
env0_first_0:                 episode reward: -0.2500,                 loss: 0.1852
env0_second_0:                 episode reward: 0.2500,                 loss: 0.1903
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 3561/50000 (7.1220%),                 avg. length: 570.75,                last time consumption/overall running time: 55.3069s / 10053.6522 s
env0_first_0:                 episode reward: -0.7500,                 loss: 0.2130
env0_second_0:                 episode reward: 0.7500,                 loss: 0.2135
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 3581/50000 (7.1620%),                 avg. length: 637.95,                last time consumption/overall running time: 62.4706s / 10116.1228 s
env0_first_0:                 episode reward: -0.3500,                 loss: 0.1924
env0_second_0:                 episode reward: 0.3500,                 loss: 0.2058
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3601/50000 (7.2020%),                 avg. length: 595.9,                last time consumption/overall running time: 57.6756s / 10173.7984 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.2129
env0_second_0:                 episode reward: 1.3500,                 loss: 0.2161
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3621/50000 (7.2420%),                 avg. length: 623.85,                last time consumption/overall running time: 59.7348s / 10233.5332 s
env0_first_0:                 episode reward: -1.4000,                 loss: 0.1896
env0_second_0:                 episode reward: 1.4000,                 loss: 0.1970
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 3641/50000 (7.2820%),                 avg. length: 636.7,                last time consumption/overall running time: 61.3238s / 10294.8570 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1802
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1823
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 3661/50000 (7.3220%),                 avg. length: 641.65,                last time consumption/overall running time: 63.6540s / 10358.5110 s
env0_first_0:                 episode reward: -0.5000,                 loss: 0.2092
env0_second_0:                 episode reward: 0.5000,                 loss: 0.2180
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3681/50000 (7.3620%),                 avg. length: 635.05,                last time consumption/overall running time: 62.3935s / 10420.9046 s
env0_first_0:                 episode reward: -0.3000,                 loss: 0.1862
env0_second_0:                 episode reward: 0.3000,                 loss: 0.1923
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 3701/50000 (7.4020%),                 avg. length: 655.6,                last time consumption/overall running time: 62.6325s / 10483.5370 s
env0_first_0:                 episode reward: -0.4000,                 loss: 0.1913
env0_second_0:                 episode reward: 0.4000,                 loss: 0.1882
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 3721/50000 (7.4420%),                 avg. length: 619.7,                last time consumption/overall running time: 60.0145s / 10543.5516 s
env0_first_0:                 episode reward: -1.1000,                 loss: 0.1858
env0_second_0:                 episode reward: 1.1000,                 loss: 0.1984
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 3741/50000 (7.4820%),                 avg. length: 709.95,                last time consumption/overall running time: 66.1963s / 10609.7478 s
env0_first_0:                 episode reward: -0.6000,                 loss: 0.1606
env0_second_0:                 episode reward: 0.6000,                 loss: 0.1788
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 3761/50000 (7.5220%),                 avg. length: 629.75,                last time consumption/overall running time: 60.9902s / 10670.7380 s
env0_first_0:                 episode reward: -0.8500,                 loss: 0.1811
env0_second_0:                 episode reward: 0.8500,                 loss: 0.1922
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 3781/50000 (7.5620%),                 avg. length: 659.25,                last time consumption/overall running time: 63.6386s / 10734.3766 s
env0_first_0:                 episode reward: -1.3000,                 loss: 0.1831
env0_second_0:                 episode reward: 1.3000,                 loss: 0.2030
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3801/50000 (7.6020%),                 avg. length: 644.15,                last time consumption/overall running time: 61.9020s / 10796.2786 s
env0_first_0:                 episode reward: -0.9000,                 loss: 0.1598
env0_second_0:                 episode reward: 0.9000,                 loss: 0.1766
env1_first_0:                 episode reward: -1.9500,                 loss: nan
env1_second_0:                 episode reward: 1.9500,                 loss: nan
Episode: 3821/50000 (7.6420%),                 avg. length: 614.45,                last time consumption/overall running time: 59.9037s / 10856.1824 s
env0_first_0:                 episode reward: -1.5500,                 loss: 0.1586
env0_second_0:                 episode reward: 1.5500,                 loss: 0.1645
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 3841/50000 (7.6820%),                 avg. length: 664.45,                last time consumption/overall running time: 64.3424s / 10920.5248 s
env0_first_0:                 episode reward: -1.7000,                 loss: 0.1657
env0_second_0:                 episode reward: 1.7000,                 loss: 0.1737
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 3861/50000 (7.7220%),                 avg. length: 687.75,                last time consumption/overall running time: 65.6182s / 10986.1430 s
env0_first_0:                 episode reward: -0.7000,                 loss: 0.1843
env0_second_0:                 episode reward: 0.7000,                 loss: 0.1963
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 3881/50000 (7.7620%),                 avg. length: 642.6,                last time consumption/overall running time: 61.4644s / 11047.6074 s
env0_first_0:                 episode reward: -1.6000,                 loss: 0.1521
env0_second_0:                 episode reward: 1.6000,                 loss: 0.1589
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3901/50000 (7.8020%),                 avg. length: 694.35,                last time consumption/overall running time: 66.2074s / 11113.8148 s
env0_first_0:                 episode reward: -1.8000,                 loss: 0.1273
env0_second_0:                 episode reward: 1.8000,                 loss: 0.1364
env1_first_0:                 episode reward: -1.8500,                 loss: nan
env1_second_0:                 episode reward: 1.8500,                 loss: nan
Episode: 3921/50000 (7.8420%),                 avg. length: 708.9,                last time consumption/overall running time: 67.4726s / 11181.2874 s
env0_first_0:                 episode reward: -2.0000,                 loss: 0.1351
env0_second_0:                 episode reward: 2.0000,                 loss: 0.1440
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3941/50000 (7.8820%),                 avg. length: 693.25,                last time consumption/overall running time: 65.7950s / 11247.0824 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1537
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1630
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 3961/50000 (7.9220%),                 avg. length: 614.4,                last time consumption/overall running time: 59.5960s / 11306.6784 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1360
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1450
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 3981/50000 (7.9620%),                 avg. length: 690.45,                last time consumption/overall running time: 65.4598s / 11372.1382 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.1392
env0_second_0:                 episode reward: 2.4000,                 loss: 0.1481
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 4001/50000 (8.0020%),                 avg. length: 615.55,                last time consumption/overall running time: 59.2152s / 11431.3534 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.1281
env0_second_0:                 episode reward: 2.6500,                 loss: 0.1433
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4021/50000 (8.0420%),                 avg. length: 649.45,                last time consumption/overall running time: 62.1847s / 11493.5381 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.1234
env0_second_0:                 episode reward: 2.3500,                 loss: 0.1485
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 4041/50000 (8.0820%),                 avg. length: 604.85,                last time consumption/overall running time: 57.7648s / 11551.3029 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1291
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1495
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 4061/50000 (8.1220%),                 avg. length: 651.3,                last time consumption/overall running time: 62.1393s / 11613.4422 s
env0_first_0:                 episode reward: -2.2000,                 loss: 0.1347
env0_second_0:                 episode reward: 2.2000,                 loss: 0.1508
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4081/50000 (8.1620%),                 avg. length: 626.1,                last time consumption/overall running time: 59.3578s / 11672.8000 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.1331
env0_second_0:                 episode reward: 2.1000,                 loss: 0.1466
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4101/50000 (8.2020%),                 avg. length: 642.25,                last time consumption/overall running time: 60.6041s / 11733.4041 s
env0_first_0:                 episode reward: -1.1500,                 loss: 0.1979
env0_second_0:                 episode reward: 1.1500,                 loss: 0.2203
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 4121/50000 (8.2420%),                 avg. length: 605.9,                last time consumption/overall running time: 57.7371s / 11791.1412 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.1548
env0_second_0:                 episode reward: 2.7000,                 loss: 0.1565
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 4141/50000 (8.2820%),                 avg. length: 608.9,                last time consumption/overall running time: 58.6871s / 11849.8282 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1645
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1829
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 4161/50000 (8.3220%),                 avg. length: 698.25,                last time consumption/overall running time: 65.6297s / 11915.4579 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.1567
env0_second_0:                 episode reward: 2.5500,                 loss: 0.1763
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 4181/50000 (8.3620%),                 avg. length: 646.4,                last time consumption/overall running time: 61.5704s / 11977.0283 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.1279
env0_second_0:                 episode reward: 2.6500,                 loss: 0.1388
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4201/50000 (8.4020%),                 avg. length: 623.1,                last time consumption/overall running time: 60.3879s / 12037.4162 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1539
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1747
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 4221/50000 (8.4420%),                 avg. length: 595.25,                last time consumption/overall running time: 57.5720s / 12094.9883 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.1626
env0_second_0:                 episode reward: 1.9500,                 loss: 0.1822
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 4241/50000 (8.4820%),                 avg. length: 681.35,                last time consumption/overall running time: 64.7841s / 12159.7724 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.2036
env0_second_0:                 episode reward: 2.3500,                 loss: 0.2161
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 4261/50000 (8.5220%),                 avg. length: 639.05,                last time consumption/overall running time: 61.7276s / 12221.5000 s
env0_first_0:                 episode reward: -2.3000,                 loss: 0.1391
env0_second_0:                 episode reward: 2.3000,                 loss: 0.1567
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 4281/50000 (8.5620%),                 avg. length: 629.05,                last time consumption/overall running time: 60.2386s / 12281.7385 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1193
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1407
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4301/50000 (8.6020%),                 avg. length: 607.9,                last time consumption/overall running time: 57.7585s / 12339.4970 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1068
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1167
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4321/50000 (8.6420%),                 avg. length: 606.35,                last time consumption/overall running time: 58.7216s / 12398.2186 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1122
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1278
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 4341/50000 (8.6820%),                 avg. length: 643.2,                last time consumption/overall running time: 61.6329s / 12459.8515 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1126
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1252
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 4361/50000 (8.7220%),                 avg. length: 628.1,                last time consumption/overall running time: 60.4218s / 12520.2733 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1276
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1467
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 4381/50000 (8.7620%),                 avg. length: 596.55,                last time consumption/overall running time: 57.6752s / 12577.9485 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0981
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1000
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4401/50000 (8.8020%),                 avg. length: 657.65,                last time consumption/overall running time: 62.7693s / 12640.7178 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1228
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1436
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4421/50000 (8.8420%),                 avg. length: 617.1,                last time consumption/overall running time: 59.3228s / 12700.0406 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0992
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1240
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 4441/50000 (8.8820%),                 avg. length: 669.95,                last time consumption/overall running time: 63.3108s / 12763.3514 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.0998
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1053
env1_first_0:                 episode reward: -2.6000,                 loss: nan
env1_second_0:                 episode reward: 2.6000,                 loss: nan
Episode: 4461/50000 (8.9220%),                 avg. length: 636.8,                last time consumption/overall running time: 60.8938s / 12824.2452 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1379
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1786
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 4481/50000 (8.9620%),                 avg. length: 594.3,                last time consumption/overall running time: 57.4165s / 12881.6616 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1202
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1457
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 4501/50000 (9.0020%),                 avg. length: 687.2,                last time consumption/overall running time: 63.8306s / 12945.4923 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1082
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1275
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 4521/50000 (9.0420%),                 avg. length: 610.3,                last time consumption/overall running time: 58.3126s / 13003.8048 s
env0_first_0:                 episode reward: -2.1000,                 loss: 0.1035
env0_second_0:                 episode reward: 2.1000,                 loss: 0.1377
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 4541/50000 (9.0820%),                 avg. length: 606.1,                last time consumption/overall running time: 58.2768s / 13062.0816 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1262
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1474
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 4561/50000 (9.1220%),                 avg. length: 548.4,                last time consumption/overall running time: 53.8842s / 13115.9658 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1828
env0_second_0:                 episode reward: 3.0000,                 loss: 0.2137
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 4581/50000 (9.1620%),                 avg. length: 611.3,                last time consumption/overall running time: 58.6891s / 13174.6549 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1055
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1425
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4601/50000 (9.2020%),                 avg. length: 564.45,                last time consumption/overall running time: 53.9992s / 13228.6541 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1269
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1695
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 4621/50000 (9.2420%),                 avg. length: 610.9,                last time consumption/overall running time: 58.5945s / 13287.2487 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1583
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1993
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 4641/50000 (9.2820%),                 avg. length: 552.9,                last time consumption/overall running time: 52.9577s / 13340.2064 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1650
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1942
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 4661/50000 (9.3220%),                 avg. length: 560.45,                last time consumption/overall running time: 53.7473s / 13393.9536 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.1328
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1635
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4681/50000 (9.3620%),                 avg. length: 591.0,                last time consumption/overall running time: 57.2341s / 13451.1878 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1601
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1735
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 4701/50000 (9.4020%),                 avg. length: 579.55,                last time consumption/overall running time: 56.4079s / 13507.5957 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1430
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1595
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 4721/50000 (9.4420%),                 avg. length: 595.6,                last time consumption/overall running time: 58.0240s / 13565.6197 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1281
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1730
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 4741/50000 (9.4820%),                 avg. length: 615.0,                last time consumption/overall running time: 58.8330s / 13624.4527 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1703
env0_second_0:                 episode reward: 3.4000,                 loss: 0.2098
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 4761/50000 (9.5220%),                 avg. length: 604.2,                last time consumption/overall running time: 58.0555s / 13682.5082 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1480
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1760
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 4781/50000 (9.5620%),                 avg. length: 585.7,                last time consumption/overall running time: 56.2573s / 13738.7655 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1086
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1348
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 4801/50000 (9.6020%),                 avg. length: 534.85,                last time consumption/overall running time: 52.5279s / 13791.2934 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1446
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1727
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 4821/50000 (9.6420%),                 avg. length: 559.05,                last time consumption/overall running time: 54.0872s / 13845.3806 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1427
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1708
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 4841/50000 (9.6820%),                 avg. length: 576.2,                last time consumption/overall running time: 55.8827s / 13901.2633 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1418
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1657
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 4861/50000 (9.7220%),                 avg. length: 529.85,                last time consumption/overall running time: 51.7580s / 13953.0213 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1650
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1907
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 4881/50000 (9.7620%),                 avg. length: 581.45,                last time consumption/overall running time: 56.2918s / 14009.3131 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1338
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1544
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4901/50000 (9.8020%),                 avg. length: 571.6,                last time consumption/overall running time: 55.0677s / 14064.3808 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1574
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1731
env1_first_0:                 episode reward: -4.1500,                 loss: nan
env1_second_0:                 episode reward: 4.1500,                 loss: nan
Episode: 4921/50000 (9.8420%),                 avg. length: 589.5,                last time consumption/overall running time: 56.5678s / 14120.9486 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0959
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1192
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 4941/50000 (9.8820%),                 avg. length: 587.85,                last time consumption/overall running time: 57.7205s / 14178.6690 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1266
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1480
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 4961/50000 (9.9220%),                 avg. length: 670.65,                last time consumption/overall running time: 63.5955s / 14242.2646 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1357
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1626
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 4981/50000 (9.9620%),                 avg. length: 605.45,                last time consumption/overall running time: 57.7945s / 14300.0591 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1286
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1692
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5001/50000 (10.0020%),                 avg. length: 645.25,                last time consumption/overall running time: 61.2240s / 14361.2831 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1248
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1512
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5021/50000 (10.0420%),                 avg. length: 564.25,                last time consumption/overall running time: 54.6149s / 14415.8980 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1102
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1444
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5041/50000 (10.0820%),                 avg. length: 603.0,                last time consumption/overall running time: 58.3563s / 14474.2543 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1319
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1721
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5061/50000 (10.1220%),                 avg. length: 604.55,                last time consumption/overall running time: 57.6485s / 14531.9028 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1371
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1760
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 5081/50000 (10.1620%),                 avg. length: 587.25,                last time consumption/overall running time: 56.2645s / 14588.1673 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1404
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1869
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5101/50000 (10.2020%),                 avg. length: 608.2,                last time consumption/overall running time: 58.8735s / 14647.0408 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1147
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1406
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5121/50000 (10.2420%),                 avg. length: 580.9,                last time consumption/overall running time: 57.2546s / 14704.2954 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.1258
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1625
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 5141/50000 (10.2820%),                 avg. length: 584.75,                last time consumption/overall running time: 56.6955s / 14760.9910 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1198
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1481
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5161/50000 (10.3220%),                 avg. length: 619.85,                last time consumption/overall running time: 59.0328s / 14820.0238 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1063
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1481
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 5181/50000 (10.3620%),                 avg. length: 598.45,                last time consumption/overall running time: 57.8051s / 14877.8289 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1519
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1929
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5201/50000 (10.4020%),                 avg. length: 636.65,                last time consumption/overall running time: 60.5967s / 14938.4256 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1349
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1962
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 5221/50000 (10.4420%),                 avg. length: 578.35,                last time consumption/overall running time: 55.8334s / 14994.2590 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1202
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1399
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 5241/50000 (10.4820%),                 avg. length: 638.6,                last time consumption/overall running time: 61.0576s / 15055.3166 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1455
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1745
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5261/50000 (10.5220%),                 avg. length: 561.35,                last time consumption/overall running time: 56.2463s / 15111.5629 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1650
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1969
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 5281/50000 (10.5620%),                 avg. length: 599.7,                last time consumption/overall running time: 57.8440s / 15169.4070 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1556
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1834
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 5301/50000 (10.6020%),                 avg. length: 568.85,                last time consumption/overall running time: 55.1122s / 15224.5192 s
env0_first_0:                 episode reward: -2.4000,                 loss: 0.2047
env0_second_0:                 episode reward: 2.4000,                 loss: 0.2460
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5321/50000 (10.6420%),                 avg. length: 632.8,                last time consumption/overall running time: 60.8532s / 15285.3724 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1741
env0_second_0:                 episode reward: 3.0000,                 loss: 0.2107
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5341/50000 (10.6820%),                 avg. length: 556.45,                last time consumption/overall running time: 53.5792s / 15338.9516 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1492
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1773
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5361/50000 (10.7220%),                 avg. length: 589.05,                last time consumption/overall running time: 56.6935s / 15395.6451 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1384
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1664
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5381/50000 (10.7620%),                 avg. length: 635.7,                last time consumption/overall running time: 61.0305s / 15456.6756 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0959
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1203
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 5401/50000 (10.8020%),                 avg. length: 617.8,                last time consumption/overall running time: 58.8293s / 15515.5049 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1320
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1658
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 5421/50000 (10.8420%),                 avg. length: 605.9,                last time consumption/overall running time: 57.9088s / 15573.4137 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.1106
env0_second_0:                 episode reward: 2.7500,                 loss: 0.1421
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5441/50000 (10.8820%),                 avg. length: 573.5,                last time consumption/overall running time: 54.9390s / 15628.3527 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1313
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1578
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5461/50000 (10.9220%),                 avg. length: 560.15,                last time consumption/overall running time: 55.9617s / 15684.3144 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1379
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1521
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 5481/50000 (10.9620%),                 avg. length: 631.95,                last time consumption/overall running time: 60.0868s / 15744.4011 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1212
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1422
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 5501/50000 (11.0020%),                 avg. length: 608.05,                last time consumption/overall running time: 58.8724s / 15803.2735 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1507
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1832
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 5521/50000 (11.0420%),                 avg. length: 625.0,                last time consumption/overall running time: 59.2733s / 15862.5468 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1671
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1873
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 5541/50000 (11.0820%),                 avg. length: 570.25,                last time consumption/overall running time: 54.0836s / 15916.6305 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1575
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1799
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 5561/50000 (11.1220%),                 avg. length: 547.55,                last time consumption/overall running time: 54.0267s / 15970.6572 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1375
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1666
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 5581/50000 (11.1620%),                 avg. length: 577.7,                last time consumption/overall running time: 55.7260s / 16026.3831 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.1404
env0_second_0:                 episode reward: 4.0000,                 loss: 0.1624
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 5601/50000 (11.2020%),                 avg. length: 567.8,                last time consumption/overall running time: 55.6173s / 16082.0005 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1611
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1834
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 5621/50000 (11.2420%),                 avg. length: 609.4,                last time consumption/overall running time: 58.3579s / 16140.3584 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1215
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1346
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5641/50000 (11.2820%),                 avg. length: 615.9,                last time consumption/overall running time: 59.0079s / 16199.3663 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.1173
env0_second_0:                 episode reward: 3.9500,                 loss: 0.1534
env1_first_0:                 episode reward: -2.5000,                 loss: nan
env1_second_0:                 episode reward: 2.5000,                 loss: nan
Episode: 5661/50000 (11.3220%),                 avg. length: 644.7,                last time consumption/overall running time: 60.4354s / 16259.8017 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.2184
env0_second_0:                 episode reward: 1.9500,                 loss: 0.2504
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 5681/50000 (11.3620%),                 avg. length: 598.85,                last time consumption/overall running time: 57.5218s / 16317.3235 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1139
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1292
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5701/50000 (11.4020%),                 avg. length: 628.15,                last time consumption/overall running time: 59.5562s / 16376.8798 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1511
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1862
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 5721/50000 (11.4420%),                 avg. length: 607.65,                last time consumption/overall running time: 58.3498s / 16435.2296 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.0936
env0_second_0:                 episode reward: 4.0000,                 loss: 0.1196
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5741/50000 (11.4820%),                 avg. length: 587.5,                last time consumption/overall running time: 56.8501s / 16492.0797 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1108
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1228
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 5761/50000 (11.5220%),                 avg. length: 589.65,                last time consumption/overall running time: 57.6153s / 16549.6949 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1297
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1589
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5781/50000 (11.5620%),                 avg. length: 539.9,                last time consumption/overall running time: 53.2041s / 16602.8990 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1668
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1952
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 5801/50000 (11.6020%),                 avg. length: 606.95,                last time consumption/overall running time: 59.7832s / 16662.6822 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1956
env0_second_0:                 episode reward: 3.4000,                 loss: 0.2238
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 5821/50000 (11.6420%),                 avg. length: 619.9,                last time consumption/overall running time: 60.2542s / 16722.9365 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1416
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1580
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 5841/50000 (11.6820%),                 avg. length: 601.6,                last time consumption/overall running time: 58.4588s / 16781.3953 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1775
env0_second_0:                 episode reward: 3.2000,                 loss: 0.2035
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 5861/50000 (11.7220%),                 avg. length: 549.4,                last time consumption/overall running time: 53.7152s / 16835.1105 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1522
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1833
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 5881/50000 (11.7620%),                 avg. length: 545.3,                last time consumption/overall running time: 53.0729s / 16888.1834 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1294
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1769
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 5901/50000 (11.8020%),                 avg. length: 604.65,                last time consumption/overall running time: 58.0836s / 16946.2670 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.0978
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1384
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 5921/50000 (11.8420%),                 avg. length: 575.65,                last time consumption/overall running time: 55.5043s / 17001.7713 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1195
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1451
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 5941/50000 (11.8820%),                 avg. length: 609.75,                last time consumption/overall running time: 59.5507s / 17061.3220 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0825
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1107
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 5961/50000 (11.9220%),                 avg. length: 588.4,                last time consumption/overall running time: 57.4404s / 17118.7623 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1241
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1483
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 5981/50000 (11.9620%),                 avg. length: 556.25,                last time consumption/overall running time: 54.2938s / 17173.0561 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1150
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1457
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6001/50000 (12.0020%),                 avg. length: 597.8,                last time consumption/overall running time: 57.8568s / 17230.9129 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1568
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1884
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 6021/50000 (12.0420%),                 avg. length: 614.7,                last time consumption/overall running time: 58.8515s / 17289.7645 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1271
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1484
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6041/50000 (12.0820%),                 avg. length: 602.35,                last time consumption/overall running time: 58.1351s / 17347.8996 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1267
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1586
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 6061/50000 (12.1220%),                 avg. length: 578.2,                last time consumption/overall running time: 56.3994s / 17404.2990 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1357
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1493
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 6081/50000 (12.1620%),                 avg. length: 633.7,                last time consumption/overall running time: 60.9668s / 17465.2657 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1508
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1846
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 6101/50000 (12.2020%),                 avg. length: 604.65,                last time consumption/overall running time: 57.6307s / 17522.8964 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1354
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1845
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 6121/50000 (12.2420%),                 avg. length: 584.1,                last time consumption/overall running time: 56.9299s / 17579.8264 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1492
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1925
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6141/50000 (12.2820%),                 avg. length: 700.6,                last time consumption/overall running time: 66.8328s / 17646.6591 s
env0_first_0:                 episode reward: -2.7000,                 loss: 0.1711
env0_second_0:                 episode reward: 2.7000,                 loss: 0.2047
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 6161/50000 (12.3220%),                 avg. length: 647.2,                last time consumption/overall running time: 61.9291s / 17708.5882 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1052
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1312
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 6181/50000 (12.3620%),                 avg. length: 609.7,                last time consumption/overall running time: 59.6113s / 17768.1995 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.1241
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1446
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 6201/50000 (12.4020%),                 avg. length: 588.2,                last time consumption/overall running time: 56.7450s / 17824.9445 s
env0_first_0:                 episode reward: -2.8000,                 loss: 0.1174
env0_second_0:                 episode reward: 2.8000,                 loss: 0.1370
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 6221/50000 (12.4420%),                 avg. length: 578.65,                last time consumption/overall running time: 56.6496s / 17881.5940 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1348
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1537
env1_first_0:                 episode reward: -4.0000,                 loss: nan
env1_second_0:                 episode reward: 4.0000,                 loss: nan
Episode: 6241/50000 (12.4820%),                 avg. length: 587.95,                last time consumption/overall running time: 57.1051s / 17938.6991 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1264
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1754
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 6261/50000 (12.5220%),                 avg. length: 554.0,                last time consumption/overall running time: 54.5219s / 17993.2210 s
env0_first_0:                 episode reward: -4.6000,                 loss: 0.1158
env0_second_0:                 episode reward: 4.6000,                 loss: 0.1185
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 6281/50000 (12.5620%),                 avg. length: 605.05,                last time consumption/overall running time: 58.1893s / 18051.4103 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1208
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1409
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 6301/50000 (12.6020%),                 avg. length: 620.9,                last time consumption/overall running time: 59.9059s / 18111.3163 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1351
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1569
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 6321/50000 (12.6420%),                 avg. length: 602.5,                last time consumption/overall running time: 58.1666s / 18169.4829 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1286
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1631
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6341/50000 (12.6820%),                 avg. length: 572.25,                last time consumption/overall running time: 56.8699s / 18226.3528 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1458
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1691
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 6361/50000 (12.7220%),                 avg. length: 639.95,                last time consumption/overall running time: 61.6142s / 18287.9669 s
env0_first_0:                 episode reward: -2.6500,                 loss: 0.1595
env0_second_0:                 episode reward: 2.6500,                 loss: 0.1781
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6381/50000 (12.7620%),                 avg. length: 610.4,                last time consumption/overall running time: 59.6211s / 18347.5881 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1459
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1657
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6401/50000 (12.8020%),                 avg. length: 559.25,                last time consumption/overall running time: 54.8601s / 18402.4482 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1127
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1339
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 6421/50000 (12.8420%),                 avg. length: 595.6,                last time consumption/overall running time: 58.1709s / 18460.6191 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1489
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1737
env1_first_0:                 episode reward: -2.7500,                 loss: nan
env1_second_0:                 episode reward: 2.7500,                 loss: nan
Episode: 6441/50000 (12.8820%),                 avg. length: 615.0,                last time consumption/overall running time: 59.9377s / 18520.5568 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1663
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1997
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 6461/50000 (12.9220%),                 avg. length: 552.15,                last time consumption/overall running time: 53.3711s / 18573.9279 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1528
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1740
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 6481/50000 (12.9620%),                 avg. length: 538.2,                last time consumption/overall running time: 53.0806s / 18627.0086 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1235
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1483
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 6501/50000 (13.0020%),                 avg. length: 581.65,                last time consumption/overall running time: 56.2558s / 18683.2644 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.1333
env0_second_0:                 episode reward: 4.0000,                 loss: 0.1445
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 6521/50000 (13.0420%),                 avg. length: 604.8,                last time consumption/overall running time: 58.7581s / 18742.0225 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1419
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1578
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6541/50000 (13.0820%),                 avg. length: 528.3,                last time consumption/overall running time: 52.5115s / 18794.5340 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1793
env0_second_0:                 episode reward: 3.7000,                 loss: 0.2010
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 6561/50000 (13.1220%),                 avg. length: 577.0,                last time consumption/overall running time: 55.7742s / 18850.3082 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1516
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1920
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6581/50000 (13.1620%),                 avg. length: 549.0,                last time consumption/overall running time: 52.9784s / 18903.2867 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1172
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1397
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 6601/50000 (13.2020%),                 avg. length: 621.95,                last time consumption/overall running time: 59.9166s / 18963.2033 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1904
env0_second_0:                 episode reward: 3.1500,                 loss: 0.2419
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 6621/50000 (13.2420%),                 avg. length: 575.35,                last time consumption/overall running time: 56.4945s / 19019.6977 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1665
env0_second_0:                 episode reward: 3.0000,                 loss: 0.2315
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 6641/50000 (13.2820%),                 avg. length: 589.4,                last time consumption/overall running time: 56.0135s / 19075.7112 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1610
env0_second_0:                 episode reward: 3.2500,                 loss: 0.2060
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6661/50000 (13.3220%),                 avg. length: 543.0,                last time consumption/overall running time: 52.7722s / 19128.4834 s
env0_first_0:                 episode reward: -4.2500,                 loss: 0.1320
env0_second_0:                 episode reward: 4.2500,                 loss: 0.1647
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6681/50000 (13.3620%),                 avg. length: 550.35,                last time consumption/overall running time: 53.4318s / 19181.9152 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1596
env0_second_0:                 episode reward: 3.4000,                 loss: 0.2240
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 6701/50000 (13.4020%),                 avg. length: 573.9,                last time consumption/overall running time: 55.7383s / 19237.6535 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1897
env0_second_0:                 episode reward: 3.1500,                 loss: 0.2129
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 6721/50000 (13.4420%),                 avg. length: 526.1,                last time consumption/overall running time: 51.7637s / 19289.4172 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1371
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1979
env1_first_0:                 episode reward: -4.7000,                 loss: nan
env1_second_0:                 episode reward: 4.7000,                 loss: nan
Episode: 6741/50000 (13.4820%),                 avg. length: 596.9,                last time consumption/overall running time: 58.0513s / 19347.4685 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1294
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1499
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 6761/50000 (13.5220%),                 avg. length: 593.95,                last time consumption/overall running time: 57.6439s / 19405.1124 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1317
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1692
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 6781/50000 (13.5620%),                 avg. length: 577.45,                last time consumption/overall running time: 56.6060s / 19461.7184 s
env0_first_0:                 episode reward: -4.4000,                 loss: 0.1392
env0_second_0:                 episode reward: 4.4000,                 loss: 0.1705
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 6801/50000 (13.6020%),                 avg. length: 542.3,                last time consumption/overall running time: 53.8690s / 19515.5874 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1736
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1942
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 6821/50000 (13.6420%),                 avg. length: 653.4,                last time consumption/overall running time: 62.2135s / 19577.8009 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1123
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1483
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 6841/50000 (13.6820%),                 avg. length: 575.85,                last time consumption/overall running time: 55.0789s / 19632.8797 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0817
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1574
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 6861/50000 (13.7220%),                 avg. length: 616.95,                last time consumption/overall running time: 58.8830s / 19691.7628 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1340
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1791
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6881/50000 (13.7620%),                 avg. length: 614.85,                last time consumption/overall running time: 58.6447s / 19750.4075 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.1330
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1670
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 6901/50000 (13.8020%),                 avg. length: 558.6,                last time consumption/overall running time: 54.4393s / 19804.8468 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1122
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1345
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 6921/50000 (13.8420%),                 avg. length: 608.3,                last time consumption/overall running time: 58.6860s / 19863.5328 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.0953
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1456
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 6941/50000 (13.8820%),                 avg. length: 576.65,                last time consumption/overall running time: 56.3798s / 19919.9125 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1117
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1577
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 6961/50000 (13.9220%),                 avg. length: 570.65,                last time consumption/overall running time: 55.4933s / 19975.4058 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1763
env0_second_0:                 episode reward: 3.5500,                 loss: 0.2011
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 6981/50000 (13.9620%),                 avg. length: 636.75,                last time consumption/overall running time: 61.5308s / 20036.9366 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.1060
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1425
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 7001/50000 (14.0020%),                 avg. length: 610.05,                last time consumption/overall running time: 59.2138s / 20096.1504 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1107
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1389
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 7021/50000 (14.0420%),                 avg. length: 569.2,                last time consumption/overall running time: 55.6952s / 20151.8457 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1399
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1622
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 7041/50000 (14.0820%),                 avg. length: 595.4,                last time consumption/overall running time: 58.0325s / 20209.8782 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1813
env0_second_0:                 episode reward: 3.2500,                 loss: 0.2051
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 7061/50000 (14.1220%),                 avg. length: 585.65,                last time consumption/overall running time: 56.7861s / 20266.6643 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1863
env0_second_0:                 episode reward: 3.6000,                 loss: 0.2070
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 7081/50000 (14.1620%),                 avg. length: 606.6,                last time consumption/overall running time: 58.0673s / 20324.7317 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1775
env0_second_0:                 episode reward: 3.3000,                 loss: 0.2022
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7101/50000 (14.2020%),                 avg. length: 550.0,                last time consumption/overall running time: 53.6932s / 20378.4249 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1944
env0_second_0:                 episode reward: 3.1000,                 loss: 0.2231
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7121/50000 (14.2420%),                 avg. length: 593.6,                last time consumption/overall running time: 57.5011s / 20435.9260 s
env0_first_0:                 episode reward: -4.0000,                 loss: 0.1270
env0_second_0:                 episode reward: 4.0000,                 loss: 0.1471
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 7141/50000 (14.2820%),                 avg. length: 579.35,                last time consumption/overall running time: 56.4490s / 20492.3751 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.2044
env0_second_0:                 episode reward: 3.4500,                 loss: 0.2284
env1_first_0:                 episode reward: -2.1000,                 loss: nan
env1_second_0:                 episode reward: 2.1000,                 loss: nan
Episode: 7161/50000 (14.3220%),                 avg. length: 627.75,                last time consumption/overall running time: 61.2023s / 20553.5773 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1896
env0_second_0:                 episode reward: 3.3500,                 loss: 0.2147
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7181/50000 (14.3620%),                 avg. length: 575.0,                last time consumption/overall running time: 56.7182s / 20610.2956 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1498
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1768
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 7201/50000 (14.4020%),                 avg. length: 539.5,                last time consumption/overall running time: 53.5169s / 20663.8124 s
env0_first_0:                 episode reward: -4.1000,                 loss: 0.1204
env0_second_0:                 episode reward: 4.1000,                 loss: 0.1366
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7221/50000 (14.4420%),                 avg. length: 581.9,                last time consumption/overall running time: 56.5923s / 20720.4048 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1034
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1267
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 7241/50000 (14.4820%),                 avg. length: 551.05,                last time consumption/overall running time: 53.7093s / 20774.1141 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1218
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1463
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7261/50000 (14.5220%),                 avg. length: 552.2,                last time consumption/overall running time: 53.3384s / 20827.4524 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1371
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1667
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 7281/50000 (14.5620%),                 avg. length: 682.3,                last time consumption/overall running time: 64.4582s / 20891.9107 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1786
env0_second_0:                 episode reward: 3.1000,                 loss: 0.2179
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 7301/50000 (14.6020%),                 avg. length: 600.1,                last time consumption/overall running time: 57.7788s / 20949.6895 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1486
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1642
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7321/50000 (14.6420%),                 avg. length: 566.3,                last time consumption/overall running time: 54.4172s / 21004.1066 s
env0_first_0:                 episode reward: -2.9000,                 loss: 0.1554
env0_second_0:                 episode reward: 2.9000,                 loss: 0.1783
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7341/50000 (14.6820%),                 avg. length: 587.05,                last time consumption/overall running time: 57.2467s / 21061.3534 s
env0_first_0:                 episode reward: -3.0000,                 loss: 0.1267
env0_second_0:                 episode reward: 3.0000,                 loss: 0.1442
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 7361/50000 (14.7220%),                 avg. length: 634.75,                last time consumption/overall running time: 59.8674s / 21121.2207 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.1287
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1870
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 7381/50000 (14.7620%),                 avg. length: 669.35,                last time consumption/overall running time: 63.1832s / 21184.4040 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0739
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0896
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 7401/50000 (14.8020%),                 avg. length: 587.95,                last time consumption/overall running time: 56.8309s / 21241.2349 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.1498
env0_second_0:                 episode reward: 2.5500,                 loss: 0.1721
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 7421/50000 (14.8420%),                 avg. length: 560.0,                last time consumption/overall running time: 54.7014s / 21295.9363 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1284
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1372
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 7441/50000 (14.8820%),                 avg. length: 613.2,                last time consumption/overall running time: 59.6952s / 21355.6314 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1083
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1178
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 7461/50000 (14.9220%),                 avg. length: 660.85,                last time consumption/overall running time: 63.3619s / 21418.9933 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1181
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1607
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 7481/50000 (14.9620%),                 avg. length: 627.55,                last time consumption/overall running time: 60.6174s / 21479.6107 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1681
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1847
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 7501/50000 (15.0020%),                 avg. length: 643.6,                last time consumption/overall running time: 62.4060s / 21542.0167 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1002
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1180
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7521/50000 (15.0420%),                 avg. length: 583.9,                last time consumption/overall running time: 57.2397s / 21599.2565 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1118
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1363
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 7541/50000 (15.0820%),                 avg. length: 622.95,                last time consumption/overall running time: 59.7625s / 21659.0189 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1326
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1656
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7561/50000 (15.1220%),                 avg. length: 662.45,                last time consumption/overall running time: 63.2478s / 21722.2668 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1287
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1595
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7581/50000 (15.1620%),                 avg. length: 594.15,                last time consumption/overall running time: 57.4255s / 21779.6923 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.1563
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1741
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7601/50000 (15.2020%),                 avg. length: 593.4,                last time consumption/overall running time: 57.6206s / 21837.3129 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.1163
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1367
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7621/50000 (15.2420%),                 avg. length: 670.3,                last time consumption/overall running time: 64.0448s / 21901.3578 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1068
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1302
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7641/50000 (15.2820%),                 avg. length: 625.85,                last time consumption/overall running time: 59.3870s / 21960.7448 s
env0_first_0:                 episode reward: -2.8500,                 loss: 0.1971
env0_second_0:                 episode reward: 2.8500,                 loss: 0.2251
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 7661/50000 (15.3220%),                 avg. length: 651.8,                last time consumption/overall running time: 62.3348s / 22023.0796 s
env0_first_0:                 episode reward: -4.1500,                 loss: 0.1244
env0_second_0:                 episode reward: 4.1500,                 loss: 0.1358
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 7681/50000 (15.3620%),                 avg. length: 620.95,                last time consumption/overall running time: 59.6862s / 22082.7658 s
env0_first_0:                 episode reward: -3.8500,                 loss: 0.1428
env0_second_0:                 episode reward: 3.8500,                 loss: 0.1623
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7701/50000 (15.4020%),                 avg. length: 599.2,                last time consumption/overall running time: 57.8678s / 22140.6336 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.1311
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1516
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 7721/50000 (15.4420%),                 avg. length: 642.4,                last time consumption/overall running time: 61.3006s / 22201.9342 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1262
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1514
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7741/50000 (15.4820%),                 avg. length: 586.1,                last time consumption/overall running time: 56.9569s / 22258.8912 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.1549
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1728
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 7761/50000 (15.5220%),                 avg. length: 630.65,                last time consumption/overall running time: 60.5327s / 22319.4239 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1130
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1318
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 7781/50000 (15.5620%),                 avg. length: 601.2,                last time consumption/overall running time: 58.7957s / 22378.2196 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1154
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1378
env1_first_0:                 episode reward: -4.1000,                 loss: nan
env1_second_0:                 episode reward: 4.1000,                 loss: nan
Episode: 7801/50000 (15.6020%),                 avg. length: 639.45,                last time consumption/overall running time: 62.1021s / 22440.3217 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1203
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1372
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 7821/50000 (15.6420%),                 avg. length: 616.0,                last time consumption/overall running time: 59.2943s / 22499.6159 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.1359
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1509
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7841/50000 (15.6820%),                 avg. length: 614.05,                last time consumption/overall running time: 59.5162s / 22559.1322 s
env0_first_0:                 episode reward: -3.9000,                 loss: 0.1383
env0_second_0:                 episode reward: 3.9000,                 loss: 0.1564
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7861/50000 (15.7220%),                 avg. length: 585.95,                last time consumption/overall running time: 56.7650s / 22615.8972 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1746
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1929
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 7881/50000 (15.7620%),                 avg. length: 680.8,                last time consumption/overall running time: 64.6195s / 22680.5167 s
env0_first_0:                 episode reward: -1.3500,                 loss: 0.2582
env0_second_0:                 episode reward: 1.3500,                 loss: 0.2954
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 7901/50000 (15.8020%),                 avg. length: 647.4,                last time consumption/overall running time: 61.1573s / 22741.6740 s
env0_first_0:                 episode reward: -3.1500,                 loss: 0.1464
env0_second_0:                 episode reward: 3.1500,                 loss: 0.1557
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 7921/50000 (15.8420%),                 avg. length: 655.6,                last time consumption/overall running time: 63.2828s / 22804.9568 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1680
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1928
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7941/50000 (15.8820%),                 avg. length: 657.3,                last time consumption/overall running time: 63.5137s / 22868.4705 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1383
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1528
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7961/50000 (15.9220%),                 avg. length: 657.55,                last time consumption/overall running time: 62.7401s / 22931.2106 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.1326
env0_second_0:                 episode reward: 3.7000,                 loss: 0.1571
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 7981/50000 (15.9620%),                 avg. length: 733.05,                last time consumption/overall running time: 68.7220s / 22999.9325 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1057
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1451
env1_first_0:                 episode reward: -3.4500,                 loss: nan
env1_second_0:                 episode reward: 3.4500,                 loss: nan
Episode: 8001/50000 (16.0020%),                 avg. length: 713.7,                last time consumption/overall running time: 68.1102s / 23068.0428 s
env0_first_0:                 episode reward: -0.1000,                 loss: 0.3402
env0_second_0:                 episode reward: 0.1000,                 loss: 0.3964
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 8021/50000 (16.0420%),                 avg. length: 750.1,                last time consumption/overall running time: 69.9191s / 23137.9619 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.1903
env0_second_0:                 episode reward: 2.3500,                 loss: 0.2153
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 8041/50000 (16.0820%),                 avg. length: 657.0,                last time consumption/overall running time: 62.5568s / 23200.5186 s
env0_first_0:                 episode reward: -1.9500,                 loss: 0.2293
env0_second_0:                 episode reward: 1.9500,                 loss: 0.2448
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 8061/50000 (16.1220%),                 avg. length: 705.55,                last time consumption/overall running time: 66.2965s / 23266.8152 s
env0_first_0:                 episode reward: -2.7500,                 loss: 0.1702
env0_second_0:                 episode reward: 2.7500,                 loss: 0.1844
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 8081/50000 (16.1620%),                 avg. length: 658.8,                last time consumption/overall running time: 61.2876s / 23328.1027 s
env0_first_0:                 episode reward: -3.4500,                 loss: 0.1650
env0_second_0:                 episode reward: 3.4500,                 loss: 0.1788
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 8101/50000 (16.2020%),                 avg. length: 654.65,                last time consumption/overall running time: 61.3058s / 23389.4086 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.1950
env0_second_0:                 episode reward: 3.1000,                 loss: 0.2072
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 8121/50000 (16.2420%),                 avg. length: 661.95,                last time consumption/overall running time: 63.6667s / 23453.0753 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1539
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1858
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 8141/50000 (16.2820%),                 avg. length: 720.75,                last time consumption/overall running time: 68.6012s / 23521.6765 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.1082
env0_second_0:                 episode reward: 3.7500,                 loss: 0.1273
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 8161/50000 (16.3220%),                 avg. length: 626.45,                last time consumption/overall running time: 60.0696s / 23581.7461 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1206
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1429
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 8181/50000 (16.3620%),                 avg. length: 648.4,                last time consumption/overall running time: 62.0874s / 23643.8335 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.1442
env0_second_0:                 episode reward: 3.0500,                 loss: 0.1786
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 8201/50000 (16.4020%),                 avg. length: 641.2,                last time consumption/overall running time: 60.4343s / 23704.2678 s
env0_first_0:                 episode reward: -3.9500,                 loss: 0.0964
env0_second_0:                 episode reward: 3.9500,                 loss: 0.1285
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8221/50000 (16.4420%),                 avg. length: 649.85,                last time consumption/overall running time: 62.5400s / 23766.8078 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1183
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1517
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8241/50000 (16.4820%),                 avg. length: 646.4,                last time consumption/overall running time: 62.2403s / 23829.0481 s
env0_first_0:                 episode reward: -3.8000,                 loss: 0.1223
env0_second_0:                 episode reward: 3.8000,                 loss: 0.1541
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 8261/50000 (16.5220%),                 avg. length: 590.9,                last time consumption/overall running time: 57.7151s / 23886.7632 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.1382
env0_second_0:                 episode reward: 3.4000,                 loss: 0.1683
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 8281/50000 (16.5620%),                 avg. length: 627.85,                last time consumption/overall running time: 60.6131s / 23947.3763 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1703
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1948
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 8301/50000 (16.6020%),                 avg. length: 683.25,                last time consumption/overall running time: 65.2412s / 24012.6175 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1053
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1393
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8321/50000 (16.6420%),                 avg. length: 699.4,                last time consumption/overall running time: 66.1487s / 24078.7662 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.1298
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1395
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8341/50000 (16.6820%),                 avg. length: 629.65,                last time consumption/overall running time: 60.7566s / 24139.5228 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0975
env0_second_0:                 episode reward: 3.5500,                 loss: 0.1229
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 8361/50000 (16.7220%),                 avg. length: 729.0,                last time consumption/overall running time: 69.0480s / 24208.5707 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0913
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1131
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 8381/50000 (16.7620%),                 avg. length: 681.8,                last time consumption/overall running time: 65.5508s / 24274.1215 s
env0_first_0:                 episode reward: -4.2000,                 loss: 0.0897
env0_second_0:                 episode reward: 4.2000,                 loss: 0.1191
env1_first_0:                 episode reward: -3.0000,                 loss: nan
env1_second_0:                 episode reward: 3.0000,                 loss: nan
Episode: 8401/50000 (16.8020%),                 avg. length: 723.6,                last time consumption/overall running time: 68.1248s / 24342.2463 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0826
env0_second_0:                 episode reward: 3.6500,                 loss: 0.1063
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 8421/50000 (16.8420%),                 avg. length: 624.5,                last time consumption/overall running time: 60.3036s / 24402.5499 s
env0_first_0:                 episode reward: -3.2000,                 loss: 0.1037
env0_second_0:                 episode reward: 3.2000,                 loss: 0.1267
env1_first_0:                 episode reward: -4.3000,                 loss: nan
env1_second_0:                 episode reward: 4.3000,                 loss: nan
Episode: 8441/50000 (16.8820%),                 avg. length: 700.55,                last time consumption/overall running time: 65.9848s / 24468.5348 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0906
env0_second_0:                 episode reward: 4.0500,                 loss: 0.1099
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 8461/50000 (16.9220%),                 avg. length: 668.9,                last time consumption/overall running time: 64.0536s / 24532.5884 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.1081
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1186
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 8481/50000 (16.9620%),                 avg. length: 679.5,                last time consumption/overall running time: 63.8606s / 24596.4489 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0746
env0_second_0:                 episode reward: 3.3000,                 loss: 0.1009
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 8501/50000 (17.0020%),                 avg. length: 678.05,                last time consumption/overall running time: 63.4013s / 24659.8502 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.1159
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1417
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 8521/50000 (17.0420%),                 avg. length: 706.0,                last time consumption/overall running time: 65.9840s / 24725.8342 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.1165
env0_second_0:                 episode reward: 2.9500,                 loss: 0.1328
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8541/50000 (17.0820%),                 avg. length: 697.5,                last time consumption/overall running time: 65.6308s / 24791.4649 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.1378
env0_second_0:                 episode reward: 3.2500,                 loss: 0.1455
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8561/50000 (17.1220%),                 avg. length: 801.7,                last time consumption/overall running time: 73.7790s / 24865.2439 s
env0_first_0:                 episode reward: -3.3500,                 loss: 0.0785
env0_second_0:                 episode reward: 3.3500,                 loss: 0.1038
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8581/50000 (17.1620%),                 avg. length: 817.65,                last time consumption/overall running time: 75.9814s / 24941.2253 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0702
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0800
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 8601/50000 (17.2020%),                 avg. length: 744.75,                last time consumption/overall running time: 70.5184s / 25011.7437 s
env0_first_0:                 episode reward: -2.4500,                 loss: 0.1044
env0_second_0:                 episode reward: 2.4500,                 loss: 0.1166
env1_first_0:                 episode reward: -3.2500,                 loss: nan
env1_second_0:                 episode reward: 3.2500,                 loss: nan
Episode: 8621/50000 (17.2420%),                 avg. length: 813.25,                last time consumption/overall running time: 75.4493s / 25087.1930 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0504
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0763
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 8641/50000 (17.2820%),                 avg. length: 862.75,                last time consumption/overall running time: 79.6226s / 25166.8156 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0513
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0688
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8661/50000 (17.3220%),                 avg. length: 920.3,                last time consumption/overall running time: 84.6612s / 25251.4767 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0459
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0651
env1_first_0:                 episode reward: -3.1000,                 loss: nan
env1_second_0:                 episode reward: 3.1000,                 loss: nan
Episode: 8681/50000 (17.3620%),                 avg. length: 873.6,                last time consumption/overall running time: 80.9583s / 25332.4351 s
env0_first_0:                 episode reward: -2.3500,                 loss: 0.0567
env0_second_0:                 episode reward: 2.3500,                 loss: 0.0736
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 8701/50000 (17.4020%),                 avg. length: 898.65,                last time consumption/overall running time: 82.0542s / 25414.4893 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0827
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0924
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 8721/50000 (17.4420%),                 avg. length: 823.05,                last time consumption/overall running time: 76.4630s / 25490.9523 s
env0_first_0:                 episode reward: -3.0500,                 loss: 0.0601
env0_second_0:                 episode reward: 3.0500,                 loss: 0.0712
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8741/50000 (17.4820%),                 avg. length: 863.2,                last time consumption/overall running time: 78.3273s / 25569.2796 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0485
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0646
env1_first_0:                 episode reward: -3.2000,                 loss: nan
env1_second_0:                 episode reward: 3.2000,                 loss: nan
Episode: 8761/50000 (17.5220%),                 avg. length: 909.55,                last time consumption/overall running time: 82.2426s / 25651.5222 s
env0_first_0:                 episode reward: -3.6500,                 loss: 0.0609
env0_second_0:                 episode reward: 3.6500,                 loss: 0.0699
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 8781/50000 (17.5620%),                 avg. length: 841.0,                last time consumption/overall running time: 78.4983s / 25730.0205 s
env0_first_0:                 episode reward: -2.9500,                 loss: 0.0681
env0_second_0:                 episode reward: 2.9500,                 loss: 0.0906
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 8801/50000 (17.6020%),                 avg. length: 783.2,                last time consumption/overall running time: 72.7779s / 25802.7984 s
env0_first_0:                 episode reward: -3.4000,                 loss: 0.0800
env0_second_0:                 episode reward: 3.4000,                 loss: 0.0899
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 8821/50000 (17.6420%),                 avg. length: 894.95,                last time consumption/overall running time: 81.4565s / 25884.2549 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0256
env0_second_0:                 episode reward: 3.5000,                 loss: 0.0488
env1_first_0:                 episode reward: -3.7000,                 loss: nan
env1_second_0:                 episode reward: 3.7000,                 loss: nan
Episode: 8841/50000 (17.6820%),                 avg. length: 808.9,                last time consumption/overall running time: 75.5651s / 25959.8200 s
env0_first_0:                 episode reward: -3.7500,                 loss: 0.0605
env0_second_0:                 episode reward: 3.7500,                 loss: 0.0839
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 8861/50000 (17.7220%),                 avg. length: 801.4,                last time consumption/overall running time: 73.9506s / 26033.7705 s
env0_first_0:                 episode reward: -3.5500,                 loss: 0.0757
env0_second_0:                 episode reward: 3.5500,                 loss: 0.0984
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 8881/50000 (17.7620%),                 avg. length: 708.7,                last time consumption/overall running time: 66.2759s / 26100.0465 s
env0_first_0:                 episode reward: -3.5000,                 loss: 0.0911
env0_second_0:                 episode reward: 3.5000,                 loss: 0.1250
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 8901/50000 (17.8020%),                 avg. length: 835.5,                last time consumption/overall running time: 77.2221s / 26177.2686 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0238
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0498
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 8921/50000 (17.8420%),                 avg. length: 812.25,                last time consumption/overall running time: 75.3280s / 26252.5965 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0835
env0_second_0:                 episode reward: 3.1000,                 loss: 0.1096
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 8941/50000 (17.8820%),                 avg. length: 725.85,                last time consumption/overall running time: 67.7192s / 26320.3157 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0836
env0_second_0:                 episode reward: 3.6000,                 loss: 0.1079
env1_first_0:                 episode reward: -3.4000,                 loss: nan
env1_second_0:                 episode reward: 3.4000,                 loss: nan
Episode: 8961/50000 (17.9220%),                 avg. length: 902.3,                last time consumption/overall running time: 82.1898s / 26402.5056 s
env0_first_0:                 episode reward: -3.3000,                 loss: 0.0478
env0_second_0:                 episode reward: 3.3000,                 loss: 0.0680
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 8981/50000 (17.9620%),                 avg. length: 827.4,                last time consumption/overall running time: 76.1278s / 26478.6334 s
env0_first_0:                 episode reward: -4.0500,                 loss: 0.0738
env0_second_0:                 episode reward: 4.0500,                 loss: 0.0898
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 9001/50000 (18.0020%),                 avg. length: 871.75,                last time consumption/overall running time: 79.6222s / 26558.2556 s
env0_first_0:                 episode reward: -3.7000,                 loss: 0.0655
env0_second_0:                 episode reward: 3.7000,                 loss: 0.0981
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 9021/50000 (18.0420%),                 avg. length: 928.25,                last time consumption/overall running time: 85.3637s / 26643.6193 s
env0_first_0:                 episode reward: -2.5500,                 loss: 0.0654
env0_second_0:                 episode reward: 2.5500,                 loss: 0.0893
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 9041/50000 (18.0820%),                 avg. length: 895.9,                last time consumption/overall running time: 82.0122s / 26725.6316 s
env0_first_0:                 episode reward: -3.2500,                 loss: 0.0354
env0_second_0:                 episode reward: 3.2500,                 loss: 0.0545
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 9061/50000 (18.1220%),                 avg. length: 927.1,                last time consumption/overall running time: 83.5158s / 26809.1474 s
env0_first_0:                 episode reward: -3.6000,                 loss: 0.0403
env0_second_0:                 episode reward: 3.6000,                 loss: 0.0689
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 9081/50000 (18.1620%),                 avg. length: 1048.35,                last time consumption/overall running time: 93.9355s / 26903.0828 s
env0_first_0:                 episode reward: -3.1000,                 loss: 0.0361
env0_second_0:                 episode reward: 3.1000,                 loss: 0.0644
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 9101/50000 (18.2020%),                 avg. length: 1224.0,                last time consumption/overall running time: 110.5759s / 27013.6587 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.0080
env0_second_0:                 episode reward: 2.8500,                 loss: 0.0153
env1_first_0:                 episode reward: -3.3000,                 loss: nan
env1_second_0:                 episode reward: 3.3000,                 loss: nan
Episode: 9121/50000 (18.2420%),                 avg. length: 1191.1,                last time consumption/overall running time: 105.6724s / 27119.3312 s
env0_first_0:                 episode reward: -3.9000,                 loss: -0.0023
env0_second_0:                 episode reward: 3.9000,                 loss: 0.0142
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 9141/50000 (18.2820%),                 avg. length: 1091.3,                last time consumption/overall running time: 97.9632s / 27217.2944 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.0026
env0_second_0:                 episode reward: 2.9000,                 loss: 0.0160
env1_first_0:                 episode reward: -3.3500,                 loss: nan
env1_second_0:                 episode reward: 3.3500,                 loss: nan
Episode: 9161/50000 (18.3220%),                 avg. length: 1373.2,                last time consumption/overall running time: 119.4326s / 27336.7270 s
env0_first_0:                 episode reward: -3.7000,                 loss: -0.0247
env0_second_0:                 episode reward: 3.7000,                 loss: -0.0189
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 9181/50000 (18.3620%),                 avg. length: 1876.3,                last time consumption/overall running time: 161.1373s / 27497.8642 s
env0_first_0:                 episode reward: -2.5500,                 loss: -0.0637
env0_second_0:                 episode reward: 2.5500,                 loss: -0.0476
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 9201/50000 (18.4020%),                 avg. length: 2014.1,                last time consumption/overall running time: 172.2552s / 27670.1195 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.0673
env0_second_0:                 episode reward: 3.1000,                 loss: -0.0559
env1_first_0:                 episode reward: -3.0500,                 loss: nan
env1_second_0:                 episode reward: 3.0500,                 loss: nan
Episode: 9221/50000 (18.4420%),                 avg. length: 2285.5,                last time consumption/overall running time: 196.4248s / 27866.5443 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.1020
env0_second_0:                 episode reward: 2.8500,                 loss: -0.0898
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 9241/50000 (18.4820%),                 avg. length: 2253.15,                last time consumption/overall running time: 193.0699s / 28059.6142 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.0997
env0_second_0:                 episode reward: 2.6500,                 loss: -0.0812
env1_first_0:                 episode reward: -2.2000,                 loss: nan
env1_second_0:                 episode reward: 2.2000,                 loss: nan
Episode: 9261/50000 (18.5220%),                 avg. length: 2631.8,                last time consumption/overall running time: 223.4913s / 28283.1055 s
env0_first_0:                 episode reward: -1.4500,                 loss: -0.1179
env0_second_0:                 episode reward: 1.4500,                 loss: -0.1011
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 9281/50000 (18.5620%),                 avg. length: 2908.65,                last time consumption/overall running time: 248.8217s / 28531.9272 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.1319
env0_second_0:                 episode reward: 1.4000,                 loss: -0.1062
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 9301/50000 (18.6020%),                 avg. length: 2944.95,                last time consumption/overall running time: 249.2877s / 28781.2149 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.1392
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1211
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 9321/50000 (18.6420%),                 avg. length: 2801.2,                last time consumption/overall running time: 239.1619s / 29020.3767 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1312
env0_second_0:                 episode reward: 1.0500,                 loss: -0.1044
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 9341/50000 (18.6820%),                 avg. length: 2924.55,                last time consumption/overall running time: 248.9556s / 29269.3324 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1251
env0_second_0:                 episode reward: 0.8000,                 loss: -0.0981
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9361/50000 (18.7220%),                 avg. length: 2952.05,                last time consumption/overall running time: 251.1590s / 29520.4913 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1553
env0_second_0:                 episode reward: 0.6500,                 loss: -0.1195
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 9381/50000 (18.7620%),                 avg. length: 2997.55,                last time consumption/overall running time: 253.3498s / 29773.8411 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1684
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1340
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 9401/50000 (18.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.8914s / 30027.7325 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.1782
env0_second_0:                 episode reward: 0.8000,                 loss: -0.1495
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9421/50000 (18.8420%),                 avg. length: 2843.9,                last time consumption/overall running time: 241.3074s / 30269.0399 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1572
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1287
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 9441/50000 (18.8820%),                 avg. length: 2945.45,                last time consumption/overall running time: 252.6473s / 30521.6872 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1808
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1411
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 9461/50000 (18.9220%),                 avg. length: 2901.6,                last time consumption/overall running time: 246.9721s / 30768.6593 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.1721
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1460
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 9481/50000 (18.9620%),                 avg. length: 2959.0,                last time consumption/overall running time: 249.4426s / 31018.1019 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.1751
env0_second_0:                 episode reward: 0.9000,                 loss: -0.1526
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 9501/50000 (19.0020%),                 avg. length: 2930.85,                last time consumption/overall running time: 248.7166s / 31266.8185 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1776
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1461
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 9521/50000 (19.0420%),                 avg. length: 2983.8,                last time consumption/overall running time: 250.3465s / 31517.1650 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1871
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1639
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9541/50000 (19.0820%),                 avg. length: 2945.05,                last time consumption/overall running time: 249.2144s / 31766.3794 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1818
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1515
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 9561/50000 (19.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.3448s / 32019.7242 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1905
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1627
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9581/50000 (19.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 251.5325s / 32271.2567 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.1719
env0_second_0:                 episode reward: 1.0500,                 loss: -0.1448
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 9601/50000 (19.2020%),                 avg. length: 2954.6,                last time consumption/overall running time: 250.3926s / 32521.6493 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1624
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1270
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9621/50000 (19.2420%),                 avg. length: 2947.2,                last time consumption/overall running time: 251.0670s / 32772.7163 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1843
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1485
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 9641/50000 (19.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 251.8274s / 33024.5436 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1892
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1601
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9661/50000 (19.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.1132s / 33276.6568 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1874
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1580
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9681/50000 (19.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.0259s / 33529.6827 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1747
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1557
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9701/50000 (19.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 251.7482s / 33781.4308 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1893
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1688
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9721/50000 (19.4420%),                 avg. length: 2925.85,                last time consumption/overall running time: 247.2691s / 34028.6999 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1765
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1419
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9741/50000 (19.4820%),                 avg. length: 2927.05,                last time consumption/overall running time: 247.2907s / 34275.9906 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1782
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1500
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 9761/50000 (19.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.1872s / 34532.1778 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.1833
env0_second_0:                 episode reward: 0.7000,                 loss: -0.1585
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9781/50000 (19.5620%),                 avg. length: 2974.4,                last time consumption/overall running time: 253.8870s / 34786.0647 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1899
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1596
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9801/50000 (19.6020%),                 avg. length: 2908.35,                last time consumption/overall running time: 246.4383s / 35032.5031 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1802
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1441
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9821/50000 (19.6420%),                 avg. length: 2953.35,                last time consumption/overall running time: 251.0379s / 35283.5409 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1840
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1511
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9841/50000 (19.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2078s / 35538.7487 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1927
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1641
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9861/50000 (19.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5628s / 35794.3115 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.1904
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1690
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 9881/50000 (19.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.8369s / 36049.1484 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.1874
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1622
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 9901/50000 (19.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.5478s / 36302.6962 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.1906
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1617
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 9921/50000 (19.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2859s / 36558.9822 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1906
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1535
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 9941/50000 (19.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1635s / 36816.1457 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1922
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1577
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 9961/50000 (19.9220%),                 avg. length: 2998.35,                last time consumption/overall running time: 254.8294s / 37070.9751 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1757
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1434
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9981/50000 (19.9620%),                 avg. length: 2983.5,                last time consumption/overall running time: 253.0471s / 37324.0222 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.1846
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1557
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10001/50000 (20.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.9701s / 37577.9923 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.1910
env0_second_0:                 episode reward: 0.5500,                 loss: -0.1614
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 10021/50000 (20.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5293s / 37833.5216 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.2007
env0_second_0:                 episode reward: 0.6500,                 loss: -0.1802
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 10041/50000 (20.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8829s / 38089.4045 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2036
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1727
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 10061/50000 (20.1220%),                 avg. length: 2991.7,                last time consumption/overall running time: 254.8149s / 38344.2194 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1861
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1560
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 10081/50000 (20.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.1460s / 38600.3654 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1967
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1691
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 10101/50000 (20.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0656s / 38857.4309 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2021
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1722
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 10121/50000 (20.2420%),                 avg. length: 2983.1,                last time consumption/overall running time: 252.6979s / 39110.1288 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.1861
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1591
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 10141/50000 (20.2820%),                 avg. length: 2967.8,                last time consumption/overall running time: 253.1934s / 39363.3222 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1652
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1350
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 10161/50000 (20.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.1810s / 39617.5031 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2074
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1753
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 10181/50000 (20.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.1679s / 39871.6710 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1973
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1803
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10201/50000 (20.4020%),                 avg. length: 2966.75,                last time consumption/overall running time: 253.4112s / 40125.0823 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.1940
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1739
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10221/50000 (20.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.6335s / 40379.7158 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2010
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1769
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 10241/50000 (20.4820%),                 avg. length: 2747.65,                last time consumption/overall running time: 232.2498s / 40611.9655 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1799
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1459
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 10261/50000 (20.5220%),                 avg. length: 2998.35,                last time consumption/overall running time: 255.9636s / 40867.9292 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2032
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1727
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 10281/50000 (20.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2386s / 41124.1677 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2140
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1873
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 10301/50000 (20.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.7580s / 41377.9257 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2194
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1851
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 10321/50000 (20.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.8248s / 41632.7505 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2215
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1852
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 10341/50000 (20.6820%),                 avg. length: 2820.65,                last time consumption/overall running time: 241.1452s / 41873.8957 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1479
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1156
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 10361/50000 (20.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5389s / 42130.4345 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2188
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1834
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10381/50000 (20.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.9342s / 42384.3687 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2218
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1887
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 10401/50000 (20.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.5822s / 42638.9509 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2234
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1952
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 10421/50000 (20.8420%),                 avg. length: 2955.2,                last time consumption/overall running time: 252.3824s / 42891.3333 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2067
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1807
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 10441/50000 (20.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.7072s / 43146.0405 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2150
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1874
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 10461/50000 (20.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4839s / 43402.5243 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2102
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1920
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 10481/50000 (20.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9698s / 43657.4941 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2021
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1762
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10501/50000 (21.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5455s / 43913.0395 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2006
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1766
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10521/50000 (21.0420%),                 avg. length: 2958.1,                last time consumption/overall running time: 252.0905s / 44165.1301 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.1977
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1779
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 10541/50000 (21.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5219s / 44421.6520 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2265
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2024
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10561/50000 (21.1220%),                 avg. length: 2946.65,                last time consumption/overall running time: 249.2985s / 44670.9504 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2034
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1787
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 10581/50000 (21.1620%),                 avg. length: 2971.0,                last time consumption/overall running time: 253.4881s / 44924.4386 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.1949
env0_second_0:                 episode reward: 1.0000,                 loss: -0.1569
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 10601/50000 (21.2020%),                 avg. length: 2941.2,                last time consumption/overall running time: 249.2350s / 45173.6736 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1711
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1400
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 10621/50000 (21.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.4269s / 45427.1005 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1965
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1717
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10641/50000 (21.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6134s / 45682.7139 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2014
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1763
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10661/50000 (21.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9775s / 45937.6915 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2249
env0_second_0:                 episode reward: 0.3000,                 loss: -0.2010
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 10681/50000 (21.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.0901s / 46193.7815 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2122
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1918
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10701/50000 (21.4020%),                 avg. length: 2930.6,                last time consumption/overall running time: 250.4807s / 46444.2622 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1748
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1450
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 10721/50000 (21.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6004s / 46701.8625 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2197
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1967
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 10741/50000 (21.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8152s / 46958.6777 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.2130
env0_second_0:                 episode reward: 0.6500,                 loss: -0.1763
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10761/50000 (21.5220%),                 avg. length: 2978.05,                last time consumption/overall running time: 253.4931s / 47212.1709 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2058
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1800
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 10781/50000 (21.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3740s / 47468.5449 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2128
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1857
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 10801/50000 (21.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2310s / 47724.7759 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1977
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1744
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 10821/50000 (21.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8599s / 47982.6357 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2042
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1728
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10841/50000 (21.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0928s / 48239.7285 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2094
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1769
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 10861/50000 (21.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1698s / 48494.8983 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2002
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1808
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 10881/50000 (21.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9670s / 48750.8653 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2173
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1893
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 10901/50000 (21.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5154s / 49008.3807 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2323
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2052
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10921/50000 (21.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.5342s / 49262.9149 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2203
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1848
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 10941/50000 (21.8820%),                 avg. length: 2982.9,                last time consumption/overall running time: 256.0038s / 49518.9187 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2130
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1878
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 10961/50000 (21.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2029s / 49774.1216 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2002
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1826
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 10981/50000 (21.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5198s / 50029.6414 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2106
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1793
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 11001/50000 (22.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.4042s / 50284.0455 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2104
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1840
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11021/50000 (22.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4684s / 50540.5139 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2208
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1796
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11041/50000 (22.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.4015s / 50801.9155 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2182
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1929
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 11061/50000 (22.1220%),                 avg. length: 2922.35,                last time consumption/overall running time: 250.4575s / 51052.3730 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1913
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1680
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 11081/50000 (22.1620%),                 avg. length: 2997.15,                last time consumption/overall running time: 257.7956s / 51310.1686 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2060
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1815
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 11101/50000 (22.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5854s / 51567.7540 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2227
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1949
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 11121/50000 (22.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.3794s / 51823.1334 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2190
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1955
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 11141/50000 (22.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.8839s / 52078.0174 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2118
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1892
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 11161/50000 (22.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1600s / 52333.1773 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2285
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1995
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 11181/50000 (22.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9949s / 52592.1723 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2146
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1927
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11201/50000 (22.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6860s / 52847.8583 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2228
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2016
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 11221/50000 (22.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4991s / 53105.3574 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2230
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2008
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11241/50000 (22.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1310s / 53363.4884 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2070
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1808
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 11261/50000 (22.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7151s / 53622.2035 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2274
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2028
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 11281/50000 (22.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.7722s / 53875.9757 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2287
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2027
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 11301/50000 (22.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5982s / 54131.5739 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2260
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2022
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 11321/50000 (22.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7825s / 54387.3564 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2142
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1955
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 11341/50000 (22.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6716s / 54644.0281 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2240
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1910
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 11361/50000 (22.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5394s / 54899.5675 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2208
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1980
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 11381/50000 (22.7620%),                 avg. length: 2966.85,                last time consumption/overall running time: 256.2789s / 55155.8464 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2040
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1797
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 11401/50000 (22.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2384s / 55413.0848 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2221
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1960
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11421/50000 (22.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3680s / 55671.4528 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2297
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2005
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 11441/50000 (22.8820%),                 avg. length: 2980.45,                last time consumption/overall running time: 254.5518s / 55926.0046 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2103
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1881
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 11461/50000 (22.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1266s / 56183.1311 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2215
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1940
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 11481/50000 (22.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9414s / 56439.0725 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2316
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2093
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11501/50000 (23.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7620s / 56694.8345 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2373
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2003
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 11521/50000 (23.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1813s / 56953.0158 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2287
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2116
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11541/50000 (23.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5563s / 57208.5721 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2251
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1993
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 11561/50000 (23.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6484s / 57466.2205 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2320
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2103
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 11581/50000 (23.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6732s / 57721.8936 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2229
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1922
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 11601/50000 (23.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4705s / 57978.3641 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2366
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2003
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 11621/50000 (23.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5821s / 58233.9463 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2239
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2046
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 11641/50000 (23.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8541s / 58489.8004 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2293
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1882
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 11661/50000 (23.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.8817s / 58744.6820 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2336
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2056
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 11681/50000 (23.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1020s / 58999.7840 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2233
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2071
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11701/50000 (23.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.1575s / 59252.9415 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2320
env0_second_0:                 episode reward: 0.4500,                 loss: -0.2082
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 11721/50000 (23.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.7329s / 59509.6744 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2356
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2050
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 11741/50000 (23.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.7116s / 59763.3859 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2254
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1943
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 11761/50000 (23.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5877s / 60018.9736 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2259
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2005
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 11781/50000 (23.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5362s / 60274.5098 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2455
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2115
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11801/50000 (23.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9688s / 60529.4787 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2297
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2070
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 11821/50000 (23.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9355s / 60786.4141 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2275
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2051
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 11841/50000 (23.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0033s / 61043.4174 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2304
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2082
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 11861/50000 (23.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5743s / 61298.9917 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2249
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2002
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 11881/50000 (23.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9618s / 61555.9536 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2221
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1919
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 11901/50000 (23.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9947s / 61813.9483 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2254
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1916
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 11921/50000 (23.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6636s / 62071.6119 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2038
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1777
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 11941/50000 (23.8820%),                 avg. length: 2951.3,                last time consumption/overall running time: 254.3585s / 62325.9704 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2225
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1940
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11961/50000 (23.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0352s / 62583.0056 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2216
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1876
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 11981/50000 (23.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1691s / 62840.1747 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2277
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2001
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12001/50000 (24.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0651s / 63098.2397 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2222
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1934
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12021/50000 (24.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2581s / 63354.4979 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2326
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1819
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12041/50000 (24.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6626s / 63610.1605 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2310
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2008
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12061/50000 (24.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.1217s / 63864.2823 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2383
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2055
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 12081/50000 (24.1620%),                 avg. length: 2997.05,                last time consumption/overall running time: 256.0143s / 64120.2966 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2117
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1809
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12101/50000 (24.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4793s / 64376.7759 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2135
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1794
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12121/50000 (24.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9610s / 64631.7369 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2196
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1943
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 12141/50000 (24.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9814s / 64887.7184 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2290
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2025
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12161/50000 (24.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.4960s / 65141.2144 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2177
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1949
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12181/50000 (24.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2134s / 65396.4278 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2233
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1927
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 12201/50000 (24.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.9875s / 65649.4154 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2339
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2030
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 12221/50000 (24.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.1804s / 65901.5958 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2270
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1887
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 12241/50000 (24.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.5284s / 66155.1241 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2198
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1941
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12261/50000 (24.5220%),                 avg. length: 2945.75,                last time consumption/overall running time: 250.1414s / 66405.2655 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2190
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1899
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 12281/50000 (24.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5732s / 66662.8387 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2256
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1865
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 12301/50000 (24.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7322s / 66920.5709 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2109
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1872
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 12321/50000 (24.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1850s / 67178.7559 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2082
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1912
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 12341/50000 (24.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8079s / 67436.5639 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2146
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1880
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 12361/50000 (24.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.6018s / 67691.1656 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2165
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1919
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 12381/50000 (24.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.0318s / 67947.1974 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2161
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1834
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 12401/50000 (24.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3188s / 68203.5162 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2234
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1960
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12421/50000 (24.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4008s / 68459.9170 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2219
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1990
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 12441/50000 (24.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8176s / 68715.7346 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2290
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1920
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 12461/50000 (24.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.0591s / 68971.7937 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2349
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2083
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12481/50000 (24.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.0319s / 69224.8256 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2252
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2011
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12501/50000 (25.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9444s / 69483.7700 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2283
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1936
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12521/50000 (25.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6653s / 69740.4354 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2377
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2022
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12541/50000 (25.0820%),                 avg. length: 2923.25,                last time consumption/overall running time: 246.4576s / 69986.8930 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2161
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1901
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12561/50000 (25.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.6057s / 70240.4986 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2326
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1968
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 12581/50000 (25.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6106s / 70496.1093 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2267
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2032
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 12601/50000 (25.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8002s / 70751.9095 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2208
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1917
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 12621/50000 (25.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.3986s / 71005.3081 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2223
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1994
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 12641/50000 (25.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6247s / 71261.9328 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2211
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1985
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12661/50000 (25.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.4259s / 71517.3587 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2287
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2081
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 12681/50000 (25.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.2267s / 71770.5855 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2210
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1883
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 12701/50000 (25.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.9396s / 72025.5251 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2381
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1999
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12721/50000 (25.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.4955s / 72280.0206 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2344
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2122
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12741/50000 (25.4820%),                 avg. length: 2964.75,                last time consumption/overall running time: 251.6111s / 72531.6317 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2047
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1760
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 12761/50000 (25.5220%),                 avg. length: 2944.2,                last time consumption/overall running time: 252.1594s / 72783.7911 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2046
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1763
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 12781/50000 (25.5620%),                 avg. length: 2931.5,                last time consumption/overall running time: 251.1989s / 73034.9900 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2005
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1691
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 12801/50000 (25.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.9258s / 73288.9157 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2169
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1902
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 12821/50000 (25.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.5596s / 73543.4753 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2105
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1810
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 12841/50000 (25.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2942s / 73798.7695 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2146
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1845
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 12861/50000 (25.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9083s / 74054.6778 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2216
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1910
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 12881/50000 (25.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7591s / 74312.4369 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2189
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1750
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 12901/50000 (25.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8545s / 74568.2914 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2176
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1934
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 12921/50000 (25.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4169s / 74824.7083 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.1981
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1583
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 12941/50000 (25.8820%),                 avg. length: 2951.1,                last time consumption/overall running time: 251.1769s / 75075.8851 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2096
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1837
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 12961/50000 (25.9220%),                 avg. length: 2933.0,                last time consumption/overall running time: 250.8764s / 75326.7615 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.1775
env0_second_0:                 episode reward: -0.9500,                 loss: -0.1542
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 12981/50000 (25.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.3071s / 75582.0686 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2068
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1855
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13001/50000 (26.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.0334s / 75835.1020 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2134
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1896
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 13021/50000 (26.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4102s / 76091.5123 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2256
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1962
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 13041/50000 (26.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 251.4919s / 76343.0042 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1981
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 13061/50000 (26.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5779s / 76599.5821 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2329
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2064
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13081/50000 (26.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8753s / 76855.4574 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2242
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1988
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 13101/50000 (26.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.3916s / 77110.8490 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2205
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1903
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 13121/50000 (26.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6121s / 77370.4611 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2268
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1909
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 13141/50000 (26.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.3235s / 77625.7846 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2198
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1940
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 13161/50000 (26.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5219s / 77882.3065 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2182
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1944
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 13181/50000 (26.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9999s / 78139.3064 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2206
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1903
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 13201/50000 (26.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.4248s / 78393.7312 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2238
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2016
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 13221/50000 (26.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.6158s / 78646.3470 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2281
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1994
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 13241/50000 (26.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 252.5599s / 78898.9069 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2206
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1985
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 13261/50000 (26.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.4903s / 79154.3973 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2186
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1963
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 13281/50000 (26.5620%),                 avg. length: 2878.85,                last time consumption/overall running time: 246.4475s / 79400.8448 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1921
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1510
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 13301/50000 (26.6020%),                 avg. length: 2615.75,                last time consumption/overall running time: 225.0712s / 79625.9160 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1284
env0_second_0:                 episode reward: -0.9000,                 loss: -0.0608
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 13321/50000 (26.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6281s / 79881.5441 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2027
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1615
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13341/50000 (26.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1265s / 80136.6706 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2076
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1611
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13361/50000 (26.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9642s / 80392.6349 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2114
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1635
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13381/50000 (26.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6509s / 80651.2857 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2123
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1691
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 13401/50000 (26.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4400s / 80909.7257 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2177
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1610
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 13421/50000 (26.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2109s / 81166.9367 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2133
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1633
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 13441/50000 (26.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.7276s / 81423.6643 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2135
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1772
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 13461/50000 (26.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7680s / 81683.4323 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2276
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1900
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13481/50000 (26.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7404s / 81939.1727 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2108
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1776
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13501/50000 (27.0020%),                 avg. length: 2794.0,                last time consumption/overall running time: 238.5889s / 82177.7615 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.1770
env0_second_0:                 episode reward: 1.3000,                 loss: -0.1364
env1_first_0:                 episode reward: -1.0500,                 loss: nan
env1_second_0:                 episode reward: 1.0500,                 loss: nan
Episode: 13521/50000 (27.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2043s / 82435.9659 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2132
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1787
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 13541/50000 (27.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2712s / 82694.2371 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2119
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1768
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 13561/50000 (27.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9799s / 82953.2169 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2195
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1791
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 13581/50000 (27.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3360s / 83209.5530 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2088
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1821
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 13601/50000 (27.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6126s / 83468.1656 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2069
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1791
env1_first_0:                 episode reward: -0.7000,                 loss: nan
env1_second_0:                 episode reward: 0.7000,                 loss: nan
Episode: 13621/50000 (27.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7669s / 83723.9325 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2090
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1791
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 13641/50000 (27.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9991s / 83980.9316 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2061
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1818
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 13661/50000 (27.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5305s / 84238.4622 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2079
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1833
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 13681/50000 (27.3620%),                 avg. length: 2951.95,                last time consumption/overall running time: 251.4183s / 84489.8805 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.1923
env0_second_0:                 episode reward: -0.9000,                 loss: -0.1726
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 13701/50000 (27.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3603s / 84747.2408 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2068
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1827
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 13721/50000 (27.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0205s / 85005.2613 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2067
env0_second_0:                 episode reward: -0.6000,                 loss: -0.1818
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 13741/50000 (27.4820%),                 avg. length: 2863.8,                last time consumption/overall running time: 245.5755s / 85250.8368 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1814
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1547
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 13761/50000 (27.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1469s / 85507.9837 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2069
env0_second_0:                 episode reward: -0.9000,                 loss: -0.1807
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 13781/50000 (27.5620%),                 avg. length: 2904.35,                last time consumption/overall running time: 248.1130s / 85756.0967 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1954
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1700
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 13801/50000 (27.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3563s / 86013.4529 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2222
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1885
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 13821/50000 (27.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8176s / 86270.2705 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2177
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1879
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 13841/50000 (27.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3489s / 86526.6194 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2146
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1802
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13861/50000 (27.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5881s / 86784.2075 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2136
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1903
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 13881/50000 (27.7620%),                 avg. length: 2948.3,                last time consumption/overall running time: 252.5654s / 87036.7730 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1972
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1640
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 13901/50000 (27.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8587s / 87295.6317 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2157
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1761
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 13921/50000 (27.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7783s / 87553.4100 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2208
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1891
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 13941/50000 (27.8820%),                 avg. length: 2944.1,                last time consumption/overall running time: 254.0299s / 87807.4399 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2068
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1832
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 13961/50000 (27.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1068s / 88064.5466 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2190
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1893
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 13981/50000 (27.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9027s / 88322.4494 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2143
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1818
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 14001/50000 (28.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2940s / 88580.7434 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2143
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1926
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14021/50000 (28.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0516s / 88837.7949 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2217
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1896
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 14041/50000 (28.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1867s / 89092.9816 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2199
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1886
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 14061/50000 (28.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8135s / 89350.7951 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2228
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1881
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14081/50000 (28.1620%),                 avg. length: 2990.95,                last time consumption/overall running time: 256.4786s / 89607.2738 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2209
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1747
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 14101/50000 (28.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6140s / 89866.8878 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2231
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1900
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 14121/50000 (28.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7949s / 90127.6827 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2248
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1977
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14141/50000 (28.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3684s / 90385.0511 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2337
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1723
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14161/50000 (28.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2592s / 90642.3103 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2211
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1832
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 14181/50000 (28.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0548s / 90899.3651 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1969
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1660
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14201/50000 (28.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3607s / 91156.7258 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2156
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1865
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 14221/50000 (28.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2303s / 91415.9561 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2216
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1880
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 14241/50000 (28.4820%),                 avg. length: 2977.2,                last time consumption/overall running time: 255.7613s / 91671.7174 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.1816
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1486
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 14261/50000 (28.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5737s / 91928.2911 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2111
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1879
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14281/50000 (28.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7296s / 92188.0207 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2165
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1898
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 14301/50000 (28.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1769s / 92445.1975 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2521
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1945
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14321/50000 (28.6420%),                 avg. length: 1815.05,                last time consumption/overall running time: 157.8272s / 92603.0247 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.0709
env0_second_0:                 episode reward: -2.6000,                 loss: -0.0275
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 14341/50000 (28.6820%),                 avg. length: 2810.65,                last time consumption/overall running time: 241.6472s / 92844.6719 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.1622
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1283
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14361/50000 (28.7220%),                 avg. length: 2990.2,                last time consumption/overall running time: 257.8499s / 93102.5219 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2119
env0_second_0:                 episode reward: -1.1500,                 loss: -0.1857
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 14381/50000 (28.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9238s / 93360.4457 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2195
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1826
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 14401/50000 (28.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2081s / 93619.6537 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2289
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1641
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 14421/50000 (28.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7121s / 93878.3658 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2257
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1848
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14441/50000 (28.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.7061s / 94134.0719 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2278
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1817
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 14461/50000 (28.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2406s / 94391.3125 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2389
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1695
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14481/50000 (28.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5446s / 94647.8571 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2142
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1838
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 14501/50000 (29.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6745s / 94904.5316 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2161
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1805
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 14521/50000 (29.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8577s / 95165.3893 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2206
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1758
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14541/50000 (29.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2411s / 95422.6304 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2132
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1771
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 14561/50000 (29.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3501s / 95679.9805 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2217
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1931
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 14581/50000 (29.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9157s / 95935.8962 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2323
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1996
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 14601/50000 (29.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2928s / 96192.1891 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2332
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1943
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 14621/50000 (29.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4476s / 96449.6367 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2263
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1958
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 14641/50000 (29.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8788s / 96707.5155 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2290
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1992
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 14661/50000 (29.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.7919s / 96964.3074 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2288
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1980
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 14681/50000 (29.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7334s / 97223.0408 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2338
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2066
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 14701/50000 (29.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1529s / 97480.1937 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2294
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2071
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 14721/50000 (29.4420%),                 avg. length: 2924.05,                last time consumption/overall running time: 252.8420s / 97733.0357 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2053
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1802
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 14741/50000 (29.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3809s / 97991.4166 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2265
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1940
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 14761/50000 (29.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2422s / 98249.6588 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2302
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2008
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 14781/50000 (29.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3875s / 98509.0463 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2083
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1952
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 14801/50000 (29.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0247s / 98769.0710 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2218
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1864
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 14821/50000 (29.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9967s / 99028.0677 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2080
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1023
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 14841/50000 (29.6820%),                 avg. length: 2964.5,                last time consumption/overall running time: 257.3931s / 99285.4609 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2009
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1484
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 14861/50000 (29.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5613s / 99544.0222 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2265
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1936
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 14881/50000 (29.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9263s / 99802.9485 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2263
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1785
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 14901/50000 (29.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9473s / 100063.8958 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2218
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1676
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 14921/50000 (29.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3397s / 100323.2356 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2298
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1977
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 14941/50000 (29.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4809s / 100583.7165 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2206
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1945
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 14961/50000 (29.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4142s / 100843.1307 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2361
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2008
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 14981/50000 (29.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5518s / 101102.6825 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2395
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2073
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15001/50000 (30.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8752s / 101361.5577 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2339
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1924
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15021/50000 (30.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6288s / 101622.1865 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2343
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1878
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15041/50000 (30.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9084s / 101883.0950 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2165
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1878
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15061/50000 (30.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0152s / 102142.1102 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2309
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1911
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15081/50000 (30.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1230s / 102402.2331 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2250
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1729
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 15101/50000 (30.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8879s / 102662.1210 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1794
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15121/50000 (30.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0877s / 102921.2087 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2288
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1865
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15141/50000 (30.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7105s / 103178.9193 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2297
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1891
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 15161/50000 (30.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5344s / 103438.4536 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2232
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2023
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15181/50000 (30.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8418s / 103697.2955 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2305
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1860
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15201/50000 (30.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8842s / 103956.1797 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2221
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1596
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15221/50000 (30.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4052s / 104216.5849 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2249
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1944
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15241/50000 (30.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9972s / 104477.5821 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2244
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1846
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15261/50000 (30.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3668s / 104736.9488 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2348
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1986
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15281/50000 (30.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0469s / 104996.9957 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2358
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1909
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15301/50000 (30.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.5226s / 105258.5183 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2354
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1940
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15321/50000 (30.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4922s / 105518.0105 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2221
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1969
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15341/50000 (30.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9985s / 105776.0090 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2175
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1911
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 15361/50000 (30.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6594s / 106035.6684 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1840
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15381/50000 (30.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.8820s / 106297.5504 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2313
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1931
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15401/50000 (30.8020%),                 avg. length: 2460.7,                last time consumption/overall running time: 215.2118s / 106512.7622 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1562
env0_second_0:                 episode reward: -1.5500,                 loss: -0.0947
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 15421/50000 (30.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0210s / 106773.7832 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2247
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1960
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 15441/50000 (30.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7608s / 107033.5440 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2352
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2003
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15461/50000 (30.9220%),                 avg. length: 2951.85,                last time consumption/overall running time: 256.6048s / 107290.1487 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2236
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1397
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 15481/50000 (30.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.3833s / 107551.5321 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2386
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1942
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15501/50000 (31.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4195s / 107810.9516 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2328
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1926
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15521/50000 (31.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8320s / 108071.7836 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2332
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1845
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 15541/50000 (31.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9204s / 108331.7041 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2281
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1803
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 15561/50000 (31.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8083s / 108591.5124 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2273
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1974
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15581/50000 (31.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9057s / 108851.4181 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2312
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1981
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15601/50000 (31.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3669s / 109111.7850 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1776
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 15621/50000 (31.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4420s / 109372.2269 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2197
env0_second_0:                 episode reward: -0.6000,                 loss: -0.1884
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 15641/50000 (31.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9932s / 109630.2201 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2260
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2046
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15661/50000 (31.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1650s / 109889.3851 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2307
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1909
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15681/50000 (31.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2339s / 110147.6190 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2325
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1935
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15701/50000 (31.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1985s / 110406.8175 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2272
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1864
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15721/50000 (31.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6770s / 110664.4945 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2330
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2000
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15741/50000 (31.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6460s / 110924.1404 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2272
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1896
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 15761/50000 (31.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2041s / 111182.3445 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2344
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1810
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 15781/50000 (31.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2456s / 111440.5902 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2347
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2046
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15801/50000 (31.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1410s / 111699.7312 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2360
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2046
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15821/50000 (31.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2154s / 111959.9466 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2363
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2077
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15841/50000 (31.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5180s / 112220.4646 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2292
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2027
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 15861/50000 (31.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2466s / 112478.7112 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2307
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1784
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 15881/50000 (31.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2411s / 112737.9524 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2215
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1768
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 15901/50000 (31.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7372s / 112997.6896 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2337
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2010
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15921/50000 (31.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1865s / 113256.8761 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2341
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1877
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 15941/50000 (31.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8368s / 113515.7129 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2330
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2070
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 15961/50000 (31.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5566s / 113775.2695 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2293
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1969
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 15981/50000 (31.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1940s / 114036.4635 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2330
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1966
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 16001/50000 (32.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5112s / 114294.9747 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2331
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2089
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 16021/50000 (32.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2285s / 114554.2032 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2242
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1828
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16041/50000 (32.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1363s / 114813.3395 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2250
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1935
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 16061/50000 (32.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5371s / 115069.8766 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2290
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1902
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16081/50000 (32.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0615s / 115329.9381 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2324
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1677
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 16101/50000 (32.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5710s / 115589.5091 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2313
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2002
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 16121/50000 (32.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9385s / 115848.4476 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2326
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2034
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 16141/50000 (32.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9125s / 116106.3601 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2337
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1879
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16161/50000 (32.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5204s / 116364.8806 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2276
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1957
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16181/50000 (32.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6242s / 116622.5048 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2283
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1927
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16201/50000 (32.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0505s / 116882.5553 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2194
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1849
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16221/50000 (32.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9812s / 117142.5365 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2300
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1881
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16241/50000 (32.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3018s / 117400.8383 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2179
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1835
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16261/50000 (32.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6672s / 117658.5055 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2255
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1520
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 16281/50000 (32.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5376s / 117916.0431 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2233
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1739
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16301/50000 (32.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0118s / 118176.0549 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2271
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1984
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16321/50000 (32.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3300s / 118436.3849 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2049
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1797
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16341/50000 (32.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8459s / 118694.2308 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2215
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1894
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 16361/50000 (32.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5589s / 118951.7898 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2213
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1902
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16381/50000 (32.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6225s / 119211.4123 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2242
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2022
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16401/50000 (32.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0892s / 119470.5015 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2190
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1925
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16421/50000 (32.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4611s / 119729.9626 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1945
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 16441/50000 (32.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7999s / 119989.7624 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2175
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1765
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16461/50000 (32.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7722s / 120248.5346 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2320
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1859
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16481/50000 (32.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8487s / 120507.3834 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2357
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1893
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 16501/50000 (33.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2231s / 120766.6065 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2211
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1815
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16521/50000 (33.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5372s / 121026.1437 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2207
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1667
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16541/50000 (33.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4773s / 121284.6210 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2274
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1945
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 16561/50000 (33.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9814s / 121544.6024 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2272
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1984
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16581/50000 (33.1620%),                 avg. length: 2936.5,                last time consumption/overall running time: 251.3987s / 121796.0011 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2102
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1584
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 16601/50000 (33.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1041s / 122055.1053 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2237
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1909
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16621/50000 (33.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4710s / 122313.5763 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2082
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1818
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 16641/50000 (33.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5886s / 122574.1648 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2105
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1895
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16661/50000 (33.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3032s / 122833.4680 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2257
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1904
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 16681/50000 (33.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9903s / 123091.4583 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2240
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1879
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 16701/50000 (33.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8322s / 123348.2905 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2286
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2019
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16721/50000 (33.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.5073s / 123603.7979 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1983
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16741/50000 (33.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0150s / 123861.8128 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2171
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1860
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16761/50000 (33.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5611s / 124120.3740 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2249
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1974
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16781/50000 (33.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8427s / 124378.2167 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2319
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1945
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 16801/50000 (33.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6716s / 124636.8883 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2357
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2104
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16821/50000 (33.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7691s / 124895.6574 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2252
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1849
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16841/50000 (33.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3574s / 125153.0148 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2197
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1855
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 16861/50000 (33.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.3784s / 125414.3932 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2229
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1912
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16881/50000 (33.7620%),                 avg. length: 2928.45,                last time consumption/overall running time: 253.2654s / 125667.6586 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2076
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1709
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 16901/50000 (33.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6858s / 125926.3444 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2247
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2036
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 16921/50000 (33.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6129s / 126186.9573 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2073
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1823
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 16941/50000 (33.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5419s / 126446.4992 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2215
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1899
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 16961/50000 (33.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4587s / 126705.9580 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2336
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1875
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 16981/50000 (33.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4492s / 126965.4072 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2213
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1968
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17001/50000 (34.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0650s / 127225.4722 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2298
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2057
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17021/50000 (34.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7259s / 127485.1981 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2009
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 17041/50000 (34.0820%),                 avg. length: 2957.9,                last time consumption/overall running time: 256.6756s / 127741.8737 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2219
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1654
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 17061/50000 (34.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2542s / 128002.1280 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2266
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1940
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 17081/50000 (34.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5654s / 128261.6934 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2298
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1984
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 17101/50000 (34.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2012s / 128520.8946 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2359
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1996
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 17121/50000 (34.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6307s / 128779.5253 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2257
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1978
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 17141/50000 (34.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4438s / 129037.9692 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2253
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2022
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17161/50000 (34.3220%),                 avg. length: 2739.55,                last time consumption/overall running time: 237.6923s / 129275.6615 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2045
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1551
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 17181/50000 (34.3620%),                 avg. length: 2811.7,                last time consumption/overall running time: 245.0761s / 129520.7376 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1646
env0_second_0:                 episode reward: -1.0000,                 loss: -0.1400
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 17201/50000 (34.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1114s / 129781.8490 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.1981
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1676
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 17221/50000 (34.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0273s / 130042.8763 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2054
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1810
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17241/50000 (34.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2201s / 130303.0964 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2316
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1876
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17261/50000 (34.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2579s / 130562.3542 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2353
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1991
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 17281/50000 (34.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.6979s / 130824.0521 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2307
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1817
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17301/50000 (34.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6563s / 131083.7084 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2345
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1927
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 17321/50000 (34.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9555s / 131343.6640 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2284
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1915
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17341/50000 (34.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8883s / 131601.5522 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2342
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1903
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17361/50000 (34.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1451s / 131861.6973 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2112
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1809
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17381/50000 (34.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0777s / 132121.7750 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2122
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1787
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17401/50000 (34.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9344s / 132381.7094 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2266
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1974
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 17421/50000 (34.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1584s / 132641.8677 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2238
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1816
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17441/50000 (34.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5008s / 132902.3686 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1947
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 17461/50000 (34.9220%),                 avg. length: 2909.35,                last time consumption/overall running time: 252.4450s / 133154.8135 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2200
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1855
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17481/50000 (34.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6996s / 133414.5132 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2276
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1948
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 17501/50000 (35.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0502s / 133674.5634 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2351
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2049
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17521/50000 (35.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0363s / 133934.5998 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2326
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2081
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17541/50000 (35.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9448s / 134195.5446 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2303
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1998
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 17561/50000 (35.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8894s / 134455.4339 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2381
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1954
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17581/50000 (35.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0956s / 134715.5296 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2255
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1997
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17601/50000 (35.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3265s / 134974.8561 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2381
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2069
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 17621/50000 (35.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7568s / 135234.6129 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2357
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2048
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 17641/50000 (35.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3010s / 135494.9139 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2405
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1926
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17661/50000 (35.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9106s / 135754.8245 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2298
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1927
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 17681/50000 (35.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7128s / 136013.5373 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2304
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1905
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17701/50000 (35.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8166s / 136273.3539 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2242
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1901
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17721/50000 (35.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7415s / 136533.0954 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2292
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1962
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 17741/50000 (35.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2513s / 136791.3467 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2244
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1852
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17761/50000 (35.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1408s / 137050.4875 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2241
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1915
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 17781/50000 (35.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2848s / 137308.7723 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2197
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1865
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 17801/50000 (35.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6723s / 137568.4446 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2333
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1967
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17821/50000 (35.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5979s / 137827.0425 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2301
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1944
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 17841/50000 (35.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7615s / 138085.8040 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2350
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2026
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17861/50000 (35.7220%),                 avg. length: 2968.25,                last time consumption/overall running time: 257.2077s / 138343.0117 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2158
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1788
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 17881/50000 (35.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1831s / 138601.1948 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2296
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2042
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17901/50000 (35.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2832s / 138860.4780 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2370
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2066
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 17921/50000 (35.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6535s / 139120.1314 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2295
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1957
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 17941/50000 (35.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2053s / 139379.3367 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2279
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1985
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 17961/50000 (35.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7542s / 139638.0909 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2300
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2082
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 17981/50000 (35.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5992s / 139897.6902 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2311
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2009
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 18001/50000 (36.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7855s / 140156.4757 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2223
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1959
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 18021/50000 (36.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9914s / 140415.4671 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2282
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2046
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 18041/50000 (36.0820%),                 avg. length: 2939.4,                last time consumption/overall running time: 253.4088s / 140668.8759 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2092
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1838
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18061/50000 (36.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1761s / 140928.0520 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2190
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1956
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18081/50000 (36.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9097s / 141186.9617 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2297
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1911
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18101/50000 (36.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8007s / 141446.7624 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2337
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2021
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 18121/50000 (36.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3244s / 141705.0867 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2390
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2041
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18141/50000 (36.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8682s / 141962.9549 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1917
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18161/50000 (36.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0390s / 142221.9939 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2381
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2000
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 18181/50000 (36.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6748s / 142481.6687 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2495
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1939
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 18201/50000 (36.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2525s / 142741.9212 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2409
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1877
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18221/50000 (36.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5015s / 143001.4227 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2257
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2005
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 18241/50000 (36.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9782s / 143260.4009 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2310
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2014
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18261/50000 (36.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7114s / 143520.1123 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2233
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1927
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18281/50000 (36.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1032s / 143780.2154 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2410
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1682
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18301/50000 (36.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7441s / 144039.9595 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2407
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2090
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18321/50000 (36.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1671s / 144298.1267 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2362
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2002
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18341/50000 (36.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7536s / 144558.8803 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2261
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1995
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18361/50000 (36.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0013s / 144818.8816 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2360
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2066
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18381/50000 (36.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5608s / 145079.4424 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2257
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1866
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18401/50000 (36.8020%),                 avg. length: 2776.75,                last time consumption/overall running time: 239.9588s / 145319.4012 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.1759
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1480
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 18421/50000 (36.8420%),                 avg. length: 2860.65,                last time consumption/overall running time: 248.4443s / 145567.8455 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1959
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1548
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 18441/50000 (36.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6931s / 145826.5387 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2342
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1867
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18461/50000 (36.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0281s / 146086.5667 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1813
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 18481/50000 (36.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3745s / 146345.9413 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2185
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1884
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18501/50000 (37.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2185s / 146603.1597 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2350
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1956
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 18521/50000 (37.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9918s / 146860.1515 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2343
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1985
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 18541/50000 (37.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3279s / 147119.4794 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2270
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1875
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 18561/50000 (37.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7448s / 147378.2242 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2280
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1597
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 18581/50000 (37.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5321s / 147636.7563 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2327
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2043
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18601/50000 (37.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7449s / 147896.5012 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2326
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1866
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 18621/50000 (37.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3370s / 148156.8381 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2321
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1186
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18641/50000 (37.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0686s / 148415.9067 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2200
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1869
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18661/50000 (37.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4362s / 148675.3430 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2211
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1591
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18681/50000 (37.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7577s / 148935.1006 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2262
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1698
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 18701/50000 (37.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1276s / 149193.2282 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2222
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1872
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 18721/50000 (37.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6318s / 149450.8600 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2371
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1886
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18741/50000 (37.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1282s / 149708.9882 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2386
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1986
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 18761/50000 (37.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5143s / 149966.5025 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2386
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1993
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18781/50000 (37.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5499s / 150225.0524 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2345
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1867
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18801/50000 (37.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9383s / 150483.9907 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2318
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1957
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 18821/50000 (37.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0842s / 150742.0749 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2180
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1743
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 18841/50000 (37.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4822s / 151000.5572 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2299
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2013
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18861/50000 (37.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1013s / 151257.6585 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2258
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1977
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 18881/50000 (37.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5711s / 151517.2295 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2383
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2048
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 18901/50000 (37.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3568s / 151775.5864 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2346
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1882
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 18921/50000 (37.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2753s / 152034.8617 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2325
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1865
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 18941/50000 (37.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0877s / 152295.9494 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2273
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1991
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 18961/50000 (37.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7270s / 152555.6763 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2334
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2005
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 18981/50000 (37.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7343s / 152816.4106 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2280
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2096
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19001/50000 (38.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4471s / 153075.8578 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2274
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2003
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 19021/50000 (38.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4253s / 153336.2831 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2408
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1967
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19041/50000 (38.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2332s / 153592.5164 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2346
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2026
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 19061/50000 (38.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6363s / 153852.1527 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2344
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2068
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 19081/50000 (38.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0379s / 154111.1906 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2303
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2008
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19101/50000 (38.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8770s / 154369.0676 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2314
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1899
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19121/50000 (38.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3916s / 154629.4592 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2393
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2075
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 19141/50000 (38.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2394s / 154887.6986 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2419
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2048
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19161/50000 (38.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3114s / 155147.0100 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2256
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1889
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19181/50000 (38.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1345s / 155407.1446 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1865
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 19201/50000 (38.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5634s / 155666.7080 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2341
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1998
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 19221/50000 (38.4420%),                 avg. length: 2963.0,                last time consumption/overall running time: 255.6969s / 155922.4049 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2359
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1759
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19241/50000 (38.4820%),                 avg. length: 2902.35,                last time consumption/overall running time: 252.8024s / 156175.2073 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.1936
env0_second_0:                 episode reward: -1.8000,                 loss: -0.0851
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 19261/50000 (38.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8716s / 156434.0789 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2297
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1908
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19281/50000 (38.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0279s / 156692.1068 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2165
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1702
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 19301/50000 (38.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7137s / 156950.8205 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1872
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19321/50000 (38.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8135s / 157209.6340 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2364
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1520
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 19341/50000 (38.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3154s / 157468.9494 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2347
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1788
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19361/50000 (38.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4301s / 157729.3795 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2233
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1681
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19381/50000 (38.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1909s / 157988.5705 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2244
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1805
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 19401/50000 (38.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0081s / 158247.5786 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2282
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1703
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 19421/50000 (38.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6192s / 158508.1978 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2280
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1985
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19441/50000 (38.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7248s / 158767.9226 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2215
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1881
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 19461/50000 (38.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4674s / 159028.3900 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2229
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1819
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 19481/50000 (38.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8129s / 159287.2028 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2309
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1956
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19501/50000 (39.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4693s / 159546.6722 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2263
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1875
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 19521/50000 (39.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5639s / 159806.2361 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2056
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1715
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 19541/50000 (39.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7116s / 160066.9477 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1730
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19561/50000 (39.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7331s / 160326.6808 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2235
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1912
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 19581/50000 (39.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.1283s / 160588.8091 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2223
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1910
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19601/50000 (39.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5114s / 160849.3205 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2238
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1876
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 19621/50000 (39.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9671s / 161107.2876 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2166
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1565
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 19641/50000 (39.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1137s / 161367.4013 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2224
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1879
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19661/50000 (39.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2539s / 161627.6552 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2233
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1824
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19681/50000 (39.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1967s / 161885.8519 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2179
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1592
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19701/50000 (39.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.4369s / 162147.2888 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2143
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1838
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 19721/50000 (39.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8730s / 162407.1618 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2288
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1968
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19741/50000 (39.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2597s / 162666.4215 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2204
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1891
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 19761/50000 (39.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8857s / 162927.3073 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2126
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1852
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 19781/50000 (39.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9362s / 163188.2435 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2230
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1864
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 19801/50000 (39.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7816s / 163447.0251 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2271
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1985
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19821/50000 (39.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3663s / 163706.3914 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2199
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1917
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 19841/50000 (39.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6534s / 163965.0449 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2289
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1934
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19861/50000 (39.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3904s / 164223.4353 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2370
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1874
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 19881/50000 (39.7620%),                 avg. length: 1855.5,                last time consumption/overall running time: 164.9830s / 164388.4183 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.0852
env0_second_0:                 episode reward: -1.8500,                 loss: 0.0735
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 19901/50000 (39.8020%),                 avg. length: 2673.8,                last time consumption/overall running time: 234.3856s / 164622.8039 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.1230
env0_second_0:                 episode reward: -1.1500,                 loss: -0.0647
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 19921/50000 (39.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.3011s / 164884.1050 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.1999
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1239
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 19941/50000 (39.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3851s / 165142.4901 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2061
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1266
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 19961/50000 (39.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9274s / 165399.4175 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2020
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1193
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 19981/50000 (39.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7454s / 165659.1628 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2027
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1190
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 20001/50000 (40.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1024s / 165920.2652 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2080
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1414
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20021/50000 (40.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1211s / 166180.3862 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2179
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1403
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 20041/50000 (40.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6891s / 166440.0753 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2175
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1434
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 20061/50000 (40.1220%),                 avg. length: 2986.2,                last time consumption/overall running time: 258.3353s / 166698.4106 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2208
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1558
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20081/50000 (40.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7313s / 166957.1420 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2264
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1618
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20101/50000 (40.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2193s / 167217.3613 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2264
env0_second_0:                 episode reward: -0.2500,                 loss: 0.1548
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20121/50000 (40.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8499s / 167477.2112 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2284
env0_second_0:                 episode reward: -0.0500,                 loss: 0.1216
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20141/50000 (40.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7516s / 167736.9629 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2166
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0558
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 20161/50000 (40.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2464s / 167997.2092 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2111
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1163
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20181/50000 (40.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2849s / 168255.4941 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2098
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0957
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20201/50000 (40.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6143s / 168516.1085 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.1940
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1176
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 20221/50000 (40.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1889s / 168775.2974 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2019
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0888
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 20241/50000 (40.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8804s / 169035.1778 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2105
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1424
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20261/50000 (40.5220%),                 avg. length: 2790.6,                last time consumption/overall running time: 241.6807s / 169276.8585 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.1925
env0_second_0:                 episode reward: -0.8000,                 loss: -0.1397
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 20281/50000 (40.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9068s / 169534.7652 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2098
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1567
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20301/50000 (40.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4548s / 169794.2201 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2190
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1544
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20321/50000 (40.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3292s / 170053.5492 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2134
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1581
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20341/50000 (40.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0943s / 170313.6436 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2115
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1529
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20361/50000 (40.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0674s / 170572.7109 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2126
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1564
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 20381/50000 (40.7620%),                 avg. length: 2696.65,                last time consumption/overall running time: 234.2868s / 170806.9977 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.1701
env0_second_0:                 episode reward: -1.6500,                 loss: -0.1178
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 20401/50000 (40.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6642s / 171066.6620 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2151
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1528
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 20421/50000 (40.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7000s / 171324.3620 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2131
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1599
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 20441/50000 (40.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7287s / 171584.0907 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2171
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1543
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20461/50000 (40.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1035s / 171842.1942 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2143
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1547
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20481/50000 (40.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0960s / 172100.2902 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2114
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1655
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20501/50000 (41.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1574s / 172358.4475 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2103
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1670
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 20521/50000 (41.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7580s / 172616.2056 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2076
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1568
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20541/50000 (41.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.6769s / 172877.8825 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2092
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1471
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20561/50000 (41.1220%),                 avg. length: 2977.05,                last time consumption/overall running time: 258.6520s / 173136.5346 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.1978
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1405
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 20581/50000 (41.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.2005s / 173397.7351 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2140
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1487
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 20601/50000 (41.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7220s / 173658.4571 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2084
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1703
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20621/50000 (41.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0049s / 173918.4619 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2127
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1658
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20641/50000 (41.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2272s / 174178.6892 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2214
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1649
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 20661/50000 (41.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8583s / 174439.5475 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2283
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1748
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20681/50000 (41.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8625s / 174699.4100 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2208
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1505
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20701/50000 (41.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8893s / 174959.2993 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1671
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 20721/50000 (41.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6281s / 175219.9274 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2331
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1755
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20741/50000 (41.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.0699s / 175481.9974 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2379
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1825
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20761/50000 (41.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7996s / 175739.7970 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2278
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1853
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 20781/50000 (41.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6651s / 175999.4621 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2381
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1903
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20801/50000 (41.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5597s / 176260.0218 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2286
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1864
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 20821/50000 (41.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7655s / 176520.7873 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2276
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1932
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 20841/50000 (41.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8340s / 176780.6213 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2346
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1935
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 20861/50000 (41.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3263s / 177038.9475 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2346
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1918
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20881/50000 (41.7620%),                 avg. length: 2797.35,                last time consumption/overall running time: 244.1456s / 177283.0932 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.2077
env0_second_0:                 episode reward: -0.8000,                 loss: -0.1625
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 20901/50000 (41.8020%),                 avg. length: 2991.4,                last time consumption/overall running time: 259.8704s / 177542.9636 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2115
env0_second_0:                 episode reward: -1.1500,                 loss: -0.1842
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 20921/50000 (41.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6032s / 177801.5668 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2376
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2017
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 20941/50000 (41.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8001s / 178060.3669 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2312
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1935
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 20961/50000 (41.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 263.7478s / 178324.1147 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2292
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0278
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 20981/50000 (41.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7417s / 178584.8564 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2412
env0_second_0:                 episode reward: -0.2500,                 loss: -0.0476
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21001/50000 (42.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5355s / 178844.3919 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2375
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1249
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21021/50000 (42.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8316s / 179101.2235 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2389
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1640
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 21041/50000 (42.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8997s / 179359.1232 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2371
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1574
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21061/50000 (42.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4068s / 179619.5299 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2263
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1827
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 21081/50000 (42.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0962s / 179880.6261 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2240
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1816
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21101/50000 (42.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2483s / 180140.8744 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2363
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1673
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21121/50000 (42.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6861s / 180397.5605 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2252
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1718
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21141/50000 (42.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8854s / 180655.4459 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2336
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1910
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 21161/50000 (42.3220%),                 avg. length: 2967.3,                last time consumption/overall running time: 254.9586s / 180910.4045 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2273
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1754
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21181/50000 (42.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9063s / 181168.3108 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2427
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2046
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 21201/50000 (42.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8053s / 181427.1161 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2387
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1996
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21221/50000 (42.4420%),                 avg. length: 2855.2,                last time consumption/overall running time: 248.1406s / 181675.2567 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2132
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1430
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 21241/50000 (42.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6306s / 181935.8873 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2383
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1984
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21261/50000 (42.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9019s / 182195.7892 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2329
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1932
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 21281/50000 (42.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4793s / 182456.2685 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2397
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1932
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21301/50000 (42.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6884s / 182715.9570 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2422
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1788
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21321/50000 (42.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6253s / 182974.5822 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2464
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1984
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21341/50000 (42.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1958s / 183233.7780 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2368
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1937
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 21361/50000 (42.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0388s / 183491.8168 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2368
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2005
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 21381/50000 (42.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6704s / 183752.4871 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2270
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1895
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21401/50000 (42.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2965s / 184010.7836 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2243
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1880
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21421/50000 (42.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2932s / 184268.0768 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2156
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1679
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 21441/50000 (42.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4192s / 184526.4960 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2450
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1907
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 21461/50000 (42.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0904s / 184786.5865 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2379
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2039
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 21481/50000 (42.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2924s / 185044.8789 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2412
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2116
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21501/50000 (43.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2435s / 185303.1224 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2344
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1800
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21521/50000 (43.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1938s / 185562.3162 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2356
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1873
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21541/50000 (43.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8280s / 185820.1442 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2419
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2116
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 21561/50000 (43.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7162s / 186080.8604 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2387
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1696
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21581/50000 (43.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8971s / 186337.7575 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2408
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2035
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 21601/50000 (43.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0674s / 186598.8249 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2478
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2219
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21621/50000 (43.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2366s / 186859.0615 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2352
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1835
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 21641/50000 (43.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9097s / 187117.9712 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2289
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1530
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21661/50000 (43.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4618s / 187375.4331 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2353
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1970
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21681/50000 (43.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4768s / 187633.9098 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2479
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2019
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 21701/50000 (43.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5592s / 187891.4690 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2434
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2120
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21721/50000 (43.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6966s / 188149.1656 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2434
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2113
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21741/50000 (43.4820%),                 avg. length: 2982.05,                last time consumption/overall running time: 255.5936s / 188404.7592 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2220
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1694
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 21761/50000 (43.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4628s / 188664.2220 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2293
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1946
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 21781/50000 (43.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.2683s / 188925.4903 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2479
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1831
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21801/50000 (43.6020%),                 avg. length: 2980.6,                last time consumption/overall running time: 258.1877s / 189183.6780 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2275
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1840
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 21821/50000 (43.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5752s / 189441.2532 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2140
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1760
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21841/50000 (43.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7172s / 189700.9703 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2331
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1892
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21861/50000 (43.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9136s / 189960.8839 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2302
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1974
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 21881/50000 (43.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3487s / 190220.2326 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2357
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1901
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21901/50000 (43.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5200s / 190476.7526 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2431
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1754
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 21921/50000 (43.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3886s / 190733.1412 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2386
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1900
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 21941/50000 (43.8820%),                 avg. length: 2995.25,                last time consumption/overall running time: 255.9204s / 190989.0616 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2229
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1803
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 21961/50000 (43.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2058s / 191247.2674 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2529
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1605
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 21981/50000 (43.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2572s / 191504.5246 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2480
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2047
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 22001/50000 (44.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7299s / 191764.2546 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2395
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2057
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 22021/50000 (44.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9258s / 192022.1804 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2323
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1886
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22041/50000 (44.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3181s / 192278.4985 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2388
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1913
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 22061/50000 (44.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6950s / 192536.1935 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2381
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1881
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22081/50000 (44.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.0176s / 192792.2112 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2336
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1862
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22101/50000 (44.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8876s / 193049.0988 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2238
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1706
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22121/50000 (44.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.4701s / 193311.5689 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2278
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1857
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 22141/50000 (44.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0715s / 193569.6404 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2385
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1941
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 22161/50000 (44.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5241s / 193830.1644 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2283
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1911
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 22181/50000 (44.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8207s / 194088.9851 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1855
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 22201/50000 (44.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0745s / 194347.0596 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2328
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1951
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 22221/50000 (44.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0287s / 194605.0883 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2246
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1874
env1_first_0:                 episode reward: -0.4000,                 loss: nan
env1_second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 22241/50000 (44.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6357s / 194864.7240 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2376
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1998
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 22261/50000 (44.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1917s / 195122.9157 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2161
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1541
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 22281/50000 (44.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8954s / 195379.8111 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2270
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1742
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 22301/50000 (44.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.1634s / 195635.9745 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2342
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0647
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22321/50000 (44.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7773s / 195893.7518 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2392
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1298
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22341/50000 (44.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2601s / 196149.0119 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2398
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1556
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 22361/50000 (44.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0994s / 196409.1113 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2362
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1845
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22381/50000 (44.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8710s / 196668.9823 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2239
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1886
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 22401/50000 (44.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3938s / 196926.3760 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2357
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1969
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 22421/50000 (44.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2802s / 197185.6562 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2353
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1830
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22441/50000 (44.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8809s / 197445.5371 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2398
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1748
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 22461/50000 (44.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1939s / 197705.7310 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2294
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1764
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22481/50000 (44.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 263.2556s / 197968.9866 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2318
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1721
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22501/50000 (45.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1037s / 198229.0903 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2411
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1835
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 22521/50000 (45.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0975s / 198487.1877 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2336
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1911
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 22541/50000 (45.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4565s / 198744.6443 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2374
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2076
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 22561/50000 (45.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.9036s / 199006.5478 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2290
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1979
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 22581/50000 (45.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.6178s / 199269.1656 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2316
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1596
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 22601/50000 (45.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7593s / 199529.9249 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2274
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1870
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22621/50000 (45.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.7919s / 199786.7168 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2259
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1923
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 22641/50000 (45.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2413s / 200044.9580 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2351
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1840
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22661/50000 (45.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7507s / 200304.7088 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2280
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1883
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 22681/50000 (45.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6619s / 200564.3707 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2303
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1953
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 22701/50000 (45.4020%),                 avg. length: 2991.0,                last time consumption/overall running time: 258.8298s / 200823.2005 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2184
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1845
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 22721/50000 (45.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8508s / 201084.0512 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2323
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1879
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 22741/50000 (45.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1561s / 201343.2074 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2295
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1947
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 22761/50000 (45.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9566s / 201602.1640 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2315
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1753
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 22781/50000 (45.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5418s / 201860.7057 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2193
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1732
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 22801/50000 (45.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4137s / 202121.1195 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2365
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1749
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 22821/50000 (45.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.3278s / 202382.4473 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2375
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1980
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 22841/50000 (45.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8803s / 202641.3276 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2318
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1917
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 22861/50000 (45.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9877s / 202901.3153 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2270
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1990
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 22881/50000 (45.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3840s / 203160.6993 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2319
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1864
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22901/50000 (45.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8412s / 203417.5406 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2269
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1953
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 22921/50000 (45.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9054s / 203676.4460 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2379
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2017
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22941/50000 (45.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0032s / 203936.4492 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2319
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2034
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 22961/50000 (45.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0467s / 204195.4959 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2349
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2089
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 22981/50000 (45.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2543s / 204453.7501 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2451
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2042
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23001/50000 (46.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9327s / 204713.6829 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2371
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1883
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 23021/50000 (46.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0700s / 204974.7529 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2362
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1922
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 23041/50000 (46.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1583s / 205234.9112 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2261
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1855
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 23061/50000 (46.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0477s / 205494.9589 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2295
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1939
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 23081/50000 (46.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5755s / 205755.5344 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2360
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1964
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 23101/50000 (46.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6294s / 206015.1638 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2344
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1813
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23121/50000 (46.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7919s / 206273.9557 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2225
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1911
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 23141/50000 (46.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9141s / 206533.8697 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2370
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1880
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23161/50000 (46.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1215s / 206793.9912 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2237
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1791
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 23181/50000 (46.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4934s / 207054.4846 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2344
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2005
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23201/50000 (46.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9614s / 207312.4460 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2306
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1955
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 23221/50000 (46.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9970s / 207572.4431 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2465
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2161
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 23241/50000 (46.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5688s / 207831.0118 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2417
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2077
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23261/50000 (46.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4391s / 208089.4509 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2376
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1987
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 23281/50000 (46.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7044s / 208349.1552 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2371
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2052
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23301/50000 (46.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2719s / 208607.4271 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2312
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2059
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23321/50000 (46.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0450s / 208867.4721 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2398
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2032
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 23341/50000 (46.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9898s / 209127.4619 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2356
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2046
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 23361/50000 (46.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6504s / 209388.1123 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2404
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2018
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 23381/50000 (46.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0807s / 209648.1930 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2376
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2077
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23401/50000 (46.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5916s / 209907.7846 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2386
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1992
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 23421/50000 (46.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5728s / 210167.3574 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2391
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2140
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23441/50000 (46.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3248s / 210424.6822 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2216
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1903
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 23461/50000 (46.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8233s / 210682.5055 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2368
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2015
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 23481/50000 (46.9620%),                 avg. length: 2890.95,                last time consumption/overall running time: 247.3825s / 210929.8880 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2158
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1880
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 23501/50000 (47.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 264.0899s / 211193.9779 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2362
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1945
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 23521/50000 (47.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8784s / 211451.8563 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2298
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1879
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 23541/50000 (47.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6603s / 211710.5166 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2370
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1761
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 23561/50000 (47.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0934s / 211968.6100 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2329
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1490
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 23581/50000 (47.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.2455s / 212229.8555 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2344
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2010
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 23601/50000 (47.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1070s / 212490.9625 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2388
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2013
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 23621/50000 (47.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7510s / 212751.7135 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2424
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2096
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 23641/50000 (47.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7307s / 213011.4442 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2344
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1929
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 23661/50000 (47.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2938s / 213271.7380 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2371
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2177
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23681/50000 (47.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2321s / 213531.9700 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2436
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2033
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 23701/50000 (47.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9922s / 213791.9622 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2337
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1988
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 23721/50000 (47.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1932s / 214051.1555 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2299
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2029
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 23741/50000 (47.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1331s / 214310.2886 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2344
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1990
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23761/50000 (47.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7730s / 214571.0616 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2315
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1860
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 23781/50000 (47.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4511s / 214831.5127 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2228
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1874
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23801/50000 (47.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5817s / 215090.0944 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2205
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1754
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 23821/50000 (47.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.5070s / 215344.6014 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2331
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1929
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 23841/50000 (47.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3000s / 215603.9014 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2194
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1954
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 23861/50000 (47.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7283s / 215864.6297 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2323
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1991
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 23881/50000 (47.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3223s / 216123.9520 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2232
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1742
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 23901/50000 (47.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.0226s / 216384.9746 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2349
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1794
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 23921/50000 (47.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9768s / 216643.9515 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2278
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1885
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 23941/50000 (47.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4738s / 216903.4252 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2322
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1967
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 23961/50000 (47.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5871s / 217164.0124 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2318
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1978
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 23981/50000 (47.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.4106s / 217426.4230 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2294
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1762
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24001/50000 (48.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2828s / 217686.7058 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2263
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1940
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24021/50000 (48.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5990s / 217945.3048 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2215
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1934
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 24041/50000 (48.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.0960s / 218207.4008 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2339
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1928
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 24061/50000 (48.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0977s / 218466.4985 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2171
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1803
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 24081/50000 (48.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6946s / 218727.1931 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2136
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1823
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 24101/50000 (48.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2133s / 218986.4064 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2120
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1811
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 24121/50000 (48.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1403s / 219245.5466 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2225
env0_second_0:                 episode reward: 0.2500,                 loss: -0.2012
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 24141/50000 (48.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3047s / 219504.8513 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2269
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1942
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24161/50000 (48.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1285s / 219763.9799 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2235
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1840
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 24181/50000 (48.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1448s / 220024.1247 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2146
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1609
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 24201/50000 (48.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6939s / 220284.8186 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2216
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1696
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 24221/50000 (48.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6896s / 220545.5082 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2345
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1883
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 24241/50000 (48.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3876s / 220805.8958 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2317
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1890
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 24261/50000 (48.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.2443s / 221067.1401 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2246
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1866
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 24281/50000 (48.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6396s / 221325.7798 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2318
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1900
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24301/50000 (48.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8378s / 221584.6175 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2353
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2025
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 24321/50000 (48.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3286s / 221842.9462 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2340
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2080
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 24341/50000 (48.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.3946s / 222104.3408 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2340
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2064
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 24361/50000 (48.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1231s / 222361.4639 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2311
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1819
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 24381/50000 (48.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0615s / 222619.5254 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2384
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1878
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24401/50000 (48.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3967s / 222877.9222 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2230
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1958
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 24421/50000 (48.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0597s / 223137.9818 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2236
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1976
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 24441/50000 (48.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0645s / 223397.0463 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2351
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2095
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24461/50000 (48.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5013s / 223655.5476 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2394
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2008
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 24481/50000 (48.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5563s / 223915.1039 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2383
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2064
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 24501/50000 (49.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0411s / 224173.1450 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2398
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1996
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 24521/50000 (49.0420%),                 avg. length: 2926.7,                last time consumption/overall running time: 254.6776s / 224427.8227 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2109
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1839
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 24541/50000 (49.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1965s / 224687.0192 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2409
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1997
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 24561/50000 (49.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0476s / 224944.0668 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2350
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2078
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 24581/50000 (49.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1664s / 225202.2331 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2386
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1983
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24601/50000 (49.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8831s / 225462.1163 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2349
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1957
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 24621/50000 (49.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4392s / 225720.5555 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2440
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2123
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 24641/50000 (49.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9302s / 225979.4857 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2390
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1976
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 24661/50000 (49.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4483s / 226236.9340 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2318
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1982
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 24681/50000 (49.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6791s / 226496.6131 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2291
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1865
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 24701/50000 (49.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.7140s / 226758.3271 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2288
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1903
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 24721/50000 (49.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1495s / 227016.4765 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2289
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1959
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 24741/50000 (49.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5080s / 227275.9845 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2218
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2017
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 24761/50000 (49.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9698s / 227536.9543 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2366
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1931
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 24781/50000 (49.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1045s / 227797.0588 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2303
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1958
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 24801/50000 (49.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9335s / 228057.9922 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2321
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1900
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24821/50000 (49.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0782s / 228317.0705 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2202
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1677
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 24841/50000 (49.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4381s / 228576.5086 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2274
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1897
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 24861/50000 (49.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7451s / 228836.2536 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2270
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1907
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 24881/50000 (49.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6069s / 229096.8605 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2302
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1807
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 24901/50000 (49.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7624s / 229356.6229 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2244
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1832
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 24921/50000 (49.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1910s / 229616.8139 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2143
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1795
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 24941/50000 (49.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3273s / 229876.1411 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2336
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1887
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 24961/50000 (49.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6045s / 230134.7456 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2204
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1792
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 24981/50000 (49.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1373s / 230393.8829 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2159
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1787
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 25001/50000 (50.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0234s / 230653.9063 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2241
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1965
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 25021/50000 (50.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4427s / 230913.3490 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2275
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1842
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25041/50000 (50.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5053s / 231172.8542 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2323
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1963
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 25061/50000 (50.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.0230s / 231434.8773 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2286
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1940
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25081/50000 (50.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4358s / 231693.3131 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2300
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1989
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 25101/50000 (50.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6094s / 231952.9225 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2223
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1750
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25121/50000 (50.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5763s / 232213.4988 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2009
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1713
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 25141/50000 (50.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4751s / 232473.9739 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2262
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2012
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 25161/50000 (50.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4600s / 232733.4339 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2365
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1945
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 25181/50000 (50.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5085s / 232992.9424 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2399
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2093
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 25201/50000 (50.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3220s / 233252.2643 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2406
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2042
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25221/50000 (50.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0735s / 233510.3378 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2280
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1947
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 25241/50000 (50.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7026s / 233769.0404 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2372
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1914
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 25261/50000 (50.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2032s / 234027.2436 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2291
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1918
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 25281/50000 (50.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.2375s / 234288.4810 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2354
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1898
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 25301/50000 (50.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3980s / 234548.8790 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2261
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1968
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 25321/50000 (50.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1369s / 234810.0160 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2260
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1922
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 25341/50000 (50.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8614s / 235068.8773 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2278
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1896
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25361/50000 (50.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9805s / 235328.8578 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2357
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2096
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25381/50000 (50.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7015s / 235587.5593 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2352
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2050
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 25401/50000 (50.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1567s / 235846.7160 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2408
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2019
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 25421/50000 (50.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1381s / 236104.8541 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2386
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1920
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25441/50000 (50.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7745s / 236362.6286 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2386
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2047
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 25461/50000 (50.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5480s / 236621.1767 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2386
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2020
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 25481/50000 (50.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8903s / 236879.0669 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2378
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1869
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25501/50000 (51.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8266s / 237135.8935 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2285
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1859
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 25521/50000 (51.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6963s / 237394.5898 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2353
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1856
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 25541/50000 (51.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1768s / 237654.7666 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2320
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1866
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 25561/50000 (51.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9727s / 237915.7393 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2403
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2004
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 25581/50000 (51.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0957s / 238175.8351 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2436
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2030
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 25601/50000 (51.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8983s / 238435.7333 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2458
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2000
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25621/50000 (51.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1326s / 238695.8659 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2350
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1914
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 25641/50000 (51.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2729s / 238955.1388 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2311
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1948
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 25661/50000 (51.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0206s / 239215.1595 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2251
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1930
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 25681/50000 (51.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3705s / 239474.5299 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2302
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2024
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 25701/50000 (51.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2789s / 239734.8088 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2394
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2046
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25721/50000 (51.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9918s / 239992.8006 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2394
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1997
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25741/50000 (51.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5689s / 240251.3696 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2385
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2100
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25761/50000 (51.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1090s / 240511.4786 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2326
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1922
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 25781/50000 (51.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9506s / 240770.4292 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2355
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1858
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 25801/50000 (51.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4776s / 241029.9068 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2213
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1816
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 25821/50000 (51.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7529s / 241288.6597 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2275
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1869
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 25841/50000 (51.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5756s / 241548.2353 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2375
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1974
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25861/50000 (51.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7395s / 241807.9748 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2332
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1955
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 25881/50000 (51.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9489s / 242068.9237 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2301
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2024
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 25901/50000 (51.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1564s / 242324.0801 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2349
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1972
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 25921/50000 (51.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4067s / 242582.4868 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2339
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1933
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 25941/50000 (51.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8085s / 242841.2952 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2314
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1924
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25961/50000 (51.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8910s / 243098.1863 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2260
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1982
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 25981/50000 (51.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3816s / 243357.5678 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2323
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2007
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26001/50000 (52.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5139s / 243616.0817 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2343
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1980
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 26021/50000 (52.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7943s / 243875.8761 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2394
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1917
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26041/50000 (52.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3361s / 244135.2122 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2364
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1922
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 26061/50000 (52.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1736s / 244395.3859 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2180
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1849
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 26081/50000 (52.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9443s / 244654.3302 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2307
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1993
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 26101/50000 (52.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.9249s / 244915.2550 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2262
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1993
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 26121/50000 (52.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4572s / 245173.7123 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2366
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2024
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 26141/50000 (52.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8531s / 245432.5653 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2182
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1905
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 26161/50000 (52.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4445s / 245693.0098 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2343
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2015
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 26181/50000 (52.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4202s / 245952.4300 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2328
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1965
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 26201/50000 (52.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3387s / 246212.7687 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2219
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1873
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26221/50000 (52.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8376s / 246471.6063 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2250
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1960
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 26241/50000 (52.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6203s / 246729.2266 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2353
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1806
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26261/50000 (52.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.6827s / 246986.9093 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2256
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1762
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 26281/50000 (52.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0559s / 247244.9652 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2238
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1830
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26301/50000 (52.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9491s / 247503.9143 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.2104
env0_second_0:                 episode reward: -0.8000,                 loss: -0.1766
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 26321/50000 (52.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5031s / 247761.4174 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2127
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1795
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26341/50000 (52.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1194s / 248020.5368 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2282
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2000
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26361/50000 (52.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0470s / 248279.5838 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2329
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1978
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 26381/50000 (52.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6706s / 248539.2544 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2321
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1855
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26401/50000 (52.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2144s / 248797.4687 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2163
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1807
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 26421/50000 (52.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6058s / 249056.0746 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2280
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1965
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 26441/50000 (52.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6718s / 249312.7464 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2219
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1860
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26461/50000 (52.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1766s / 249571.9229 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2239
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1798
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 26481/50000 (52.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4916s / 249831.4146 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2199
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1300
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26501/50000 (53.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9014s / 250090.3159 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2277
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0431
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 26521/50000 (53.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1722s / 250348.4881 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2308
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1808
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26541/50000 (53.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2971s / 250606.7852 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2150
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1826
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 26561/50000 (53.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1158s / 250865.9010 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2230
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1910
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26581/50000 (53.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6673s / 251124.5683 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2349
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1817
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 26601/50000 (53.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8373s / 251383.4055 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2193
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1778
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 26621/50000 (53.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0856s / 251640.4911 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2297
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1747
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26641/50000 (53.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2812s / 251898.7723 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2323
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1982
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 26661/50000 (53.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3666s / 252158.1390 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2298
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1905
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 26681/50000 (53.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7005s / 252416.8395 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2256
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1753
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26701/50000 (53.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4558s / 252677.2953 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2262
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1905
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 26721/50000 (53.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6704s / 252936.9657 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2212
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1865
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 26741/50000 (53.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0883s / 253195.0541 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1562
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26761/50000 (53.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2090s / 253451.2631 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2106
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1688
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26781/50000 (53.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5752s / 253710.8382 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2230
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1924
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26801/50000 (53.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9705s / 253969.8088 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2274
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2047
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 26821/50000 (53.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1383s / 254228.9471 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2337
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1936
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 26841/50000 (53.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5217s / 254488.4688 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2346
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1775
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 26861/50000 (53.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6095s / 254747.0783 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2259
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1705
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26881/50000 (53.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7501s / 255007.8284 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2253
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1834
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 26901/50000 (53.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5313s / 255267.3596 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2230
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1857
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 26921/50000 (53.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5824s / 255525.9420 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2347
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1833
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 26941/50000 (53.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3364s / 255784.2784 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2230
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1939
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 26961/50000 (53.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.2768s / 256042.5552 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2337
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2071
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 26981/50000 (53.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.6576s / 256299.2128 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2374
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1981
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 27001/50000 (54.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7769s / 256557.9897 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2363
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1802
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 27021/50000 (54.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0421s / 256816.0318 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2327
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1899
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27041/50000 (54.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2180s / 257076.2498 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2322
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1890
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 27061/50000 (54.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1323s / 257335.3821 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2281
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1828
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27081/50000 (54.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5503s / 257592.9323 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2332
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2023
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 27101/50000 (54.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2392s / 257852.1715 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2353
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1912
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 27121/50000 (54.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8397s / 258111.0112 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2294
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1952
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 27141/50000 (54.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1114s / 258370.1226 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1916
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27161/50000 (54.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1265s / 258630.2491 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2420
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1841
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27181/50000 (54.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4168s / 258890.6659 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2266
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1970
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 27201/50000 (54.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4107s / 259151.0766 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2380
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1950
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 27221/50000 (54.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6499s / 259410.7265 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2447
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1896
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 27241/50000 (54.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6185s / 259671.3449 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2378
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1990
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 27261/50000 (54.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4179s / 259930.7629 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2328
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2270
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 27281/50000 (54.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8338s / 260190.5967 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2344
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2074
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 27301/50000 (54.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9887s / 260448.5854 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2390
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2043
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 27321/50000 (54.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2930s / 260707.8784 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2287
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1978
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 27341/50000 (54.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.2559s / 260964.1343 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2272
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1923
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27361/50000 (54.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2601s / 261221.3943 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2383
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2046
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 27381/50000 (54.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3078s / 261481.7022 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2378
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2073
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 27401/50000 (54.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2630s / 261740.9652 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2323
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1979
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 27421/50000 (54.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3225s / 261998.2877 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2397
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2097
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27441/50000 (54.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.4332s / 262254.7209 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2341
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1997
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 27461/50000 (54.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.2254s / 262509.9462 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2337
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1997
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 27481/50000 (54.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.1322s / 262771.0785 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2360
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2017
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 27501/50000 (55.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8589s / 263029.9373 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2204
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1982
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 27521/50000 (55.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2217s / 263287.1590 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2296
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1896
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 27541/50000 (55.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6958s / 263545.8548 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2345
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2034
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27561/50000 (55.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5802s / 263806.4350 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2223
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1906
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 27581/50000 (55.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9911s / 264066.4261 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2315
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1991
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 27601/50000 (55.2020%),                 avg. length: 2956.7,                last time consumption/overall running time: 253.9949s / 264320.4210 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2277
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1980
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 27621/50000 (55.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1091s / 264578.5301 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2340
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2036
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 27641/50000 (55.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8705s / 264837.4006 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2261
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1991
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 27661/50000 (55.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0823s / 265094.4829 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2311
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2014
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 27681/50000 (55.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1690s / 265354.6519 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2230
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2017
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 27701/50000 (55.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.4449s / 265614.0968 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2383
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1990
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27721/50000 (55.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2287s / 265871.3256 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2444
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2016
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 27741/50000 (55.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5141s / 266129.8396 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2372
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2138
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 27761/50000 (55.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2690s / 266389.1086 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2383
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2156
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 27781/50000 (55.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7623s / 266648.8709 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2315
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1950
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 27801/50000 (55.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8874s / 266907.7583 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2308
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1992
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 27821/50000 (55.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5597s / 267166.3180 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2358
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1997
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 27841/50000 (55.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9808s / 267424.2988 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2164
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1887
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 27861/50000 (55.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1032s / 267682.4020 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2288
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1931
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27881/50000 (55.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8902s / 267942.2922 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2334
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1990
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 27901/50000 (55.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.9230s / 268199.2152 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2306
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1952
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 27921/50000 (55.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7418s / 268459.9570 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2171
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1744
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 27941/50000 (55.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2440s / 268717.2010 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2317
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1985
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 27961/50000 (55.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5983s / 268975.7993 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2298
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1894
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 27981/50000 (55.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4575s / 269234.2568 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2363
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2025
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 28001/50000 (56.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1094s / 269493.3663 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2373
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2092
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28021/50000 (56.0420%),                 avg. length: 2983.9,                last time consumption/overall running time: 256.7434s / 269750.1096 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2172
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1050
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 28041/50000 (56.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.7557s / 270008.8653 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2243
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1758
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28061/50000 (56.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7646s / 270268.6299 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2238
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1850
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 28081/50000 (56.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 254.1973s / 270522.8272 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2318
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1905
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 28101/50000 (56.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4351s / 270781.2624 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2262
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1785
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 28121/50000 (56.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8654s / 271040.1278 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2384
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1898
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 28141/50000 (56.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2459s / 271299.3736 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2316
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1954
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 28161/50000 (56.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1878s / 271556.5614 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2326
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1976
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28181/50000 (56.3620%),                 avg. length: 2911.25,                last time consumption/overall running time: 250.7749s / 271807.3363 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2200
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1703
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 28201/50000 (56.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.5354s / 272066.8717 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2355
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1944
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 28221/50000 (56.4420%),                 avg. length: 2929.6,                last time consumption/overall running time: 252.3153s / 272319.1870 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2162
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1795
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28241/50000 (56.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8943s / 272578.0814 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2412
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1842
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28261/50000 (56.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.7111s / 272835.7925 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2415
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1913
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 28281/50000 (56.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3882s / 273094.1807 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2407
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2024
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 28301/50000 (56.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5090s / 273354.6896 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2369
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2086
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 28321/50000 (56.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8189s / 273613.5086 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2343
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1937
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28341/50000 (56.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.5275s / 273874.0361 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2250
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2004
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 28361/50000 (56.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8702s / 274133.9063 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2382
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2016
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28381/50000 (56.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3017s / 274392.2080 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2324
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1919
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 28401/50000 (56.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3298s / 274650.5378 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2372
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1909
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 28421/50000 (56.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.8290s / 274911.3668 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2319
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1903
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 28441/50000 (56.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.9387s / 275169.3055 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2171
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1736
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 28461/50000 (56.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6636s / 275428.9691 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2237
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1882
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 28481/50000 (56.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8956s / 275688.8646 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2273
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1846
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 28501/50000 (57.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.0697s / 275947.9344 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2309
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1905
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28521/50000 (57.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7797s / 276207.7141 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2290
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1844
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 28541/50000 (57.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.3623s / 276467.0764 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2228
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1840
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28561/50000 (57.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.7047s / 276727.7811 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2325
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1878
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 28581/50000 (57.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 263.2219s / 276991.0031 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2386
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1987
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 28601/50000 (57.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5132s / 277249.5163 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2268
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1915
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28621/50000 (57.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 262.4858s / 277512.0021 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2232
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1877
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 28641/50000 (57.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5139s / 277770.5160 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2339
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1877
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 28661/50000 (57.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8177s / 278029.3337 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2295
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1882
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28681/50000 (57.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.0053s / 278289.3390 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2366
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1941
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28701/50000 (57.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.1001s / 278547.4390 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2319
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1933
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 28721/50000 (57.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1243s / 278807.5633 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2239
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1864
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 28741/50000 (57.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2507s / 279066.8140 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2220
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1834
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 28761/50000 (57.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2315s / 279327.0454 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2363
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2020
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28781/50000 (57.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1064s / 279587.1518 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2356
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2008
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28801/50000 (57.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.6618s / 279847.8136 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2296
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1851
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28821/50000 (57.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2832s / 280107.0968 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2364
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1971
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 28841/50000 (57.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3255s / 280365.4222 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2311
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1886
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 28861/50000 (57.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.5499s / 280621.9722 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2311
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1864
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28881/50000 (57.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4965s / 280880.4686 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2325
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1995
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 28901/50000 (57.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.8577s / 281140.3263 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2424
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1993
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 28921/50000 (57.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0152s / 281398.3415 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2388
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1883
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28941/50000 (57.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3264s / 281658.6679 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2345
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1504
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28961/50000 (57.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3988s / 281915.0667 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2406
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1957
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 28981/50000 (57.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1201s / 282170.1868 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2399
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2082
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29001/50000 (58.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1903s / 282429.3770 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2451
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2021
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29021/50000 (58.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 253.7461s / 282683.1231 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2493
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1968
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29041/50000 (58.0820%),                 avg. length: 2916.2,                last time consumption/overall running time: 251.0333s / 282934.1564 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2056
env0_second_0:                 episode reward: -0.9500,                 loss: -0.1829
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 29061/50000 (58.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.8352s / 283189.9916 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2333
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1932
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29081/50000 (58.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3412s / 283447.3328 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2350
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1904
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 29101/50000 (58.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3320s / 283703.6648 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2288
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1890
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 29121/50000 (58.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9650s / 283959.6299 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2147
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1862
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 29141/50000 (58.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1418s / 284218.7717 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2237
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1917
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 29161/50000 (58.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.8149s / 284476.5866 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2289
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1908
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 29181/50000 (58.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.1083s / 284736.6949 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2289
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1826
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29201/50000 (58.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1564s / 284995.8513 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2330
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1719
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 29221/50000 (58.4420%),                 avg. length: 2890.05,                last time consumption/overall running time: 248.9118s / 285244.7631 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2037
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1658
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 29241/50000 (58.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.0047s / 285501.7678 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2244
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1824
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 29261/50000 (58.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4223s / 285760.1901 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2298
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1832
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 29281/50000 (58.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1759s / 286015.3660 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2224
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1848
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29301/50000 (58.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8890s / 286272.2550 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2278
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1818
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 29321/50000 (58.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3554s / 286530.6104 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2283
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1818
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 29341/50000 (58.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.2650s / 286790.8754 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2279
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2003
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29361/50000 (58.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.6110s / 287050.4864 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2300
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1899
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29381/50000 (58.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1013s / 287307.5876 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2114
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1676
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 29401/50000 (58.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8303s / 287566.4179 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2248
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1709
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 29421/50000 (58.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 261.6195s / 287828.0374 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2229
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1481
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29441/50000 (58.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4743s / 288085.5117 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2167
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1748
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 29461/50000 (58.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2998s / 288342.8114 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2321
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1684
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 29481/50000 (58.9620%),                 avg. length: 2948.3,                last time consumption/overall running time: 255.6326s / 288598.4440 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2100
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1833
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 29501/50000 (59.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.4925s / 288856.9365 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2290
env0_second_0:                 episode reward: -0.1500,                 loss: -0.2005
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 29521/50000 (59.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5598s / 289114.4963 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2372
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1921
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29541/50000 (59.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.7649s / 289374.2612 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2414
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2089
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 29561/50000 (59.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.2742s / 289631.5354 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2460
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2086
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 29581/50000 (59.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.9419s / 289890.4774 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2283
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1958
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 29601/50000 (59.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.1626s / 290145.6400 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2409
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1893
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29621/50000 (59.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.3845s / 290403.0246 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2402
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1966
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29641/50000 (59.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3639s / 290661.3884 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2396
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2026
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 29661/50000 (59.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.6246s / 290920.0130 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2385
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1901
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 29681/50000 (59.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.5958s / 291178.6087 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2342
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1442
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29701/50000 (59.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4267s / 291439.0354 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2408
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1986
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29721/50000 (59.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.9053s / 291698.9407 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2285
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1562
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 29741/50000 (59.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.3998s / 291959.3405 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2363
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1490
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29761/50000 (59.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 260.4353s / 292219.7758 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2370
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1619
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 29781/50000 (59.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.3594s / 292478.1352 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2275
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1883
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 29801/50000 (59.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.4978s / 292735.6330 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2232
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1806
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 29821/50000 (59.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.3889s / 292992.0219 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2271
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1511
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 29841/50000 (59.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.9557s / 293247.9776 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2342
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1174
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 29861/50000 (59.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.5442s / 293505.5218 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2287
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1773
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 29881/50000 (59.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.2738s / 293764.7956 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2260
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2000
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 29901/50000 (59.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 259.1593s / 294023.9549 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2383
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1914
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 29921/50000 (59.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.8638s / 294282.8187 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2333
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1872
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29941/50000 (59.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 255.6107s / 294538.4295 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2334
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1906
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 29961/50000 (59.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 256.8576s / 294795.2870 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2295
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1872
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 29981/50000 (59.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 257.1996s / 295052.4866 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2330
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1942
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30001/50000 (60.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 258.0501s / 295310.5368 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2297
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1911
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30021/50000 (60.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 247.6843s / 295558.2211 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2225
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1757
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30041/50000 (60.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 225.4773s / 295783.6984 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2195
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1764
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 30061/50000 (60.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 225.5155s / 296009.2139 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2417
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2097
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30081/50000 (60.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 225.3111s / 296234.5250 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2444
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1917
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30101/50000 (60.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 225.7305s / 296460.2555 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2401
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1977
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30121/50000 (60.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 225.5398s / 296685.7953 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2423
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1908
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 30141/50000 (60.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 224.9170s / 296910.7123 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2333
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2007
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30161/50000 (60.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 222.4765s / 297133.1888 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2359
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1986
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30181/50000 (60.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 221.8803s / 297355.0691 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2352
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1901
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30201/50000 (60.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 224.6325s / 297579.7016 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2323
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1662
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30221/50000 (60.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 224.2238s / 297803.9255 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2348
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2019
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30241/50000 (60.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 224.4689s / 298028.3944 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2172
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1749
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 30261/50000 (60.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 225.2296s / 298253.6240 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2314
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1870
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30281/50000 (60.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 225.5331s / 298479.1571 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2317
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1826
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30301/50000 (60.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 227.8144s / 298706.9715 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2169
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1798
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 30321/50000 (60.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 224.4059s / 298931.3774 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2187
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1510
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30341/50000 (60.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 227.7485s / 299159.1260 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2231
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1743
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30361/50000 (60.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 218.7866s / 299377.9126 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2345
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2032
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30381/50000 (60.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.5045s / 299574.4171 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2378
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1798
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 30401/50000 (60.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5375s / 299771.9546 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2337
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1394
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30421/50000 (60.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.9892s / 299968.9439 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2333
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1808
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 30441/50000 (60.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4626s / 300166.4065 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2194
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1813
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30461/50000 (60.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5723s / 300364.9788 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2389
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1787
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30481/50000 (60.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.9979s / 300563.9768 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2310
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1966
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30501/50000 (61.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5110s / 300762.4878 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2290
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1924
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30521/50000 (61.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.9091s / 300961.3969 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2354
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1945
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30541/50000 (61.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.3160s / 301159.7128 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2372
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1988
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30561/50000 (61.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7637s / 301356.4766 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2258
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1935
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 30581/50000 (61.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.6692s / 301555.1458 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2259
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1985
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30601/50000 (61.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.3369s / 301754.4827 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2312
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1972
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30621/50000 (61.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.8123s / 301951.2950 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2273
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1899
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30641/50000 (61.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5130s / 302149.8079 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2354
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1913
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 30661/50000 (61.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9616s / 302347.7695 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2247
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1766
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 30681/50000 (61.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5761s / 302545.3456 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2327
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1777
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 30701/50000 (61.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7823s / 302742.1279 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2338
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2051
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 30721/50000 (61.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.2436s / 302938.3715 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2363
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2027
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30741/50000 (61.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5096s / 303135.8812 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2313
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1886
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30761/50000 (61.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.7417s / 303334.6228 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2289
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1870
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30781/50000 (61.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.4836s / 303534.1064 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2317
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1705
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30801/50000 (61.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.8056s / 303730.9120 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2311
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1923
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 30821/50000 (61.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4691s / 303927.3811 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2265
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1940
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 30841/50000 (61.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3983s / 304124.7794 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2391
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1971
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 30861/50000 (61.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.9263s / 304325.7058 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2369
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1943
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30881/50000 (61.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2296s / 304523.9354 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2360
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1945
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 30901/50000 (61.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6629s / 304721.5983 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2392
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1945
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30921/50000 (61.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.0319s / 304919.6302 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2319
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1796
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 30941/50000 (61.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.2477s / 305116.8779 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2384
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1830
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 30961/50000 (61.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7639s / 305313.6418 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2398
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1965
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 30981/50000 (61.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0129s / 305509.6547 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2222
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1856
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 31001/50000 (62.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3579s / 305707.0126 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2435
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1886
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31021/50000 (62.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.7841s / 305906.7967 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2318
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1805
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31041/50000 (62.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.0425s / 306105.8392 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2172
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1755
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 31061/50000 (62.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.3432s / 306305.1823 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2313
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1953
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 31081/50000 (62.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.7504s / 306503.9327 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2282
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1894
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31101/50000 (62.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7390s / 306701.6718 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2144
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1816
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31121/50000 (62.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.1456s / 306900.8174 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2235
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1804
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 31141/50000 (62.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.4792s / 307099.2965 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2330
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1948
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31161/50000 (62.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.5527s / 307298.8493 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2291
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1891
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 31181/50000 (62.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7082s / 307496.5575 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2252
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1836
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 31201/50000 (62.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8464s / 307695.4039 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2185
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1784
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 31221/50000 (62.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5885s / 307892.9924 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2300
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1818
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 31241/50000 (62.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.0875s / 308092.0799 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2328
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1925
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 31261/50000 (62.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4073s / 308289.4872 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2325
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1688
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 31281/50000 (62.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7789s / 308487.2661 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2235
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1856
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 31301/50000 (62.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8008s / 308686.0669 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2313
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1776
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31321/50000 (62.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.9203s / 308884.9872 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2182
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1882
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 31341/50000 (62.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1417s / 309082.1289 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2337
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1796
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31361/50000 (62.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8007s / 309280.9296 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2383
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1580
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 31381/50000 (62.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9989s / 309478.9285 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2346
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1667
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 31401/50000 (62.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5742s / 309677.5027 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2315
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1265
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 31421/50000 (62.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.8809s / 309874.3836 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2317
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1622
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 31441/50000 (62.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5574s / 310072.9410 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2095
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1664
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 31461/50000 (62.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.9676s / 310271.9086 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2206
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1669
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31481/50000 (62.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4513s / 310469.3599 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2321
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1700
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 31501/50000 (63.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0402s / 310665.4000 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2227
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1796
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 31521/50000 (63.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 202.3812s / 310867.7812 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2272
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1867
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 31541/50000 (63.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6087s / 311065.3899 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2342
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1846
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 31561/50000 (63.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7442s / 311263.1341 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2336
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1987
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 31581/50000 (63.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7351s / 311460.8693 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2388
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1934
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 31601/50000 (63.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.5789s / 311660.4482 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2366
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1420
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 31621/50000 (63.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.9042s / 311856.3524 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2393
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1857
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31641/50000 (63.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7583s / 312053.1108 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2326
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1932
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 31661/50000 (63.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.0008s / 312251.1116 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2373
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1785
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 31681/50000 (63.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7829s / 312448.8945 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2376
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1898
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31701/50000 (63.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.4460s / 312648.3404 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2290
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1976
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 31721/50000 (63.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3107s / 312845.6511 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2360
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1992
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 31741/50000 (63.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.8134s / 313043.4645 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2320
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1876
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 31761/50000 (63.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.0146s / 313242.4792 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2305
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1762
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 31781/50000 (63.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.9616s / 313441.4408 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2343
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1895
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 31801/50000 (63.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.7748s / 313640.2156 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2225
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1887
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 31821/50000 (63.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.1256s / 313838.3412 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2344
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1886
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 31841/50000 (63.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7502s / 314035.0914 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2342
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1974
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 31861/50000 (63.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2548s / 314233.3463 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2191
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1636
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 31881/50000 (63.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.3684s / 314431.7146 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2148
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1783
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 31901/50000 (63.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7129s / 314629.4275 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2088
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1811
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 31921/50000 (63.8420%),                 avg. length: 2995.15,                last time consumption/overall running time: 198.6739s / 314828.1014 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2080
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1168
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 31941/50000 (63.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.6655s / 315027.7669 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2274
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1634
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 31961/50000 (63.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.2144s / 315224.9812 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2294
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1771
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 31981/50000 (63.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.2711s / 315422.2523 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2355
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1828
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 32001/50000 (64.0020%),                 avg. length: 2857.5,                last time consumption/overall running time: 189.0541s / 315611.3064 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.1872
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1524
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 32021/50000 (64.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.8681s / 315809.1745 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2091
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1829
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 32041/50000 (64.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.7208s / 316008.8953 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2310
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1895
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 32061/50000 (64.1220%),                 avg. length: 2981.7,                last time consumption/overall running time: 196.9959s / 316205.8911 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2188
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1241
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 32081/50000 (64.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.0282s / 316402.9193 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2266
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1499
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 32101/50000 (64.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7758s / 316600.6952 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2290
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1773
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 32121/50000 (64.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3064s / 316798.0016 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2212
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1744
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 32141/50000 (64.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4898s / 316994.4914 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2223
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1727
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 32161/50000 (64.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.8968s / 317192.3883 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2323
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1888
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 32181/50000 (64.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.2458s / 317389.6340 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2213
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1922
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 32201/50000 (64.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4917s / 317586.1257 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2243
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1925
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 32221/50000 (64.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.8838s / 317786.0095 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2266
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1825
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 32241/50000 (64.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.4610s / 317986.4705 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2162
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1790
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 32261/50000 (64.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8506s / 318185.3210 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2240
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1722
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 32281/50000 (64.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.5069s / 318381.8279 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2235
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1836
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 32301/50000 (64.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7266s / 318579.5545 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2232
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1729
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 32321/50000 (64.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5154s / 318777.0699 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2323
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1856
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 32341/50000 (64.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9344s / 318975.0043 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2385
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1895
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 32361/50000 (64.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5050s / 319173.5093 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2361
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1763
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 32381/50000 (64.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6180s / 319371.1273 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2353
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1968
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 32401/50000 (64.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5110s / 319568.6383 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2324
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1906
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 32421/50000 (64.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.9840s / 319767.6223 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2320
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1907
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 32441/50000 (64.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3523s / 319964.9746 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2349
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1889
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 32461/50000 (64.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.2451s / 320162.2196 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2264
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1863
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 32481/50000 (64.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 201.6511s / 320363.8707 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2254
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1297
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 32501/50000 (65.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5169s / 320562.3876 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2190
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1714
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 32521/50000 (65.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.6953s / 320762.0829 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2208
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1734
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 32541/50000 (65.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9283s / 320960.0112 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2154
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1646
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 32561/50000 (65.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.0348s / 321159.0460 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2017
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1641
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 32581/50000 (65.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.8374s / 321356.8834 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2194
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1837
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 32601/50000 (65.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6479s / 321554.5313 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2313
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1952
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 32621/50000 (65.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6308s / 321752.1621 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2345
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1838
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 32641/50000 (65.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9393s / 321950.1014 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2176
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1797
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 32661/50000 (65.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1780s / 322147.2794 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2267
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1817
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 32681/50000 (65.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.0298s / 322345.3092 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2297
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1847
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 32701/50000 (65.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.8261s / 322542.1353 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2224
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1676
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 32721/50000 (65.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.1755s / 322738.3108 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2352
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1842
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 32741/50000 (65.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.7862s / 322938.0970 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2298
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1837
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 32761/50000 (65.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.6648s / 323134.7618 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2340
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1728
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 32781/50000 (65.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.3504s / 323333.1121 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2177
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1747
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 32801/50000 (65.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.9204s / 323530.0325 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2169
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1852
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 32821/50000 (65.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1611s / 323727.1936 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2349
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1973
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 32841/50000 (65.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.1075s / 323925.3011 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2324
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1954
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 32861/50000 (65.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.9222s / 324122.2233 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2309
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1906
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 32881/50000 (65.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4816s / 324318.7049 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2149
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1587
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 32901/50000 (65.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5426s / 324516.2475 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2177
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1809
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 32921/50000 (65.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5807s / 324714.8282 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2241
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1897
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 32941/50000 (65.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7024s / 324912.5306 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2175
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1690
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 32961/50000 (65.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.9547s / 325109.4853 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2215
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1794
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 32981/50000 (65.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.2675s / 325306.7528 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2385
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2005
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 33001/50000 (66.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6902s / 325504.4430 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2345
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2032
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 33021/50000 (66.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1947s / 325701.6377 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2290
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1894
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 33041/50000 (66.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9973s / 325899.6350 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2282
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1939
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 33061/50000 (66.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.3276s / 326097.9625 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2312
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1994
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 33081/50000 (66.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5602s / 326296.5228 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2331
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2025
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 33101/50000 (66.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 201.4735s / 326497.9963 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2257
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1965
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 33121/50000 (66.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.4634s / 326697.4597 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2280
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1899
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 33141/50000 (66.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.4973s / 326896.9570 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2311
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1899
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 33161/50000 (66.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9553s / 327094.9122 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2309
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2018
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 33181/50000 (66.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.3407s / 327291.2530 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2403
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2056
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 33201/50000 (66.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5896s / 327489.8426 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2396
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2027
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 33221/50000 (66.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.6004s / 327688.4430 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2303
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1977
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 33241/50000 (66.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.3936s / 327887.8367 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2450
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2077
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 33261/50000 (66.5220%),                 avg. length: 2680.75,                last time consumption/overall running time: 177.9757s / 328065.8124 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2036
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1622
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 33281/50000 (66.5620%),                 avg. length: 2839.05,                last time consumption/overall running time: 186.2247s / 328252.0371 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1928
env0_second_0:                 episode reward: -1.0000,                 loss: -0.1208
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 33301/50000 (66.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8154s / 328450.8525 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2298
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1751
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 33321/50000 (66.6420%),                 avg. length: 2828.0,                last time consumption/overall running time: 187.7729s / 328638.6254 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2083
env0_second_0:                 episode reward: 0.6000,                 loss: -0.0197
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 33341/50000 (66.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.8460s / 328838.4714 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2388
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1677
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 33361/50000 (66.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9905s / 329036.4619 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2249
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1571
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 33381/50000 (66.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.4735s / 329235.9354 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2339
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1379
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 33401/50000 (66.8020%),                 avg. length: 2991.8,                last time consumption/overall running time: 198.9217s / 329434.8571 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2275
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1481
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 33421/50000 (66.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.0518s / 329632.9089 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2224
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1539
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 33441/50000 (66.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5108s / 329831.4196 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2193
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1418
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 33461/50000 (66.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.3612s / 330031.7808 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2206
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1433
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 33481/50000 (66.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.1287s / 330229.9095 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2129
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1514
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 33501/50000 (67.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.1303s / 330426.0399 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2069
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1076
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 33521/50000 (67.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5309s / 330623.5708 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2127
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1448
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 33541/50000 (67.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1340s / 330820.7048 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2174
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1408
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 33561/50000 (67.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0381s / 331016.7429 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2220
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1567
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 33581/50000 (67.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4817s / 331213.2246 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2257
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1594
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 33601/50000 (67.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.8903s / 331413.1150 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2249
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1532
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 33621/50000 (67.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.8059s / 331613.9208 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2247
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1680
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 33641/50000 (67.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.6560s / 331812.5768 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2318
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1584
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 33661/50000 (67.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.3663s / 332010.9431 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2222
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1465
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 33681/50000 (67.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.5076s / 332210.4507 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2304
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1789
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 33701/50000 (67.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6488s / 332408.0996 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2416
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1746
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 33721/50000 (67.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.7218s / 332607.8214 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2156
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1544
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 33741/50000 (67.4820%),                 avg. length: 2669.9,                last time consumption/overall running time: 177.9462s / 332785.7675 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.2003
env0_second_0:                 episode reward: -1.0500,                 loss: -0.1383
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 33761/50000 (67.5220%),                 avg. length: 2329.7,                last time consumption/overall running time: 155.3521s / 332941.1197 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1275
env0_second_0:                 episode reward: -1.4000,                 loss: -0.0244
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 33781/50000 (67.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.3268s / 333140.4465 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2049
env0_second_0:                 episode reward: 0.2500,                 loss: -0.0987
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 33801/50000 (67.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 201.1213s / 333341.5678 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2329
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0329
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 33821/50000 (67.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.2058s / 333541.7736 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2351
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1334
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 33841/50000 (67.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5075s / 333739.2811 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2437
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1775
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 33861/50000 (67.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 202.2969s / 333941.5780 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2389
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1664
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 33881/50000 (67.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2621s / 334139.8402 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2260
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1691
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 33901/50000 (67.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.5656s / 334340.4058 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2173
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1345
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 33921/50000 (67.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5383s / 334538.9441 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2248
env0_second_0:                 episode reward: 0.1000,                 loss: -0.0339
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 33941/50000 (67.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.0431s / 334736.9872 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2152
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1421
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 33961/50000 (67.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.8622s / 334937.8494 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2293
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1652
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 33981/50000 (67.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.1993s / 335136.0486 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2172
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1410
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 34001/50000 (68.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.5677s / 335335.6163 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2199
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1459
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 34021/50000 (68.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2970s / 335533.9133 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2319
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1609
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34041/50000 (68.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.7145s / 335732.6278 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2226
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0196
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34061/50000 (68.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5299s / 335930.1577 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2227
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0082
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34081/50000 (68.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.6972s / 336130.8549 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2317
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1367
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34101/50000 (68.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7630s / 336327.6179 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2355
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1419
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34121/50000 (68.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.0059s / 336525.6238 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2351
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1113
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34141/50000 (68.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2495s / 336723.8733 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2388
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1582
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34161/50000 (68.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8493s / 336922.7226 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2266
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1462
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34181/50000 (68.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.1332s / 337120.8558 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2358
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1558
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 34201/50000 (68.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.8172s / 337318.6730 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2218
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1594
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 34221/50000 (68.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6162s / 337516.2892 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2342
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1636
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34241/50000 (68.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5974s / 337714.8866 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2246
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1511
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34261/50000 (68.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2355s / 337913.1221 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2214
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1465
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 34281/50000 (68.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.2375s / 338110.3597 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2142
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1518
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 34301/50000 (68.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.8946s / 338308.2542 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2049
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1472
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 34321/50000 (68.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.9688s / 338505.2230 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2231
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1691
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 34341/50000 (68.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7599s / 338701.9830 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2322
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1504
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34361/50000 (68.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.5523s / 338901.5353 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2251
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1643
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 34381/50000 (68.7620%),                 avg. length: 2924.5,                last time consumption/overall running time: 193.5203s / 339095.0556 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.2115
env0_second_0:                 episode reward: -1.2000,                 loss: -0.1735
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 34401/50000 (68.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.9106s / 339293.9662 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2132
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1719
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 34421/50000 (68.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.1057s / 339492.0719 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2243
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1636
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 34441/50000 (68.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5439s / 339689.6158 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2223
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1689
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34461/50000 (68.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.1257s / 339887.7415 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2267
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1643
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34481/50000 (68.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.3977s / 340086.1392 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2377
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1821
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34501/50000 (69.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.0198s / 340285.1590 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2368
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1759
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34521/50000 (69.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4923s / 340482.6513 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2312
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1623
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34541/50000 (69.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.2432s / 340682.8946 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2244
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1574
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 34561/50000 (69.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1311s / 340880.0257 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2399
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1910
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 34581/50000 (69.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.8692s / 341076.8949 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2270
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1557
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34601/50000 (69.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9264s / 341274.8213 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2242
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1561
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34621/50000 (69.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2405s / 341473.0618 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2324
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1539
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34641/50000 (69.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5415s / 341670.6033 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2241
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1566
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 34661/50000 (69.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.3681s / 341866.9714 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2345
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1671
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 34681/50000 (69.3620%),                 avg. length: 2896.55,                last time consumption/overall running time: 191.4016s / 342058.3730 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2081
env0_second_0:                 episode reward: -1.0000,                 loss: -0.1535
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 34701/50000 (69.4020%),                 avg. length: 2367.0,                last time consumption/overall running time: 158.8525s / 342217.2255 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.1454
env0_second_0:                 episode reward: -2.6500,                 loss: -0.0916
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 34721/50000 (69.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0358s / 342413.2613 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2254
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1584
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 34741/50000 (69.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.9629s / 342610.2243 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2257
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1719
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34761/50000 (69.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.7567s / 342810.9810 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2331
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1745
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34781/50000 (69.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.5388s / 343011.5198 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2379
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1759
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 34801/50000 (69.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.4781s / 343209.9979 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2345
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1896
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34821/50000 (69.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9213s / 343407.9192 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2366
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1680
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 34841/50000 (69.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 201.4111s / 343609.3303 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2341
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1769
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34861/50000 (69.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.8406s / 343809.1709 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2309
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1641
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34881/50000 (69.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 200.9431s / 344010.1140 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2328
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1453
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34901/50000 (69.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8854s / 344208.9995 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2319
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1621
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 34921/50000 (69.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.6295s / 344405.6289 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2263
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1630
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 34941/50000 (69.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0064s / 344601.6353 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2247
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1742
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 34961/50000 (69.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4853s / 344799.1206 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2365
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1710
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 34981/50000 (69.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4618s / 344995.5824 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2407
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1849
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 35001/50000 (70.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.7476s / 345191.3300 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2432
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1786
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35021/50000 (70.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.6675s / 345387.9975 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2275
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1531
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 35041/50000 (70.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1404s / 345585.1379 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2323
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1717
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 35061/50000 (70.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6408s / 345782.7787 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2395
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1766
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35081/50000 (70.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2483s / 345981.0270 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2403
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1717
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35101/50000 (70.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.4031s / 346179.4301 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2425
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1698
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35121/50000 (70.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.8730s / 346377.3031 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2236
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1368
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35141/50000 (70.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.1979s / 346575.5011 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2110
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1478
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 35161/50000 (70.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3024s / 346772.8035 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2299
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1585
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 35181/50000 (70.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.5229s / 346969.3264 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2154
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0902
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 35201/50000 (70.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8803s / 347168.2067 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2312
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1642
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 35221/50000 (70.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.5284s / 347366.7351 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2246
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1556
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 35241/50000 (70.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7412s / 347563.4762 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2269
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1531
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35261/50000 (70.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7777s / 347761.2539 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2436
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1799
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35281/50000 (70.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.1771s / 347957.4310 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2399
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1786
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35301/50000 (70.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.5058s / 348153.9368 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2382
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1802
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35321/50000 (70.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.3625s / 348349.2993 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2317
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1825
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35341/50000 (70.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4488s / 348546.7481 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2415
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1876
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35361/50000 (70.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.4096s / 348745.1577 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2323
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1691
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35381/50000 (70.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.8219s / 348941.9796 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2212
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1544
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35401/50000 (70.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.3296s / 349141.3092 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2366
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1741
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 35421/50000 (70.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.6186s / 349338.9278 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2384
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1757
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35441/50000 (70.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7508s / 349535.6785 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2432
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1923
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35461/50000 (70.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.5159s / 349732.1944 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2426
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1898
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 35481/50000 (70.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.9421s / 349930.1365 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2125
env0_second_0:                 episode reward: -0.5000,                 loss: 0.0677
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 35501/50000 (71.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.1557s / 350129.2922 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2236
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1186
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 35521/50000 (71.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.3160s / 350327.6082 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2170
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1579
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35541/50000 (71.0820%),                 avg. length: 2944.8,                last time consumption/overall running time: 194.9946s / 350522.6028 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2051
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1460
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 35561/50000 (71.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.0296s / 350720.6323 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2259
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1545
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35581/50000 (71.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4163s / 350918.0486 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2301
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1782
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35601/50000 (71.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.0529s / 351117.1015 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2323
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1787
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35621/50000 (71.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.6069s / 351311.7084 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2431
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1860
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35641/50000 (71.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.2621s / 351507.9705 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2414
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1601
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 35661/50000 (71.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4293s / 351704.3998 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2303
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1815
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35681/50000 (71.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.7749s / 351902.1747 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2186
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1656
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35701/50000 (71.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2282s / 352100.4029 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2267
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1759
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35721/50000 (71.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.2797s / 352299.6825 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2100
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1613
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 35741/50000 (71.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1862s / 352496.8687 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2099
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1607
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 35761/50000 (71.5220%),                 avg. length: 2862.7,                last time consumption/overall running time: 187.9032s / 352684.7719 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.1832
env0_second_0:                 episode reward: -1.0000,                 loss: -0.1430
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 35781/50000 (71.5620%),                 avg. length: 2821.9,                last time consumption/overall running time: 184.6977s / 352869.4696 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.1792
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1314
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 35801/50000 (71.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.2702s / 353065.7398 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2247
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1668
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35821/50000 (71.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.9537s / 353261.6935 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2314
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1675
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 35841/50000 (71.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0067s / 353457.7002 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2173
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1555
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 35861/50000 (71.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.6559s / 353653.3561 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2165
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1665
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 35881/50000 (71.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3741s / 353850.7302 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2245
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1788
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 35901/50000 (71.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2711s / 354049.0012 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2351
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1668
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35921/50000 (71.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3722s / 354246.3734 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2174
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1574
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 35941/50000 (71.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4986s / 354442.8720 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2239
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1695
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 35961/50000 (71.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.9942s / 354639.8662 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2128
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1621
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 35981/50000 (71.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.5700s / 354836.4362 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2284
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1754
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 36001/50000 (72.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.7679s / 355032.2041 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2207
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1642
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 36021/50000 (72.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.6433s / 355227.8473 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2323
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1751
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 36041/50000 (72.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4914s / 355425.3387 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2336
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1857
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 36061/50000 (72.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1227s / 355622.4614 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2346
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1913
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 36081/50000 (72.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0791s / 355818.5405 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2334
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1008
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 36101/50000 (72.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.3404s / 356016.8810 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2268
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1346
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 36121/50000 (72.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.5681s / 356212.4491 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2257
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1617
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 36141/50000 (72.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.1859s / 356407.6351 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2348
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1553
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 36161/50000 (72.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.1315s / 356603.7666 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2293
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1790
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 36181/50000 (72.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.7947s / 356798.5613 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2304
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1811
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 36201/50000 (72.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.3626s / 356992.9238 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2304
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1619
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 36221/50000 (72.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.4535s / 357190.3773 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2407
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1823
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 36241/50000 (72.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.0396s / 357389.4169 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2107
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1648
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 36261/50000 (72.5220%),                 avg. length: 2978.65,                last time consumption/overall running time: 194.3120s / 357583.7289 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2106
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1590
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 36281/50000 (72.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 202.2715s / 357786.0005 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2019
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1494
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 36301/50000 (72.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.6316s / 357982.6320 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2166
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1456
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 36321/50000 (72.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.0481s / 358179.6802 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2009
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1544
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 36341/50000 (72.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.2234s / 358378.9035 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2246
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1754
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 36361/50000 (72.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 199.3161s / 358578.2197 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2244
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1669
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 36381/50000 (72.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3381s / 358775.5578 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2134
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1672
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 36401/50000 (72.8020%),                 avg. length: 2905.95,                last time consumption/overall running time: 190.5366s / 358966.0944 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.1885
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1334
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 36421/50000 (72.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7188s / 359162.8132 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2278
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1654
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 36441/50000 (72.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.6781s / 359357.4913 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2240
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1828
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 36461/50000 (72.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.8493s / 359553.3405 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2213
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1663
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 36481/50000 (72.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.0468s / 359748.3873 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2079
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1647
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 36501/50000 (73.0020%),                 avg. length: 2875.4,                last time consumption/overall running time: 187.7742s / 359936.1615 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.1969
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1589
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 36521/50000 (73.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.2981s / 360133.4596 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.2034
env0_second_0:                 episode reward: -0.6500,                 loss: -0.1686
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 36541/50000 (73.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.0363s / 360328.4959 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.1960
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1523
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 36561/50000 (73.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.2332s / 360526.7291 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2115
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1420
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 36581/50000 (73.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.2576s / 360722.9868 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2210
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1313
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 36601/50000 (73.2020%),                 avg. length: 2972.05,                last time consumption/overall running time: 193.3303s / 360916.3171 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2019
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1499
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 36621/50000 (73.2420%),                 avg. length: 2664.95,                last time consumption/overall running time: 178.6418s / 361094.9588 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.1640
env0_second_0:                 episode reward: -1.4000,                 loss: -0.1065
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 36641/50000 (73.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.9733s / 361291.9321 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1851
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1534
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 36661/50000 (73.3220%),                 avg. length: 2993.1,                last time consumption/overall running time: 198.0610s / 361489.9932 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2019
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1595
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 36681/50000 (73.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5346s / 361687.5277 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2392
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1956
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 36701/50000 (73.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5540s / 361885.0818 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2302
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1487
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 36721/50000 (73.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0760s / 362081.1577 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2388
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1823
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 36741/50000 (73.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.9896s / 362276.1474 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2369
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1850
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 36761/50000 (73.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4332s / 362472.5806 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2379
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1873
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 36781/50000 (73.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.5628s / 362669.1434 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2193
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1864
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 36801/50000 (73.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.1470s / 362865.2904 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2165
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1479
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 36821/50000 (73.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.7482s / 363062.0386 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2267
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1745
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 36841/50000 (73.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.8291s / 363259.8677 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2318
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1966
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 36861/50000 (73.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.5444s / 363457.4121 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2370
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1897
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 36881/50000 (73.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4461s / 363653.8582 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2216
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1724
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 36901/50000 (73.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.0803s / 363848.9385 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2284
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1612
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 36921/50000 (73.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1156s / 364046.0541 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2339
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1689
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 36941/50000 (73.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.2933s / 364242.3474 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2327
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1680
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 36961/50000 (73.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.3460s / 364438.6934 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2095
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1590
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 36981/50000 (73.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.3456s / 364636.0390 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2156
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1845
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 37001/50000 (74.0020%),                 avg. length: 2969.85,                last time consumption/overall running time: 197.2149s / 364833.2539 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2019
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1661
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 37021/50000 (74.0420%),                 avg. length: 2601.95,                last time consumption/overall running time: 172.7655s / 365006.0194 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.1742
env0_second_0:                 episode reward: -1.6000,                 loss: -0.1431
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 37041/50000 (74.0820%),                 avg. length: 2936.35,                last time consumption/overall running time: 191.0562s / 365197.0756 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.1965
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1731
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 37061/50000 (74.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.4489s / 365393.5245 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2348
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1849
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 37081/50000 (74.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.4674s / 365587.9920 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2370
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1960
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 37101/50000 (74.2020%),                 avg. length: 2946.8,                last time consumption/overall running time: 192.6670s / 365780.6590 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.1865
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1607
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 37121/50000 (74.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.1985s / 365976.8575 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2335
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1758
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 37141/50000 (74.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 198.8174s / 366175.6749 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2353
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1900
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 37161/50000 (74.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.5000s / 366372.1749 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2321
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1834
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 37181/50000 (74.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.6901s / 366565.8651 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2241
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1743
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 37201/50000 (74.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.7406s / 366757.6057 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2256
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1583
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 37221/50000 (74.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.5381s / 366949.1438 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2036
env0_second_0:                 episode reward: -0.9000,                 loss: -0.1388
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 37241/50000 (74.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.6988s / 367141.8426 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2128
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1763
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 37261/50000 (74.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.1353s / 367335.9779 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2366
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1829
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 37281/50000 (74.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.3526s / 367528.3305 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2413
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1703
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 37301/50000 (74.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 188.8638s / 367717.1943 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2389
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1652
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 37321/50000 (74.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.0814s / 367910.2758 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2333
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1379
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 37341/50000 (74.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.5629s / 368101.8386 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2213
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1505
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 37361/50000 (74.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 189.8705s / 368291.7091 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2264
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1678
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 37381/50000 (74.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.7410s / 368482.4501 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2221
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1726
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 37401/50000 (74.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.4192s / 368674.8693 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2300
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1634
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 37421/50000 (74.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.2167s / 368866.0860 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2213
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1631
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 37441/50000 (74.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.5802s / 369060.6662 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2201
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1701
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 37461/50000 (74.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.5716s / 369254.2377 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2265
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1713
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 37481/50000 (74.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.1565s / 369448.3942 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2157
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1574
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 37501/50000 (75.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.1683s / 369641.5626 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2212
env0_second_0:                 episode reward: 0.0500,                 loss: -0.5533
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 37521/50000 (75.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.6990s / 369835.2615 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2116
env0_second_0:                 episode reward: -0.2000,                 loss: 0.4081
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 37541/50000 (75.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.5232s / 370027.7847 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2059
env0_second_0:                 episode reward: 0.0500,                 loss: 0.3668
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 37561/50000 (75.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.0920s / 370219.8768 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2287
env0_second_0:                 episode reward: -0.0500,                 loss: 0.1157
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 37581/50000 (75.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.7031s / 370410.5799 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2273
env0_second_0:                 episode reward: 0.0000,                 loss: 0.1587
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 37601/50000 (75.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.0750s / 370604.6548 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2248
env0_second_0:                 episode reward: 0.0000,                 loss: 0.1278
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 37621/50000 (75.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.4460s / 370797.1008 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2143
env0_second_0:                 episode reward: -0.1000,                 loss: 0.0506
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 37641/50000 (75.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.8778s / 370989.9786 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2247
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0271
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 37661/50000 (75.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.3774s / 371180.3560 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2227
env0_second_0:                 episode reward: -0.1500,                 loss: 0.0298
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 37681/50000 (75.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.0370s / 371370.3930 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2191
env0_second_0:                 episode reward: -0.0500,                 loss: -0.0814
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 37701/50000 (75.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.1081s / 371563.5011 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2210
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0942
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 37721/50000 (75.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.9185s / 371754.4196 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2233
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1467
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 37741/50000 (75.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.3099s / 371945.7295 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2343
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1323
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 37761/50000 (75.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.8796s / 372137.6090 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2459
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1662
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 37781/50000 (75.5620%),                 avg. length: 2918.9,                last time consumption/overall running time: 186.8737s / 372324.4827 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.2024
env0_second_0:                 episode reward: -1.5500,                 loss: -0.1177
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 37801/50000 (75.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.8171s / 372517.2998 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2189
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1500
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 37821/50000 (75.6420%),                 avg. length: 2982.9,                last time consumption/overall running time: 189.9727s / 372707.2725 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2274
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1628
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 37841/50000 (75.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.8755s / 372899.1480 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2316
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1732
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 37861/50000 (75.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.4443s / 373090.5923 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2259
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1625
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 37881/50000 (75.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.6477s / 373281.2400 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2238
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1557
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 37901/50000 (75.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.0034s / 373473.2434 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2314
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1357
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 37921/50000 (75.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.2679s / 373664.5113 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2370
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1470
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 37941/50000 (75.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.7876s / 373856.2989 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2302
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1742
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 37961/50000 (75.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.3051s / 374049.6040 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2422
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1658
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 37981/50000 (75.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.0378s / 374241.6418 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2330
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1708
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 38001/50000 (76.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.4548s / 374434.0966 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2240
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1359
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 38021/50000 (76.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.1613s / 374627.2579 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2338
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1392
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 38041/50000 (76.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.9283s / 374818.1862 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2276
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1583
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38061/50000 (76.1220%),                 avg. length: 2996.85,                last time consumption/overall running time: 191.4097s / 375009.5959 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2250
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1844
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 38081/50000 (76.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.2504s / 375201.8463 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2316
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1942
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 38101/50000 (76.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.9764s / 375394.8227 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2334
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1870
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 38121/50000 (76.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.4694s / 375585.2921 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2399
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1947
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 38141/50000 (76.2820%),                 avg. length: 2944.55,                last time consumption/overall running time: 188.3641s / 375773.6562 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2119
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1588
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 38161/50000 (76.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.4757s / 375964.1319 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2273
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1802
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38181/50000 (76.3620%),                 avg. length: 2927.05,                last time consumption/overall running time: 187.4405s / 376151.5724 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2146
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1639
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 38201/50000 (76.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.3753s / 376342.9478 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.2121
env0_second_0:                 episode reward: -0.8500,                 loss: -0.1704
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 38221/50000 (76.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 189.5297s / 376532.4775 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2312
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1821
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 38241/50000 (76.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.4680s / 376725.9454 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2278
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1524
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38261/50000 (76.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.9665s / 376916.9120 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2368
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1620
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 38281/50000 (76.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.2152s / 377109.1272 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2421
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1822
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 38301/50000 (76.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.9859s / 377302.1131 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2417
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1725
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38321/50000 (76.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.1267s / 377492.2398 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2340
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1694
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 38341/50000 (76.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.0959s / 377684.3357 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2194
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1584
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 38361/50000 (76.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.4088s / 377877.7446 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2383
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1676
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 38381/50000 (76.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.5554s / 378072.2999 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2335
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1686
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 38401/50000 (76.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.0806s / 378264.3805 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2340
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1685
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 38421/50000 (76.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.1639s / 378455.5444 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2325
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1589
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 38441/50000 (76.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.4510s / 378645.9953 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2061
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1105
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 38461/50000 (76.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.6653s / 378836.6607 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2397
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1987
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 38481/50000 (76.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.6938s / 379028.3544 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2399
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2053
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38501/50000 (77.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.0874s / 379219.4418 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2323
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1764
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38521/50000 (77.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.5318s / 379410.9736 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2239
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1934
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 38541/50000 (77.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.3246s / 379605.2981 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2415
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1984
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 38561/50000 (77.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.7319s / 379797.0300 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2338
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1896
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38581/50000 (77.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.0715s / 379988.1016 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2168
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1804
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 38601/50000 (77.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.9579s / 380180.0595 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.2027
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1751
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38621/50000 (77.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.2930s / 380371.3525 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2160
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1843
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 38641/50000 (77.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.3364s / 380562.6889 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2287
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2005
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 38661/50000 (77.3220%),                 avg. length: 2923.7,                last time consumption/overall running time: 185.8920s / 380748.5808 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2282
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1863
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 38681/50000 (77.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.1676s / 380940.7484 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2351
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1798
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 38701/50000 (77.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.2765s / 381132.0248 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2396
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1763
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 38721/50000 (77.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 189.7834s / 381321.8082 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2307
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1648
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 38741/50000 (77.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.3187s / 381513.1269 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2350
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1736
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 38761/50000 (77.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.1277s / 381705.2546 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2176
env0_second_0:                 episode reward: 0.4500,                 loss: -0.0963
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 38781/50000 (77.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.2827s / 381896.5373 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2237
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1546
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 38801/50000 (77.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.3177s / 382087.8550 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2361
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1183
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 38821/50000 (77.6420%),                 avg. length: 2971.35,                last time consumption/overall running time: 190.3193s / 382278.1743 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2084
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1322
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 38841/50000 (77.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.7164s / 382469.8907 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2149
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1307
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 38861/50000 (77.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.4371s / 382661.3278 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2306
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1590
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38881/50000 (77.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.5324s / 382852.8602 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2185
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1550
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 38901/50000 (77.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.5161s / 383044.3763 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2207
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1662
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 38921/50000 (77.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.4380s / 383237.8143 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2288
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1722
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 38941/50000 (77.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.9593s / 383429.7737 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2337
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1707
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 38961/50000 (77.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.1361s / 383621.9097 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2272
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1496
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 38981/50000 (77.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.8589s / 383814.7686 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2256
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1760
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 39001/50000 (78.0020%),                 avg. length: 2858.85,                last time consumption/overall running time: 182.9841s / 383997.7528 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.1861
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1329
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 39021/50000 (78.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.8312s / 384188.5839 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2164
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1717
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 39041/50000 (78.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.4043s / 384379.9883 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2320
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1738
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 39061/50000 (78.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.5605s / 384570.5488 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2368
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1620
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39081/50000 (78.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.9940s / 384761.5428 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2342
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1529
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39101/50000 (78.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.4149s / 384940.9577 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2381
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1745
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 39121/50000 (78.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.4700s / 385112.4277 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2375
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1783
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39141/50000 (78.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.4077s / 385284.8354 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2332
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1869
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39161/50000 (78.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.9956s / 385457.8310 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2363
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1845
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39181/50000 (78.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2086s / 385629.0396 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2361
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1810
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39201/50000 (78.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.1760s / 385799.2156 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2357
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1806
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39221/50000 (78.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.8781s / 385970.0937 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2464
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1830
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 39241/50000 (78.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.9594s / 386142.0531 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2413
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1812
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 39261/50000 (78.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1473s / 386313.2004 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2391
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1768
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39281/50000 (78.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.8210s / 386483.0215 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2359
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1852
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39301/50000 (78.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.1926s / 386653.2141 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2347
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1738
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39321/50000 (78.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6611s / 386823.8752 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2383
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1742
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39341/50000 (78.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.0245s / 386996.8996 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2185
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1629
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39361/50000 (78.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.7757s / 387170.6753 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2380
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1880
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39381/50000 (78.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.0744s / 387343.7497 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2377
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1498
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 39401/50000 (78.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.5877s / 387513.3374 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2280
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1725
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 39421/50000 (78.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6211s / 387683.9585 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2339
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1678
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 39441/50000 (78.8820%),                 avg. length: 2686.6,                last time consumption/overall running time: 152.1539s / 387836.1124 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.1689
env0_second_0:                 episode reward: -1.5500,                 loss: -0.1045
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 39461/50000 (78.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0815s / 388007.1939 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2311
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1785
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 39481/50000 (78.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.5421s / 388177.7360 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2237
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1843
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 39501/50000 (79.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.9370s / 388350.6730 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.2173
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1669
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39521/50000 (79.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.0799s / 388522.7529 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2422
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1651
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39541/50000 (79.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8313s / 388694.5842 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2350
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1382
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39561/50000 (79.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.4844s / 388867.0686 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2488
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1808
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 39581/50000 (79.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.2239s / 389039.2926 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2481
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1655
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39601/50000 (79.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.3765s / 389211.6690 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2342
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1812
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39621/50000 (79.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.7507s / 389385.4197 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2392
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1662
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39641/50000 (79.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.5942s / 389558.0139 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2325
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1312
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 39661/50000 (79.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8904s / 389729.9043 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2430
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1697
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 39681/50000 (79.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.8145s / 389903.7189 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2296
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1315
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39701/50000 (79.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.0531s / 390076.7720 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2288
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1755
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39721/50000 (79.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.0772s / 390249.8492 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2296
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1613
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 39741/50000 (79.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.4069s / 390421.2561 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2315
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1546
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 39761/50000 (79.5220%),                 avg. length: 2983.15,                last time consumption/overall running time: 173.2405s / 390594.4965 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2266
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1624
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 39781/50000 (79.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.2193s / 390764.7159 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2412
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1869
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 39801/50000 (79.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.4405s / 390936.1564 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2209
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1514
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 39821/50000 (79.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.9816s / 391110.1379 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2358
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1768
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 39841/50000 (79.6820%),                 avg. length: 2367.65,                last time consumption/overall running time: 137.5797s / 391247.7177 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.1606
env0_second_0:                 episode reward: -2.2500,                 loss: 0.1863
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 39861/50000 (79.7220%),                 avg. length: 2752.15,                last time consumption/overall running time: 157.6053s / 391405.3230 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.1691
env0_second_0:                 episode reward: -1.3000,                 loss: -0.0329
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 39881/50000 (79.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.2524s / 391578.5753 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2347
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1172
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 39901/50000 (79.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.6151s / 391751.1904 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2367
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1483
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39921/50000 (79.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1428s / 391922.3332 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2351
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1482
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39941/50000 (79.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.1958s / 392092.5290 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2355
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1646
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 39961/50000 (79.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.3757s / 392262.9047 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2247
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1089
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 39981/50000 (79.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2040s / 392434.1087 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2324
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1408
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 40001/50000 (80.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.5073s / 392605.6160 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2115
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1108
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 40021/50000 (80.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.4116s / 392775.0276 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2311
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1134
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40041/50000 (80.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.4163s / 392944.4438 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.1976
env0_second_0:                 episode reward: -0.6000,                 loss: -0.1252
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 40061/50000 (80.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8455s / 393116.2894 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2311
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1355
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40081/50000 (80.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.1599s / 393286.4492 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2335
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1553
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 40101/50000 (80.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1623s / 393457.6115 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2337
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1428
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 40121/50000 (80.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.4126s / 393628.0242 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2374
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0938
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40141/50000 (80.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.8900s / 393797.9142 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2279
env0_second_0:                 episode reward: 0.1500,                 loss: -0.0393
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 40161/50000 (80.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0398s / 393968.9540 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2382
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1140
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 40181/50000 (80.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.3803s / 394141.3343 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2369
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1424
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 40201/50000 (80.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.3071s / 394313.6414 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2364
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1480
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40221/50000 (80.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.9907s / 394486.6321 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2231
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1211
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40241/50000 (80.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.4997s / 394658.1318 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2293
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1399
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40261/50000 (80.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0177s / 394829.1494 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2361
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1468
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 40281/50000 (80.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.5056s / 395001.6551 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2346
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1616
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40301/50000 (80.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.1575s / 395173.8125 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2435
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1669
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 40321/50000 (80.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6066s / 395344.4191 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2285
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1701
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 40341/50000 (80.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2067s / 395515.6258 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2327
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1680
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 40361/50000 (80.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.7979s / 395688.4237 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2351
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1635
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 40381/50000 (80.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.4700s / 395858.8937 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2415
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1781
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 40401/50000 (80.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.1805s / 396031.0742 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2443
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1736
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40421/50000 (80.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0772s / 396202.1514 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2315
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1751
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 40441/50000 (80.8820%),                 avg. length: 2920.05,                last time consumption/overall running time: 166.0399s / 396368.1913 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.1970
env0_second_0:                 episode reward: -1.7000,                 loss: -0.1606
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 40461/50000 (80.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.5151s / 396539.7064 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2253
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1686
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 40481/50000 (80.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.7825s / 396711.4889 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2327
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1871
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40501/50000 (81.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.5034s / 396882.9923 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2352
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1666
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 40521/50000 (81.0420%),                 avg. length: 2984.05,                last time consumption/overall running time: 169.0557s / 397052.0479 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2321
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1797
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 40541/50000 (81.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2773s / 397223.3252 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2268
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1372
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 40561/50000 (81.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3589s / 397394.6841 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2266
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1507
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 40581/50000 (81.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.1838s / 397566.8679 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2307
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1547
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40601/50000 (81.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.8849s / 397737.7529 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2292
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1622
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40621/50000 (81.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.3917s / 397908.1446 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2162
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1403
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 40641/50000 (81.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.3663s / 398078.5109 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2298
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1593
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 40661/50000 (81.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.2637s / 398250.7746 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2215
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1635
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 40681/50000 (81.3620%),                 avg. length: 2968.65,                last time consumption/overall running time: 169.4909s / 398420.2655 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2151
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1709
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 40701/50000 (81.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0603s / 398591.3259 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2314
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1665
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40721/50000 (81.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0027s / 398762.3286 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2335
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1764
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 40741/50000 (81.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.1346s / 398931.4632 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2322
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1574
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40761/50000 (81.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.6414s / 399103.1046 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2239
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1596
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 40781/50000 (81.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0690s / 399274.1736 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2239
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1617
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 40801/50000 (81.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.6384s / 399445.8120 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2281
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1701
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 40821/50000 (81.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.4848s / 399618.2968 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2334
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1455
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40841/50000 (81.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3459s / 399789.6427 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2330
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1601
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 40861/50000 (81.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.1634s / 399961.8062 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2361
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1550
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40881/50000 (81.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3805s / 400133.1866 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2263
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1509
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40901/50000 (81.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 175.5127s / 400308.6993 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2281
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1546
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 40921/50000 (81.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 174.5710s / 400483.2703 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2217
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1559
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40941/50000 (81.8820%),                 avg. length: 2949.2,                last time consumption/overall running time: 171.2324s / 400654.5027 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.2072
env0_second_0:                 episode reward: -0.8000,                 loss: -0.1528
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 40961/50000 (81.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.4705s / 400827.9731 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2254
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1480
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 40981/50000 (81.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.3800s / 401001.3532 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2359
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1467
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41001/50000 (82.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.5932s / 401174.9463 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2279
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1573
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 41021/50000 (82.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.3171s / 401348.2635 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2292
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1853
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 41041/50000 (82.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1828s / 401519.4462 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.2236
env0_second_0:                 episode reward: 0.5000,                 loss: -0.1755
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 41061/50000 (82.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.7940s / 401690.2402 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2021
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1596
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 41081/50000 (82.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.5014s / 401862.7416 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2319
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1545
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 41101/50000 (82.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.1542s / 402034.8958 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2283
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1245
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 41121/50000 (82.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 174.9440s / 402209.8398 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2309
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1432
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41141/50000 (82.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.4574s / 402382.2972 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2368
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1454
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41161/50000 (82.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.4954s / 402553.7926 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2254
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1558
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 41181/50000 (82.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.1246s / 402726.9172 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2358
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1366
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41201/50000 (82.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.9724s / 402899.8896 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2327
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1585
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41221/50000 (82.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.2029s / 403072.0925 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2287
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1420
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 41241/50000 (82.4820%),                 avg. length: 2992.3,                last time consumption/overall running time: 169.7223s / 403241.8147 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2172
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1478
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 41261/50000 (82.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.2495s / 403415.0642 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2341
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1767
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41281/50000 (82.5620%),                 avg. length: 2924.5,                last time consumption/overall running time: 170.7481s / 403585.8123 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2192
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1524
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 41301/50000 (82.6020%),                 avg. length: 2998.2,                last time consumption/overall running time: 171.0142s / 403756.8265 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.2082
env0_second_0:                 episode reward: 0.8000,                 loss: -0.1428
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 41321/50000 (82.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.9200s / 403928.7464 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2336
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1641
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 41341/50000 (82.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.0393s / 404100.7857 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2351
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1720
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41361/50000 (82.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.0168s / 404272.8025 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2376
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1386
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41381/50000 (82.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 174.2337s / 404447.0362 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2220
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1253
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 41401/50000 (82.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.9011s / 404618.9373 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2378
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1612
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41421/50000 (82.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.3358s / 404792.2731 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2305
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1561
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41441/50000 (82.8820%),                 avg. length: 2809.15,                last time consumption/overall running time: 161.7549s / 404954.0280 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.2039
env0_second_0:                 episode reward: -0.7500,                 loss: -0.1317
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 41461/50000 (82.9220%),                 avg. length: 2943.55,                last time consumption/overall running time: 170.2317s / 405124.2597 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2317
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1531
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 41481/50000 (82.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3453s / 405295.6050 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2330
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1264
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41501/50000 (83.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8562s / 405467.4612 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2428
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1582
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 41521/50000 (83.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.1148s / 405639.5760 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2327
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1636
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41541/50000 (83.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.7271s / 405812.3031 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2347
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1669
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41561/50000 (83.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.7602s / 405986.0633 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2343
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1610
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41581/50000 (83.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.4375s / 406158.5009 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2380
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1756
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 41601/50000 (83.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0947s / 406329.5955 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2327
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1363
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 41621/50000 (83.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1568s / 406500.7523 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2395
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1431
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 41641/50000 (83.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.0024s / 406670.7547 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2278
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0112
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41661/50000 (83.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0999s / 406841.8546 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2282
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1046
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41681/50000 (83.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.5949s / 407013.4495 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2266
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1692
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41701/50000 (83.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.5770s / 407183.0265 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2345
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1739
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41721/50000 (83.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.8485s / 407352.8750 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2317
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1684
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41741/50000 (83.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8417s / 407524.7167 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1727
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 41761/50000 (83.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8276s / 407696.5443 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2187
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1326
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 41781/50000 (83.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.2762s / 407866.8205 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2316
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1444
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41801/50000 (83.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.1288s / 408035.9493 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2349
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1494
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41821/50000 (83.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.4035s / 408205.3528 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2303
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1350
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 41841/50000 (83.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.6883s / 408375.0410 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2266
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1536
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41861/50000 (83.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.5353s / 408545.5763 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2342
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1684
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 41881/50000 (83.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.5891s / 408716.1654 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2291
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1676
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 41901/50000 (83.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6526s / 408886.8180 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2275
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1640
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41921/50000 (83.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7794s / 409056.5974 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2230
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1200
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 41941/50000 (83.8820%),                 avg. length: 2950.75,                last time consumption/overall running time: 166.9879s / 409223.5853 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2273
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1248
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 41961/50000 (83.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7947s / 409393.3800 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2409
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1888
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 41981/50000 (83.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7632s / 409563.1431 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2340
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1815
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42001/50000 (84.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0339s / 409734.1771 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2293
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1797
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 42021/50000 (84.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.5511s / 409904.7282 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2276
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1764
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 42041/50000 (84.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1076s / 410075.8358 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2310
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1805
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 42061/50000 (84.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7625s / 410245.5983 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2327
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1951
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 42081/50000 (84.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.0527s / 410414.6510 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2355
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1859
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 42101/50000 (84.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.4524s / 410585.1034 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2306
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1690
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 42121/50000 (84.2420%),                 avg. length: 2928.2,                last time consumption/overall running time: 166.1196s / 410751.2229 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.2080
env0_second_0:                 episode reward: -0.5500,                 loss: -0.1433
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 42141/50000 (84.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7213s / 410920.9442 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2306
env0_second_0:                 episode reward: -0.3500,                 loss: -0.0571
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 42161/50000 (84.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.3400s / 411091.2842 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2370
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1526
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 42181/50000 (84.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3050s / 411262.5892 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2407
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1458
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42201/50000 (84.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.4174s / 411434.0066 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2387
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1721
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 42221/50000 (84.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.2021s / 411604.2087 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2382
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1203
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 42241/50000 (84.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6047s / 411774.8135 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2389
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1449
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42261/50000 (84.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.4959s / 411943.3094 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2384
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1752
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 42281/50000 (84.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.5927s / 412112.9020 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2311
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1506
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42301/50000 (84.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.6300s / 412282.5320 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2325
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1685
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42321/50000 (84.6420%),                 avg. length: 2760.4,                last time consumption/overall running time: 157.5065s / 412440.0385 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.1951
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1221
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 42341/50000 (84.6820%),                 avg. length: 2793.0,                last time consumption/overall running time: 158.0535s / 412598.0920 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.1766
env0_second_0:                 episode reward: -1.8500,                 loss: -0.0917
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 42361/50000 (84.7220%),                 avg. length: 2993.7,                last time consumption/overall running time: 170.3884s / 412768.4804 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2126
env0_second_0:                 episode reward: -0.6000,                 loss: -0.1169
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 42381/50000 (84.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2562s / 412939.7366 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2228
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1419
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 42401/50000 (84.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2409s / 413110.9775 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2321
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1574
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 42421/50000 (84.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.4415s / 413280.4190 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2310
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1252
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 42441/50000 (84.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7697s / 413450.1887 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2334
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0778
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42461/50000 (84.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.0583s / 413619.2470 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2117
env0_second_0:                 episode reward: -0.1500,                 loss: -0.0926
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42481/50000 (84.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.8916s / 413789.1387 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2242
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1207
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 42501/50000 (85.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2765s / 413960.4152 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2308
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1196
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 42521/50000 (85.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.0833s / 414130.4985 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2246
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1621
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 42541/50000 (85.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.9247s / 414299.4232 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2208
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1521
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 42561/50000 (85.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.0954s / 414471.5186 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2185
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1531
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 42581/50000 (85.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.6720s / 414641.1906 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2395
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0433
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 42601/50000 (85.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.1059s / 414811.2965 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2331
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1383
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 42621/50000 (85.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.3172s / 414981.6137 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2232
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1570
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 42641/50000 (85.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.7693s / 415150.3830 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2181
env0_second_0:                 episode reward: -0.3000,                 loss: -0.0969
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 42661/50000 (85.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.4949s / 415321.8779 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2326
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1690
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 42681/50000 (85.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6605s / 415492.5384 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2295
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1712
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 42701/50000 (85.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.9605s / 415665.4988 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2386
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1845
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42721/50000 (85.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2760s / 415836.7748 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2222
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1049
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42741/50000 (85.4820%),                 avg. length: 2911.1,                last time consumption/overall running time: 165.4653s / 416002.2401 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2287
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1611
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 42761/50000 (85.5220%),                 avg. length: 2054.85,                last time consumption/overall running time: 117.1788s / 416119.4189 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.1370
env0_second_0:                 episode reward: -3.9000,                 loss: -0.0607
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 42781/50000 (85.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.5990s / 416289.0179 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2224
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1620
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 42801/50000 (85.6020%),                 avg. length: 2871.6,                last time consumption/overall running time: 162.7584s / 416451.7763 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2101
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1434
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 42821/50000 (85.6420%),                 avg. length: 2861.35,                last time consumption/overall running time: 161.8162s / 416613.5926 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.2041
env0_second_0:                 episode reward: -1.1500,                 loss: -0.1607
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 42841/50000 (85.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.9740s / 416782.5666 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2291
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1306
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 42861/50000 (85.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.0876s / 416951.6542 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2348
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1554
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 42881/50000 (85.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.8175s / 417121.4716 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2309
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1476
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 42901/50000 (85.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.3060s / 417290.7776 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2255
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1394
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 42921/50000 (85.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.4367s / 417460.2143 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2341
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1503
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 42941/50000 (85.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.4103s / 417629.6246 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2180
env0_second_0:                 episode reward: -0.5000,                 loss: -0.0808
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 42961/50000 (85.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.9834s / 417800.6080 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2317
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1365
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 42981/50000 (85.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0812s / 417971.6892 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2358
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1363
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43001/50000 (86.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.3334s / 418142.0225 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2300
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1650
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 43021/50000 (86.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.5521s / 418313.5746 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2306
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1668
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 43041/50000 (86.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.7748s / 418487.3494 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2343
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1649
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 43061/50000 (86.1220%),                 avg. length: 2307.75,                last time consumption/overall running time: 135.7391s / 418623.0885 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.1541
env0_second_0:                 episode reward: -2.1500,                 loss: -0.0115
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 43081/50000 (86.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.5859s / 418795.6744 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2475
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1229
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 43101/50000 (86.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.7812s / 418967.4557 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2449
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1365
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43121/50000 (86.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.3665s / 419136.8222 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2386
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1698
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 43141/50000 (86.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.9547s / 419305.7769 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2426
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1718
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 43161/50000 (86.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.5344s / 419475.3113 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2423
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1493
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 43181/50000 (86.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.6936s / 419644.0049 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2422
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1549
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43201/50000 (86.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.4755s / 419812.4804 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2283
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1338
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 43221/50000 (86.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.1282s / 419981.6086 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2359
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1422
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 43241/50000 (86.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.9666s / 420150.5751 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2368
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1732
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43261/50000 (86.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1024s / 420321.6775 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2334
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1604
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 43281/50000 (86.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.5088s / 420491.1863 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2310
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1598
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43301/50000 (86.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.6980s / 420660.8843 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2367
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1373
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 43321/50000 (86.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6284s / 420831.5127 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2319
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1516
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 43341/50000 (86.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.9744s / 421000.4872 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2355
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1680
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43361/50000 (86.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6623s / 421171.1495 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2297
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1613
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 43381/50000 (86.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.8863s / 421341.0358 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2380
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1655
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 43401/50000 (86.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.9441s / 421510.9799 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2330
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1607
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 43421/50000 (86.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.4800s / 421681.4598 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2307
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1768
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 43441/50000 (86.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7414s / 421851.2012 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2385
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1750
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 43461/50000 (86.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.5162s / 422020.7174 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2364
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1763
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 43481/50000 (86.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.0463s / 422190.7637 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2420
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1901
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43501/50000 (87.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.3238s / 422359.0875 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2402
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1488
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 43521/50000 (87.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.9982s / 422528.0857 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2382
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1083
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 43541/50000 (87.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.4895s / 422697.5751 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2237
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1469
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 43561/50000 (87.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7292s / 422867.3043 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2261
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1285
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 43581/50000 (87.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.3702s / 423035.6746 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2352
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1391
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43601/50000 (87.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.5013s / 423204.1758 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2351
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1572
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 43621/50000 (87.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.3004s / 423372.4762 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2456
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1789
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 43641/50000 (87.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.5840s / 423541.0602 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2365
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1555
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 43661/50000 (87.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.8242s / 423709.8844 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2420
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1767
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 43681/50000 (87.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.2812s / 423879.1656 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2327
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1648
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 43701/50000 (87.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.5609s / 424047.7264 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2316
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1687
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 43721/50000 (87.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.4913s / 424216.2177 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2403
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1276
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 43741/50000 (87.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.9715s / 424385.1893 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2276
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1588
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 43761/50000 (87.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.5842s / 424555.7735 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2269
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1653
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 43781/50000 (87.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.0506s / 424724.8241 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2031
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1028
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 43801/50000 (87.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.6331s / 424893.4572 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2347
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1786
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 43821/50000 (87.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 168.3794s / 425061.8366 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2357
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1852
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 43841/50000 (87.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.1586s / 425230.9952 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2352
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1873
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 43861/50000 (87.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.2816s / 425400.2768 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2428
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1899
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 43881/50000 (87.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1269s / 425571.4037 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2396
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1822
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 43901/50000 (87.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.5258s / 425740.9295 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2396
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1864
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 43921/50000 (87.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7443s / 425910.6738 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2399
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2000
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 43941/50000 (87.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6343s / 426081.3081 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2422
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1918
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 43961/50000 (87.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.4844s / 426253.7925 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2375
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1744
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 43981/50000 (87.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.7518s / 426426.5444 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2342
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1793
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44001/50000 (88.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.9118s / 426597.4562 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2409
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1836
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 44021/50000 (88.0420%),                 avg. length: 2996.55,                last time consumption/overall running time: 170.3912s / 426767.8474 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2825
env0_second_0:                 episode reward: -1.5000,                 loss: -0.1909
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 44041/50000 (88.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6408s / 426938.4882 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2266
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1880
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 44061/50000 (88.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0146s / 427109.5028 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2341
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1929
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 44081/50000 (88.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.7264s / 427280.2292 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2405
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1832
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44101/50000 (88.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.0518s / 427453.2810 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2395
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1788
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44121/50000 (88.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.9256s / 427624.2066 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2453
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1898
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 44141/50000 (88.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.8700s / 427794.0767 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2361
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1850
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 44161/50000 (88.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.9998s / 427964.0765 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2410
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1827
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44181/50000 (88.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3369s / 428135.4134 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2412
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1994
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 44201/50000 (88.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3210s / 428306.7344 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2441
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1888
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44221/50000 (88.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8936s / 428478.6279 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2406
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1579
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 44241/50000 (88.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8311s / 428650.4590 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2501
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1919
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44261/50000 (88.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0916s / 428821.5506 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2375
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1778
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 44281/50000 (88.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.9543s / 428994.5049 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2458
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1917
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 44301/50000 (88.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.2602s / 429164.7651 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2442
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1759
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44321/50000 (88.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.7360s / 429334.5011 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2435
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1734
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44341/50000 (88.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0755s / 429505.5765 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2396
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1650
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44361/50000 (88.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.7801s / 429676.3566 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2414
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1743
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 44381/50000 (88.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.8559s / 429847.2126 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2424
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1853
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 44401/50000 (88.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.7121s / 430018.9246 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2335
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1757
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44421/50000 (88.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1934s / 430190.1180 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2345
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1764
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44441/50000 (88.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.6087s / 430359.7267 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2377
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1745
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44461/50000 (88.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.8023s / 430531.5290 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2408
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1509
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44481/50000 (88.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.9053s / 430701.4343 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2245
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1632
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 44501/50000 (89.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.8061s / 430872.2404 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2330
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1771
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 44521/50000 (89.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.3818s / 431042.6222 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2389
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1849
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44541/50000 (89.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.9370s / 431212.5592 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2339
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1880
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 44561/50000 (89.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.6787s / 431382.2379 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2463
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1735
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44581/50000 (89.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.6976s / 431552.9355 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2409
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1889
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 44601/50000 (89.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.1161s / 431724.0517 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2287
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1602
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 44621/50000 (89.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.0211s / 431895.0728 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2370
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1513
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 44641/50000 (89.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 169.4824s / 432064.5552 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2416
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1901
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44661/50000 (89.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.2369s / 432235.7921 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2438
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1818
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44681/50000 (89.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3754s / 432407.1676 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2386
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1671
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44701/50000 (89.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.5186s / 432577.6861 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2366
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1619
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44721/50000 (89.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 170.3703s / 432748.0564 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2397
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1685
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 44741/50000 (89.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 171.3321s / 432919.3885 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2352
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1697
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44761/50000 (89.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.0885s / 433091.4771 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2343
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1716
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44781/50000 (89.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 174.1547s / 433265.6318 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2236
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1540
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44801/50000 (89.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 172.5891s / 433438.2209 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2238
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1575
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44821/50000 (89.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 173.2898s / 433611.5107 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2206
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1490
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 44841/50000 (89.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 174.5565s / 433786.0672 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2316
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1772
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44861/50000 (89.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.0952s / 433967.1623 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2282
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1524
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 44881/50000 (89.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.7532s / 434150.9156 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2242
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1345
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44901/50000 (89.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.2789s / 434333.1945 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2127
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1352
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 44921/50000 (89.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.2321s / 434512.4266 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2185
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1266
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44941/50000 (89.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.9364s / 434691.3630 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2259
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1410
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 44961/50000 (89.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.6891s / 434869.0521 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2375
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1598
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 44981/50000 (89.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.1226s / 435047.1747 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2316
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1437
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 45001/50000 (90.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 187.6967s / 435234.8714 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2385
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1651
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45021/50000 (90.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.8009s / 435415.6723 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2145
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1293
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 45041/50000 (90.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.3090s / 435605.9813 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2361
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1732
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45061/50000 (90.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 188.3247s / 435794.3060 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2344
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1175
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45081/50000 (90.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.7111s / 435975.0172 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2349
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1686
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 45101/50000 (90.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.2477s / 436156.2649 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2242
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1819
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 45121/50000 (90.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.5565s / 436338.8214 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2271
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1860
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45141/50000 (90.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 185.8794s / 436524.7008 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2375
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1846
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 45161/50000 (90.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.5689s / 436704.2697 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2336
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1831
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 45181/50000 (90.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.8362s / 436885.1059 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2263
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1651
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45201/50000 (90.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.5148s / 437064.6207 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2308
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1634
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45221/50000 (90.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.5397s / 437247.1604 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2271
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1556
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 45241/50000 (90.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.8363s / 437429.9966 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2271
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1057
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45261/50000 (90.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.0760s / 437613.0726 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2301
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1603
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 45281/50000 (90.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.4838s / 437791.5564 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2198
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0728
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45301/50000 (90.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 188.9401s / 437980.4964 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2457
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1405
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45321/50000 (90.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.7650s / 438175.2614 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2391
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1699
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 45341/50000 (90.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 184.4028s / 438359.6642 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2336
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1694
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45361/50000 (90.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.3700s / 438543.0342 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2413
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1749
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 45381/50000 (90.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 186.0684s / 438729.1026 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2489
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2053
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45401/50000 (90.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 184.2450s / 438913.3476 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2372
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1790
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 45421/50000 (90.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 186.4072s / 439099.7548 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2453
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2032
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45441/50000 (90.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.3835s / 439279.1383 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2445
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1976
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 45461/50000 (90.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.5567s / 439461.6950 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2410
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1704
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 45481/50000 (90.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.4418s / 439640.1368 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2368
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1784
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 45501/50000 (91.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.7729s / 439817.9097 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2409
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1762
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45521/50000 (91.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.1245s / 439998.0342 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2382
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0934
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45541/50000 (91.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.2869s / 440176.3211 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2362
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1680
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45561/50000 (91.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.5444s / 440354.8654 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2301
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1127
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 45581/50000 (91.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.0387s / 440535.9041 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2271
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1681
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45601/50000 (91.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.4743s / 440714.3784 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2240
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1581
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 45621/50000 (91.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.4376s / 440892.8160 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2286
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1552
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45641/50000 (91.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.4689s / 441073.2850 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2343
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1345
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 45661/50000 (91.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.6295s / 441251.9145 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2447
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1868
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45681/50000 (91.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.7229s / 441431.6374 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2436
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1958
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 45701/50000 (91.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.2339s / 441609.8713 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2440
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1922
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 45721/50000 (91.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.6165s / 441788.4878 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2334
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1852
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 45741/50000 (91.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 197.1785s / 441985.6663 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2345
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1469
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 45761/50000 (91.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.5304s / 442177.1967 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2357
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1728
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 45781/50000 (91.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.6508s / 442360.8475 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2317
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1648
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 45801/50000 (91.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.6024s / 442544.4498 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2493
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1910
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 45821/50000 (91.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 195.2901s / 442739.7399 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2456
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1673
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45841/50000 (91.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.1639s / 442933.9038 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2438
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1845
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45861/50000 (91.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 184.8384s / 443118.7421 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2381
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1796
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45881/50000 (91.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.5103s / 443301.2524 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2399
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1774
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45901/50000 (91.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.1340s / 443484.3864 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2405
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1662
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 45921/50000 (91.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 185.8649s / 443670.2513 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2374
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1849
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 45941/50000 (91.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.9195s / 443854.1708 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2321
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1678
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 45961/50000 (91.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.7139s / 444034.8847 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2358
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1846
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 45981/50000 (91.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.8537s / 444217.7384 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2425
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1968
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 46001/50000 (92.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 186.7532s / 444404.4916 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2449
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1957
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 46021/50000 (92.0420%),                 avg. length: 2978.45,                last time consumption/overall running time: 180.5561s / 444585.0477 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.2162
env0_second_0:                 episode reward: -1.0000,                 loss: -0.1605
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 46041/50000 (92.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.1332s / 444768.1809 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2368
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1902
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46061/50000 (92.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.0501s / 444951.2310 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2352
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1816
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 46081/50000 (92.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.4280s / 445134.6590 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2381
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1713
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 46101/50000 (92.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.4923s / 445316.1514 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2255
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1757
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 46121/50000 (92.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.7331s / 445494.8845 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2399
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1728
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46141/50000 (92.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.2897s / 445678.1742 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2342
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1597
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46161/50000 (92.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.3754s / 445857.5496 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2336
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1520
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46181/50000 (92.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.9805s / 446035.5301 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2393
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1843
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46201/50000 (92.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.7115s / 446217.2416 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2341
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1809
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46221/50000 (92.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 186.8827s / 446404.1243 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2379
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1652
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46241/50000 (92.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.6593s / 446586.7836 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2251
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1734
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 46261/50000 (92.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.2230s / 446765.0066 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2369
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1812
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 46281/50000 (92.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.0074s / 446944.0140 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2305
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1662
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 46301/50000 (92.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.0800s / 447120.0940 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2311
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1865
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46321/50000 (92.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.2753s / 447311.3693 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2350
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1600
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 46341/50000 (92.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.8911s / 447490.2604 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2376
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1728
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 46361/50000 (92.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.7870s / 447668.0474 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2383
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1491
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 46381/50000 (92.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.9683s / 447847.0157 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2384
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1732
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 46401/50000 (92.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.9458s / 448025.9615 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2372
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1857
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 46421/50000 (92.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.4723s / 448205.4338 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2328
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1564
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 46441/50000 (92.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.1408s / 448382.5746 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2420
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1479
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 46461/50000 (92.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.3660s / 448558.9407 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2324
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1544
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46481/50000 (92.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.0090s / 448739.9497 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2334
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1689
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 46501/50000 (93.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.9889s / 448917.9386 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2373
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1584
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 46521/50000 (93.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.6478s / 449095.5863 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2318
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1606
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 46541/50000 (93.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 184.7204s / 449280.3067 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2379
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1725
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 46561/50000 (93.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.3196s / 449461.6263 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2343
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1528
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 46581/50000 (93.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.0597s / 449639.6861 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2408
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1665
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46601/50000 (93.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.8934s / 449818.5794 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2217
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1617
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 46621/50000 (93.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.6036s / 449997.1830 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2234
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1677
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 46641/50000 (93.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.5937s / 450175.7767 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2345
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1980
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 46661/50000 (93.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.7571s / 450355.5338 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2268
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1747
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46681/50000 (93.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.5341s / 450537.0679 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2317
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1855
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46701/50000 (93.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.3022s / 450716.3700 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2357
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1774
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 46721/50000 (93.4420%),                 avg. length: 2870.1,                last time consumption/overall running time: 171.3887s / 450887.7587 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.2050
env0_second_0:                 episode reward: -1.1000,                 loss: -0.0574
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 46741/50000 (93.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.6392s / 451067.3980 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2277
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1737
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 46761/50000 (93.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.7957s / 451246.1937 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2457
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1844
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 46781/50000 (93.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.0718s / 451424.2655 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2386
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1710
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 46801/50000 (93.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.6534s / 451602.9188 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2424
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1386
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46821/50000 (93.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.9755s / 451780.8943 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2404
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1587
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46841/50000 (93.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.0859s / 451959.9802 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2325
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1567
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 46861/50000 (93.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.8354s / 452140.8156 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2220
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1666
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46881/50000 (93.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.8273s / 452319.6428 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2307
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1583
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46901/50000 (93.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.3787s / 452498.0215 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2356
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1827
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46921/50000 (93.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.6873s / 452675.7088 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2456
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1783
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 46941/50000 (93.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.7951s / 452854.5039 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2362
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1681
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 46961/50000 (93.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.7517s / 453034.2556 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2274
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1396
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 46981/50000 (93.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.3130s / 453214.5686 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2322
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1751
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 47001/50000 (94.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.6435s / 453396.2121 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2391
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1940
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 47021/50000 (94.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.8813s / 453579.0933 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2346
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1880
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47041/50000 (94.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 184.8142s / 453763.9076 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2410
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1920
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47061/50000 (94.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.6788s / 453945.5864 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2355
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1578
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47081/50000 (94.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.7748s / 454124.3612 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2370
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1283
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 47101/50000 (94.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.8461s / 454305.2073 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2198
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1558
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 47121/50000 (94.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.6749s / 454486.8822 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2137
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1166
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 47141/50000 (94.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.2944s / 454668.1766 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2335
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1608
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 47161/50000 (94.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.2624s / 454851.4390 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2309
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1406
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47181/50000 (94.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.5624s / 455032.0014 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2443
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1704
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47201/50000 (94.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.3209s / 455209.3223 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2427
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1736
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 47221/50000 (94.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.3047s / 455388.6270 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2375
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1643
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 47241/50000 (94.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.8380s / 455568.4650 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2345
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1689
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 47261/50000 (94.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.1993s / 455749.6644 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2372
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1755
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 47281/50000 (94.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.4445s / 455929.1088 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2330
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1615
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 47301/50000 (94.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 187.6575s / 456116.7664 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2190
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1718
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 47321/50000 (94.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.0005s / 456296.7669 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2295
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1670
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47341/50000 (94.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.4663s / 456477.2331 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2308
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1645
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 47361/50000 (94.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.5132s / 456655.7463 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2238
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1410
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 47381/50000 (94.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.5066s / 456835.2529 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2340
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1813
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47401/50000 (94.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.5854s / 457014.8382 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2391
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1433
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 47421/50000 (94.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.1944s / 457192.0326 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2119
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0981
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 47441/50000 (94.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.6367s / 457370.6693 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2357
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1585
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 47461/50000 (94.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.1264s / 457551.7957 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2286
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1578
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 47481/50000 (94.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.1545s / 457730.9502 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2240
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1593
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 47501/50000 (95.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.7843s / 457909.7345 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2303
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1601
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 47521/50000 (95.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.4006s / 458087.1351 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2444
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1693
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 47541/50000 (95.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.8542s / 458266.9894 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2394
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1697
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 47561/50000 (95.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.9624s / 458447.9517 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2364
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1699
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 47581/50000 (95.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.6696s / 458627.6214 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2354
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1895
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 47601/50000 (95.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.4841s / 458806.1055 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2387
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1785
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 47621/50000 (95.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.1833s / 458986.2888 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2379
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1777
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 47641/50000 (95.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.4070s / 459166.6958 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2426
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1759
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47661/50000 (95.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.0479s / 459345.7437 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2384
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1456
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 47681/50000 (95.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 184.2563s / 459529.9999 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2239
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1463
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 47701/50000 (95.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 188.9109s / 459718.9109 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2318
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1622
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47721/50000 (95.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 188.3839s / 459907.2947 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2260
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1464
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 47741/50000 (95.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 187.1403s / 460094.4350 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2325
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1534
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 47761/50000 (95.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 187.4832s / 460281.9182 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2346
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1546
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 47781/50000 (95.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.4798s / 460474.3980 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2097
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1678
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 47801/50000 (95.6020%),                 avg. length: 2838.15,                last time consumption/overall running time: 178.4114s / 460652.8093 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.1903
env0_second_0:                 episode reward: -0.4500,                 loss: -0.1215
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 47821/50000 (95.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.0432s / 460830.8525 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2300
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1529
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 47841/50000 (95.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.3279s / 461013.1804 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2253
env0_second_0:                 episode reward: -0.1000,                 loss: -0.0779
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 47861/50000 (95.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 186.9612s / 461200.1416 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2258
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1064
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 47881/50000 (95.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.8974s / 461381.0390 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2351
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1280
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 47901/50000 (95.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 187.6987s / 461568.7376 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2290
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1221
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 47921/50000 (95.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 196.0946s / 461764.8322 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2351
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1720
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 47941/50000 (95.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 208.7492s / 461973.5814 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2430
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1536
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 47961/50000 (95.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.9065s / 462155.4879 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2365
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1635
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 47981/50000 (95.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.7104s / 462333.1983 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2288
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1552
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48001/50000 (96.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.4711s / 462510.6694 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2292
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1434
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48021/50000 (96.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.0202s / 462688.6896 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2285
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1341
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48041/50000 (96.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.4348s / 462867.1244 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2395
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1587
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 48061/50000 (96.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.5330s / 463044.6574 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2334
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1748
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48081/50000 (96.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.1851s / 463222.8426 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2332
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1647
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48101/50000 (96.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.4993s / 463401.3419 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2365
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1756
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48121/50000 (96.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 184.4243s / 463585.7661 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2431
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1767
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48141/50000 (96.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.0785s / 463762.8446 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2302
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1809
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48161/50000 (96.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.3119s / 463941.1566 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2406
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1856
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 48181/50000 (96.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.2248s / 464118.3814 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2389
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1646
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48201/50000 (96.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.6383s / 464297.0197 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2325
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1791
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 48221/50000 (96.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.7099s / 464477.7296 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2350
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1780
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48241/50000 (96.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.2615s / 464656.9911 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2409
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1542
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48261/50000 (96.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.9884s / 464837.9795 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2385
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1654
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48281/50000 (96.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.7023s / 465021.6817 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2286
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1665
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48301/50000 (96.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.8837s / 465203.5655 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2445
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1424
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48321/50000 (96.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.0699s / 465385.6354 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2335
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1729
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48341/50000 (96.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.2716s / 465567.9070 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2450
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1830
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48361/50000 (96.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.6059s / 465747.5128 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2375
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1672
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48381/50000 (96.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.4843s / 465924.9971 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2430
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1332
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48401/50000 (96.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.9887s / 466103.9858 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2412
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1679
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48421/50000 (96.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.1605s / 466283.1463 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2452
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1565
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48441/50000 (96.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.8698s / 466461.0161 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2447
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1953
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48461/50000 (96.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 184.5925s / 466645.6085 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2429
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2016
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 48481/50000 (96.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.8785s / 466822.4870 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.2390
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1840
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48501/50000 (97.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 185.3950s / 467007.8820 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2345
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1817
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48521/50000 (97.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.1361s / 467184.0181 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2485
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1873
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48541/50000 (97.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.4854s / 467364.5035 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2402
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1732
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 48561/50000 (97.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.8374s / 467542.3409 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2371
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1631
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48581/50000 (97.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 192.7755s / 467735.1164 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2278
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1483
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 48601/50000 (97.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.8809s / 467911.9974 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2429
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1673
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48621/50000 (97.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.0065s / 468091.0039 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2418
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1624
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48641/50000 (97.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.5465s / 468270.5504 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2288
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1616
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 48661/50000 (97.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 187.0963s / 468457.6468 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2443
env0_second_0:                 episode reward: 0.0500,                 loss: -0.0916
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48681/50000 (97.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.9695s / 468635.6163 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2475
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1720
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 48701/50000 (97.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.1282s / 468817.7444 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2412
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1595
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48721/50000 (97.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 186.1228s / 469003.8672 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2369
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1741
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 48741/50000 (97.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.3526s / 469183.2198 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2421
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1474
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 48761/50000 (97.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.5510s / 469360.7708 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2347
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1389
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 48781/50000 (97.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.7339s / 469540.5046 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2419
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1679
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 48801/50000 (97.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.6654s / 469718.1700 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2480
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1829
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48821/50000 (97.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.7801s / 469895.9501 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2452
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1876
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48841/50000 (97.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.0595s / 470074.0095 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2457
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1851
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 48861/50000 (97.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.6423s / 470251.6518 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2396
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1746
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 48881/50000 (97.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.6782s / 470432.3301 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2388
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1689
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 48901/50000 (97.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.9671s / 470612.2972 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2442
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1688
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 48921/50000 (97.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.2129s / 470794.5101 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2317
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0861
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 48941/50000 (97.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 193.3501s / 470987.8601 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2355
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1405
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 48961/50000 (97.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 188.7243s / 471176.5844 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2279
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1599
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 48981/50000 (97.9620%),                 avg. length: 2753.35,                last time consumption/overall running time: 179.1765s / 471355.7609 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.1912
env0_second_0:                 episode reward: -1.1000,                 loss: -0.1315
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 49001/50000 (98.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.4419s / 471547.2028 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2328
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1500
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 49021/50000 (98.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.8284s / 471738.0312 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2242
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1351
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 49041/50000 (98.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 188.7298s / 471926.7610 s
env0_first_0:                 episode reward: -0.2500,                 loss: -0.2290
env0_second_0:                 episode reward: 0.2500,                 loss: -0.1583
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49061/50000 (98.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.0886s / 472103.8497 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2410
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1590
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 49081/50000 (98.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 175.3744s / 472279.2240 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2349
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1741
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49101/50000 (98.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.3428s / 472457.5668 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2337
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1535
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49121/50000 (98.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.7581s / 472636.3249 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2251
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1611
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 49141/50000 (98.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.6145s / 472816.9394 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2262
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1603
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49161/50000 (98.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.0516s / 472993.9910 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2437
env0_second_0:                 episode reward: 0.0000,                 loss: -0.0370
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 49181/50000 (98.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.5153s / 473171.5063 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.2356
env0_second_0:                 episode reward: 0.2000,                 loss: -0.1680
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 49201/50000 (98.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 189.3616s / 473360.8679 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2373
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1445
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49221/50000 (98.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 194.2501s / 473555.1181 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2430
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1435
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49241/50000 (98.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 185.8527s / 473740.9707 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2447
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1668
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 49261/50000 (98.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 182.9685s / 473923.9392 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2426
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1673
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49281/50000 (98.5620%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.0850s / 474107.0242 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2420
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1848
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49301/50000 (98.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.8806s / 474286.9048 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2407
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1700
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49321/50000 (98.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.5410s / 474477.4458 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2346
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1563
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49341/50000 (98.6820%),                 avg. length: 2973.0,                last time consumption/overall running time: 176.0934s / 474653.5392 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2276
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1584
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 49361/50000 (98.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 190.0059s / 474843.5451 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2326
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1725
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 49381/50000 (98.7620%),                 avg. length: 2999.0,                last time consumption/overall running time: 185.5325s / 475029.0776 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2438
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1785
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49401/50000 (98.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 185.0631s / 475214.1407 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2306
env0_second_0:                 episode reward: -0.3500,                 loss: -0.1760
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 49421/50000 (98.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.3907s / 475394.5314 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2381
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1664
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49441/50000 (98.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 181.3632s / 475575.8946 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2409
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1777
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 49461/50000 (98.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.6107s / 475755.5053 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.2426
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1642
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 49481/50000 (98.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.4502s / 475931.9554 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2457
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1813
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49501/50000 (99.0020%),                 avg. length: 2999.0,                last time consumption/overall running time: 189.8366s / 476121.7921 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2376
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1753
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49521/50000 (99.0420%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.8952s / 476300.6873 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2547
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1603
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 49541/50000 (99.0820%),                 avg. length: 2999.0,                last time consumption/overall running time: 175.4359s / 476476.1232 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2487
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1466
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49561/50000 (99.1220%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.9461s / 476654.0693 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2398
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1245
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49581/50000 (99.1620%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.3401s / 476831.4094 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2467
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1738
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49601/50000 (99.2020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.8374s / 477011.2468 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2490
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1863
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49621/50000 (99.2420%),                 avg. length: 2999.0,                last time consumption/overall running time: 185.9710s / 477197.2178 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2462
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1715
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 49641/50000 (99.2820%),                 avg. length: 2999.0,                last time consumption/overall running time: 174.3577s / 477371.5755 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2401
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1713
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 49661/50000 (99.3220%),                 avg. length: 2999.0,                last time consumption/overall running time: 180.0024s / 477551.5779 sLoad SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
Load SlimeVolley-v0 environment in type slimevolley.
Env observation space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (12,), float32) action space: Discrete(6)
/home/zihan/research/MARS/mars/rl/agents/nash_ppo.py:181: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  s,a,r,s_prime,prob_a,done_mask =    torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst).to(self.device), \
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: 0.2000,                 loss: -0.2466
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1795
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 49681/50000 (99.3620%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.3843s / 477730.9622 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2472
env0_second_0:                 episode reward: 0.0000,                 loss: -0.1960
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49701/50000 (99.4020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.5025s / 477910.4648 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2503
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1820
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 49721/50000 (99.4420%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.1446s / 478093.6094 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2414
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1860
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 49741/50000 (99.4820%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.3300s / 478270.9394 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2338
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1817
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 49761/50000 (99.5220%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.1954s / 478447.1348 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.2413
env0_second_0:                 episode reward: -0.1000,                 loss: -0.1784
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 49781/50000 (99.5620%),                 avg. length: 2984.4,                last time consumption/overall running time: 184.3926s / 478631.5274 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.2412
env0_second_0:                 episode reward: -0.3000,                 loss: -0.1856
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49801/50000 (99.6020%),                 avg. length: 2999.0,                last time consumption/overall running time: 183.5949s / 478815.1223 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2408
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1833
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 49821/50000 (99.6420%),                 avg. length: 2999.0,                last time consumption/overall running time: 177.2773s / 478992.3995 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.2483
env0_second_0:                 episode reward: -0.0500,                 loss: -0.1858
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 49841/50000 (99.6820%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.0457s / 479170.4453 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.2445
env0_second_0:                 episode reward: -0.4000,                 loss: -0.1706
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 49861/50000 (99.7220%),                 avg. length: 2999.0,                last time consumption/overall running time: 191.3090s / 479361.7543 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.2318
env0_second_0:                 episode reward: -0.2500,                 loss: -0.1752
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49881/50000 (99.7620%),                 avg. length: 2647.6,                last time consumption/overall running time: 166.6840s / 479528.4383 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.1687
env0_second_0:                 episode reward: -1.3500,                 loss: -0.1220
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 49901/50000 (99.8020%),                 avg. length: 2999.0,                last time consumption/overall running time: 179.0739s / 479707.5122 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2301
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1378
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 49921/50000 (99.8420%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.0818s / 479883.5940 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2310
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1482
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 49941/50000 (99.8820%),                 avg. length: 2999.0,                last time consumption/overall running time: 176.0006s / 480059.5946 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.2319
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1630
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 49961/50000 (99.9220%),                 avg. length: 2999.0,                last time consumption/overall running time: 186.5016s / 480246.0962 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2274
env0_second_0:                 episode reward: 0.1000,                 loss: -0.1397
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 49981/50000 (99.9620%),                 avg. length: 2999.0,                last time consumption/overall running time: 178.0442s / 480424.1404 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.2332
env0_second_0:                 episode reward: 0.0500,                 loss: -0.1265
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
