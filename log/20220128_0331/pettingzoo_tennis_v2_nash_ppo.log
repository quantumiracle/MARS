pygame 2.0.1 (SDL 2.0.14, Python 3.7.0)
Hello from the pygame community. https://www.pygame.org/contribute.html
tennis_v2 pettingzoo
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:26: UserWarning: Converting G to a CSC matrix; may take a while.
  warn("Converting G to a CSC matrix; may take a while.")
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/ecos/ecos.py:29: UserWarning: Converting A to a CSC matrix; may take a while.
  warn("Converting A to a CSC matrix; may take a while.")
random seed: [91, 69]
<SubprocVectorEnv instance>
No agent are not learnable.
Arguments:  {'env_name': 'tennis_v2', 'env_type': 'pettingzoo', 'num_envs': 2, 'ram': True, 'seed': 'random', 'algorithm': 'NashPPO', 'algorithm_spec': {'episodic_update': True, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.01, 'eps_decay': 30000, 'lambda': 0.95, 'eps_clip': 0.2, 'K_epoch': 4, 'GAE': True}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 10000, 'max_steps_per_episode': 10000, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False, 'policy': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax'}, 'value': {'hidden_dim_list': [128, 128, 128, 128], 'hidden_activation': 'ReLU', 'output_activation': False}}, 'marl_method': 'nash_ppo', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': True}}
Save models to : /home/zihan/research/MARS/data/model/20220128_0331/pettingzoo_tennis_v2_nash_ppo. 
 Save logs to: /home/zihan/research/MARS/data/log/20220128_0331/pettingzoo_tennis_v2_nash_ppo.
Episode: 1/10000 (0.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 56.2888s / 56.2888 s
env0_first_0:                 episode reward: -33.0000,                 loss: -0.0033
env0_second_0:                 episode reward: 33.0000,                 loss: -0.0051
env1_first_0:                 episode reward: -9.0000,                 loss: nan
env1_second_0:                 episode reward: 9.0000,                 loss: nan
Episode: 21/10000 (0.2100%),                 avg. length: 6597.65,                last time consumption/overall running time: 734.5071s / 790.7959 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.0484
env0_second_0:                 episode reward: 1.6500,                 loss: -0.0381
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 41/10000 (0.4100%),                 avg. length: 4977.75,                last time consumption/overall running time: 552.9652s / 1343.7611 s
env0_first_0:                 episode reward: 11.8500,                 loss: -0.0235
env0_second_0:                 episode reward: -11.8500,                 loss: -0.0075
env1_first_0:                 episode reward: 23.5500,                 loss: nan
env1_second_0:                 episode reward: -23.5500,                 loss: nan
Episode: 61/10000 (0.6100%),                 avg. length: 4425.5,                last time consumption/overall running time: 497.1235s / 1840.8847 s
env0_first_0:                 episode reward: 27.5000,                 loss: 0.0205
env0_second_0:                 episode reward: -27.5000,                 loss: 0.0117
env1_first_0:                 episode reward: 21.8000,                 loss: nan
env1_second_0:                 episode reward: -21.8000,                 loss: nan
Episode: 81/10000 (0.8100%),                 avg. length: 5117.25,                last time consumption/overall running time: 573.3923s / 2414.2770 s
env0_first_0:                 episode reward: 54.8000,                 loss: -0.0144
env0_second_0:                 episode reward: -54.8000,                 loss: -0.0312
env1_first_0:                 episode reward: 46.1500,                 loss: nan
env1_second_0:                 episode reward: -46.1500,                 loss: nan
Episode: 101/10000 (1.0100%),                 avg. length: 3693.95,                last time consumption/overall running time: 421.4289s / 2835.7059 s
env0_first_0:                 episode reward: 7.0000,                 loss: -0.1183
env0_second_0:                 episode reward: -7.0000,                 loss: -0.1166
env1_first_0:                 episode reward: 7.9000,                 loss: nan
env1_second_0:                 episode reward: -7.9000,                 loss: nan
Episode: 121/10000 (1.2100%),                 avg. length: 5694.1,                last time consumption/overall running time: 646.7775s / 3482.4834 s
env0_first_0:                 episode reward: -3.3000,                 loss: -0.0435
env0_second_0:                 episode reward: 3.3000,                 loss: -0.0544
env1_first_0:                 episode reward: 29.2000,                 loss: nan
env1_second_0:                 episode reward: -29.2000,                 loss: nan
Episode: 141/10000 (1.4100%),                 avg. length: 6642.0,                last time consumption/overall running time: 752.6484s / 4235.1318 s
env0_first_0:                 episode reward: 14.7000,                 loss: -0.0830
env0_second_0:                 episode reward: -14.7000,                 loss: -0.0807
env1_first_0:                 episode reward: 2.7500,                 loss: nan
env1_second_0:                 episode reward: -2.7500,                 loss: nan
Episode: 161/10000 (1.6100%),                 avg. length: 8258.65,                last time consumption/overall running time: 936.2597s / 5171.3915 s
env0_first_0:                 episode reward: -12.7500,                 loss: -0.0485
env0_second_0:                 episode reward: 12.7500,                 loss: -0.0467
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 181/10000 (1.8100%),                 avg. length: 7024.4,                last time consumption/overall running time: 791.3157s / 5962.7072 s
env0_first_0:                 episode reward: -10.1000,                 loss: -0.0427
env0_second_0:                 episode reward: 10.1000,                 loss: -0.0475
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 201/10000 (2.0100%),                 avg. length: 8532.2,                last time consumption/overall running time: 956.2977s / 6919.0049 s
env0_first_0:                 episode reward: -30.7500,                 loss: 0.0378
env0_second_0:                 episode reward: 30.7500,                 loss: 0.0377
env1_first_0:                 episode reward: -29.3500,                 loss: nan
env1_second_0:                 episode reward: 29.3500,                 loss: nan
Episode: 221/10000 (2.2100%),                 avg. length: 9369.85,                last time consumption/overall running time: 1054.5572s / 7973.5621 s
env0_first_0:                 episode reward: -15.1000,                 loss: 0.1060
env0_second_0:                 episode reward: 15.1000,                 loss: 0.1143
env1_first_0:                 episode reward: -13.3000,                 loss: nan
env1_second_0:                 episode reward: 13.3000,                 loss: nan
Episode: 241/10000 (2.4100%),                 avg. length: 8853.85,                last time consumption/overall running time: 1002.2531s / 8975.8152 s
env0_first_0:                 episode reward: -43.7500,                 loss: 0.0929
env0_second_0:                 episode reward: 43.7500,                 loss: 0.0894
env1_first_0:                 episode reward: -39.7000,                 loss: nan
env1_second_0:                 episode reward: 39.7000,                 loss: nan
Episode: 261/10000 (2.6100%),                 avg. length: 7288.1,                last time consumption/overall running time: 824.5326s / 9800.3478 s
env0_first_0:                 episode reward: -15.6500,                 loss: -0.0251
env0_second_0:                 episode reward: 15.6500,                 loss: -0.0214
env1_first_0:                 episode reward: -7.1000,                 loss: nan
env1_second_0:                 episode reward: 7.1000,                 loss: nan
Episode: 281/10000 (2.8100%),                 avg. length: 9721.4,                last time consumption/overall running time: 1086.1332s / 10886.4811 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.1971
env0_second_0:                 episode reward: 0.1500,                 loss: -0.1959
env1_first_0:                 episode reward: -9.9500,                 loss: nan
env1_second_0:                 episode reward: 9.9500,                 loss: nan
Episode: 301/10000 (3.0100%),                 avg. length: 9975.0,                last time consumption/overall running time: 1098.6750s / 11985.1561 s
env0_first_0:                 episode reward: -8.9000,                 loss: -0.2639
env0_second_0:                 episode reward: 8.9000,                 loss: -0.2558
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 321/10000 (3.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1119.3580s / 13104.5141 s
env0_first_0:                 episode reward: -15.0500,                 loss: -0.2333
env0_second_0:                 episode reward: 15.0500,                 loss: -0.2247
env1_first_0:                 episode reward: -16.0000,                 loss: nan
env1_second_0:                 episode reward: 16.0000,                 loss: nan
Episode: 341/10000 (3.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.3232s / 14211.8373 s
env0_first_0:                 episode reward: -16.1500,                 loss: -0.2321
env0_second_0:                 episode reward: 16.1500,                 loss: -0.2244
env1_first_0:                 episode reward: -10.9500,                 loss: nan
env1_second_0:                 episode reward: 10.9500,                 loss: nan
Episode: 361/10000 (3.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1089.2162s / 15301.0535 s
env0_first_0:                 episode reward: -20.6000,                 loss: -0.2261
env0_second_0:                 episode reward: 20.6000,                 loss: -0.2230
env1_first_0:                 episode reward: -20.9000,                 loss: nan
env1_second_0:                 episode reward: 20.9000,                 loss: nan
Episode: 381/10000 (3.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1104.8998s / 16405.9533 s
env0_first_0:                 episode reward: -10.6000,                 loss: -0.2732
env0_second_0:                 episode reward: 10.6000,                 loss: -0.2620
env1_first_0:                 episode reward: -10.1500,                 loss: nan
env1_second_0:                 episode reward: 10.1500,                 loss: nan
Episode: 401/10000 (4.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1089.1146s / 17495.0680 s
env0_first_0:                 episode reward: -15.5000,                 loss: -0.2798
env0_second_0:                 episode reward: 15.5000,                 loss: -0.2736
env1_first_0:                 episode reward: -10.8500,                 loss: nan
env1_second_0:                 episode reward: 10.8500,                 loss: nan
Episode: 421/10000 (4.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.8257s / 18587.8937 s
env0_first_0:                 episode reward: -16.2500,                 loss: -0.2784
env0_second_0:                 episode reward: 16.2500,                 loss: -0.2721
env1_first_0:                 episode reward: -10.0000,                 loss: nan
env1_second_0:                 episode reward: 10.0000,                 loss: nan
Episode: 441/10000 (4.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1083.2362s / 19671.1298 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.2993
env0_second_0:                 episode reward: 9.0500,                 loss: -0.2926
env1_first_0:                 episode reward: -7.2000,                 loss: nan
env1_second_0:                 episode reward: 7.2000,                 loss: nan
Episode: 461/10000 (4.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1094.3307s / 20765.4606 s
env0_first_0:                 episode reward: -6.7000,                 loss: -0.3003
env0_second_0:                 episode reward: 6.7000,                 loss: -0.2929
env1_first_0:                 episode reward: -7.8000,                 loss: nan
env1_second_0:                 episode reward: 7.8000,                 loss: nan
Episode: 481/10000 (4.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1090.0359s / 21855.4965 s
env0_first_0:                 episode reward: -12.1500,                 loss: -0.2855
env0_second_0:                 episode reward: 12.1500,                 loss: -0.2762
env1_first_0:                 episode reward: -10.3000,                 loss: nan
env1_second_0:                 episode reward: 10.3000,                 loss: nan
Episode: 501/10000 (5.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1089.8073s / 22945.3038 s
env0_first_0:                 episode reward: -8.6000,                 loss: -0.2891
env0_second_0:                 episode reward: 8.6000,                 loss: -0.2818
env1_first_0:                 episode reward: -7.5500,                 loss: nan
env1_second_0:                 episode reward: 7.5500,                 loss: nan
Episode: 521/10000 (5.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1079.0739s / 24024.3777 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.2845
env0_second_0:                 episode reward: 4.7000,                 loss: -0.2811
env1_first_0:                 episode reward: -2.8000,                 loss: nan
env1_second_0:                 episode reward: 2.8000,                 loss: nan
Episode: 541/10000 (5.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1077.7039s / 25102.0816 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.3205
env0_second_0:                 episode reward: 2.9000,                 loss: -0.3131
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 561/10000 (5.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1093.6172s / 26195.6989 s
env0_first_0:                 episode reward: -9.2000,                 loss: -0.2606
env0_second_0:                 episode reward: 9.2000,                 loss: -0.2485
env1_first_0:                 episode reward: -17.7000,                 loss: nan
env1_second_0:                 episode reward: 17.7000,                 loss: nan
Episode: 581/10000 (5.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1087.5520s / 27283.2509 s
env0_first_0:                 episode reward: -10.9000,                 loss: -0.2916
env0_second_0:                 episode reward: 10.9000,                 loss: -0.2804
env1_first_0:                 episode reward: -16.8000,                 loss: nan
env1_second_0:                 episode reward: 16.8000,                 loss: nan
Episode: 601/10000 (6.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1085.4312s / 28368.6821 s
env0_first_0:                 episode reward: -8.6500,                 loss: -0.3051
env0_second_0:                 episode reward: 8.6500,                 loss: -0.2957
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 621/10000 (6.2100%),                 avg. length: 9912.55,                last time consumption/overall running time: 1075.5162s / 29444.1982 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.2090
env0_second_0:                 episode reward: -6.5500,                 loss: -0.2029
env1_first_0:                 episode reward: 15.4500,                 loss: nan
env1_second_0:                 episode reward: -15.4500,                 loss: nan
Episode: 641/10000 (6.4100%),                 avg. length: 9931.8,                last time consumption/overall running time: 1085.9299s / 30530.1281 s
env0_first_0:                 episode reward: 24.0500,                 loss: -0.2442
env0_second_0:                 episode reward: -24.0500,                 loss: -0.2327
env1_first_0:                 episode reward: 7.5500,                 loss: nan
env1_second_0:                 episode reward: -7.5500,                 loss: nan
Episode: 661/10000 (6.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1084.5263s / 31614.6544 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.2791
env0_second_0:                 episode reward: 2.3500,                 loss: -0.2719
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 681/10000 (6.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1088.9697s / 32703.6241 s
env0_first_0:                 episode reward: -8.3500,                 loss: -0.2477
env0_second_0:                 episode reward: 8.3500,                 loss: -0.2349
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 701/10000 (7.0100%),                 avg. length: 9793.1,                last time consumption/overall running time: 1074.3454s / 33777.9695 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.2563
env0_second_0:                 episode reward: 2.1500,                 loss: -0.2504
env1_first_0:                 episode reward: -7.1500,                 loss: nan
env1_second_0:                 episode reward: 7.1500,                 loss: nan
Episode: 721/10000 (7.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1086.6102s / 34864.5797 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.1861
env0_second_0:                 episode reward: 7.5000,                 loss: -0.1792
env1_first_0:                 episode reward: -14.9500,                 loss: nan
env1_second_0:                 episode reward: 14.9500,                 loss: nan
Episode: 741/10000 (7.4100%),                 avg. length: 9811.7,                last time consumption/overall running time: 1057.8223s / 35922.4020 s
env0_first_0:                 episode reward: 7.3500,                 loss: -0.2733
env0_second_0:                 episode reward: -7.3500,                 loss: -0.2696
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 761/10000 (7.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1060.3313s / 36982.7333 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.2280
env0_second_0:                 episode reward: -4.8500,                 loss: -0.2205
env1_first_0:                 episode reward: -16.5000,                 loss: nan
env1_second_0:                 episode reward: 16.5000,                 loss: nan
Episode: 781/10000 (7.8100%),                 avg. length: 9917.8,                last time consumption/overall running time: 1066.9097s / 38049.6430 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.2795
env0_second_0:                 episode reward: 2.0000,                 loss: -0.2728
env1_first_0:                 episode reward: -2.5500,                 loss: nan
env1_second_0:                 episode reward: 2.5500,                 loss: nan
Episode: 801/10000 (8.0100%),                 avg. length: 9930.95,                last time consumption/overall running time: 1080.1325s / 39129.7755 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2746
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2629
env1_first_0:                 episode reward: 8.9000,                 loss: nan
env1_second_0:                 episode reward: -8.9000,                 loss: nan
Episode: 821/10000 (8.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1093.5024s / 40223.2779 s
env0_first_0:                 episode reward: -5.5000,                 loss: -0.2372
env0_second_0:                 episode reward: 5.5000,                 loss: -0.2301
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 841/10000 (8.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1078.7514s / 41302.0293 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.2535
env0_second_0:                 episode reward: 0.1000,                 loss: -0.2430
env1_first_0:                 episode reward: -2.1500,                 loss: nan
env1_second_0:                 episode reward: 2.1500,                 loss: nan
Episode: 861/10000 (8.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.2014s / 42394.2308 s
env0_first_0:                 episode reward: -6.5500,                 loss: -0.2492
env0_second_0:                 episode reward: 6.5500,                 loss: -0.2366
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 881/10000 (8.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1086.4044s / 43480.6352 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.2678
env0_second_0:                 episode reward: 1.6000,                 loss: -0.2574
env1_first_0:                 episode reward: 1.2500,                 loss: nan
env1_second_0:                 episode reward: -1.2500,                 loss: nan
Episode: 901/10000 (9.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1088.5913s / 44569.2265 s
env0_first_0:                 episode reward: -14.6500,                 loss: -0.2144
env0_second_0:                 episode reward: 14.6500,                 loss: -0.1961
env1_first_0:                 episode reward: -26.4500,                 loss: nan
env1_second_0:                 episode reward: 26.4500,                 loss: nan
Episode: 921/10000 (9.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1093.9117s / 45663.1382 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.2299
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2180
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 941/10000 (9.4100%),                 avg. length: 9790.7,                last time consumption/overall running time: 1056.5400s / 46719.6783 s
env0_first_0:                 episode reward: 11.8500,                 loss: -0.2396
env0_second_0:                 episode reward: -11.8500,                 loss: -0.2243
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 961/10000 (9.6100%),                 avg. length: 9977.6,                last time consumption/overall running time: 1079.0921s / 47798.7703 s
env0_first_0:                 episode reward: 6.6000,                 loss: -0.2232
env0_second_0:                 episode reward: -6.6000,                 loss: -0.2048
env1_first_0:                 episode reward: 9.2500,                 loss: nan
env1_second_0:                 episode reward: -9.2500,                 loss: nan
Episode: 981/10000 (9.8100%),                 avg. length: 9930.35,                last time consumption/overall running time: 1068.8453s / 48867.6157 s
env0_first_0:                 episode reward: 3.8000,                 loss: -0.2441
env0_second_0:                 episode reward: -3.8000,                 loss: -0.2321
env1_first_0:                 episode reward: 3.0500,                 loss: nan
env1_second_0:                 episode reward: -3.0500,                 loss: nan
Episode: 1001/10000 (10.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1082.8230s / 49950.4387 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.2389
env0_second_0:                 episode reward: 1.0000,                 loss: -0.2204
env1_first_0:                 episode reward: -15.7000,                 loss: nan
env1_second_0:                 episode reward: 15.7000,                 loss: nan
Episode: 1021/10000 (10.2100%),                 avg. length: 9839.0,                last time consumption/overall running time: 1062.1355s / 51012.5743 s
env0_first_0:                 episode reward: -27.2000,                 loss: -0.2236
env0_second_0:                 episode reward: 27.2000,                 loss: -0.2127
env1_first_0:                 episode reward: 3.3500,                 loss: nan
env1_second_0:                 episode reward: -3.3500,                 loss: nan
Episode: 1041/10000 (10.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1074.6044s / 52087.1787 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.2373
env0_second_0:                 episode reward: 7.5000,                 loss: -0.2272
env1_first_0:                 episode reward: -12.8000,                 loss: nan
env1_second_0:                 episode reward: 12.8000,                 loss: nan
Episode: 1061/10000 (10.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1079.9816s / 53167.1602 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.2097
env0_second_0:                 episode reward: 1.8000,                 loss: -0.2015
env1_first_0:                 episode reward: -3.5500,                 loss: nan
env1_second_0:                 episode reward: 3.5500,                 loss: nan
Episode: 1081/10000 (10.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1056.4254s / 54223.5856 s
env0_first_0:                 episode reward: -3.9500,                 loss: -0.2542
env0_second_0:                 episode reward: 3.9500,                 loss: -0.2459
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 1101/10000 (11.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1068.9979s / 55292.5835 s
env0_first_0:                 episode reward: -2.7000,                 loss: -0.2481
env0_second_0:                 episode reward: 2.7000,                 loss: -0.2318
env1_first_0:                 episode reward: -4.2000,                 loss: nan
env1_second_0:                 episode reward: 4.2000,                 loss: nan
Episode: 1121/10000 (11.2100%),                 avg. length: 9952.7,                last time consumption/overall running time: 1078.9635s / 56371.5470 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.2206
env0_second_0:                 episode reward: -6.5000,                 loss: -0.2066
env1_first_0:                 episode reward: 3.8000,                 loss: nan
env1_second_0:                 episode reward: -3.8000,                 loss: nan
Episode: 1141/10000 (11.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1061.9875s / 57433.5346 s
env0_first_0:                 episode reward: -15.6500,                 loss: -0.2453
env0_second_0:                 episode reward: 15.6500,                 loss: -0.2337
env1_first_0:                 episode reward: -11.9500,                 loss: nan
env1_second_0:                 episode reward: 11.9500,                 loss: nan
Episode: 1161/10000 (11.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1075.7201s / 58509.2546 s
env0_first_0:                 episode reward: -10.3500,                 loss: -0.2295
env0_second_0:                 episode reward: 10.3500,                 loss: -0.2151
env1_first_0:                 episode reward: -3.5000,                 loss: nan
env1_second_0:                 episode reward: 3.5000,                 loss: nan
Episode: 1181/10000 (11.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1090.4196s / 59599.6742 s
env0_first_0:                 episode reward: -4.9500,                 loss: -0.2486
env0_second_0:                 episode reward: 4.9500,                 loss: -0.2334
env1_first_0:                 episode reward: 8.2500,                 loss: nan
env1_second_0:                 episode reward: -8.2500,                 loss: nan
Episode: 1201/10000 (12.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1078.2668s / 60677.9410 s
env0_first_0:                 episode reward: -7.0000,                 loss: -0.2455
env0_second_0:                 episode reward: 7.0000,                 loss: -0.2309
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 1221/10000 (12.2100%),                 avg. length: 9889.9,                last time consumption/overall running time: 1052.3937s / 61730.3347 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.2307
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2122
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 1241/10000 (12.4100%),                 avg. length: 9596.7,                last time consumption/overall running time: 1018.9531s / 62749.2878 s
env0_first_0:                 episode reward: 9.6000,                 loss: -0.2340
env0_second_0:                 episode reward: -9.6000,                 loss: -0.2149
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 1261/10000 (12.6100%),                 avg. length: 9568.25,                last time consumption/overall running time: 1006.8087s / 63756.0965 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.2437
env0_second_0:                 episode reward: -5.7000,                 loss: -0.2251
env1_first_0:                 episode reward: 5.8500,                 loss: nan
env1_second_0:                 episode reward: -5.8500,                 loss: nan
Episode: 1281/10000 (12.8100%),                 avg. length: 9453.8,                last time consumption/overall running time: 1012.5298s / 64768.6264 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.2194
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2013
env1_first_0:                 episode reward: -6.0000,                 loss: nan
env1_second_0:                 episode reward: 6.0000,                 loss: nan
Episode: 1301/10000 (13.0100%),                 avg. length: 9514.55,                last time consumption/overall running time: 1009.9675s / 65778.5939 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.2286
env0_second_0:                 episode reward: 2.4000,                 loss: -0.2130
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 1321/10000 (13.2100%),                 avg. length: 9771.85,                last time consumption/overall running time: 1034.2190s / 66812.8128 s
env0_first_0:                 episode reward: -7.3000,                 loss: -0.2280
env0_second_0:                 episode reward: 7.3000,                 loss: -0.2080
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 1341/10000 (13.4100%),                 avg. length: 9909.1,                last time consumption/overall running time: 1033.6883s / 67846.5012 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.2462
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2328
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 1361/10000 (13.6100%),                 avg. length: 9787.9,                last time consumption/overall running time: 1019.7626s / 68866.2638 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.2508
env0_second_0:                 episode reward: 2.9000,                 loss: -0.2358
env1_first_0:                 episode reward: 8.3500,                 loss: nan
env1_second_0:                 episode reward: -8.3500,                 loss: nan
Episode: 1381/10000 (13.8100%),                 avg. length: 9853.85,                last time consumption/overall running time: 1022.2655s / 69888.5293 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.2354
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2215
env1_first_0:                 episode reward: -12.5500,                 loss: nan
env1_second_0:                 episode reward: 12.5500,                 loss: nan
Episode: 1401/10000 (14.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1035.5866s / 70924.1159 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.2691
env0_second_0:                 episode reward: 1.8000,                 loss: -0.2535
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 1421/10000 (14.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1029.7243s / 71953.8402 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.2233
env0_second_0:                 episode reward: 2.6500,                 loss: -0.2016
env1_first_0:                 episode reward: -14.7500,                 loss: nan
env1_second_0:                 episode reward: 14.7500,                 loss: nan
Episode: 1441/10000 (14.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.2134s / 72990.0536 s
env0_first_0:                 episode reward: -15.0000,                 loss: -0.2073
env0_second_0:                 episode reward: 15.0000,                 loss: -0.1807
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 1461/10000 (14.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.5751s / 74037.6287 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2728
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2526
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 1481/10000 (14.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.6803s / 75092.3090 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.2783
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2580
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1501/10000 (15.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.9745s / 76138.2835 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.2811
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2638
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 1521/10000 (15.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.9321s / 77176.2155 s
env0_first_0:                 episode reward: 4.2000,                 loss: -0.2623
env0_second_0:                 episode reward: -4.2000,                 loss: -0.2433
env1_first_0:                 episode reward: 5.3000,                 loss: nan
env1_second_0:                 episode reward: -5.3000,                 loss: nan
Episode: 1541/10000 (15.4100%),                 avg. length: 9825.2,                last time consumption/overall running time: 1010.0469s / 78186.2624 s
env0_first_0:                 episode reward: 6.7500,                 loss: -0.2285
env0_second_0:                 episode reward: -6.7500,                 loss: -0.2099
env1_first_0:                 episode reward: 8.2000,                 loss: nan
env1_second_0:                 episode reward: -8.2000,                 loss: nan
Episode: 1561/10000 (15.6100%),                 avg. length: 9998.05,                last time consumption/overall running time: 1048.1758s / 79234.4382 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.2717
env0_second_0:                 episode reward: -3.8500,                 loss: -0.2515
env1_first_0:                 episode reward: 8.4500,                 loss: nan
env1_second_0:                 episode reward: -8.4500,                 loss: nan
Episode: 1581/10000 (15.8100%),                 avg. length: 9949.05,                last time consumption/overall running time: 1041.0703s / 80275.5084 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.2768
env0_second_0:                 episode reward: -3.3000,                 loss: -0.2564
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1601/10000 (16.0100%),                 avg. length: 9909.05,                last time consumption/overall running time: 1036.0053s / 81311.5137 s
env0_first_0:                 episode reward: 5.3000,                 loss: -0.2790
env0_second_0:                 episode reward: -5.3000,                 loss: -0.2618
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 1621/10000 (16.2100%),                 avg. length: 9907.8,                last time consumption/overall running time: 1041.1709s / 82352.6846 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.2630
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2438
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 1641/10000 (16.4100%),                 avg. length: 9993.1,                last time consumption/overall running time: 1040.0945s / 83392.7792 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.2613
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2382
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 1661/10000 (16.6100%),                 avg. length: 9824.9,                last time consumption/overall running time: 1015.7827s / 84408.5619 s
env0_first_0:                 episode reward: 3.1000,                 loss: -0.2596
env0_second_0:                 episode reward: -3.1000,                 loss: -0.2379
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 1681/10000 (16.8100%),                 avg. length: 9873.85,                last time consumption/overall running time: 1037.4227s / 85445.9846 s
env0_first_0:                 episode reward: 0.4500,                 loss: -0.2589
env0_second_0:                 episode reward: -0.4500,                 loss: -0.2361
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1701/10000 (17.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.1224s / 86491.1070 s
env0_first_0:                 episode reward: 9.1500,                 loss: -0.2485
env0_second_0:                 episode reward: -9.1500,                 loss: -0.2248
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 1721/10000 (17.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.3662s / 87530.4732 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.2537
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2257
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 1741/10000 (17.4100%),                 avg. length: 9642.3,                last time consumption/overall running time: 993.1191s / 88523.5923 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.2510
env0_second_0:                 episode reward: -0.8000,                 loss: -0.2291
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 1761/10000 (17.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1040.9457s / 89564.5380 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.2748
env0_second_0:                 episode reward: 4.0000,                 loss: -0.2509
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 1781/10000 (17.8100%),                 avg. length: 9978.35,                last time consumption/overall running time: 1051.9715s / 90616.5095 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.2630
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2379
env1_first_0:                 episode reward: -11.9000,                 loss: nan
env1_second_0:                 episode reward: 11.9000,                 loss: nan
Episode: 1801/10000 (18.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.0778s / 91666.5873 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.2785
env0_second_0:                 episode reward: 2.6000,                 loss: -0.2579
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 1821/10000 (18.2100%),                 avg. length: 9879.4,                last time consumption/overall running time: 1032.2149s / 92698.8022 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2543
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2233
env1_first_0:                 episode reward: -5.9500,                 loss: nan
env1_second_0:                 episode reward: 5.9500,                 loss: nan
Episode: 1841/10000 (18.4100%),                 avg. length: 9966.25,                last time consumption/overall running time: 1025.8660s / 93724.6682 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.2775
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2510
env1_first_0:                 episode reward: -0.6000,                 loss: nan
env1_second_0:                 episode reward: 0.6000,                 loss: nan
Episode: 1861/10000 (18.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.2634s / 94777.9317 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.2866
env0_second_0:                 episode reward: 3.5500,                 loss: -0.2659
env1_first_0:                 episode reward: -4.5500,                 loss: nan
env1_second_0:                 episode reward: 4.5500,                 loss: nan
Episode: 1881/10000 (18.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.2113s / 95828.1430 s
env0_first_0:                 episode reward: -9.3000,                 loss: -0.2807
env0_second_0:                 episode reward: 9.3000,                 loss: -0.2585
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 1901/10000 (19.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.4383s / 96885.5813 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3156
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2965
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 1921/10000 (19.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.0527s / 97936.6340 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2789
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2570
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 1941/10000 (19.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1055.7703s / 98992.4044 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.2962
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2786
env1_first_0:                 episode reward: -5.3500,                 loss: nan
env1_second_0:                 episode reward: 5.3500,                 loss: nan
Episode: 1961/10000 (19.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.4714s / 100029.8757 s
env0_first_0:                 episode reward: -2.2000,                 loss: -0.3351
env0_second_0:                 episode reward: 2.2000,                 loss: -0.3220
env1_first_0:                 episode reward: -1.2000,                 loss: nan
env1_second_0:                 episode reward: 1.2000,                 loss: nan
Episode: 1981/10000 (19.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.9557s / 101077.8314 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3223
env0_second_0:                 episode reward: -0.1000,                 loss: -0.3075
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2001/10000 (20.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1052.2089s / 102130.0403 s
env0_first_0:                 episode reward: -1.5500,                 loss: -0.3123
env0_second_0:                 episode reward: 1.5500,                 loss: -0.2883
env1_first_0:                 episode reward: -3.8000,                 loss: nan
env1_second_0:                 episode reward: 3.8000,                 loss: nan
Episode: 2021/10000 (20.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.9048s / 103169.9451 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.2903
env0_second_0:                 episode reward: -3.9000,                 loss: -0.2696
env1_first_0:                 episode reward: -20.8500,                 loss: nan
env1_second_0:                 episode reward: 20.8500,                 loss: nan
Episode: 2041/10000 (20.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1052.4100s / 104222.3551 s
env0_first_0:                 episode reward: -10.3000,                 loss: -0.2695
env0_second_0:                 episode reward: 10.3000,                 loss: -0.2500
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 2061/10000 (20.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1059.3682s / 105281.7233 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.2930
env0_second_0:                 episode reward: 1.2500,                 loss: -0.2783
env1_first_0:                 episode reward: -5.1000,                 loss: nan
env1_second_0:                 episode reward: 5.1000,                 loss: nan
Episode: 2081/10000 (20.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.7859s / 106345.5092 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3036
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2878
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2101/10000 (21.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1056.2960s / 107401.8052 s
env0_first_0:                 episode reward: 2.1000,                 loss: -0.3239
env0_second_0:                 episode reward: -2.1000,                 loss: -0.3107
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2121/10000 (21.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.1044s / 108464.9096 s
env0_first_0:                 episode reward: 24.3500,                 loss: -0.1843
env0_second_0:                 episode reward: -24.3500,                 loss: -0.1532
env1_first_0:                 episode reward: 9.9000,                 loss: nan
env1_second_0:                 episode reward: -9.9000,                 loss: nan
Episode: 2141/10000 (21.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1023.3391s / 109488.2487 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.2955
env0_second_0:                 episode reward: -2.9000,                 loss: -0.2751
env1_first_0:                 episode reward: 7.8000,                 loss: nan
env1_second_0:                 episode reward: -7.8000,                 loss: nan
Episode: 2161/10000 (21.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1040.5897s / 110528.8384 s
env0_first_0:                 episode reward: -6.6000,                 loss: -0.2507
env0_second_0:                 episode reward: 6.6000,                 loss: -0.2293
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 2181/10000 (21.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.1217s / 111591.9600 s
env0_first_0:                 episode reward: -5.1000,                 loss: -0.2712
env0_second_0:                 episode reward: 5.1000,                 loss: -0.2455
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 2201/10000 (22.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.8615s / 112629.8215 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.2668
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2429
env1_first_0:                 episode reward: 8.0000,                 loss: nan
env1_second_0:                 episode reward: -8.0000,                 loss: nan
Episode: 2221/10000 (22.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.3281s / 113667.1497 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.3028
env0_second_0:                 episode reward: 2.1500,                 loss: -0.2815
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 2241/10000 (22.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1071.4489s / 114738.5986 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3161
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2943
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 2261/10000 (22.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1041.8601s / 115780.4587 s
env0_first_0:                 episode reward: 4.9500,                 loss: -0.3083
env0_second_0:                 episode reward: -4.9500,                 loss: -0.2864
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 2281/10000 (22.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1066.2659s / 116846.7247 s
env0_first_0:                 episode reward: -8.3000,                 loss: -0.3125
env0_second_0:                 episode reward: 8.3000,                 loss: -0.2905
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 2301/10000 (23.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1055.0077s / 117901.7324 s
env0_first_0:                 episode reward: -6.0500,                 loss: -0.3164
env0_second_0:                 episode reward: 6.0500,                 loss: -0.2916
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 2321/10000 (23.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1034.5141s / 118936.2465 s
env0_first_0:                 episode reward: 3.9500,                 loss: -0.3128
env0_second_0:                 episode reward: -3.9500,                 loss: -0.2855
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2341/10000 (23.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1044.5693s / 119980.8157 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3101
env0_second_0:                 episode reward: -2.3000,                 loss: -0.2870
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2361/10000 (23.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1046.1445s / 121026.9602 s
env0_first_0:                 episode reward: 1.7500,                 loss: -0.3110
env0_second_0:                 episode reward: -1.7500,                 loss: -0.2855
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 2381/10000 (23.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1031.9633s / 122058.9235 s
env0_first_0:                 episode reward: -4.1500,                 loss: -0.2859
env0_second_0:                 episode reward: 4.1500,                 loss: -0.2587
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 2401/10000 (24.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.9935s / 123104.9170 s
env0_first_0:                 episode reward: 7.1000,                 loss: -0.2672
env0_second_0:                 episode reward: -7.1000,                 loss: -0.2364
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 2421/10000 (24.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1043.7206s / 124148.6376 s
env0_first_0:                 episode reward: 4.9000,                 loss: -0.2772
env0_second_0:                 episode reward: -4.9000,                 loss: -0.2493
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 2441/10000 (24.4100%),                 avg. length: 9959.7,                last time consumption/overall running time: 1045.3861s / 125194.0237 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.2762
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2466
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2461/10000 (24.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1061.0889s / 126255.1126 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3074
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2875
env1_first_0:                 episode reward: 5.4500,                 loss: nan
env1_second_0:                 episode reward: -5.4500,                 loss: nan
Episode: 2481/10000 (24.8100%),                 avg. length: 9791.2,                last time consumption/overall running time: 1011.9752s / 127267.0879 s
env0_first_0:                 episode reward: 9.3000,                 loss: -0.2628
env0_second_0:                 episode reward: -9.3000,                 loss: -0.2302
env1_first_0:                 episode reward: 9.0000,                 loss: nan
env1_second_0:                 episode reward: -9.0000,                 loss: nan
Episode: 2501/10000 (25.0100%),                 avg. length: 9961.75,                last time consumption/overall running time: 1039.7852s / 128306.8731 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3410
env0_second_0:                 episode reward: -1.2000,                 loss: -0.3120
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 2521/10000 (25.2100%),                 avg. length: 9751.8,                last time consumption/overall running time: 1021.6411s / 129328.5142 s
env0_first_0:                 episode reward: 2.6500,                 loss: -0.3109
env0_second_0:                 episode reward: -2.6500,                 loss: -0.2923
env1_first_0:                 episode reward: 3.4500,                 loss: nan
env1_second_0:                 episode reward: -3.4500,                 loss: nan
Episode: 2541/10000 (25.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.9613s / 130371.4755 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3265
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3038
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 2561/10000 (25.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1040.5240s / 131411.9995 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.3255
env0_second_0:                 episode reward: 3.5500,                 loss: -0.3017
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2581/10000 (25.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.8534s / 132463.8528 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.2995
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2715
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 2601/10000 (26.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.0159s / 133508.8687 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.3489
env0_second_0:                 episode reward: -3.0000,                 loss: -0.3256
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2621/10000 (26.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.3836s / 134551.2524 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3235
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2888
env1_first_0:                 episode reward: -1.5500,                 loss: nan
env1_second_0:                 episode reward: 1.5500,                 loss: nan
Episode: 2641/10000 (26.4100%),                 avg. length: 9909.95,                last time consumption/overall running time: 1038.0932s / 135589.3455 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.3025
env0_second_0:                 episode reward: 3.8500,                 loss: -0.2633
env1_first_0:                 episode reward: -5.2000,                 loss: nan
env1_second_0:                 episode reward: 5.2000,                 loss: nan
Episode: 2661/10000 (26.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1043.1608s / 136632.5063 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.3221
env0_second_0:                 episode reward: 1.2500,                 loss: -0.2859
env1_first_0:                 episode reward: -6.0500,                 loss: nan
env1_second_0:                 episode reward: 6.0500,                 loss: nan
Episode: 2681/10000 (26.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.0130s / 137671.5193 s
env0_first_0:                 episode reward: -10.4500,                 loss: -0.2841
env0_second_0:                 episode reward: 10.4500,                 loss: -0.2397
env1_first_0:                 episode reward: -4.2500,                 loss: nan
env1_second_0:                 episode reward: 4.2500,                 loss: nan
Episode: 2701/10000 (27.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.2136s / 138721.7329 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3212
env0_second_0:                 episode reward: -0.5500,                 loss: -0.2869
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 2721/10000 (27.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.2247s / 139760.9576 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3158
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2828
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 2741/10000 (27.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.9838s / 140785.9413 s
env0_first_0:                 episode reward: 1.9000,                 loss: -0.3098
env0_second_0:                 episode reward: -1.9000,                 loss: -0.2739
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 2761/10000 (27.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.1070s / 141816.0484 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3269
env0_second_0:                 episode reward: 0.3000,                 loss: -0.3035
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 2781/10000 (27.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1028.7321s / 142844.7804 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3187
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2902
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 2801/10000 (28.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1040.1980s / 143884.9784 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.3022
env0_second_0:                 episode reward: 1.9000,                 loss: -0.2676
env1_first_0:                 episode reward: -7.7500,                 loss: nan
env1_second_0:                 episode reward: 7.7500,                 loss: nan
Episode: 2821/10000 (28.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.6878s / 144915.6662 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3125
env0_second_0:                 episode reward: -1.4000,                 loss: -0.2802
env1_first_0:                 episode reward: -4.8500,                 loss: nan
env1_second_0:                 episode reward: 4.8500,                 loss: nan
Episode: 2841/10000 (28.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1019.4538s / 145935.1200 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3289
env0_second_0:                 episode reward: -0.7500,                 loss: -0.3057
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 2861/10000 (28.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.1568s / 146967.2768 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3245
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2910
env1_first_0:                 episode reward: -4.6000,                 loss: nan
env1_second_0:                 episode reward: 4.6000,                 loss: nan
Episode: 2881/10000 (28.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.4077s / 147999.6845 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3400
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3147
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 2901/10000 (29.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.2777s / 149036.9622 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3532
env0_second_0:                 episode reward: -2.4000,                 loss: -0.3248
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 2921/10000 (29.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1026.3563s / 150063.3185 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3348
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3035
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 2941/10000 (29.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.6901s / 151100.0086 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3464
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3203
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 2961/10000 (29.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1056.0584s / 152156.0670 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3447
env0_second_0:                 episode reward: -2.3500,                 loss: -0.3207
env1_first_0:                 episode reward: 1.9000,                 loss: nan
env1_second_0:                 episode reward: -1.9000,                 loss: nan
Episode: 2981/10000 (29.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.2362s / 153210.3032 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3329
env0_second_0:                 episode reward: 0.0000,                 loss: -0.3039
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 3001/10000 (30.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.9066s / 154256.2097 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3337
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3146
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 3021/10000 (30.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.6832s / 155303.8929 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3369
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3125
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3041/10000 (30.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.9425s / 156355.8354 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3380
env0_second_0:                 episode reward: -2.8500,                 loss: -0.3132
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 3061/10000 (30.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.3284s / 157392.1639 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.3023
env0_second_0:                 episode reward: 6.3000,                 loss: -0.2793
env1_first_0:                 episode reward: 4.5500,                 loss: nan
env1_second_0:                 episode reward: -4.5500,                 loss: nan
Episode: 3081/10000 (30.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1055.7486s / 158447.9124 s
env0_first_0:                 episode reward: 1.2500,                 loss: -0.3448
env0_second_0:                 episode reward: -1.2500,                 loss: -0.3129
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3101/10000 (31.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1079.0978s / 159527.0102 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3372
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3020
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3121/10000 (31.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.7834s / 160569.7936 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3391
env0_second_0:                 episode reward: -2.0000,                 loss: -0.3073
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 3141/10000 (31.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.5943s / 161592.3879 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3398
env0_second_0:                 episode reward: 0.0500,                 loss: -0.3104
env1_first_0:                 episode reward: 0.0000,                 loss: nan
env1_second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 3161/10000 (31.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.8992s / 162620.2871 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3348
env0_second_0:                 episode reward: 0.1500,                 loss: -0.3045
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 3181/10000 (31.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1016.9681s / 163637.2552 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.3195
env0_second_0:                 episode reward: 2.8500,                 loss: -0.2760
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3201/10000 (32.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1048.6378s / 164685.8930 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3113
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2739
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 3221/10000 (32.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1080.8336s / 165766.7266 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3135
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2819
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 3241/10000 (32.4100%),                 avg. length: 9923.35,                last time consumption/overall running time: 1032.5771s / 166799.3037 s
env0_first_0:                 episode reward: -10.9500,                 loss: -0.2962
env0_second_0:                 episode reward: 10.9500,                 loss: -0.2643
env1_first_0:                 episode reward: -4.7500,                 loss: nan
env1_second_0:                 episode reward: 4.7500,                 loss: nan
Episode: 3261/10000 (32.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1044.7009s / 167844.0046 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.3333
env0_second_0:                 episode reward: 1.2500,                 loss: -0.3061
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 3281/10000 (32.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1028.2310s / 168872.2356 s
env0_first_0:                 episode reward: -12.0000,                 loss: -0.3036
env0_second_0:                 episode reward: 12.0000,                 loss: -0.2559
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3301/10000 (33.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.2834s / 169922.5190 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3079
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2663
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 3321/10000 (33.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.4466s / 170975.9656 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3223
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2619
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3341/10000 (33.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.6054s / 172033.5710 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.3311
env0_second_0:                 episode reward: 1.2000,                 loss: -0.2902
env1_first_0:                 episode reward: -1.8000,                 loss: nan
env1_second_0:                 episode reward: 1.8000,                 loss: nan
Episode: 3361/10000 (33.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.3349s / 173084.9059 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3318
env0_second_0:                 episode reward: -0.5000,                 loss: -0.2970
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 3381/10000 (33.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1033.1172s / 174118.0231 s
env0_first_0:                 episode reward: 4.7500,                 loss: -0.2670
env0_second_0:                 episode reward: -4.7500,                 loss: -0.2105
env1_first_0:                 episode reward: 6.4500,                 loss: nan
env1_second_0:                 episode reward: -6.4500,                 loss: nan
Episode: 3401/10000 (34.0100%),                 avg. length: 9675.4,                last time consumption/overall running time: 1005.5259s / 175123.5490 s
env0_first_0:                 episode reward: -6.3000,                 loss: -0.2667
env0_second_0:                 episode reward: 6.3000,                 loss: -0.2200
env1_first_0:                 episode reward: -12.1000,                 loss: nan
env1_second_0:                 episode reward: 12.1000,                 loss: nan
Episode: 3421/10000 (34.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1062.3731s / 176185.9222 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3141
env0_second_0:                 episode reward: -1.5500,                 loss: -0.2458
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 3441/10000 (34.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.1406s / 177249.0627 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3318
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2976
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 3461/10000 (34.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.5035s / 178285.5663 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3463
env0_second_0:                 episode reward: -0.0500,                 loss: -0.3128
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3481/10000 (34.8100%),                 avg. length: 9809.0,                last time consumption/overall running time: 1003.8964s / 179289.4627 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3143
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2811
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3501/10000 (35.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1023.2750s / 180312.7377 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3284
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2982
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 3521/10000 (35.2100%),                 avg. length: 9985.35,                last time consumption/overall running time: 1022.1502s / 181334.8879 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.3144
env0_second_0:                 episode reward: 0.7500,                 loss: -0.2790
env1_first_0:                 episode reward: 4.6500,                 loss: nan
env1_second_0:                 episode reward: -4.6500,                 loss: nan
Episode: 3541/10000 (35.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1017.7923s / 182352.6802 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3289
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2950
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 3561/10000 (35.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.3928s / 183380.0731 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.3135
env0_second_0:                 episode reward: 1.4000,                 loss: -0.2681
env1_first_0:                 episode reward: -6.9000,                 loss: nan
env1_second_0:                 episode reward: 6.9000,                 loss: nan
Episode: 3581/10000 (35.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.4975s / 184402.5705 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3316
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2961
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 3601/10000 (36.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.7630s / 185407.3335 s
env0_first_0:                 episode reward: -4.4500,                 loss: -0.2885
env0_second_0:                 episode reward: 4.4500,                 loss: -0.2390
env1_first_0:                 episode reward: -19.1500,                 loss: nan
env1_second_0:                 episode reward: 19.1500,                 loss: nan
Episode: 3621/10000 (36.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1009.0243s / 186416.3578 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3290
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2976
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 3641/10000 (36.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1013.2389s / 187429.5967 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3050
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2591
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 3661/10000 (36.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.0434s / 188453.6402 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3125
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2557
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 3681/10000 (36.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.9085s / 189511.5487 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.3291
env0_second_0:                 episode reward: 1.4000,                 loss: -0.2874
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 3701/10000 (37.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1009.1438s / 190520.6925 s
env0_first_0:                 episode reward: -3.1000,                 loss: -0.3128
env0_second_0:                 episode reward: 3.1000,                 loss: -0.2712
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 3721/10000 (37.2100%),                 avg. length: 9797.5,                last time consumption/overall running time: 990.5598s / 191511.2523 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.3092
env0_second_0:                 episode reward: 1.2500,                 loss: -0.2659
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 3741/10000 (37.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1023.2924s / 192534.5447 s
env0_first_0:                 episode reward: -0.9500,                 loss: -0.3066
env0_second_0:                 episode reward: 0.9500,                 loss: -0.2631
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 3761/10000 (37.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.4780s / 193565.0227 s
env0_first_0:                 episode reward: -2.0000,                 loss: -0.3151
env0_second_0:                 episode reward: 2.0000,                 loss: -0.2769
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 3781/10000 (37.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.7747s / 194604.7974 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.2977
env0_second_0:                 episode reward: 2.2500,                 loss: -0.2489
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 3801/10000 (38.0100%),                 avg. length: 9995.1,                last time consumption/overall running time: 1084.8433s / 195689.6408 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.3236
env0_second_0:                 episode reward: 2.1000,                 loss: -0.2840
env1_first_0:                 episode reward: 0.6500,                 loss: nan
env1_second_0:                 episode reward: -0.6500,                 loss: nan
Episode: 3821/10000 (38.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.2154s / 196746.8562 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3265
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2907
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 3841/10000 (38.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1033.0897s / 197779.9459 s
env0_first_0:                 episode reward: 1.6500,                 loss: -0.3130
env0_second_0:                 episode reward: -1.6500,                 loss: -0.2664
env1_first_0:                 episode reward: 1.6000,                 loss: nan
env1_second_0:                 episode reward: -1.6000,                 loss: nan
Episode: 3861/10000 (38.6100%),                 avg. length: 9606.55,                last time consumption/overall running time: 985.7714s / 198765.7173 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3274
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2810
env1_first_0:                 episode reward: -0.2500,                 loss: nan
env1_second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 3881/10000 (38.8100%),                 avg. length: 9993.15,                last time consumption/overall running time: 1050.7393s / 199816.4566 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.3187
env0_second_0:                 episode reward: 2.3000,                 loss: -0.2519
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 3901/10000 (39.0100%),                 avg. length: 9633.35,                last time consumption/overall running time: 1005.7539s / 200822.2105 s
env0_first_0:                 episode reward: -4.3500,                 loss: -0.2784
env0_second_0:                 episode reward: 4.3500,                 loss: -0.2299
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 3921/10000 (39.2100%),                 avg. length: 9835.2,                last time consumption/overall running time: 1010.3931s / 201832.6036 s
env0_first_0:                 episode reward: -5.9500,                 loss: -0.2836
env0_second_0:                 episode reward: 5.9500,                 loss: -0.1833
env1_first_0:                 episode reward: -17.3000,                 loss: nan
env1_second_0:                 episode reward: 17.3000,                 loss: nan
Episode: 3941/10000 (39.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1012.8242s / 202845.4278 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.3054
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2306
env1_first_0:                 episode reward: -11.5500,                 loss: nan
env1_second_0:                 episode reward: 11.5500,                 loss: nan
Episode: 3961/10000 (39.6100%),                 avg. length: 9758.65,                last time consumption/overall running time: 1021.4806s / 203866.9084 s
env0_first_0:                 episode reward: -25.3500,                 loss: -0.2731
env0_second_0:                 episode reward: 25.3500,                 loss: -0.2137
env1_first_0:                 episode reward: -3.1500,                 loss: nan
env1_second_0:                 episode reward: 3.1500,                 loss: nan
Episode: 3981/10000 (39.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1038.5245s / 204905.4329 s
env0_first_0:                 episode reward: -15.7500,                 loss: -0.2786
env0_second_0:                 episode reward: 15.7500,                 loss: -0.2324
env1_first_0:                 episode reward: -17.7500,                 loss: nan
env1_second_0:                 episode reward: 17.7500,                 loss: nan
Episode: 4001/10000 (40.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.1909s / 205941.6238 s
env0_first_0:                 episode reward: -21.0000,                 loss: -0.2442
env0_second_0:                 episode reward: 21.0000,                 loss: -0.1882
env1_first_0:                 episode reward: -22.2000,                 loss: nan
env1_second_0:                 episode reward: 22.2000,                 loss: nan
Episode: 4021/10000 (40.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.8674s / 206978.4912 s
env0_first_0:                 episode reward: -11.8500,                 loss: -0.2692
env0_second_0:                 episode reward: 11.8500,                 loss: -0.2031
env1_first_0:                 episode reward: -20.1500,                 loss: nan
env1_second_0:                 episode reward: 20.1500,                 loss: nan
Episode: 4041/10000 (40.4100%),                 avg. length: 9980.85,                last time consumption/overall running time: 1017.4864s / 207995.9775 s
env0_first_0:                 episode reward: -4.6000,                 loss: -0.2874
env0_second_0:                 episode reward: 4.6000,                 loss: -0.2306
env1_first_0:                 episode reward: -2.0000,                 loss: nan
env1_second_0:                 episode reward: 2.0000,                 loss: nan
Episode: 4061/10000 (40.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1020.7214s / 209016.6989 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.3065
env0_second_0:                 episode reward: 1.4000,                 loss: -0.2469
env1_first_0:                 episode reward: -1.9000,                 loss: nan
env1_second_0:                 episode reward: 1.9000,                 loss: nan
Episode: 4081/10000 (40.8100%),                 avg. length: 9144.05,                last time consumption/overall running time: 925.6134s / 209942.3124 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2434
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1550
env1_first_0:                 episode reward: 5.0000,                 loss: nan
env1_second_0:                 episode reward: -5.0000,                 loss: nan
Episode: 4101/10000 (41.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.1015s / 210964.4139 s
env0_first_0:                 episode reward: -8.2000,                 loss: -0.3214
env0_second_0:                 episode reward: 8.2000,                 loss: -0.2673
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4121/10000 (41.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1021.6309s / 211986.0448 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3300
env0_second_0:                 episode reward: 0.5000,                 loss: -0.2477
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 4141/10000 (41.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.6888s / 213008.7336 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3450
env0_second_0:                 episode reward: 0.6000,                 loss: -0.1903
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 4161/10000 (41.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.0761s / 214030.8097 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3287
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2394
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 4181/10000 (41.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1029.1340s / 215059.9437 s
env0_first_0:                 episode reward: 4.8500,                 loss: -0.2658
env0_second_0:                 episode reward: -4.8500,                 loss: -0.1544
env1_first_0:                 episode reward: 6.8000,                 loss: nan
env1_second_0:                 episode reward: -6.8000,                 loss: nan
Episode: 4201/10000 (42.0100%),                 avg. length: 9762.3,                last time consumption/overall running time: 1010.9051s / 216070.8488 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.3147
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2476
env1_first_0:                 episode reward: -1.7500,                 loss: nan
env1_second_0:                 episode reward: 1.7500,                 loss: nan
Episode: 4221/10000 (42.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.0852s / 217102.9340 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.3354
env0_second_0:                 episode reward: 1.6000,                 loss: -0.2809
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 4241/10000 (42.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1035.7275s / 218138.6615 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.3316
env0_second_0:                 episode reward: 4.7000,                 loss: -0.2681
env1_first_0:                 episode reward: -2.4500,                 loss: nan
env1_second_0:                 episode reward: 2.4500,                 loss: nan
Episode: 4261/10000 (42.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1041.1427s / 219179.8042 s
env0_first_0:                 episode reward: -5.2500,                 loss: -0.2964
env0_second_0:                 episode reward: 5.2500,                 loss: -0.2242
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 4281/10000 (42.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.0531s / 220201.8573 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.2903
env0_second_0:                 episode reward: 4.9000,                 loss: -0.2219
env1_first_0:                 episode reward: -5.4500,                 loss: nan
env1_second_0:                 episode reward: 5.4500,                 loss: nan
Episode: 4301/10000 (43.0100%),                 avg. length: 8180.8,                last time consumption/overall running time: 840.9867s / 221042.8441 s
env0_first_0:                 episode reward: 5.7000,                 loss: -0.2507
env0_second_0:                 episode reward: -5.7000,                 loss: -0.1492
env1_first_0:                 episode reward: 6.6000,                 loss: nan
env1_second_0:                 episode reward: -6.6000,                 loss: nan
Episode: 4321/10000 (43.2100%),                 avg. length: 9132.15,                last time consumption/overall running time: 934.0758s / 221976.9199 s
env0_first_0:                 episode reward: 8.4500,                 loss: -0.3067
env0_second_0:                 episode reward: -8.4500,                 loss: -0.2098
env1_first_0:                 episode reward: 10.9500,                 loss: nan
env1_second_0:                 episode reward: -10.9500,                 loss: nan
Episode: 4341/10000 (43.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1011.6225s / 222988.5424 s
env0_first_0:                 episode reward: -1.8000,                 loss: -0.3182
env0_second_0:                 episode reward: 1.8000,                 loss: -0.2309
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 4361/10000 (43.6100%),                 avg. length: 9884.35,                last time consumption/overall running time: 1014.6942s / 224003.2366 s
env0_first_0:                 episode reward: -19.8000,                 loss: -0.2670
env0_second_0:                 episode reward: 19.8000,                 loss: -0.1809
env1_first_0:                 episode reward: -5.6000,                 loss: nan
env1_second_0:                 episode reward: 5.6000,                 loss: nan
Episode: 4381/10000 (43.8100%),                 avg. length: 9556.7,                last time consumption/overall running time: 978.9872s / 224982.2238 s
env0_first_0:                 episode reward: -17.2000,                 loss: -0.1340
env0_second_0:                 episode reward: 17.2000,                 loss: 0.0219
env1_first_0:                 episode reward: -11.2000,                 loss: nan
env1_second_0:                 episode reward: 11.2000,                 loss: nan
Episode: 4401/10000 (44.0100%),                 avg. length: 8295.75,                last time consumption/overall running time: 853.3164s / 225835.5403 s
env0_first_0:                 episode reward: -119.2500,                 loss: 0.1278
env0_second_0:                 episode reward: 119.2500,                 loss: 0.2871
env1_first_0:                 episode reward: -137.1500,                 loss: nan
env1_second_0:                 episode reward: 137.1500,                 loss: nan
Episode: 4421/10000 (44.2100%),                 avg. length: 8842.35,                last time consumption/overall running time: 916.2237s / 226751.7639 s
env0_first_0:                 episode reward: -15.1500,                 loss: -0.2150
env0_second_0:                 episode reward: 15.1500,                 loss: -0.0961
env1_first_0:                 episode reward: -16.9500,                 loss: nan
env1_second_0:                 episode reward: 16.9500,                 loss: nan
Episode: 4441/10000 (44.4100%),                 avg. length: 9931.55,                last time consumption/overall running time: 1005.8074s / 227757.5713 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3266
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2489
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 4461/10000 (44.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.0240s / 228789.5953 s
env0_first_0:                 episode reward: -5.3000,                 loss: -0.3188
env0_second_0:                 episode reward: 5.3000,                 loss: -0.2248
env1_first_0:                 episode reward: 2.2000,                 loss: nan
env1_second_0:                 episode reward: -2.2000,                 loss: nan
Episode: 4481/10000 (44.8100%),                 avg. length: 9271.2,                last time consumption/overall running time: 932.0014s / 229721.5967 s
env0_first_0:                 episode reward: -14.7500,                 loss: -0.2137
env0_second_0:                 episode reward: 14.7500,                 loss: -0.0921
env1_first_0:                 episode reward: -6.2500,                 loss: nan
env1_second_0:                 episode reward: 6.2500,                 loss: nan
Episode: 4501/10000 (45.0100%),                 avg. length: 8836.15,                last time consumption/overall running time: 900.4991s / 230622.0958 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.1426
env0_second_0:                 episode reward: 0.6500,                 loss: 0.0915
env1_first_0:                 episode reward: -2.9000,                 loss: nan
env1_second_0:                 episode reward: 2.9000,                 loss: nan
Episode: 4521/10000 (45.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1007.1424s / 231629.2382 s
env0_first_0:                 episode reward: 4.4000,                 loss: -0.3090
env0_second_0:                 episode reward: -4.4000,                 loss: -0.2337
env1_first_0:                 episode reward: -0.3500,                 loss: nan
env1_second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 4541/10000 (45.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1046.9743s / 232676.2125 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3392
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2751
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 4561/10000 (45.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.5455s / 233727.7580 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3140
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2534
env1_first_0:                 episode reward: 2.2500,                 loss: nan
env1_second_0:                 episode reward: -2.2500,                 loss: nan
Episode: 4581/10000 (45.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1002.1645s / 234729.9225 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3377
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2847
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4601/10000 (46.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.3514s / 235777.2739 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3435
env0_second_0:                 episode reward: 0.7000,                 loss: -0.2964
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 4621/10000 (46.2100%),                 avg. length: 9964.5,                last time consumption/overall running time: 1031.9469s / 236809.2208 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3137
env0_second_0:                 episode reward: -1.4000,                 loss: -0.2484
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 4641/10000 (46.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1079.8055s / 237889.0262 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3348
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2855
env1_first_0:                 episode reward: 2.4000,                 loss: nan
env1_second_0:                 episode reward: -2.4000,                 loss: nan
Episode: 4661/10000 (46.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1014.8312s / 238903.8574 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.3226
env0_second_0:                 episode reward: -3.2500,                 loss: -0.2647
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 4681/10000 (46.8100%),                 avg. length: 9962.3,                last time consumption/overall running time: 1008.6390s / 239912.4964 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3357
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2379
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 4701/10000 (47.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1012.8560s / 240925.3524 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.3353
env0_second_0:                 episode reward: -4.0500,                 loss: -0.2039
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 4721/10000 (47.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.6420s / 242017.9944 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.3203
env0_second_0:                 episode reward: 2.1000,                 loss: -0.1213
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4741/10000 (47.4100%),                 avg. length: 9925.7,                last time consumption/overall running time: 1040.1498s / 243058.1442 s
env0_first_0:                 episode reward: -3.8500,                 loss: -0.2894
env0_second_0:                 episode reward: 3.8500,                 loss: 0.0346
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 4761/10000 (47.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.0496s / 244088.1938 s
env0_first_0:                 episode reward: -14.4000,                 loss: -0.2770
env0_second_0:                 episode reward: 14.4000,                 loss: -0.1234
env1_first_0:                 episode reward: -7.7000,                 loss: nan
env1_second_0:                 episode reward: 7.7000,                 loss: nan
Episode: 4781/10000 (47.8100%),                 avg. length: 9994.4,                last time consumption/overall running time: 1039.5997s / 245127.7935 s
env0_first_0:                 episode reward: -13.2500,                 loss: -0.2594
env0_second_0:                 episode reward: 13.2500,                 loss: -0.1645
env1_first_0:                 episode reward: -15.1500,                 loss: nan
env1_second_0:                 episode reward: 15.1500,                 loss: nan
Episode: 4801/10000 (48.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1034.4400s / 246162.2335 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.3121
env0_second_0:                 episode reward: 2.3000,                 loss: -0.2333
env1_first_0:                 episode reward: -8.9500,                 loss: nan
env1_second_0:                 episode reward: 8.9500,                 loss: nan
Episode: 4821/10000 (48.2100%),                 avg. length: 8636.6,                last time consumption/overall running time: 894.6856s / 247056.9192 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.2617
env0_second_0:                 episode reward: -1.9500,                 loss: -0.1493
env1_first_0:                 episode reward: 4.5000,                 loss: nan
env1_second_0:                 episode reward: -4.5000,                 loss: nan
Episode: 4841/10000 (48.4100%),                 avg. length: 9928.8,                last time consumption/overall running time: 1051.9265s / 248108.8456 s
env0_first_0:                 episode reward: -3.4000,                 loss: -0.3131
env0_second_0:                 episode reward: 3.4000,                 loss: -0.2315
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 4861/10000 (48.6100%),                 avg. length: 9970.25,                last time consumption/overall running time: 1034.2187s / 249143.0643 s
env0_first_0:                 episode reward: -4.0000,                 loss: -0.3047
env0_second_0:                 episode reward: 4.0000,                 loss: -0.2137
env1_first_0:                 episode reward: -4.4000,                 loss: nan
env1_second_0:                 episode reward: 4.4000,                 loss: nan
Episode: 4881/10000 (48.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1060.7666s / 250203.8309 s
env0_first_0:                 episode reward: -2.4500,                 loss: -0.3199
env0_second_0:                 episode reward: 2.4500,                 loss: -0.2492
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 4901/10000 (49.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1050.9906s / 251254.8215 s
env0_first_0:                 episode reward: -0.7500,                 loss: -0.3257
env0_second_0:                 episode reward: 0.7500,                 loss: -0.2448
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 4921/10000 (49.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1033.0111s / 252287.8326 s
env0_first_0:                 episode reward: -8.8500,                 loss: -0.2906
env0_second_0:                 episode reward: 8.8500,                 loss: -0.1219
env1_first_0:                 episode reward: -1.7000,                 loss: nan
env1_second_0:                 episode reward: 1.7000,                 loss: nan
Episode: 4941/10000 (49.4100%),                 avg. length: 9619.7,                last time consumption/overall running time: 1049.9813s / 253337.8139 s
env0_first_0:                 episode reward: 1.2000,                 loss: -0.3020
env0_second_0:                 episode reward: -1.2000,                 loss: -0.2011
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 4961/10000 (49.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1028.5775s / 254366.3914 s
env0_first_0:                 episode reward: -1.5000,                 loss: -0.3427
env0_second_0:                 episode reward: 1.5000,                 loss: -0.2856
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 4981/10000 (49.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1046.8035s / 255413.1949 s
env0_first_0:                 episode reward: -1.7000,                 loss: -0.3276
env0_second_0:                 episode reward: 1.7000,                 loss: -0.2612
env1_first_0:                 episode reward: -1.6500,                 loss: nan
env1_second_0:                 episode reward: 1.6500,                 loss: nan
Episode: 5001/10000 (50.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.4968s / 256460.6917 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3300
env0_second_0:                 episode reward: 1.3500,                 loss: -0.2621
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 5021/10000 (50.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1070.1691s / 257530.8608 s
env0_first_0:                 episode reward: 0.8000,                 loss: -0.3247
env0_second_0:                 episode reward: -0.8000,                 loss: -0.2730
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5041/10000 (50.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1043.3945s / 258574.2553 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3233
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2580
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 5061/10000 (50.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.9403s / 259607.1956 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3115
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2176
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 5081/10000 (50.8100%),                 avg. length: 9612.65,                last time consumption/overall running time: 1008.3899s / 260615.5856 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.2874
env0_second_0:                 episode reward: -2.3000,                 loss: -0.1960
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 5101/10000 (51.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1053.2197s / 261668.8053 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3408
env0_second_0:                 episode reward: -1.1500,                 loss: -0.2869
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 5121/10000 (51.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.3422s / 262708.1475 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3588
env0_second_0:                 episode reward: -1.1000,                 loss: -0.3055
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 5141/10000 (51.4100%),                 avg. length: 9937.45,                last time consumption/overall running time: 1030.2914s / 263738.4389 s
env0_first_0:                 episode reward: 8.9500,                 loss: -0.2994
env0_second_0:                 episode reward: -8.9500,                 loss: -0.2308
env1_first_0:                 episode reward: 13.0500,                 loss: nan
env1_second_0:                 episode reward: -13.0500,                 loss: nan
Episode: 5161/10000 (51.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.6044s / 264775.0434 s
env0_first_0:                 episode reward: 6.3500,                 loss: -0.3365
env0_second_0:                 episode reward: -6.3500,                 loss: -0.2731
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5181/10000 (51.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.9026s / 265805.9460 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3387
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2748
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 5201/10000 (52.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1052.9480s / 266858.8940 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3585
env0_second_0:                 episode reward: -0.5000,                 loss: -0.3051
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 5221/10000 (52.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1066.1867s / 267925.0807 s
env0_first_0:                 episode reward: 1.5500,                 loss: -0.3487
env0_second_0:                 episode reward: -1.5500,                 loss: -0.3086
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 5241/10000 (52.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1062.3197s / 268987.4003 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3591
env0_second_0:                 episode reward: -2.4500,                 loss: -0.3176
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 5261/10000 (52.6100%),                 avg. length: 9955.5,                last time consumption/overall running time: 1017.0883s / 270004.4886 s
env0_first_0:                 episode reward: 6.5000,                 loss: -0.3336
env0_second_0:                 episode reward: -6.5000,                 loss: -0.2789
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 5281/10000 (52.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1026.2904s / 271030.7790 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3305
env0_second_0:                 episode reward: -0.3000,                 loss: -0.2664
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 5301/10000 (53.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1043.8667s / 272074.6457 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3177
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2266
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 5321/10000 (53.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.1956s / 273121.8413 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.3308
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2574
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 5341/10000 (53.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1057.5854s / 274179.4267 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3064
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2382
env1_first_0:                 episode reward: -1.3000,                 loss: nan
env1_second_0:                 episode reward: 1.3000,                 loss: nan
Episode: 5361/10000 (53.6100%),                 avg. length: 9464.3,                last time consumption/overall running time: 995.2971s / 275174.7238 s
env0_first_0:                 episode reward: 3.6500,                 loss: -0.3010
env0_second_0:                 episode reward: -3.6500,                 loss: -0.2391
env1_first_0:                 episode reward: 2.0500,                 loss: nan
env1_second_0:                 episode reward: -2.0500,                 loss: nan
Episode: 5381/10000 (53.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1064.6895s / 276239.4134 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3305
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2654
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 5401/10000 (54.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1069.9898s / 277309.4032 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.2892
env0_second_0:                 episode reward: 0.3500,                 loss: -0.2138
env1_first_0:                 episode reward: -8.4500,                 loss: nan
env1_second_0:                 episode reward: 8.4500,                 loss: nan
Episode: 5421/10000 (54.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1064.2578s / 278373.6610 s
env0_first_0:                 episode reward: -6.3500,                 loss: -0.3267
env0_second_0:                 episode reward: 6.3500,                 loss: -0.2526
env1_first_0:                 episode reward: -2.8500,                 loss: nan
env1_second_0:                 episode reward: 2.8500,                 loss: nan
Episode: 5441/10000 (54.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.3496s / 279410.0106 s
env0_first_0:                 episode reward: -3.4500,                 loss: -0.3148
env0_second_0:                 episode reward: 3.4500,                 loss: -0.2482
env1_first_0:                 episode reward: -7.9000,                 loss: nan
env1_second_0:                 episode reward: 7.9000,                 loss: nan
Episode: 5461/10000 (54.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.2321s / 280446.2428 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3313
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2574
env1_first_0:                 episode reward: 2.5000,                 loss: nan
env1_second_0:                 episode reward: -2.5000,                 loss: nan
Episode: 5481/10000 (54.8100%),                 avg. length: 9779.75,                last time consumption/overall running time: 1011.7800s / 281458.0228 s
env0_first_0:                 episode reward: 54.4500,                 loss: -0.1156
env0_second_0:                 episode reward: -54.4500,                 loss: 0.0346
env1_first_0:                 episode reward: 39.4500,                 loss: nan
env1_second_0:                 episode reward: -39.4500,                 loss: nan
Episode: 5501/10000 (55.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1098.6306s / 282556.6535 s
env0_first_0:                 episode reward: -2.1500,                 loss: -0.3010
env0_second_0:                 episode reward: 2.1500,                 loss: -0.2228
env1_first_0:                 episode reward: -4.6500,                 loss: nan
env1_second_0:                 episode reward: 4.6500,                 loss: nan
Episode: 5521/10000 (55.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1052.1954s / 283608.8488 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.3406
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2769
env1_first_0:                 episode reward: 2.6500,                 loss: nan
env1_second_0:                 episode reward: -2.6500,                 loss: nan
Episode: 5541/10000 (55.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.7671s / 284631.6160 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3514
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2965
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 5561/10000 (55.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.7546s / 285659.3705 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3502
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2995
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 5581/10000 (55.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1044.2451s / 286703.6156 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3413
env0_second_0:                 episode reward: -2.8500,                 loss: -0.2685
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 5601/10000 (56.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.1105s / 287757.7262 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3545
env0_second_0:                 episode reward: -3.5500,                 loss: -0.2807
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 5621/10000 (56.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1038.1425s / 288795.8687 s
env0_first_0:                 episode reward: 0.3000,                 loss: -0.3738
env0_second_0:                 episode reward: -0.3000,                 loss: -0.3302
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 5641/10000 (56.4100%),                 avg. length: 9908.65,                last time consumption/overall running time: 1055.2743s / 289851.1431 s
env0_first_0:                 episode reward: 2.8000,                 loss: -0.3714
env0_second_0:                 episode reward: -2.8000,                 loss: -0.3169
env1_first_0:                 episode reward: 1.7500,                 loss: nan
env1_second_0:                 episode reward: -1.7500,                 loss: nan
Episode: 5661/10000 (56.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1012.9767s / 290864.1198 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3630
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3078
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 5681/10000 (56.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1018.6716s / 291882.7913 s
env0_first_0:                 episode reward: -3.5500,                 loss: -0.3078
env0_second_0:                 episode reward: 3.5500,                 loss: -0.2527
env1_first_0:                 episode reward: -6.7000,                 loss: nan
env1_second_0:                 episode reward: 6.7000,                 loss: nan
Episode: 5701/10000 (57.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1037.0369s / 292919.8283 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3239
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2547
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 5721/10000 (57.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1035.1942s / 293955.0225 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3186
env0_second_0:                 episode reward: -0.3500,                 loss: -0.2509
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 5741/10000 (57.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.5475s / 294994.5701 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3511
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2877
env1_first_0:                 episode reward: 1.7000,                 loss: nan
env1_second_0:                 episode reward: -1.7000,                 loss: nan
Episode: 5761/10000 (57.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.3082s / 296024.8783 s
env0_first_0:                 episode reward: -4.9000,                 loss: -0.3560
env0_second_0:                 episode reward: 4.9000,                 loss: -0.2921
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 5781/10000 (57.8100%),                 avg. length: 9854.55,                last time consumption/overall running time: 1009.1588s / 297034.0371 s
env0_first_0:                 episode reward: -3.6500,                 loss: -0.2989
env0_second_0:                 episode reward: 3.6500,                 loss: -0.2144
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 5801/10000 (58.0100%),                 avg. length: 9166.2,                last time consumption/overall running time: 928.3713s / 297962.4084 s
env0_first_0:                 episode reward: -37.5000,                 loss: -0.1886
env0_second_0:                 episode reward: 37.5000,                 loss: -0.1078
env1_first_0:                 episode reward: -31.9000,                 loss: nan
env1_second_0:                 episode reward: 31.9000,                 loss: nan
Episode: 5821/10000 (58.2100%),                 avg. length: 9949.25,                last time consumption/overall running time: 1028.1316s / 298990.5400 s
env0_first_0:                 episode reward: -18.8500,                 loss: -0.2737
env0_second_0:                 episode reward: 18.8500,                 loss: -0.2019
env1_first_0:                 episode reward: -10.1000,                 loss: nan
env1_second_0:                 episode reward: 10.1000,                 loss: nan
Episode: 5841/10000 (58.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1016.7046s / 300007.2446 s
env0_first_0:                 episode reward: -17.7000,                 loss: -0.2940
env0_second_0:                 episode reward: 17.7000,                 loss: -0.1696
env1_first_0:                 episode reward: -4.5000,                 loss: nan
env1_second_0:                 episode reward: 4.5000,                 loss: nan
Episode: 5861/10000 (58.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.8780s / 301012.1225 s
env0_first_0:                 episode reward: -2.5000,                 loss: -0.3062
env0_second_0:                 episode reward: 2.5000,                 loss: -0.2342
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 5881/10000 (58.8100%),                 avg. length: 9964.45,                last time consumption/overall running time: 1047.1005s / 302059.2230 s
env0_first_0:                 episode reward: 2.0000,                 loss: -0.3109
env0_second_0:                 episode reward: -2.0000,                 loss: -0.2030
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 5901/10000 (59.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.4128s / 303110.6358 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3155
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2385
env1_first_0:                 episode reward: -4.8000,                 loss: nan
env1_second_0:                 episode reward: 4.8000,                 loss: nan
Episode: 5921/10000 (59.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1006.0982s / 304116.7339 s
env0_first_0:                 episode reward: 0.0000,                 loss: -0.3353
env0_second_0:                 episode reward: 0.0000,                 loss: -0.2741
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 5941/10000 (59.4100%),                 avg. length: 9846.15,                last time consumption/overall running time: 998.8610s / 305115.5949 s
env0_first_0:                 episode reward: -7.7500,                 loss: -0.2787
env0_second_0:                 episode reward: 7.7500,                 loss: -0.1929
env1_first_0:                 episode reward: -8.7500,                 loss: nan
env1_second_0:                 episode reward: 8.7500,                 loss: nan
Episode: 5961/10000 (59.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1038.3402s / 306153.9351 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.3264
env0_second_0:                 episode reward: -0.2000,                 loss: -0.2673
env1_first_0:                 episode reward: 1.4500,                 loss: nan
env1_second_0:                 episode reward: -1.4500,                 loss: nan
Episode: 5981/10000 (59.8100%),                 avg. length: 9876.55,                last time consumption/overall running time: 1013.5916s / 307167.5267 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.3215
env0_second_0:                 episode reward: -3.3000,                 loss: -0.2350
env1_first_0:                 episode reward: 2.7000,                 loss: nan
env1_second_0:                 episode reward: -2.7000,                 loss: nan
Episode: 6001/10000 (60.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1020.2473s / 308187.7740 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.3482
env0_second_0:                 episode reward: 0.4500,                 loss: -0.2548
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 6021/10000 (60.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1009.8364s / 309197.6104 s
env0_first_0:                 episode reward: -0.0500,                 loss: -0.3225
env0_second_0:                 episode reward: 0.0500,                 loss: -0.2375
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 6041/10000 (60.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1048.0910s / 310245.7013 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3434
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2868
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 6061/10000 (60.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1042.6626s / 311288.3639 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.3562
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2145
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 6081/10000 (60.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1013.0135s / 312301.3774 s
env0_first_0:                 episode reward: 3.3000,                 loss: -0.3603
env0_second_0:                 episode reward: -3.3000,                 loss: -0.2994
env1_first_0:                 episode reward: 4.7500,                 loss: nan
env1_second_0:                 episode reward: -4.7500,                 loss: nan
Episode: 6101/10000 (61.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1002.1867s / 313303.5641 s
env0_first_0:                 episode reward: 2.1500,                 loss: -0.3729
env0_second_0:                 episode reward: -2.1500,                 loss: -0.2989
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 6121/10000 (61.2100%),                 avg. length: 8919.05,                last time consumption/overall running time: 948.7985s / 314252.3626 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.2843
env0_second_0:                 episode reward: -0.9000,                 loss: -0.1821
env1_first_0:                 episode reward: -5.7000,                 loss: nan
env1_second_0:                 episode reward: 5.7000,                 loss: nan
Episode: 6141/10000 (61.4100%),                 avg. length: 9958.55,                last time consumption/overall running time: 1008.8868s / 315261.2494 s
env0_first_0:                 episode reward: -1.8500,                 loss: -0.2979
env0_second_0:                 episode reward: 1.8500,                 loss: -0.2339
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 6161/10000 (61.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1034.0052s / 316295.2546 s
env0_first_0:                 episode reward: -2.6000,                 loss: -0.3275
env0_second_0:                 episode reward: 2.6000,                 loss: -0.2597
env1_first_0:                 episode reward: -3.6000,                 loss: nan
env1_second_0:                 episode reward: 3.6000,                 loss: nan
Episode: 6181/10000 (61.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.0458s / 317317.3003 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.3416
env0_second_0:                 episode reward: 1.7500,                 loss: -0.2571
env1_first_0:                 episode reward: -1.6000,                 loss: nan
env1_second_0:                 episode reward: 1.6000,                 loss: nan
Episode: 6201/10000 (62.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1003.0762s / 318320.3766 s
env0_first_0:                 episode reward: -10.9000,                 loss: -0.3276
env0_second_0:                 episode reward: 10.9000,                 loss: -0.2483
env1_first_0:                 episode reward: -6.4000,                 loss: nan
env1_second_0:                 episode reward: 6.4000,                 loss: nan
Episode: 6221/10000 (62.2100%),                 avg. length: 8855.7,                last time consumption/overall running time: 945.4240s / 319265.8006 s
env0_first_0:                 episode reward: 19.2000,                 loss: -0.2394
env0_second_0:                 episode reward: -19.2000,                 loss: -0.0117
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 6241/10000 (62.4100%),                 avg. length: 9959.1,                last time consumption/overall running time: 1028.4006s / 320294.2012 s
env0_first_0:                 episode reward: 2.0500,                 loss: -0.3465
env0_second_0:                 episode reward: -2.0500,                 loss: 0.0849
env1_first_0:                 episode reward: 4.4500,                 loss: nan
env1_second_0:                 episode reward: -4.4500,                 loss: nan
Episode: 6261/10000 (62.6100%),                 avg. length: 9842.4,                last time consumption/overall running time: 1007.5165s / 321301.7177 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3454
env0_second_0:                 episode reward: 0.4000,                 loss: -0.1467
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 6281/10000 (62.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1026.1496s / 322327.8672 s
env0_first_0:                 episode reward: -1.3500,                 loss: -0.3245
env0_second_0:                 episode reward: 1.3500,                 loss: -0.1083
env1_first_0:                 episode reward: 0.8000,                 loss: nan
env1_second_0:                 episode reward: -0.8000,                 loss: nan
Episode: 6301/10000 (63.0100%),                 avg. length: 9751.7,                last time consumption/overall running time: 1065.1093s / 323392.9766 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.3430
env0_second_0:                 episode reward: -3.0000,                 loss: -0.1978
env1_first_0:                 episode reward: 3.0000,                 loss: nan
env1_second_0:                 episode reward: -3.0000,                 loss: nan
Episode: 6321/10000 (63.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1018.1777s / 324411.1543 s
env0_first_0:                 episode reward: -5.8500,                 loss: -0.3522
env0_second_0:                 episode reward: 5.8500,                 loss: -0.3040
env1_first_0:                 episode reward: -0.8500,                 loss: nan
env1_second_0:                 episode reward: 0.8500,                 loss: nan
Episode: 6341/10000 (63.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1026.0644s / 325437.2187 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3709
env0_second_0:                 episode reward: 0.1500,                 loss: -0.3310
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 6361/10000 (63.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1002.1504s / 326439.3691 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3762
env0_second_0:                 episode reward: 0.5000,                 loss: -0.3351
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 6381/10000 (63.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1011.2975s / 327450.6665 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3501
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2993
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 6401/10000 (64.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1022.9455s / 328473.6120 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3230
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2787
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 6421/10000 (64.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1023.1121s / 329496.7241 s
env0_first_0:                 episode reward: 1.1500,                 loss: -0.3739
env0_second_0:                 episode reward: -1.1500,                 loss: -0.3337
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 6441/10000 (64.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1006.6853s / 330503.4094 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3484
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2962
env1_first_0:                 episode reward: -3.8500,                 loss: nan
env1_second_0:                 episode reward: 3.8500,                 loss: nan
Episode: 6461/10000 (64.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1007.4689s / 331510.8784 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3400
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2770
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 6481/10000 (64.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1005.8918s / 332516.7702 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.2948
env0_second_0:                 episode reward: 2.9000,                 loss: -0.2114
env1_first_0:                 episode reward: -17.8500,                 loss: nan
env1_second_0:                 episode reward: 17.8500,                 loss: nan
Episode: 6501/10000 (65.0100%),                 avg. length: 9904.8,                last time consumption/overall running time: 994.1997s / 333510.9698 s
env0_first_0:                 episode reward: -6.7500,                 loss: -0.3104
env0_second_0:                 episode reward: 6.7500,                 loss: -0.2128
env1_first_0:                 episode reward: -2.6500,                 loss: nan
env1_second_0:                 episode reward: 2.6500,                 loss: nan
Episode: 6521/10000 (65.2100%),                 avg. length: 9416.4,                last time consumption/overall running time: 958.1458s / 334469.1157 s
env0_first_0:                 episode reward: -4.5000,                 loss: -0.2950
env0_second_0:                 episode reward: 4.5000,                 loss: -0.1867
env1_first_0:                 episode reward: -7.2500,                 loss: nan
env1_second_0:                 episode reward: 7.2500,                 loss: nan
Episode: 6541/10000 (65.4100%),                 avg. length: 9558.7,                last time consumption/overall running time: 967.5821s / 335436.6978 s
env0_first_0:                 episode reward: 2.3000,                 loss: -0.3158
env0_second_0:                 episode reward: -2.3000,                 loss: -0.2092
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 6561/10000 (65.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1015.2587s / 336451.9565 s
env0_first_0:                 episode reward: -4.4000,                 loss: -0.3369
env0_second_0:                 episode reward: 4.4000,                 loss: -0.2443
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 6581/10000 (65.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1001.4678s / 337453.4243 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3583
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2349
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 6601/10000 (66.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.0701s / 338480.4944 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3228
env0_second_0:                 episode reward: 1.1000,                 loss: -0.1858
env1_first_0:                 episode reward: 2.1000,                 loss: nan
env1_second_0:                 episode reward: -2.1000,                 loss: nan
Episode: 6621/10000 (66.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1029.7929s / 339510.2873 s
env0_first_0:                 episode reward: 1.3000,                 loss: -0.3135
env0_second_0:                 episode reward: -1.3000,                 loss: -0.2321
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 6641/10000 (66.4100%),                 avg. length: 9936.3,                last time consumption/overall running time: 986.0860s / 340496.3733 s
env0_first_0:                 episode reward: 3.1500,                 loss: -0.3400
env0_second_0:                 episode reward: -3.1500,                 loss: -0.2627
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 6661/10000 (66.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1009.9544s / 341506.3277 s
env0_first_0:                 episode reward: 1.4000,                 loss: -0.3582
env0_second_0:                 episode reward: -1.4000,                 loss: -0.2899
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 6681/10000 (66.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 980.3528s / 342486.6805 s
env0_first_0:                 episode reward: -2.8500,                 loss: -0.3456
env0_second_0:                 episode reward: 2.8500,                 loss: -0.2741
env1_first_0:                 episode reward: 0.8500,                 loss: nan
env1_second_0:                 episode reward: -0.8500,                 loss: nan
Episode: 6701/10000 (67.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 983.1211s / 343469.8017 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3462
env0_second_0:                 episode reward: 0.8500,                 loss: -0.2715
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 6721/10000 (67.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1007.3022s / 344477.1039 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.3121
env0_second_0:                 episode reward: 1.0000,                 loss: -0.2271
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 6741/10000 (67.4100%),                 avg. length: 9768.5,                last time consumption/overall running time: 1004.0661s / 345481.1700 s
env0_first_0:                 episode reward: 4.7000,                 loss: -0.2954
env0_second_0:                 episode reward: -4.7000,                 loss: -0.2037
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 6761/10000 (67.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 998.8389s / 346480.0089 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.3368
env0_second_0:                 episode reward: 1.2000,                 loss: -0.2466
env1_first_0:                 episode reward: -2.4000,                 loss: nan
env1_second_0:                 episode reward: 2.4000,                 loss: nan
Episode: 6781/10000 (67.8100%),                 avg. length: 9845.0,                last time consumption/overall running time: 989.4920s / 347469.5010 s
env0_first_0:                 episode reward: 3.8500,                 loss: -0.3206
env0_second_0:                 episode reward: -3.8500,                 loss: -0.0713
env1_first_0:                 episode reward: 2.8000,                 loss: nan
env1_second_0:                 episode reward: -2.8000,                 loss: nan
Episode: 6801/10000 (68.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 991.5562s / 348461.0572 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.3500
env0_second_0:                 episode reward: 0.3500,                 loss: -0.1423
env1_first_0:                 episode reward: 0.7500,                 loss: nan
env1_second_0:                 episode reward: -0.7500,                 loss: nan
Episode: 6821/10000 (68.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1003.7678s / 349464.8250 s
env0_first_0:                 episode reward: 5.5500,                 loss: -0.2945
env0_second_0:                 episode reward: -5.5500,                 loss: -0.1997
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 6841/10000 (68.4100%),                 avg. length: 9553.15,                last time consumption/overall running time: 963.0267s / 350427.8516 s
env0_first_0:                 episode reward: 17.3000,                 loss: -0.2331
env0_second_0:                 episode reward: -17.3000,                 loss: -0.1445
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 6861/10000 (68.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1017.9750s / 351445.8267 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3504
env0_second_0:                 episode reward: -1.6000,                 loss: -0.2924
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 6881/10000 (68.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1005.6584s / 352451.4850 s
env0_first_0:                 episode reward: 1.0500,                 loss: -0.3496
env0_second_0:                 episode reward: -1.0500,                 loss: -0.2829
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 6901/10000 (69.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 998.1295s / 353449.6145 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3438
env0_second_0:                 episode reward: -0.5500,                 loss: -0.2689
env1_first_0:                 episode reward: 0.9500,                 loss: nan
env1_second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 6921/10000 (69.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 987.7266s / 354437.3411 s
env0_first_0:                 episode reward: 3.0000,                 loss: -0.3402
env0_second_0:                 episode reward: -3.0000,                 loss: -0.2697
env1_first_0:                 episode reward: 0.7000,                 loss: nan
env1_second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 6941/10000 (69.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.0075s / 355441.3486 s
env0_first_0:                 episode reward: 0.2000,                 loss: -0.3408
env0_second_0:                 episode reward: -0.2000,                 loss: -0.1909
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 6961/10000 (69.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 998.3796s / 356439.7282 s
env0_first_0:                 episode reward: -0.3500,                 loss: -0.3591
env0_second_0:                 episode reward: 0.3500,                 loss: -0.2981
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 6981/10000 (69.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1005.0724s / 357444.8006 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.3443
env0_second_0:                 episode reward: -0.7000,                 loss: -0.1533
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 7001/10000 (70.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1000.3403s / 358445.1409 s
env0_first_0:                 episode reward: 1.8000,                 loss: -0.3549
env0_second_0:                 episode reward: -1.8000,                 loss: -0.2042
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 7021/10000 (70.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 997.9377s / 359443.0786 s
env0_first_0:                 episode reward: 6.9500,                 loss: -0.3530
env0_second_0:                 episode reward: -6.9500,                 loss: -0.2845
env1_first_0:                 episode reward: 4.2000,                 loss: nan
env1_second_0:                 episode reward: -4.2000,                 loss: nan
Episode: 7041/10000 (70.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1017.2570s / 360460.3356 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3556
env0_second_0:                 episode reward: -0.5500,                 loss: -0.3040
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 7061/10000 (70.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 987.7805s / 361448.1161 s
env0_first_0:                 episode reward: -0.7000,                 loss: -0.3440
env0_second_0:                 episode reward: 0.7000,                 loss: -0.2860
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7081/10000 (70.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 999.6492s / 362447.7653 s
env0_first_0:                 episode reward: 1.4500,                 loss: -0.3308
env0_second_0:                 episode reward: -1.4500,                 loss: -0.2280
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 7101/10000 (71.0100%),                 avg. length: 9883.45,                last time consumption/overall running time: 990.6761s / 363438.4414 s
env0_first_0:                 episode reward: -7.5000,                 loss: -0.3542
env0_second_0:                 episode reward: 7.5000,                 loss: -0.2828
env1_first_0:                 episode reward: 4.8500,                 loss: nan
env1_second_0:                 episode reward: -4.8500,                 loss: nan
Episode: 7121/10000 (71.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.7452s / 364471.1867 s
env0_first_0:                 episode reward: 1.6000,                 loss: -0.3605
env0_second_0:                 episode reward: -1.6000,                 loss: -0.3059
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 7141/10000 (71.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1019.1804s / 365490.3671 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3552
env0_second_0:                 episode reward: -2.4000,                 loss: -0.3001
env1_first_0:                 episode reward: -2.9500,                 loss: nan
env1_second_0:                 episode reward: 2.9500,                 loss: nan
Episode: 7161/10000 (71.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1012.0715s / 366502.4386 s
env0_first_0:                 episode reward: -1.6500,                 loss: -0.3709
env0_second_0:                 episode reward: 1.6500,                 loss: -0.3203
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 7181/10000 (71.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 993.1520s / 367495.5905 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3564
env0_second_0:                 episode reward: -1.5000,                 loss: -0.3011
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 7201/10000 (72.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 997.1374s / 368492.7279 s
env0_first_0:                 episode reward: 0.9500,                 loss: -0.3510
env0_second_0:                 episode reward: -0.9500,                 loss: -0.2890
env1_first_0:                 episode reward: 3.4000,                 loss: nan
env1_second_0:                 episode reward: -3.4000,                 loss: nan
Episode: 7221/10000 (72.2100%),                 avg. length: 9555.1,                last time consumption/overall running time: 962.0333s / 369454.7612 s
env0_first_0:                 episode reward: -2.3000,                 loss: -0.2975
env0_second_0:                 episode reward: 2.3000,                 loss: -0.2200
env1_first_0:                 episode reward: 3.3000,                 loss: nan
env1_second_0:                 episode reward: -3.3000,                 loss: nan
Episode: 7241/10000 (72.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1020.2827s / 370475.0439 s
env0_first_0:                 episode reward: 2.8500,                 loss: -0.3680
env0_second_0:                 episode reward: -2.8500,                 loss: -0.3144
env1_first_0:                 episode reward: 4.1500,                 loss: nan
env1_second_0:                 episode reward: -4.1500,                 loss: nan
Episode: 7261/10000 (72.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1023.4510s / 371498.4949 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3681
env0_second_0:                 episode reward: 0.5500,                 loss: -0.3186
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 7281/10000 (72.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1034.6669s / 372533.1618 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3620
env0_second_0:                 episode reward: 0.3000,                 loss: -0.2989
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 7301/10000 (73.0100%),                 avg. length: 9407.85,                last time consumption/overall running time: 957.2005s / 373490.3623 s
env0_first_0:                 episode reward: -1.2500,                 loss: -0.2953
env0_second_0:                 episode reward: 1.2500,                 loss: -0.2082
env1_first_0:                 episode reward: 4.3000,                 loss: nan
env1_second_0:                 episode reward: -4.3000,                 loss: nan
Episode: 7321/10000 (73.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.5668s / 374517.9291 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.3527
env0_second_0:                 episode reward: -2.9500,                 loss: -0.2876
env1_first_0:                 episode reward: 2.5500,                 loss: nan
env1_second_0:                 episode reward: -2.5500,                 loss: nan
Episode: 7341/10000 (73.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1008.7358s / 375526.6649 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.3601
env0_second_0:                 episode reward: -0.3500,                 loss: -0.3148
env1_first_0:                 episode reward: 0.5500,                 loss: nan
env1_second_0:                 episode reward: -0.5500,                 loss: nan
Episode: 7361/10000 (73.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.4384s / 376566.1033 s
env0_first_0:                 episode reward: 0.5500,                 loss: -0.3645
env0_second_0:                 episode reward: -0.5500,                 loss: -0.3159
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 7381/10000 (73.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 995.8890s / 377561.9923 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.3268
env0_second_0:                 episode reward: 0.4500,                 loss: -0.2548
env1_first_0:                 episode reward: 0.2500,                 loss: nan
env1_second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 7401/10000 (74.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.3772s / 378589.3695 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3435
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2550
env1_first_0:                 episode reward: -2.0500,                 loss: nan
env1_second_0:                 episode reward: 2.0500,                 loss: nan
Episode: 7421/10000 (74.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 998.3842s / 379587.7536 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3453
env0_second_0:                 episode reward: -0.9000,                 loss: -0.2813
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 7441/10000 (74.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 992.8821s / 380580.6357 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3608
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3084
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 7461/10000 (74.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1019.6472s / 381600.2829 s
env0_first_0:                 episode reward: 0.8500,                 loss: -0.3691
env0_second_0:                 episode reward: -0.8500,                 loss: -0.3213
env1_first_0:                 episode reward: 1.9500,                 loss: nan
env1_second_0:                 episode reward: -1.9500,                 loss: nan
Episode: 7481/10000 (74.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1010.7344s / 382611.0173 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3600
env0_second_0:                 episode reward: 0.1000,                 loss: -0.3110
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 7501/10000 (75.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 998.8394s / 383609.8567 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.3590
env0_second_0:                 episode reward: 1.0000,                 loss: -0.3111
env1_first_0:                 episode reward: 0.1000,                 loss: nan
env1_second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 7521/10000 (75.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1019.7514s / 384629.6081 s
env0_first_0:                 episode reward: -0.1500,                 loss: -0.3455
env0_second_0:                 episode reward: 0.1500,                 loss: -0.2832
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 7541/10000 (75.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1018.7147s / 385648.3228 s
env0_first_0:                 episode reward: 0.2500,                 loss: -0.3404
env0_second_0:                 episode reward: -0.2500,                 loss: -0.2738
env1_first_0:                 episode reward: 0.5000,                 loss: nan
env1_second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 7561/10000 (75.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1004.3763s / 386652.6990 s
env0_first_0:                 episode reward: 0.0500,                 loss: -0.3413
env0_second_0:                 episode reward: -0.0500,                 loss: -0.2815
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7581/10000 (75.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 990.8426s / 387643.5416 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.3488
env0_second_0:                 episode reward: -0.5000,                 loss: -0.3006
env1_first_0:                 episode reward: 0.9000,                 loss: nan
env1_second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 7601/10000 (76.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.5796s / 388668.1212 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3639
env0_second_0:                 episode reward: 0.1000,                 loss: -0.3111
env1_first_0:                 episode reward: 1.0500,                 loss: nan
env1_second_0:                 episode reward: -1.0500,                 loss: nan
Episode: 7621/10000 (76.2100%),                 avg. length: 9960.9,                last time consumption/overall running time: 994.9924s / 389663.1136 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3538
env0_second_0:                 episode reward: -1.7000,                 loss: -0.3008
env1_first_0:                 episode reward: -5.8000,                 loss: nan
env1_second_0:                 episode reward: 5.8000,                 loss: nan
Episode: 7641/10000 (76.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 983.2491s / 390646.3627 s
env0_first_0:                 episode reward: -1.9000,                 loss: -0.3471
env0_second_0:                 episode reward: 1.9000,                 loss: -0.2881
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 7661/10000 (76.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 994.2814s / 391640.6442 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3722
env0_second_0:                 episode reward: 1.3000,                 loss: -0.3232
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 7681/10000 (76.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 981.8991s / 392622.5432 s
env0_first_0:                 episode reward: -0.1000,                 loss: -0.3678
env0_second_0:                 episode reward: 0.1000,                 loss: -0.3269
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 7701/10000 (77.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1013.5248s / 393636.0681 s
env0_first_0:                 episode reward: -0.8000,                 loss: -0.3617
env0_second_0:                 episode reward: 0.8000,                 loss: -0.2896
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 7721/10000 (77.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1065.4230s / 394701.4911 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3461
env0_second_0:                 episode reward: 1.1000,                 loss: -0.2715
env1_first_0:                 episode reward: -1.0000,                 loss: nan
env1_second_0:                 episode reward: 1.0000,                 loss: nan
Episode: 7741/10000 (77.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1054.5400s / 395756.0310 s
env0_first_0:                 episode reward: -1.2000,                 loss: -0.3508
env0_second_0:                 episode reward: 1.2000,                 loss: 0.0753
env1_first_0:                 episode reward: -1.1500,                 loss: nan
env1_second_0:                 episode reward: 1.1500,                 loss: nan
Episode: 7761/10000 (77.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1081.9441s / 396837.9752 s
env0_first_0:                 episode reward: -2.1000,                 loss: -0.3412
env0_second_0:                 episode reward: 2.1000,                 loss: -0.1273
env1_first_0:                 episode reward: -1.5000,                 loss: nan
env1_second_0:                 episode reward: 1.5000,                 loss: nan
Episode: 7781/10000 (77.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1083.3481s / 397921.3232 s
env0_first_0:                 episode reward: -1.0000,                 loss: -0.3621
env0_second_0:                 episode reward: 1.0000,                 loss: -0.2933
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 7801/10000 (78.0100%),                 avg. length: 9833.3,                last time consumption/overall running time: 1006.9057s / 398928.2290 s
env0_first_0:                 episode reward: 6.5500,                 loss: -0.3444
env0_second_0:                 episode reward: -6.5500,                 loss: -0.2442
env1_first_0:                 episode reward: 5.4000,                 loss: nan
env1_second_0:                 episode reward: -5.4000,                 loss: nan
Episode: 7821/10000 (78.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1064.3569s / 399992.5858 s
env0_first_0:                 episode reward: -0.6500,                 loss: -0.3607
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2642
env1_first_0:                 episode reward: -0.9500,                 loss: nan
env1_second_0:                 episode reward: 0.9500,                 loss: nan
Episode: 7841/10000 (78.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1107.5893s / 401100.1752 s
env0_first_0:                 episode reward: 4.1000,                 loss: -0.3542
env0_second_0:                 episode reward: -4.1000,                 loss: -0.2671
env1_first_0:                 episode reward: 3.7500,                 loss: nan
env1_second_0:                 episode reward: -3.7500,                 loss: nan
Episode: 7861/10000 (78.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1139.9921s / 402240.1673 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3575
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2815
env1_first_0:                 episode reward: 1.6500,                 loss: nan
env1_second_0:                 episode reward: -1.6500,                 loss: nan
Episode: 7881/10000 (78.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1090.2187s / 403330.3860 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.3524
env0_second_0:                 episode reward: -2.9500,                 loss: -0.2363
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 7901/10000 (79.0100%),                 avg. length: 7537.55,                last time consumption/overall running time: 764.1424s / 404094.5284 s
env0_first_0:                 episode reward: 42.4000,                 loss: -0.0463
env0_second_0:                 episode reward: -42.4000,                 loss: 0.3333
env1_first_0:                 episode reward: 37.8500,                 loss: nan
env1_second_0:                 episode reward: -37.8500,                 loss: nan
Episode: 7921/10000 (79.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.0712s / 405141.5996 s
env0_first_0:                 episode reward: 10.0500,                 loss: -0.2959
env0_second_0:                 episode reward: -10.0500,                 loss: -0.1622
env1_first_0:                 episode reward: 4.0000,                 loss: nan
env1_second_0:                 episode reward: -4.0000,                 loss: nan
Episode: 7941/10000 (79.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1074.0355s / 406215.6351 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3233
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2142
env1_first_0:                 episode reward: 1.4000,                 loss: nan
env1_second_0:                 episode reward: -1.4000,                 loss: nan
Episode: 7961/10000 (79.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1108.0659s / 407323.7010 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3338
env0_second_0:                 episode reward: 0.4000,                 loss: -0.2468
env1_first_0:                 episode reward: -1.1000,                 loss: nan
env1_second_0:                 episode reward: 1.1000,                 loss: nan
Episode: 7981/10000 (79.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1095.2114s / 408418.9124 s
env0_first_0:                 episode reward: 0.1500,                 loss: -0.2906
env0_second_0:                 episode reward: -0.1500,                 loss: -0.1905
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 8001/10000 (80.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.5101s / 409550.4225 s
env0_first_0:                 episode reward: 3.5500,                 loss: -0.3421
env0_second_0:                 episode reward: -3.5500,                 loss: 0.0772
env1_first_0:                 episode reward: 3.6000,                 loss: nan
env1_second_0:                 episode reward: -3.6000,                 loss: nan
Episode: 8021/10000 (80.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1111.6502s / 410662.0727 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3476
env0_second_0:                 episode reward: 0.8500,                 loss: -0.0119
env1_first_0:                 episode reward: 0.1500,                 loss: nan
env1_second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 8041/10000 (80.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1155.5910s / 411817.6637 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.3542
env0_second_0:                 episode reward: 0.4500,                 loss: -0.1522
env1_first_0:                 episode reward: -0.6500,                 loss: nan
env1_second_0:                 episode reward: 0.6500,                 loss: nan
Episode: 8061/10000 (80.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1126.0629s / 412943.7266 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3533
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2525
env1_first_0:                 episode reward: -0.5500,                 loss: nan
env1_second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 8081/10000 (80.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1077.8525s / 414021.5790 s
env0_first_0:                 episode reward: 0.7500,                 loss: -0.3580
env0_second_0:                 episode reward: -0.7500,                 loss: -0.2671
env1_first_0:                 episode reward: 1.1000,                 loss: nan
env1_second_0:                 episode reward: -1.1000,                 loss: nan
Episode: 8101/10000 (81.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1092.0849s / 415113.6639 s
env0_first_0:                 episode reward: 2.6000,                 loss: -0.3611
env0_second_0:                 episode reward: -2.6000,                 loss: -0.2913
env1_first_0:                 episode reward: 1.0000,                 loss: nan
env1_second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 8121/10000 (81.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.6535s / 416219.3174 s
env0_first_0:                 episode reward: -2.9000,                 loss: -0.3291
env0_second_0:                 episode reward: 2.9000,                 loss: -0.2502
env1_first_0:                 episode reward: -6.6000,                 loss: nan
env1_second_0:                 episode reward: 6.6000,                 loss: nan
Episode: 8141/10000 (81.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1135.0078s / 417354.3252 s
env0_first_0:                 episode reward: 2.7000,                 loss: -0.3495
env0_second_0:                 episode reward: -2.7000,                 loss: -0.2678
env1_first_0:                 episode reward: -2.2500,                 loss: nan
env1_second_0:                 episode reward: 2.2500,                 loss: nan
Episode: 8161/10000 (81.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1094.5454s / 418448.8706 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3220
env0_second_0:                 episode reward: 0.3000,                 loss: -0.1691
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 8181/10000 (81.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1102.1685s / 419551.0392 s
env0_first_0:                 episode reward: -8.9000,                 loss: -0.3345
env0_second_0:                 episode reward: 8.9000,                 loss: -0.2148
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8201/10000 (82.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1075.6285s / 420626.6676 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3367
env0_second_0:                 episode reward: 0.5000,                 loss: -0.2605
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 8221/10000 (82.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1097.5978s / 421724.2654 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.3237
env0_second_0:                 episode reward: 2.6500,                 loss: -0.2513
env1_first_0:                 episode reward: 0.3500,                 loss: nan
env1_second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 8241/10000 (82.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1105.6587s / 422829.9241 s
env0_first_0:                 episode reward: -1.7500,                 loss: -0.3482
env0_second_0:                 episode reward: 1.7500,                 loss: -0.2866
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 8261/10000 (82.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1101.9218s / 423931.8459 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3465
env0_second_0:                 episode reward: 0.5000,                 loss: -0.2784
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 8281/10000 (82.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1115.0514s / 425046.8973 s
env0_first_0:                 episode reward: -0.9000,                 loss: -0.3482
env0_second_0:                 episode reward: 0.9000,                 loss: -0.2836
env1_first_0:                 episode reward: -0.2000,                 loss: nan
env1_second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 8301/10000 (83.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1129.3698s / 426176.2671 s
env0_first_0:                 episode reward: -1.4000,                 loss: -0.3360
env0_second_0:                 episode reward: 1.4000,                 loss: -0.2646
env1_first_0:                 episode reward: -0.1000,                 loss: nan
env1_second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 8321/10000 (83.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1051.8772s / 427228.1443 s
env0_first_0:                 episode reward: -2.6500,                 loss: -0.3095
env0_second_0:                 episode reward: 2.6500,                 loss: -0.2092
env1_first_0:                 episode reward: -2.7000,                 loss: nan
env1_second_0:                 episode reward: 2.7000,                 loss: nan
Episode: 8341/10000 (83.4100%),                 avg. length: 4410.05,                last time consumption/overall running time: 503.2696s / 427731.4140 s
env0_first_0:                 episode reward: -12.1500,                 loss: -0.0842
env0_second_0:                 episode reward: 12.1500,                 loss: 0.1740
env1_first_0:                 episode reward: -9.9000,                 loss: nan
env1_second_0:                 episode reward: 9.9000,                 loss: nan
Episode: 8361/10000 (83.6100%),                 avg. length: 3040.55,                last time consumption/overall running time: 350.9471s / 428082.3610 s
env0_first_0:                 episode reward: -50.7000,                 loss: -0.0305
env0_second_0:                 episode reward: 50.7000,                 loss: 0.1279
env1_first_0:                 episode reward: -52.2500,                 loss: nan
env1_second_0:                 episode reward: 52.2500,                 loss: nan
Episode: 8381/10000 (83.8100%),                 avg. length: 7409.35,                last time consumption/overall running time: 875.3072s / 428957.6683 s
env0_first_0:                 episode reward: -130.8000,                 loss: 0.1208
env0_second_0:                 episode reward: 130.8000,                 loss: 0.2293
env1_first_0:                 episode reward: -68.2000,                 loss: nan
env1_second_0:                 episode reward: 68.2000,                 loss: nan
Episode: 8401/10000 (84.0100%),                 avg. length: 9776.9,                last time consumption/overall running time: 1044.2395s / 430001.9077 s
env0_first_0:                 episode reward: -160.1500,                 loss: 0.2344
env0_second_0:                 episode reward: 160.1500,                 loss: 0.4096
env1_first_0:                 episode reward: -174.2000,                 loss: nan
env1_second_0:                 episode reward: 174.2000,                 loss: nan
Episode: 8421/10000 (84.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1101.8982s / 431103.8059 s
env0_first_0:                 episode reward: -219.0000,                 loss: 0.3018
env0_second_0:                 episode reward: 219.0000,                 loss: 0.4383
env1_first_0:                 episode reward: -218.9000,                 loss: nan
env1_second_0:                 episode reward: 218.9000,                 loss: nan
Episode: 8441/10000 (84.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1073.8383s / 432177.6442 s
env0_first_0:                 episode reward: -218.7500,                 loss: 0.2686
env0_second_0:                 episode reward: 218.7500,                 loss: 0.3278
env1_first_0:                 episode reward: -218.9500,                 loss: nan
env1_second_0:                 episode reward: 218.9500,                 loss: nan
Episode: 8461/10000 (84.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1140.7708s / 433318.4150 s
env0_first_0:                 episode reward: -218.5000,                 loss: 0.2561
env0_second_0:                 episode reward: 218.5000,                 loss: 0.2857
env1_first_0:                 episode reward: -218.4500,                 loss: nan
env1_second_0:                 episode reward: 218.4500,                 loss: nan
Episode: 8481/10000 (84.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1077.0666s / 434395.4816 s
env0_first_0:                 episode reward: -218.1500,                 loss: 0.2306
env0_second_0:                 episode reward: 218.1500,                 loss: 0.2526
env1_first_0:                 episode reward: -218.7000,                 loss: nan
env1_second_0:                 episode reward: 218.7000,                 loss: nan
Episode: 8501/10000 (85.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1020.7572s / 435416.2388 s
env0_first_0:                 episode reward: -218.7000,                 loss: 0.2304
env0_second_0:                 episode reward: 218.7000,                 loss: 0.2418
env1_first_0:                 episode reward: -218.5000,                 loss: nan
env1_second_0:                 episode reward: 218.5000,                 loss: nan
Episode: 8521/10000 (85.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1131.7084s / 436547.9472 s
env0_first_0:                 episode reward: -219.0000,                 loss: 0.2372
env0_second_0:                 episode reward: 219.0000,                 loss: 0.2431
env1_first_0:                 episode reward: -218.8500,                 loss: nan
env1_second_0:                 episode reward: 218.8500,                 loss: nan
Episode: 8541/10000 (85.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1073.3047s / 437621.2520 s
env0_first_0:                 episode reward: -219.0000,                 loss: 0.2315
env0_second_0:                 episode reward: 219.0000,                 loss: 0.2352
env1_first_0:                 episode reward: -219.0000,                 loss: nan
env1_second_0:                 episode reward: 219.0000,                 loss: nan
Episode: 8561/10000 (85.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1011.7756s / 438633.0275 s
env0_first_0:                 episode reward: -218.8000,                 loss: 0.2288
env0_second_0:                 episode reward: 218.8000,                 loss: 0.2386
env1_first_0:                 episode reward: -217.9000,                 loss: nan
env1_second_0:                 episode reward: 217.9000,                 loss: nan
Episode: 8581/10000 (85.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 998.8611s / 439631.8886 s
env0_first_0:                 episode reward: -192.1000,                 loss: 0.2646
env0_second_0:                 episode reward: 192.1000,                 loss: 0.2679
env1_first_0:                 episode reward: -218.1500,                 loss: nan
env1_second_0:                 episode reward: 218.1500,                 loss: nan
Episode: 8601/10000 (86.0100%),                 avg. length: 9109.55,                last time consumption/overall running time: 914.2670s / 440546.1556 s
env0_first_0:                 episode reward: -126.1500,                 loss: 0.0956
env0_second_0:                 episode reward: 126.1500,                 loss: 0.2779
env1_first_0:                 episode reward: -131.3500,                 loss: nan
env1_second_0:                 episode reward: 131.3500,                 loss: nan
Episode: 8621/10000 (86.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1125.1064s / 441671.2620 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3277
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2470
env1_first_0:                 episode reward: -5.1500,                 loss: nan
env1_second_0:                 episode reward: 5.1500,                 loss: nan
Episode: 8641/10000 (86.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1150.8323s / 442822.0943 s
env0_first_0:                 episode reward: -1.3000,                 loss: -0.3503
env0_second_0:                 episode reward: 1.3000,                 loss: -0.2894
env1_first_0:                 episode reward: 2.8500,                 loss: nan
env1_second_0:                 episode reward: -2.8500,                 loss: nan
Episode: 8661/10000 (86.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1158.9429s / 443981.0372 s
env0_first_0:                 episode reward: -4.7000,                 loss: -0.3341
env0_second_0:                 episode reward: 4.7000,                 loss: -0.2732
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 8681/10000 (86.8100%),                 avg. length: 9981.85,                last time consumption/overall running time: 1107.3812s / 445088.4184 s
env0_first_0:                 episode reward: -12.8000,                 loss: -0.3144
env0_second_0:                 episode reward: 12.8000,                 loss: -0.2534
env1_first_0:                 episode reward: -7.0000,                 loss: nan
env1_second_0:                 episode reward: 7.0000,                 loss: nan
Episode: 8701/10000 (87.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1206.0852s / 446294.5036 s
env0_first_0:                 episode reward: 2.5500,                 loss: -0.3283
env0_second_0:                 episode reward: -2.5500,                 loss: -0.2082
env1_first_0:                 episode reward: -3.7500,                 loss: nan
env1_second_0:                 episode reward: 3.7500,                 loss: nan
Episode: 8721/10000 (87.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1160.0425s / 447454.5461 s
env0_first_0:                 episode reward: 5.9000,                 loss: -0.3533
env0_second_0:                 episode reward: -5.9000,                 loss: -0.2704
env1_first_0:                 episode reward: 3.7000,                 loss: nan
env1_second_0:                 episode reward: -3.7000,                 loss: nan
Episode: 8741/10000 (87.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1121.0381s / 448575.5842 s
env0_first_0:                 episode reward: -0.4500,                 loss: -0.3541
env0_second_0:                 episode reward: 0.4500,                 loss: -0.2623
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 8761/10000 (87.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1118.6583s / 449694.2425 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3219
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2522
env1_first_0:                 episode reward: 1.3000,                 loss: nan
env1_second_0:                 episode reward: -1.3000,                 loss: nan
Episode: 8781/10000 (87.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1173.6801s / 450867.9226 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3500
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2401
env1_first_0:                 episode reward: -0.7500,                 loss: nan
env1_second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 8801/10000 (88.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1110.6302s / 451978.5528 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3371
env0_second_0:                 episode reward: -1.1000,                 loss: -0.2176
env1_first_0:                 episode reward: -1.4500,                 loss: nan
env1_second_0:                 episode reward: 1.4500,                 loss: nan
Episode: 8821/10000 (88.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1150.0896s / 453128.6423 s
env0_first_0:                 episode reward: 0.5000,                 loss: -0.2682
env0_second_0:                 episode reward: -0.5000,                 loss: -0.1642
env1_first_0:                 episode reward: 4.2500,                 loss: nan
env1_second_0:                 episode reward: -4.2500,                 loss: nan
Episode: 8841/10000 (88.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1101.8310s / 454230.4734 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.3096
env0_second_0:                 episode reward: 2.3500,                 loss: -0.2054
env1_first_0:                 episode reward: -3.9500,                 loss: nan
env1_second_0:                 episode reward: 3.9500,                 loss: nan
Episode: 8861/10000 (88.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1056.1495s / 455286.6228 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3109
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2299
env1_first_0:                 episode reward: -2.3500,                 loss: nan
env1_second_0:                 episode reward: 2.3500,                 loss: nan
Episode: 8881/10000 (88.8100%),                 avg. length: 9884.4,                last time consumption/overall running time: 1088.3247s / 456374.9475 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.2878
env0_second_0:                 episode reward: -2.9500,                 loss: -0.2094
env1_first_0:                 episode reward: -7.3500,                 loss: nan
env1_second_0:                 episode reward: 7.3500,                 loss: nan
Episode: 8901/10000 (89.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1016.5098s / 457391.4574 s
env0_first_0:                 episode reward: -1.6000,                 loss: -0.2756
env0_second_0:                 episode reward: 1.6000,                 loss: -0.1996
env1_first_0:                 episode reward: -9.5000,                 loss: nan
env1_second_0:                 episode reward: 9.5000,                 loss: nan
Episode: 8921/10000 (89.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.3905s / 458415.8479 s
env0_first_0:                 episode reward: 3.3500,                 loss: -0.3359
env0_second_0:                 episode reward: -3.3500,                 loss: -0.2340
env1_first_0:                 episode reward: 3.2000,                 loss: nan
env1_second_0:                 episode reward: -3.2000,                 loss: nan
Episode: 8941/10000 (89.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1088.5469s / 459504.3947 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3336
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2705
env1_first_0:                 episode reward: 0.4000,                 loss: nan
env1_second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 8961/10000 (89.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1168.6056s / 460673.0003 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3209
env0_second_0:                 episode reward: 0.6000,                 loss: -0.2444
env1_first_0:                 episode reward: -0.4500,                 loss: nan
env1_second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 8981/10000 (89.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1180.2344s / 461853.2348 s
env0_first_0:                 episode reward: 0.1000,                 loss: -0.3435
env0_second_0:                 episode reward: -0.1000,                 loss: -0.2592
env1_first_0:                 episode reward: -0.8000,                 loss: nan
env1_second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 9001/10000 (90.0100%),                 avg. length: 9758.65,                last time consumption/overall running time: 1153.5572s / 463006.7920 s
env0_first_0:                 episode reward: -9.0500,                 loss: -0.3255
env0_second_0:                 episode reward: 9.0500,                 loss: -0.2471
env1_first_0:                 episode reward: 5.7000,                 loss: nan
env1_second_0:                 episode reward: -5.7000,                 loss: nan
Episode: 9021/10000 (90.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1146.8286s / 464153.6206 s
env0_first_0:                 episode reward: 2.4000,                 loss: -0.3268
env0_second_0:                 episode reward: -2.4000,                 loss: -0.2196
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 9041/10000 (90.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1140.3903s / 465294.0109 s
env0_first_0:                 episode reward: -0.5500,                 loss: -0.3298
env0_second_0:                 episode reward: 0.5500,                 loss: -0.2620
env1_first_0:                 episode reward: -1.2500,                 loss: nan
env1_second_0:                 episode reward: 1.2500,                 loss: nan
Episode: 9061/10000 (90.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1045.0232s / 466339.0341 s
env0_first_0:                 episode reward: 2.7500,                 loss: -0.3561
env0_second_0:                 episode reward: -2.7500,                 loss: -0.2840
env1_first_0:                 episode reward: 3.1500,                 loss: nan
env1_second_0:                 episode reward: -3.1500,                 loss: nan
Episode: 9081/10000 (90.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1011.6477s / 467350.6818 s
env0_first_0:                 episode reward: 0.6500,                 loss: -0.3606
env0_second_0:                 episode reward: -0.6500,                 loss: -0.2882
env1_first_0:                 episode reward: 1.8000,                 loss: nan
env1_second_0:                 episode reward: -1.8000,                 loss: nan
Episode: 9101/10000 (91.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1006.5493s / 468357.2310 s
env0_first_0:                 episode reward: 1.7000,                 loss: -0.3307
env0_second_0:                 episode reward: -1.7000,                 loss: -0.2511
env1_first_0:                 episode reward: 1.5500,                 loss: nan
env1_second_0:                 episode reward: -1.5500,                 loss: nan
Episode: 9121/10000 (91.2100%),                 avg. length: 7725.75,                last time consumption/overall running time: 801.1737s / 469158.4047 s
env0_first_0:                 episode reward: 5.0000,                 loss: -0.1436
env0_second_0:                 episode reward: -5.0000,                 loss: 0.2862
env1_first_0:                 episode reward: 6.9000,                 loss: nan
env1_second_0:                 episode reward: -6.9000,                 loss: nan
Episode: 9141/10000 (91.4100%),                 avg. length: 9687.9,                last time consumption/overall running time: 946.4060s / 470104.8107 s
env0_first_0:                 episode reward: 14.9500,                 loss: -0.2505
env0_second_0:                 episode reward: -14.9500,                 loss: 0.0729
env1_first_0:                 episode reward: 17.0000,                 loss: nan
env1_second_0:                 episode reward: -17.0000,                 loss: nan
Episode: 9161/10000 (91.6100%),                 avg. length: 9263.65,                last time consumption/overall running time: 963.4299s / 471068.2405 s
env0_first_0:                 episode reward: 10.7500,                 loss: -0.2120
env0_second_0:                 episode reward: -10.7500,                 loss: -0.0381
env1_first_0:                 episode reward: 11.1500,                 loss: nan
env1_second_0:                 episode reward: -11.1500,                 loss: nan
Episode: 9181/10000 (91.8100%),                 avg. length: 9076.75,                last time consumption/overall running time: 891.6231s / 471959.8636 s
env0_first_0:                 episode reward: 13.2000,                 loss: -0.1647
env0_second_0:                 episode reward: -13.2000,                 loss: -0.0415
env1_first_0:                 episode reward: 17.8500,                 loss: nan
env1_second_0:                 episode reward: -17.8500,                 loss: nan
Episode: 9201/10000 (92.0100%),                 avg. length: 9767.25,                last time consumption/overall running time: 1001.1768s / 472961.0404 s
env0_first_0:                 episode reward: 8.1000,                 loss: -0.2849
env0_second_0:                 episode reward: -8.1000,                 loss: -0.0927
env1_first_0:                 episode reward: 6.4000,                 loss: nan
env1_second_0:                 episode reward: -6.4000,                 loss: nan
Episode: 9221/10000 (92.2100%),                 avg. length: 9800.4,                last time consumption/overall running time: 973.1797s / 473934.2201 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3057
env0_second_0:                 episode reward: 0.5000,                 loss: -0.0607
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 9241/10000 (92.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 978.3025s / 474912.5226 s
env0_first_0:                 episode reward: 0.9000,                 loss: -0.3404
env0_second_0:                 episode reward: -0.9000,                 loss: -0.1242
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9261/10000 (92.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.8054s / 475937.3281 s
env0_first_0:                 episode reward: -2.2500,                 loss: -0.3305
env0_second_0:                 episode reward: 2.2500,                 loss: 0.1257
env1_first_0:                 episode reward: -2.3000,                 loss: nan
env1_second_0:                 episode reward: 2.3000,                 loss: nan
Episode: 9281/10000 (92.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1032.7670s / 476970.0951 s
env0_first_0:                 episode reward: -1.1000,                 loss: -0.3238
env0_second_0:                 episode reward: 1.1000,                 loss: -0.0527
env1_first_0:                 episode reward: -8.1500,                 loss: nan
env1_second_0:                 episode reward: 8.1500,                 loss: nan
Episode: 9301/10000 (93.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1039.3524s / 478009.4475 s
env0_first_0:                 episode reward: 1.1000,                 loss: -0.3293
env0_second_0:                 episode reward: -1.1000,                 loss: -0.1050
env1_first_0:                 episode reward: -0.5000,                 loss: nan
env1_second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 9321/10000 (93.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1034.5016s / 479043.9491 s
env0_first_0:                 episode reward: -0.6000,                 loss: -0.3060
env0_second_0:                 episode reward: 0.6000,                 loss: 0.0212
env1_first_0:                 episode reward: -3.9000,                 loss: nan
env1_second_0:                 episode reward: 3.9000,                 loss: nan
Episode: 9341/10000 (93.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1005.5075s / 480049.4565 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3251
env0_second_0:                 episode reward: 0.3000,                 loss: 0.0911
env1_first_0:                 episode reward: 0.0500,                 loss: nan
env1_second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 9361/10000 (93.6100%),                 avg. length: 9813.65,                last time consumption/overall running time: 995.1953s / 481044.6519 s
env0_first_0:                 episode reward: 0.3500,                 loss: -0.2880
env0_second_0:                 episode reward: -0.3500,                 loss: 0.0554
env1_first_0:                 episode reward: -1.3500,                 loss: nan
env1_second_0:                 episode reward: 1.3500,                 loss: nan
Episode: 9381/10000 (93.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1030.6703s / 482075.3222 s
env0_first_0:                 episode reward: -27.2000,                 loss: -0.2254
env0_second_0:                 episode reward: 27.2000,                 loss: 0.1989
env1_first_0:                 episode reward: -19.6000,                 loss: nan
env1_second_0:                 episode reward: 19.6000,                 loss: nan
Episode: 9401/10000 (94.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1000.0872s / 483075.4094 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3390
env0_second_0:                 episode reward: -1.9500,                 loss: -0.2197
env1_first_0:                 episode reward: 2.1500,                 loss: nan
env1_second_0:                 episode reward: -2.1500,                 loss: nan
Episode: 9421/10000 (94.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1036.9966s / 484112.4060 s
env0_first_0:                 episode reward: 1.0000,                 loss: -0.3510
env0_second_0:                 episode reward: -1.0000,                 loss: -0.2678
env1_first_0:                 episode reward: 2.0000,                 loss: nan
env1_second_0:                 episode reward: -2.0000,                 loss: nan
Episode: 9441/10000 (94.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1029.2272s / 485141.6332 s
env0_first_0:                 episode reward: 1.5000,                 loss: -0.3428
env0_second_0:                 episode reward: -1.5000,                 loss: -0.2655
env1_first_0:                 episode reward: 1.3500,                 loss: nan
env1_second_0:                 episode reward: -1.3500,                 loss: nan
Episode: 9461/10000 (94.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1024.8334s / 486166.4666 s
env0_first_0:                 episode reward: 2.9500,                 loss: -0.3493
env0_second_0:                 episode reward: -2.9500,                 loss: -0.2486
env1_first_0:                 episode reward: 2.9500,                 loss: nan
env1_second_0:                 episode reward: -2.9500,                 loss: nan
Episode: 9481/10000 (94.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 992.8384s / 487159.3050 s
env0_first_0:                 episode reward: 2.3500,                 loss: -0.3274
env0_second_0:                 episode reward: -2.3500,                 loss: -0.2343
env1_first_0:                 episode reward: 2.3500,                 loss: nan
env1_second_0:                 episode reward: -2.3500,                 loss: nan
Episode: 9501/10000 (95.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1063.5857s / 488222.8907 s
env0_first_0:                 episode reward: 3.2500,                 loss: -0.3390
env0_second_0:                 episode reward: -3.2500,                 loss: -0.1692
env1_first_0:                 episode reward: 1.5000,                 loss: nan
env1_second_0:                 episode reward: -1.5000,                 loss: nan
Episode: 9521/10000 (95.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1068.9333s / 489291.8240 s
env0_first_0:                 episode reward: 4.0500,                 loss: -0.3520
env0_second_0:                 episode reward: -4.0500,                 loss: -0.2633
env1_first_0:                 episode reward: -4.0500,                 loss: nan
env1_second_0:                 episode reward: 4.0500,                 loss: nan
Episode: 9541/10000 (95.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1043.4426s / 490335.2666 s
env0_first_0:                 episode reward: 1.9500,                 loss: -0.3505
env0_second_0:                 episode reward: -1.9500,                 loss: -0.2858
env1_first_0:                 episode reward: 1.2000,                 loss: nan
env1_second_0:                 episode reward: -1.2000,                 loss: nan
Episode: 9561/10000 (95.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1006.5465s / 491341.8131 s
env0_first_0:                 episode reward: -1.0500,                 loss: -0.2968
env0_second_0:                 episode reward: 1.0500,                 loss: -0.2334
env1_first_0:                 episode reward: -4.3500,                 loss: nan
env1_second_0:                 episode reward: 4.3500,                 loss: nan
Episode: 9581/10000 (95.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1027.2237s / 492369.0368 s
env0_first_0:                 episode reward: -2.4000,                 loss: -0.3630
env0_second_0:                 episode reward: 2.4000,                 loss: -0.2782
env1_first_0:                 episode reward: 0.6000,                 loss: nan
env1_second_0:                 episode reward: -0.6000,                 loss: nan
Episode: 9601/10000 (96.0100%),                 avg. length: 9879.15,                last time consumption/overall running time: 1045.9445s / 493414.9813 s
env0_first_0:                 episode reward: -18.7500,                 loss: -0.2800
env0_second_0:                 episode reward: 18.7500,                 loss: -0.2077
env1_first_0:                 episode reward: -17.5000,                 loss: nan
env1_second_0:                 episode reward: 17.5000,                 loss: nan
Episode: 9621/10000 (96.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1102.6951s / 494517.6765 s
env0_first_0:                 episode reward: -10.6500,                 loss: -0.3066
env0_second_0:                 episode reward: 10.6500,                 loss: -0.2243
env1_first_0:                 episode reward: -3.6500,                 loss: nan
env1_second_0:                 episode reward: 3.6500,                 loss: nan
Episode: 9641/10000 (96.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1035.3443s / 495553.0208 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3380
env0_second_0:                 episode reward: -1.3500,                 loss: -0.2723
env1_first_0:                 episode reward: 0.3000,                 loss: nan
env1_second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 9661/10000 (96.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1002.2275s / 496555.2483 s
env0_first_0:                 episode reward: -0.8500,                 loss: -0.3177
env0_second_0:                 episode reward: 0.8500,                 loss: -0.1233
env1_first_0:                 episode reward: -0.9000,                 loss: nan
env1_second_0:                 episode reward: 0.9000,                 loss: nan
Episode: 9681/10000 (96.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 994.5153s / 497549.7636 s
env0_first_0:                 episode reward: 0.4000,                 loss: -0.3640
env0_second_0:                 episode reward: -0.4000,                 loss: -0.2401
env1_first_0:                 episode reward: -0.0500,                 loss: nan
env1_second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 9701/10000 (97.0100%),                 avg. length: 9816.15,                last time consumption/overall running time: 972.5918s / 498522.3555 s
env0_first_0:                 episode reward: 3.9000,                 loss: -0.3391
env0_second_0:                 episode reward: -3.9000,                 loss: -0.2103
env1_first_0:                 episode reward: 2.9000,                 loss: nan
env1_second_0:                 episode reward: -2.9000,                 loss: nan
Episode: 9721/10000 (97.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 993.9847s / 499516.3401 s
env0_first_0:                 episode reward: 2.2500,                 loss: -0.3632
env0_second_0:                 episode reward: -2.2500,                 loss: -0.2803
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 9741/10000 (97.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1069.4928s / 500585.8330 s
env0_first_0:                 episode reward: 2.9000,                 loss: -0.3433
env0_second_0:                 episode reward: -2.9000,                 loss: -0.3008
env1_first_0:                 episode reward: 1.8500,                 loss: nan
env1_second_0:                 episode reward: -1.8500,                 loss: nan
Episode: 9761/10000 (97.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1129.2892s / 501715.1222 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3705
env0_second_0:                 episode reward: -0.6000,                 loss: -0.3225
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 9781/10000 (97.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1065.7203s / 502780.8425 sLoad tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
Load tennis_v2 environment in type pettingzoo.
Env observation space: Box(0.0, 1.0, (128,), float32) action space: Discrete(18)
/home/zihan/research/MARS/mars/rl/agents/nash_ppo.py:181: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1641801999604/work/torch/csrc/utils/tensor_new.cpp:210.)
  s,a,r,s_prime,prob_a,done_mask =    torch.tensor(s_lst, dtype=torch.float).to(self.device), torch.tensor(a_lst).to(self.device), \
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.
  out=out, **kwargs)
/home/zihan/anaconda3/envs/x/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)

env0_first_0:                 episode reward: -0.6500,                 loss: -0.3115
env0_second_0:                 episode reward: 0.6500,                 loss: -0.2328
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9801/10000 (98.0100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1047.3566s / 503828.1991 s
env0_first_0:                 episode reward: -0.2000,                 loss: -0.3108
env0_second_0:                 episode reward: 0.2000,                 loss: -0.2417
env1_first_0:                 episode reward: -1.4000,                 loss: nan
env1_second_0:                 episode reward: 1.4000,                 loss: nan
Episode: 9821/10000 (98.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1231.6985s / 505059.8977 s
env0_first_0:                 episode reward: -0.3000,                 loss: -0.3404
env0_second_0:                 episode reward: 0.3000,                 loss: -0.2749
env1_first_0:                 episode reward: 0.4500,                 loss: nan
env1_second_0:                 episode reward: -0.4500,                 loss: nan
Episode: 9841/10000 (98.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1156.7993s / 506216.6970 s
env0_first_0:                 episode reward: 0.7000,                 loss: -0.3541
env0_second_0:                 episode reward: -0.7000,                 loss: -0.3002
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 9861/10000 (98.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1177.7231s / 507394.4201 s
env0_first_0:                 episode reward: -2.3500,                 loss: -0.3513
env0_second_0:                 episode reward: 2.3500,                 loss: -0.2789
env1_first_0:                 episode reward: -4.9000,                 loss: nan
env1_second_0:                 episode reward: 4.9000,                 loss: nan
Episode: 9881/10000 (98.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1080.5961s / 508475.0162 s
env0_first_0:                 episode reward: 1.3500,                 loss: -0.3559
env0_second_0:                 episode reward: -1.3500,                 loss: -0.3042
env1_first_0:                 episode reward: 0.2000,                 loss: nan
env1_second_0:                 episode reward: -0.2000,                 loss: nan
Episode: 9901/10000 (99.0100%),                 avg. length: 9931.55,                last time consumption/overall running time: 966.6023s / 509441.6185 s
env0_first_0:                 episode reward: 2.4500,                 loss: -0.3030
env0_second_0:                 episode reward: -2.4500,                 loss: -0.2273
env1_first_0:                 episode reward: 2.6000,                 loss: nan
env1_second_0:                 episode reward: -2.6000,                 loss: nan
Episode: 9921/10000 (99.2100%),                 avg. length: 9999.0,                last time consumption/overall running time: 977.2418s / 510418.8604 s
env0_first_0:                 episode reward: -0.4000,                 loss: -0.3677
env0_second_0:                 episode reward: 0.4000,                 loss: -0.3119
env1_first_0:                 episode reward: -0.3000,                 loss: nan
env1_second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 9941/10000 (99.4100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1098.9378s / 511517.7982 s
env0_first_0:                 episode reward: 0.6000,                 loss: -0.3649
env0_second_0:                 episode reward: -0.6000,                 loss: -0.2896
env1_first_0:                 episode reward: 2.3000,                 loss: nan
env1_second_0:                 episode reward: -2.3000,                 loss: nan
Episode: 9961/10000 (99.6100%),                 avg. length: 9999.0,                last time consumption/overall running time: 985.4295s / 512503.2276 s
env0_first_0:                 episode reward: 1.8500,                 loss: -0.3654
env0_second_0:                 episode reward: -1.8500,                 loss: -0.2871
env1_first_0:                 episode reward: 1.1500,                 loss: nan
env1_second_0:                 episode reward: -1.1500,                 loss: nan
Episode: 9981/10000 (99.8100%),                 avg. length: 9999.0,                last time consumption/overall running time: 1164.7875s / 513668.0151 s
env0_first_0:                 episode reward: -0.5000,                 loss: -0.3207
env0_second_0:                 episode reward: 0.5000,                 loss: -0.2556
env1_first_0:                 episode reward: -0.1500,                 loss: nan
env1_second_0:                 episode reward: 0.1500,                 loss: nan
