Cannot import pettingzoo env:  combat_jet_v1
surround_v2 pettingzoo
type:  pettingzoo
surround_v2
Load surround_v2 environment in type pettingzoo.
Env observation space: <bound method aec_to_parallel_wrapper.observation_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f7f3a186cf8>> action space: <bound method aec_to_parallel_wrapper.action_space of <pettingzoo.utils.conversions.aec_to_parallel_wrapper object at 0x7f7f3a186cf8>>
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f7f3a183128>
random seed: [577, 783, 773, 412, 810]
<mars.env.wrappers.mars_wrappers.SSVecWrapper object at 0x7f7f3a183128>
discrete_policy 5 Discrete(5)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=25, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 5 Discrete(5)
NashDQNBase(
  (net): ImpalaCNN(
    (cnn_layers): ModuleList(
      (0): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
    )
    (max_pool_layers): ModuleList(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
      (1): MaxPool2d(kernel_size=3, stride=2, padding=2, dilation=1, ceil_mode=False)
    )
    (residual_blocks_whole): ModuleList(
      (0): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
      (1): ModuleList(
        (0): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
        (1): Sequential(
          (0): ReLU()
          (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
          (2): ReLU()
          (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        )
      )
    )
    (residual_blocks): ModuleList(
      (0): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
      (1): Sequential(
        (0): ReLU()
        (1): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
        (2): ReLU()
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=same)
      )
    )
    (body): Sequential(
      (0): Flatten()
      (1): Linear(in_features=28224, out_features=128, bias=True)
      (2): ReLU()
      (3): Linear(in_features=128, out_features=128, bias=True)
      (4): ReLU()
      (5): Linear(in_features=128, out_features=25, bias=True)
      (6): Softmax(dim=-1)
    )
  )
)
discrete_policy 5 Discrete(5)
Agents No. [1] (index starting from 0) are not learnable.
Arguments:  {'env_name': 'surround_v2', 'env_type': 'pettingzoo', 'num_envs': 5, 'ram': False, 'seed': 'random', 'algorithm': 'NashDQN', 'algorithm_spec': {'episodic_update': False, 'dueling': False, 'replay_buffer_size': '1e5', 'gamma': 0.99, 'multi_step': 1, 'target_update_interval': 1000, 'eps_start': 1.0, 'eps_final': 0.001, 'eps_decay': 5000000}, 'num_process': 1, 'batch_size': 128, 'max_episodes': 50000, 'max_steps_per_episode': 300, 'train_start_frame': 0, 'optimizer': 'adam', 'learning_rate': '1e-4', 'device': 'gpu', 'update_itr': 1, 'log_avg_window': 20, 'log_interval': 20, 'render': False, 'test': False, 'exploit': False, 'load_model_idx': False, 'load_model_full_path': False, 'multiprocess': False, 'eval_models': False, 'save_path': '', 'save_interval': 2000, 'net_architecture': {'hidden_dim_list': [128, 128], 'hidden_activation': 'ReLU', 'output_activation': 'Softmax', 'channel_list': [32, 16], 'kernel_size_list': [4, 4], 'stride_list': [1, 1]}, 'marl_method': 'nash_dqn', 'marl_spec': {'min_update_interval': 20, 'score_avg_window': 10, 'global_state': False}}
Save models to : /home/zihan/research/MARS/data/model/20220615_2120/pettingzoo_surround_v2_nash_dqn. 
 Save logs to: /home/zihan/research/MARS/data/log/20220615_2120/pettingzoo_surround_v2_nash_dqn.
Episode: 1/50000 (0.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 20.9730s / 20.9730 s
first_0:                 episode reward: 1.0000,                 loss: 0.0102
second_0:                 episode reward: -1.0000,                 loss: nan
Episode: 21/50000 (0.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 598.4409s / 619.4140 s
first_0:                 episode reward: -0.5000,                 loss: 0.0095
second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 41/50000 (0.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 607.7867s / 1227.2007 s
first_0:                 episode reward: 0.4000,                 loss: 0.0091
second_0:                 episode reward: -0.4000,                 loss: nan
Episode: 61/50000 (0.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 610.1587s / 1837.3594 s
first_0:                 episode reward: 0.1500,                 loss: 0.0090
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 81/50000 (0.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 614.0270s / 2451.3864 s
first_0:                 episode reward: -0.4000,                 loss: 0.0088
second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 101/50000 (0.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 618.9747s / 3070.3611 s
first_0:                 episode reward: -0.1500,                 loss: 0.0084
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 121/50000 (0.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 623.4857s / 3693.8468 s
first_0:                 episode reward: -0.0500,                 loss: 0.0085
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 141/50000 (0.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 625.1862s / 4319.0331 s
first_0:                 episode reward: 0.1000,                 loss: 0.0085
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 161/50000 (0.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 631.1972s / 4950.2303 s
first_0:                 episode reward: -0.5500,                 loss: 0.0086
second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 181/50000 (0.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 637.6872s / 5587.9175 s
first_0:                 episode reward: -0.2000,                 loss: 0.0088
second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 201/50000 (0.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 637.4216s / 6225.3390 s
first_0:                 episode reward: 0.1000,                 loss: 0.0088
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 221/50000 (0.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 642.2813s / 6867.6203 s
first_0:                 episode reward: -0.1000,                 loss: 0.0087
second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 241/50000 (0.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 647.3053s / 7514.9256 s
first_0:                 episode reward: -0.0500,                 loss: 0.0088
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 261/50000 (0.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 653.4349s / 8168.3605 s
first_0:                 episode reward: -0.7500,                 loss: 0.0087
second_0:                 episode reward: 0.7500,                 loss: nan
Episode: 281/50000 (0.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 660.6787s / 8829.0392 s
first_0:                 episode reward: 0.3000,                 loss: 0.0087
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 301/50000 (0.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 663.0355s / 9492.0748 s
first_0:                 episode reward: -0.8000,                 loss: 0.0090
second_0:                 episode reward: 0.8000,                 loss: nan
Episode: 321/50000 (0.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 668.0246s / 10160.0994 s
first_0:                 episode reward: -0.3000,                 loss: 0.0090
second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 341/50000 (0.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 673.4497s / 10833.5491 s
first_0:                 episode reward: -0.1000,                 loss: 0.0088
second_0:                 episode reward: 0.1000,                 loss: nan
Episode: 361/50000 (0.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 674.1721s / 11507.7212 s
first_0:                 episode reward: 0.0500,                 loss: 0.0089
second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 381/50000 (0.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 674.2060s / 12181.9272 s
first_0:                 episode reward: 0.1000,                 loss: 0.0090
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 401/50000 (0.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 675.7310s / 12857.6582 s
first_0:                 episode reward: 0.1500,                 loss: 0.0094
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 421/50000 (0.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 677.1028s / 13534.7611 s
first_0:                 episode reward: -0.5500,                 loss: 0.0093
second_0:                 episode reward: 0.5500,                 loss: nan
Episode: 441/50000 (0.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 5496.5829s / 19031.3440 s
first_0:                 episode reward: 0.3000,                 loss: 0.0093
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 461/50000 (0.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 682.8055s / 19714.1495 s
first_0:                 episode reward: -0.3500,                 loss: 0.0093
second_0:                 episode reward: 0.3500,                 loss: nan
Episode: 481/50000 (0.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 677.1261s / 20391.2756 s
first_0:                 episode reward: 0.2500,                 loss: 0.0092
second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 501/50000 (1.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 677.7384s / 21069.0140 s
first_0:                 episode reward: 0.0000,                 loss: 0.0094
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 521/50000 (1.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 678.0936s / 21747.1076 s
first_0:                 episode reward: -0.0500,                 loss: 0.0093
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 541/50000 (1.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 676.2852s / 22423.3928 s
first_0:                 episode reward: 0.0500,                 loss: 0.0092
second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 561/50000 (1.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 677.1842s / 23100.5770 s
first_0:                 episode reward: -0.4500,                 loss: 0.0092
second_0:                 episode reward: 0.4500,                 loss: nan
Episode: 581/50000 (1.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 677.6979s / 23778.2748 s
first_0:                 episode reward: -0.2000,                 loss: 0.0090
second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 601/50000 (1.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 693.4990s / 24471.7739 s
first_0:                 episode reward: -0.1500,                 loss: 0.0092
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 621/50000 (1.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 673.8039s / 25145.5778 s
first_0:                 episode reward: 0.1000,                 loss: 0.0093
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 641/50000 (1.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 673.9306s / 25819.5084 s
first_0:                 episode reward: -0.3000,                 loss: 0.0094
second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 661/50000 (1.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 675.1198s / 26494.6282 s
first_0:                 episode reward: 0.1000,                 loss: 0.0092
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 681/50000 (1.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 673.8206s / 27168.4488 s
first_0:                 episode reward: -0.0500,                 loss: 0.0092
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 701/50000 (1.4020%),                 avg. length: 299.0,                last time consumption/overall running time: 676.9297s / 27845.3785 s
first_0:                 episode reward: 0.1500,                 loss: 0.0092
second_0:                 episode reward: -0.1500,                 loss: nan
Episode: 721/50000 (1.4420%),                 avg. length: 299.0,                last time consumption/overall running time: 679.2542s / 28524.6327 s
first_0:                 episode reward: 0.9500,                 loss: 0.0092
second_0:                 episode reward: -0.9500,                 loss: nan
Episode: 741/50000 (1.4820%),                 avg. length: 299.0,                last time consumption/overall running time: 687.5661s / 29212.1988 s
first_0:                 episode reward: 0.0000,                 loss: 0.0094
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 761/50000 (1.5220%),                 avg. length: 299.0,                last time consumption/overall running time: 671.8779s / 29884.0767 s
first_0:                 episode reward: 0.3500,                 loss: 0.0093
second_0:                 episode reward: -0.3500,                 loss: nan
Episode: 781/50000 (1.5620%),                 avg. length: 299.0,                last time consumption/overall running time: 673.7981s / 30557.8748 s
first_0:                 episode reward: 0.3000,                 loss: 0.0093
second_0:                 episode reward: -0.3000,                 loss: nan
Episode: 801/50000 (1.6020%),                 avg. length: 299.0,                last time consumption/overall running time: 675.6730s / 31233.5479 s
first_0:                 episode reward: -0.5000,                 loss: 0.0093
second_0:                 episode reward: 0.5000,                 loss: nan
Episode: 821/50000 (1.6420%),                 avg. length: 299.0,                last time consumption/overall running time: 679.1875s / 31912.7354 s
first_0:                 episode reward: -0.2500,                 loss: 0.0092
second_0:                 episode reward: 0.2500,                 loss: nan
Episode: 841/50000 (1.6820%),                 avg. length: 299.0,                last time consumption/overall running time: 683.0361s / 32595.7715 s
first_0:                 episode reward: -0.4000,                 loss: 0.0093
second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 861/50000 (1.7220%),                 avg. length: 299.0,                last time consumption/overall running time: 675.7342s / 33271.5057 s
first_0:                 episode reward: -0.2000,                 loss: 0.0093
second_0:                 episode reward: 0.2000,                 loss: nan
Episode: 881/50000 (1.7620%),                 avg. length: 299.0,                last time consumption/overall running time: 676.5183s / 33948.0240 s
first_0:                 episode reward: 0.5000,                 loss: 0.0093
second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 901/50000 (1.8020%),                 avg. length: 299.0,                last time consumption/overall running time: 676.6361s / 34624.6601 s
first_0:                 episode reward: -0.0500,                 loss: 0.0091
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 921/50000 (1.8420%),                 avg. length: 299.0,                last time consumption/overall running time: 674.9327s / 35299.5928 s
first_0:                 episode reward: 0.5000,                 loss: 0.0090
second_0:                 episode reward: -0.5000,                 loss: nan
Episode: 941/50000 (1.8820%),                 avg. length: 299.0,                last time consumption/overall running time: 673.9022s / 35973.4950 s
first_0:                 episode reward: 0.0500,                 loss: 0.0091
second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 961/50000 (1.9220%),                 avg. length: 299.0,                last time consumption/overall running time: 675.1494s / 36648.6444 s
first_0:                 episode reward: -0.1500,                 loss: 0.0094
second_0:                 episode reward: 0.1500,                 loss: nan
Episode: 981/50000 (1.9620%),                 avg. length: 299.0,                last time consumption/overall running time: 675.6900s / 37324.3344 s
first_0:                 episode reward: 0.0000,                 loss: 0.0092
second_0:                 episode reward: 0.0000,                 loss: nan
Episode: 1001/50000 (2.0020%),                 avg. length: 299.0,                last time consumption/overall running time: 676.3989s / 38000.7333 s
first_0:                 episode reward: 0.9000,                 loss: 0.0092
second_0:                 episode reward: -0.9000,                 loss: nan
Episode: 1021/50000 (2.0420%),                 avg. length: 299.0,                last time consumption/overall running time: 675.6388s / 38676.3721 s
first_0:                 episode reward: 0.2500,                 loss: 0.0093
second_0:                 episode reward: -0.2500,                 loss: nan
Episode: 1041/50000 (2.0820%),                 avg. length: 299.0,                last time consumption/overall running time: 674.0621s / 39350.4341 s
first_0:                 episode reward: 0.7000,                 loss: 0.0091
second_0:                 episode reward: -0.7000,                 loss: nan
Episode: 1061/50000 (2.1220%),                 avg. length: 299.0,                last time consumption/overall running time: 677.3956s / 40027.8298 s
first_0:                 episode reward: 0.1000,                 loss: 0.0091
second_0:                 episode reward: -0.1000,                 loss: nan
Episode: 1081/50000 (2.1620%),                 avg. length: 299.0,                last time consumption/overall running time: 677.7845s / 40705.6142 s
first_0:                 episode reward: -0.0500,                 loss: 0.0091
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1101/50000 (2.2020%),                 avg. length: 299.0,                last time consumption/overall running time: 678.8492s / 41384.4635 s
first_0:                 episode reward: 0.0500,                 loss: 0.0091
second_0:                 episode reward: -0.0500,                 loss: nan
Episode: 1121/50000 (2.2420%),                 avg. length: 299.0,                last time consumption/overall running time: 678.4475s / 42062.9110 s
first_0:                 episode reward: -0.4000,                 loss: 0.0093
second_0:                 episode reward: 0.4000,                 loss: nan
Episode: 1141/50000 (2.2820%),                 avg. length: 299.0,                last time consumption/overall running time: 680.5156s / 42743.4266 s
first_0:                 episode reward: -0.3000,                 loss: 0.0093
second_0:                 episode reward: 0.3000,                 loss: nan
Episode: 1161/50000 (2.3220%),                 avg. length: 299.0,                last time consumption/overall running time: 676.9484s / 43420.3750 s
first_0:                 episode reward: -0.0500,                 loss: 0.0092
second_0:                 episode reward: 0.0500,                 loss: nan
Episode: 1181/50000 (2.3620%),                 avg. length: 299.0,                last time consumption/overall running time: 677.9780s / 44098.3530 s